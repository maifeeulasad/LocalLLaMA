{
  "kind": "Listing",
  "data": {
    "after": "t3_1mfgqb0",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article: https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\n\nContext:\n\nThis insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.\n\nhttps://arxiv.org/abs/2506.21734\n\nA number of people have reproduced it if anyone is worried about that: https://x.com/VictorTaelin/status/1950512015899840768\nhttps://github.com/sapientinc/HRM/issues/12",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM solved thinking more than current \"thinking\" models (this needs more hype)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3i48",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article: &lt;a href=\"https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It&amp;#39;s seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href=\"https://x.com/VictorTaelin/status/1950512015899840768\"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt;\n&lt;a href=\"https://github.com/sapientinc/HRM/issues/12\"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?auto=webp&amp;s=3649785728b7c459cdac6d4444cd5a58a31ff8a2",
                  "width": 994,
                  "height": 610
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a32ed96622f166ae158780da40b7af8d982e72f9",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2621c7644d6e96ab551888e43049a8b94e227fc5",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da54eeb0e463cf73d8f5292e9b099e338494438",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1caf030ca1b3e5b1df5e052afe93497a9bb543",
                    "width": 640,
                    "height": 392
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88919a9c5b50d0a4d76e2c2bb7e87bb3c4eed852",
                    "width": 960,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg3i48",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754174679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. \n\n\nEdit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. \n\nThanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)",
          "author_fullname": "t2_67bb0jdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source model that is as intelligent as Claude Sonnet 4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqejn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 244,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 244,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754151213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754140871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt;\n\n&lt;p&gt;Edit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfqejn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vishwa1238",
          "discussion_type": null,
          "num_comments": 247,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754140871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is just a little remark that if you haven't you definitely should try qwen code [https://github.com/QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)   \nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. \n\nThe metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.\n\nSo this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. ",
          "author_fullname": "t2_n9dnke1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code + Qwen Coder 30b 3A is insane",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuiri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 121,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 121,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just a little remark that if you haven&amp;#39;t you definitely should try qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br/&gt;\nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I&amp;#39;m working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt;\n\n&lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I&amp;#39;m very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt;\n\n&lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don&amp;#39;t know jack shit about coding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?auto=webp&amp;s=689c8ef1840eeb6bb6bb53a5908a342a3dac7558",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2382140736e501c0e5f725eb9004d88daaf4ddc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fd612828e87be93104f75281f9696b4b69068",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef0b7f26ce8aa8b563291337614fb5656c8ab07",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=faaeb8e0dc19d67afd0d3197e7b7a5a0095a6f09",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eebbb21f63208ab9ba4a6a50b9890a328d60d830",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edc81e58030926a09a63f9377700b5cd6432c27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfuiri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flashy_Management962",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754151475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "all I need....",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgj0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 1281,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1281,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGttwXkoUDYh4g0k8yxukZYUVavOKH_4hrW2zTWYkis.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754105691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?auto=webp&amp;s=56a0a807d6543ba43266157bcaa0b48793bcd3df",
                  "width": 1024,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0faf4ee7c2bcdc1e4161739543ede55c4684b2b8",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2e17cf5370e4cca658ed8a629406a0522d68d7e",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb58e48e0282066e26e22c2d00d814c00c5ded2",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2db9397b5cffd5e90a0cc1250191ed983494cb",
                    "width": 960,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "FTO20S33694FU83Br474HCo8srCRihH9Gw7Oe1llTgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mfgj0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 108,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgj0g/all_i_need/",
          "stickied": false,
          "url": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "subreddit_subscribers": 509052,
          "created_utc": 1754105691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[A new PR](https://github.com/ggml-org/llama.cpp/pull/15026) was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated [\\#14939](https://github.com/ggml-org/llama.cpp/pull/14939) seemed to get stuck. The new PR description reads: \"**this PR will NOT attempt to implement MTP**\", with great progress being made in short time. (Amazing!!!)\n\nGiven that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.\n\nDisclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!\n\nPS: does MTP already work on/with MLX?",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvxdo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754155024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15026\"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5&amp;#39;s models in llama.cpp, as the original, highly anticipated &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt;\n\n&lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it&amp;#39;s not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I am eternally grateful for everybody&amp;#39;s contribution to the field, as LLMs allow me to code what I couldn&amp;#39;t code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt;\n\n&lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?auto=webp&amp;s=acc8e28264bfccdff7bd8f443fed065d174ea9e0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=192b46649336ccde82b3df88264f14e8c3af5057",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d87e157d503386c5e4e8e7524115ddb67fb4d525",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2253f5fea3f3de0ade685e30f24f2fe47d41bed",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=076b473df32cd3180067a97b9fe5da2c8755fd75",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc3f7d46febf1a47049086579c27beff59d8b7b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfebb15c0a042499f527df12e1302a2dc2a17608",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvxdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754155024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using `llama.cpp` with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.\n\n**KV Quantization**\n\n* **KV cache quantization matters a lot**. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use `q5_1` for a good balance of memory usage and performance. It works well in PPL tests and in practice.\n\n**Offloading Strategy**\n\n* You're bottlenecked by your **system RAM bandwidth** when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.\n* Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n\n**Memory Tuning for CPU Offloading**\n\n* System memory speed has a major impact on throughput when using partial offloading.\n* Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.\n* On **AM4** platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.\n* On **AM5**, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.\n* Poor memory tuning will bottleneck your CPU offloading even with a fast processor.\n\n**ubatch (Prompt Batch Size)**\n\n* Higher `ubatch` values significantly improve prompt processing (PP) performance.\n* Try values like `768` or `1024`. You’ll use more VRAM, but it’s often worth it for the speedup.\n* If you’re VRAM-limited, lower this until it fits.\n\n**Extra Performance Boost**\n\n* Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA\\_SET\\_ROWS=1 ./llama-server -md /path/to/model etc.\n\n**Speculative Decoding Tips (SD)**\n\nSpeculative decoding is supported in `llama.cpp`, but there are a couple important caveats:\n\n1. **KV cache quant affects acceptance rate heavily.** Using `q4_0` for the draft model’s KV cache *halves* the acceptance rate in my testing. Use `q5_1` or even `q8_0` for the draft model KV cache for much better performance.\n2. **Draft model context handling is broken after filling the draft KV cache.** Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.\n3. **Draft parameters matter a lot**. In my testing, using `--draft-p-min 0.85 --draft-min 2 --draft-max 12` gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.\n\nFor SD, try using **Qwen 3 0.6B** as the draft model. It’s fast and works well, as long as you avoid the issues above.\n\nIf you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp",
          "author_fullname": "t2_66tlmx2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9qn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 82,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 82,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you&amp;#39;re offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use &lt;code&gt;q5_1&lt;/code&gt; for a good balance of memory usage and performance. It works well in PPL tests and in practice.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You&amp;#39;re bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt;\n&lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt;\n&lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you&amp;#39;re comfortable doing so.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt;\n&lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt;\n&lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You’ll use more VRAM, but it’s often worth it for the speedup.&lt;/li&gt;\n&lt;li&gt;If you’re VRAM-limited, lower this until it fits.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model’s KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;code&gt;q5_1&lt;/code&gt; or even &lt;code&gt;q8_0&lt;/code&gt; for the draft model KV cache for much better performance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It’s fast and works well, as long as you avoid the issues above.&lt;/p&gt;\n\n&lt;p&gt;If you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfs9qn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AliNT77",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754145881,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news about the open source models that OpenAI promised to release ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1e80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754169032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg1e80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754169032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.\n\nThe take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.\n\nWe need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3d62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt;\n\n&lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt;\n\n&lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg3d62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754174302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just shipped something I'm really excited about! 🚀\nI was scrolling through my feed and saw Sebastian Raschka, PhD 's incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo.\nThat got me thinking... what if I could bring this to pure C? 🤔\nInspired by Andrej Karpathy's legendary llama2.c approach (seriously, if you haven't seen it, check it out), I decided to take on the challenge of implementing Qwen3's 30B parameter model with 128 experts in a single C file.\nThe result? Qwen_MOE_C - a complete inference engine that:\n✅ Handles sparse MoE computation (only 8 out of 128 experts active)\n✅ Supports Grouped Query Attention with proper head ratios\n✅ Uses memory mapping for efficiency (~30GB models)\n✅ Zero external dependencies (just libc + libm)\nThe beauty of this approach is the same as llama2.c - you can understand every line, it's hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency.\nHuge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems.\nSometimes the best way to truly understand something is to build it from scratch. 🛠️\nLink to the project: \nhttps://github.com/h9-tec/Qwen_MOE_C",
          "author_fullname": "t2_59165ick",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen moe in C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxas1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158458,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just shipped something I&amp;#39;m really excited about! 🚀\nI was scrolling through my feed and saw Sebastian Raschka, PhD &amp;#39;s incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo.\nThat got me thinking... what if I could bring this to pure C? 🤔\nInspired by Andrej Karpathy&amp;#39;s legendary llama2.c approach (seriously, if you haven&amp;#39;t seen it, check it out), I decided to take on the challenge of implementing Qwen3&amp;#39;s 30B parameter model with 128 experts in a single C file.\nThe result? Qwen_MOE_C - a complete inference engine that:\n✅ Handles sparse MoE computation (only 8 out of 128 experts active)\n✅ Supports Grouped Query Attention with proper head ratios\n✅ Uses memory mapping for efficiency (~30GB models)\n✅ Zero external dependencies (just libc + libm)\nThe beauty of this approach is the same as llama2.c - you can understand every line, it&amp;#39;s hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency.\nHuge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems.\nSometimes the best way to truly understand something is to build it from scratch. 🛠️\nLink to the project: \n&lt;a href=\"https://github.com/h9-tec/Qwen_MOE_C\"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?auto=webp&amp;s=e5cdc2248f745a90988be621db7d6b6be3916db2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbd8f00d966699e72ff3a93f578256c7537d2135",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3afe3fea53292db66353b624f6471827586e1c10",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023f7331df4213637fe882078ebd6fb043d92076",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99f557d7c6c92fc79bbc53fbd9519fb90c4895ac",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55d41da3d53c6240a1417760903816f861d7b857",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=869ae3899c29fb68da394dceac5b0968c64c5e51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfxas1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1Hesham",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754158458,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.\n\nI already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an [Awesome list](https://github.com/sindresorhus/awesome).\n\nTo have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: [https://aibenchmarks.net/](https://aibenchmarks.net/)\n\nAwesome AI Benchmarks GitHub repository available here: [https://github.com/panilya/awesome-ai-benchmarks](https://github.com/panilya/awesome-ai-benchmarks)\n\nWould be happy to hear any feedback on this and whether it can be useful for you :)",
          "author_fullname": "t2_7arjztn1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100+ AI Benchmarks list",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwckf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754156092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt;\n\n&lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href=\"https://github.com/sindresorhus/awesome\"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href=\"https://aibenchmarks.net/\"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href=\"https://github.com/panilya/awesome-ai-benchmarks\"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?auto=webp&amp;s=41e93dde0345b37cb0200da115b435ce77df3930",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93761f053553e9b1cfb16372cdf0e7dac51f1f5e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b480826b22f435da297cc6f87648622fc3892fe6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08c9c7135e744081cfec8c73e2a489375eb611be",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63909a628f633b135db8a2f2ad24f375a444a9b7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcf264d960a58dd8659a36b59ab18b32c8762353",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6522fe068f5cf8114a6d791bc9bbc352abd7a38",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfwckf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panilyaU",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754156092,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn't quite process them correctly.\n    \nThis is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.\n    \nSpecifically, this fixes the following:\n    \n- Chat dates are maintained\n- Chat hierarchy is preserved\n- Empty conversations are skipped\n- Parent-child relationships are maintained\n    \nIn addition, it will skip malformed conversations and try to import each chat only once using a `imported.json` file.\n    \nYou can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to `~/chatgpt/chatgpt-export.json`.\n    \nI recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your `webui.db` file.\n    \nAfter importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.\n    \nI like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the `imported.json` file correctly.\n    \nThis works for me, and I hope it works for you too! PRs and issues are welcome.",
          "author_fullname": "t2_44be3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convert your ChatGTP exported conversations to something that Open-WebUI can import",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg12k4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7aaf741530697098b43e0167db0c1c5b5305d19b",
          "edited": 1754169308,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754168175,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn&amp;#39;t quite process them correctly.&lt;/p&gt;\n\n&lt;p&gt;This is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this fixes the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Chat dates are maintained&lt;/li&gt;\n&lt;li&gt;Chat hierarchy is preserved&lt;/li&gt;\n&lt;li&gt;Empty conversations are skipped&lt;/li&gt;\n&lt;li&gt;Parent-child relationships are maintained&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In addition, it will skip malformed conversations and try to import each chat only once using a &lt;code&gt;imported.json&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;You can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to &lt;code&gt;~/chatgpt/chatgpt-export.json&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your &lt;code&gt;webui.db&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;After importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.&lt;/p&gt;\n\n&lt;p&gt;I like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the &lt;code&gt;imported.json&lt;/code&gt; file correctly.&lt;/p&gt;\n\n&lt;p&gt;This works for me, and I hope it works for you too! PRs and issues are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?auto=webp&amp;s=e22d770e4c02c756fc73d7d90a385e5b112f15b5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed1ab2121b7badab4580a484ba08cc844d5e16e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9afd23fae3b50eabe8826fbaa8cb3d0f9a3e2c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ddddd80938d359fbb1d0dd7d1abbf05f3a86de7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17fc3d36981f925574075a8743bee7f8626049a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=998b355322b3740f1ade9410e49eaa05da91b8fc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0e7de199e0866090536334a1fd28c6c4dff2939",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg12k4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "scubanarc",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/",
          "stickied": false,
          "url": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754168175,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_prnin4bw1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GNOME AI Virtual Assistant \"Newelle\" Reaches Version 1.0 Milestone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1evr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754169081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg1evr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FastDecode1",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/",
          "stickied": false,
          "url": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "subreddit_subscribers": 509052,
          "created_utc": 1754169081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our \"own\" R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.\n\nWhat are your thoughts?\n\nUpdate: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)",
          "author_fullname": "t2_qnf2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It's time to run your own R1, Kimi ... and split the cost of it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrunn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754146028,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I&amp;#39;m thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n\n&lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfrunn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HammerSpb",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754144811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!",
          "author_fullname": "t2_13hqmc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have built my own, poor mans Lovable - testing out Cerebras AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0uw8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=5b85dbf1e37f569740ca84504972c21bb5463678",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754167627,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/restyler/poor-mans-lovable",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?auto=webp&amp;s=2d2f3ab9d396d0b9ffc59352c0ad43c2be2c6d47",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cf2bdf2998419d6d274f237ffbcc2b5ad34d663",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e31d2c9aed096970a6183172f8674918abd3da69",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c2a06ca5a7606d5a5f78e2a727ce72eb1898757",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=642596b8bbafb58939fce5813d2fe60c81392d6f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc7356ab40d5babbd50137dd1b88c3efc5a874a3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=496d2912020b57e4bdcbc111110a288bf2bd8366",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg0uw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "superjet1",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0uw8/i_have_built_my_own_poor_mans_lovable_testing_out/",
          "stickied": false,
          "url": "https://github.com/restyler/poor-mans-lovable",
          "subreddit_subscribers": 509052,
          "created_utc": 1754167627,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.\n\nI ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison.\nI'm also planning to run some fine-tuning benchmarks later.",
          "author_fullname": "t2_a85vzco5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 57,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnq2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 54,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 54,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YkVLBy_Uabot6hZJtvt-2lgE4cyPGdL7zFQtJSHp-Xk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754132261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn&amp;#39;t find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt;\n\n&lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I&amp;#39;m quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I&amp;#39;ve also included the RTX 4090 results for a broader comparison.\nI&amp;#39;m also planning to run some fine-tuning benchmarks later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?auto=webp&amp;s=5c7238d3074b0c94ed28a56fd05d130ca36e4062",
                  "width": 1280,
                  "height": 525
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8aa6f94f1fd0b07761158c32a4f411fea4ff01e",
                    "width": 108,
                    "height": 44
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fd7073605b2305ef0bdfa02c28ed14504e5cadf",
                    "width": 216,
                    "height": 88
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f51b098c5db731c8b7427cd4c5e5918907eaf170",
                    "width": 320,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f",
                    "width": 640,
                    "height": 262
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f3fce6514c1c2d99e8304633aabf8e744412a87",
                    "width": 960,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4087fbfd9cd44fbf60685b1fe8cf9814b669d42c",
                    "width": 1080,
                    "height": 442
                  }
                ],
                "variants": {},
                "id": "tc9bMPtFnkPdsr6iuVPGSgsfnHwUyG7fExgbegYm3H4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfnq2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kargafe",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/",
          "stickied": false,
          "url": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "subreddit_subscribers": 509052,
          "created_utc": 1754132261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Model Name|Organization|HuggingFace Link|Size|Modality|\n|:-|:-|:-|:-|:-|\n|dots.ocr|REDnote Hilab|[https://huggingface.co/rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)|3B|Image-Text-to-Text|\n||||||\n|GLM 4.5|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5](https://huggingface.co/zai-org/GLM-4.5)|355B-A32B|Text-to-Text|\n|GLM 4.5 Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Base](https://huggingface.co/zai-org/GLM-4.5-Base)|355B-A32B|Text-to-Text|\n|GLM 4.5-Air|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air](https://huggingface.co/zai-org/GLM-4.5-Air)|106B-A12B|Text-to-Text|\n|GLM 4.5 Air Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air-Base](https://huggingface.co/zai-org/GLM-4.5-Air-Base)|106B-A12B|Text-to-Text|\n||||||\n|Qwen3 235B-A22B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)|235B-A22B|Text-to-Text|\n|Qwen3 235B-A22B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507)|235B-A22B|Text-to-Text|\n|Qwen3 30B-A3B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)|30B-A3B|Text-to-Text|\n|Qwen3 30B-A3B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)|30B-A3B|Text-to-Text|\n|Qwen3 Coder 480B-A35B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct)|480B-A35B|Text-to-Text|\n|Qwen3 Coder 30B-A3B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct)|30B-A3B|Text-to-Text|\n||||||\n|Kimi K2 Instruct|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct)|1T-32B|Text-to-Text|\n|Kimi K2 Base|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base)|1T-32B|Text-to-Text|\n||||||\n|Intern S1|Shanghai AI Laboratory - Intern|[https://huggingface.co/internlm/Intern-S1](https://huggingface.co/internlm/Intern-S1)|241B-A22B|Image-Text-to-Text|\n||||||\n|Llama-3.3 Nemotron Super 49B v1.5|Nvidia|[https://huggingface.co/nvidia/Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)|49B|Text-to-Text|\n|OpenReasoning Nemotron 1.5B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B)|1.5B|Text-to-Text|\n|OpenReasoning Nemotron 7B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B)|7B|Text-to-Text|\n|OpenReasoning Nemotron 14B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B)|14B|Text-to-Text|\n|OpenReasoning Nemotron 32B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B)|32B|Text-to-Text|\n||||||\n|step3|StepFun|[https://huggingface.co/stepfun-ai/step3](https://huggingface.co/stepfun-ai/step3)|321B-A38B|Text-to-Text|\n||||||\n|SmallThinker 21B-A3B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)|21B-A3B|Text-to-Text|\n|SmallThinker 4B-A0.6B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct)|4B-A0.6B|Text-to-Text|\n||||||\n|Seed X Instruct-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B](https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B)|7B|Machine Translation|\n|Seed X PPO-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B](https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B)|7B|Machine Translation|\n||||||\n|Magistral Small 2507|Mistral|[https://huggingface.co/mistralai/Magistral-Small-2507](https://huggingface.co/mistralai/Magistral-Small-2507)|24B|Text-to-Text|\n|Devstral Small 2507|Mistral|[https://huggingface.co/mistralai/Devstral-Small-2507](https://huggingface.co/mistralai/Devstral-Small-2507)|24B|Text-to-Text|\n|Voxtral Small 24B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)|24B|Audio-Text-to-Text|\n|Voxtral Mini 3B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)|3B|Audio-Text-to-Text|\n||||||\n|AFM 4.5B|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B](https://huggingface.co/arcee-ai/AFM-4.5B)|4.5B|Text-to-Text|\n|AFM 4.5B Base|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B-Base](https://huggingface.co/arcee-ai/AFM-4.5B-Base)|4B|Text-to-Text|\n||||||\n|Ling lite-1.5 2506|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ling-lite-1.5-2506](https://huggingface.co/inclusionAI/Ling-lite-1.5-2506)|16B|Text-to-Text|\n|Ming Lite Omni-1.5|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5](https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5)|20.3B|Text-Audio-Video-Image-To-Text|\n||||||\n|UIGEN X 32B 0727|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-32B-0727](https://huggingface.co/Tesslate/UIGEN-X-32B-0727)|32B|Text-to-Text|\n|UIGEN X 4B 0729|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729)|4B|Text-to-Text|\n|UIGEN X 8B|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)|8B|Text-to-Text|\n||||||\n|command a vision 07-2025|Cohere|[https://huggingface.co/CohereLabs/command-a-vision-07-2025](https://huggingface.co/CohereLabs/command-a-vision-07-2025)|112B|Image-Text-to-Text|\n||||||\n|KAT V1 40B|Kwaipilot|[https://huggingface.co/Kwaipilot/KAT-V1-40B](https://huggingface.co/Kwaipilot/KAT-V1-40B)|40B|Text-to-Text|\n||||||\n|EXAONE 4.0.1 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B)|32B|Text-to-Text|\n|EXAONE 4.0.1 2B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)|2B|Text-to-Text|\n|EXAONE 4.0 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)|32B|Text-to-Text|\n||||||\n|cogito v2 preview deepseek-671B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE)|671B-A37B|Text-to-Text|\n|cogito v2 preview llama-405B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B)|405B|Text-to-Text|\n|cogito v2 preview llama-109B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE)|109B-A17B|Image-Text-to-Text|\n|cogito v2 preview llama-70B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B)|70B|Text-to-Text|\n||||||\n|A.X 4.0 VL Light|SK Telecom|[https://huggingface.co/skt/A.X-4.0-VL-Light](https://huggingface.co/skt/A.X-4.0-VL-Light)|8B|Image-Text-to-Text|\n|A.X 3.1|SK Telecom|[https://huggingface.co/skt/A.X-3.1](https://huggingface.co/skt/A.X-3.1)|35B|Text-to-Text|\n|olmOCR 7B 0725|AllenAI|[https://huggingface.co/allenai/olmOCR-7B-0725](https://huggingface.co/allenai/olmOCR-7B-0725)|7B|Image-Text-to-Text|\n||||||\n|kanana 1.5 15.7B-A3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct)|7B-A3B|Text-to-Text|\n|kanana 1.5v 3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct)|3B|Image-Text-to-Text|\n||||||\n|Tri 7B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-7B](https://huggingface.co/trillionlabs/Tri-7B)|7B|Text-to-Text|\n|Tri 21B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-21B](https://huggingface.co/trillionlabs/Tri-21B)|21B|Text-to-Text|\n|Tri 70B preview SFT|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)|70B|Text-to-Text|\n\nI tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.\n\nThis list doesn’t even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)\n\nHope this can serve as a breakdown of the latest models.\n\n*Feel free to tag me if I missed any you think should be added!*\n\n\\[EDIT\\] \n\n**I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.**\n\n**Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?**\n\n**Anyone could publish a new model—with some community approval to reduce junk and pure finetunes**?",
          "author_fullname": "t2_7zubl1l8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfaigh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 521,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 521,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754117628,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754088000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Organization&lt;/th&gt;\n&lt;th align=\"left\"&gt;HuggingFace Link&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Modality&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots.ocr&lt;/td&gt;\n&lt;td align=\"left\"&gt;REDnote Hilab&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/rednote-hilab/dots.ocr\"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5-Air&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Air Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;480B-A35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Base\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Intern S1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/internlm/Intern-S1\"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;241B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;49B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;step3&lt;/td&gt;\n&lt;td align=\"left\"&gt;StepFun&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/stepfun-ai/step3\"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;321B-A38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B-A0.6B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X Instruct-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X PPO-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Magistral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Magistral-Small-2507\"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Devstral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Devstral-Small-2507\"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Small 24B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Small-24B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Mini 3B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Mini-3B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B-Base\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ling lite-1.5 2506&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ling-lite-1.5-2506\"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ming Lite Omni-1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5\"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 32B 0727&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-32B-0727\"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 4B 0729&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;command a vision 07-2025&lt;/td&gt;\n&lt;td align=\"left\"&gt;Cohere&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/CohereLabs/command-a-vision-07-2025\"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;112B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;KAT V1 40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kwaipilot&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Kwaipilot/KAT-V1-40B\"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;671B-A37B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;109B-A17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 4.0 VL Light&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-4.0-VL-Light\"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 3.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-3.1\"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;olmOCR 7B 0725&lt;/td&gt;\n&lt;td align=\"left\"&gt;AllenAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/allenai/olmOCR-7B-0725\"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5v 3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-7B\"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-21B\"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 70B preview SFT&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I tried to compile the latest models released over the past 2–3 weeks, and its kinda like there is a ground breaking model every 2 days. I’m really glad to be living in this era of rapid progress.&lt;/p&gt;\n\n&lt;p&gt;This list doesn’t even include other modalities like 3D, image, and audio, where there&amp;#39;s also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt;\n\n&lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;[EDIT] &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyone could publish a new model—with some community approval to reduce junk and pure finetunes&lt;/strong&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?auto=webp&amp;s=83e0fd1aa924b9918306c02a99cedb9bbb2eb1cb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014ce09ab614e86be0bda115d3ee826dd4c7e72b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fb10f0400ab7291afbb905ab3dfdfb49e477ed8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b160d056e65a5fdd1da13d608db9a9c123e2d7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54fe70e1d1e50ac63262c7c7180e0173f9cc1673",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa0d50402090bd3ad6e9c270f0f950421a2c1523",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dff2cd4e982c9356a88ce61af693c4ca57815b99",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfaigh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "citaman",
          "discussion_type": null,
          "num_comments": 93,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754088000,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 I just open-sourced a fully working persistent memory system for AI assistants!\n\n\n\n🧠 Features:\n\n\\- Real-time memory capture across apps (LM Studio, VS Code, etc.)\n\n\\- Semantic search via vector embeddings\n\n\\- Tool call logging for AI self-reflection\n\n\\- Cross-platform and fully tested\n\n\\- Open source and modular\n\n\n\nBuilt with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝\n\n\n\nGitHub: [https://github.com/savantskie/persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)",
          "author_fullname": "t2_7qb4luzhc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mg5xlb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt;\n\n&lt;p&gt;🧠 Features:&lt;/p&gt;\n\n&lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt;\n\n&lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt;\n\n&lt;p&gt;- Cross-platform and fully tested&lt;/p&gt;\n\n&lt;p&gt;- Open source and modular&lt;/p&gt;\n\n&lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/savantskie/persistent-ai-memory\"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?auto=webp&amp;s=726283d31a9e66a5327266304a008d1eb52da1db",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b41ef9face9075e71937a4cd3b28923245b39a29",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6703186c20a684418ded0dc4abb9a6d490d7b023",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=860a8622d037c44a41b9ee50c96939060a76e917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ef5d63ac01ba46958961a63c5e9f43dabb1245",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=446a7e497c6b8549a8cbed43e99bc5f99e79c618",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1ab447d78f6edbe4f20129542abf92b51d827b1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5xlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Savantskie1",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754181701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new models from Skywork:\n\nWe introduce **MindLink**, a new family of large language models developed by **Kunlun Inc**. Built on **Qwen**, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.\n\n* **Plan-based Reasoning**: Without the \"think\" tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.\n* **Mathematical Framework**: It analyzes the effectiveness of both **Chain-of-Thought (CoT)** and **Plan-based Reasoning**.\n* **Adaptive Reasoning**: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.\n\n[https://huggingface.co/Skywork/MindLink-32B-0801](https://huggingface.co/Skywork/MindLink-32B-0801)\n\n[https://huggingface.co/Skywork/MindLink-72B-0801](https://huggingface.co/Skywork/MindLink-72B-0801)\n\n[https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF](https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF)\n\n\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork MindLink 32B/72B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfitwb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "#bbbdbf",
          "ups": 136,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 136,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/RNvygcC1t5B12yynYguEPL4rixiqNeO2Wo07y1iwph0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754113315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new models from Skywork:&lt;/p&gt;\n\n&lt;p&gt;We introduce &lt;strong&gt;MindLink&lt;/strong&gt;, a new family of large language models developed by &lt;strong&gt;Kunlun Inc&lt;/strong&gt;. Built on &lt;strong&gt;Qwen&lt;/strong&gt;, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;: Without the &amp;quot;think&amp;quot; tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mathematical Framework&lt;/strong&gt;: It analyzes the effectiveness of both &lt;strong&gt;Chain-of-Thought (CoT)&lt;/strong&gt; and &lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Adaptive Reasoning&lt;/strong&gt;: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-32B-0801\"&gt;https://huggingface.co/Skywork/MindLink-32B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-72B-0801\"&gt;https://huggingface.co/Skywork/MindLink-72B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF\"&gt;https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/im7w319dnjgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/im7w319dnjgf1.png?auto=webp&amp;s=47f7f89ad190a3c63e1ac553dd6a3484a9ad8ca0",
                  "width": 2042,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01cd51cd78031eda5866ff7d7ad34590c408908f",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39cd3616dd75544828cbb104491bd20bffd83ee2",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e60ba4473193249923ba5fc4a97b6f3c8979b6",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7493c60ab05796cf114bd0fa0c600e5aa06497f7",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d17b7719956d66090e1c5852b8ee404b2d44a161",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9af2b894d2bfc41eb10b3bdbb12a1975e5e87cac",
                    "width": 1080,
                    "height": 812
                  }
                ],
                "variants": {},
                "id": "dMPU8D8NEUnZXYRxA5Ryy9ueDVT8WMowE2c2asQCAgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfitwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/",
          "stickied": false,
          "url": "https://i.redd.it/im7w319dnjgf1.png",
          "subreddit_subscribers": 509052,
          "created_utc": 1754113315,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I honestly don't know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...  \nIt's something I've never been clear about.  \nFor example, when I've used Deepseek, sometimes I feel that putting it into \"thinking\" mode doesn't add much, but I haven't noticed a clear pattern like \"for this type of question I use thinking mode, for this other type I don't.\"  \nCould someone clarify this for me?\n\nI'm thinking of downloading this model:  \n**Qwen3-30B-A3B-Instruct-2507** ... or **Qwen3-30B-A3B-Thinking-2507**\n\nThe Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?",
          "author_fullname": "t2_q2iij",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking or Instruct?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mg5scj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I honestly don&amp;#39;t know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...&lt;br/&gt;\nIt&amp;#39;s something I&amp;#39;ve never been clear about.&lt;br/&gt;\nFor example, when I&amp;#39;ve used Deepseek, sometimes I feel that putting it into &amp;quot;thinking&amp;quot; mode doesn&amp;#39;t add much, but I haven&amp;#39;t noticed a clear pattern like &amp;quot;for this type of question I use thinking mode, for this other type I don&amp;#39;t.&amp;quot;&lt;br/&gt;\nCould someone clarify this for me?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of downloading this model:&lt;br/&gt;\n&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; ... or &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg5scj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "9acca9",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754181257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_144o7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI models are picking up hidden habits from each other | IBM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfll39",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=9f10577c2fcc3a35df02ac054b4815c973b08f82",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754123731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "ibm.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?auto=webp&amp;s=ffc26910e336a76ab6db1af971b2d262dbce6146",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2723bb45305d3a150f76e1937c51a2690147d015",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30aa75a6f3b7183ee11ff8f6294344790b27c98c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d4ee8618c7e067b6087a6fbd41b990eb083b72f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46e4049e81a052bc62430cfe7e667f62662c693b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec61744cbe7d95092017e4c3bdcff62c20454303",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a031cd428aa62cc550e88505f888b329ed3ed9cf",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfll39",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ab2377",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/",
          "stickied": false,
          "url": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "subreddit_subscribers": 509052,
          "created_utc": 1754123731,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.\n\nIf I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.\n\nSometimes, both sides are the same.  (E.g., \"Are you helpless or do you need my help?\")\n\nSometimes, they're so unbalanced it becomes a Mitch Hedberg-style question. (E.g., \"Have you ever tried sugar or PCP?\")\n\nSometimes, a very open-ended question is presented as either/or. (E.g., \"Is your favorite CSS color value #ff73c1 or #2141af?\" like those are the only two options.)\n\nI have found myself utterly unable to affect this behavior at all through the system prompt. I've tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with \"Don't...\" instructions) I've tried prompting it not to use \"either/or\" questions, \"A or B?\" questions, questions that limit the user's options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.\n\nAnd if I bring it up in chat, I get Qwen3's usual long obsequious apology (\"You're absolutely right, I'm sorry, I made assumptions and didn't respect your blah blah blah... I'll be sure to blah blah blah...\") and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird \"shell-shocked\" mode where it starts writing responses with three words per line that read like it's a frustrated beat poet.\n\nHave other people run into this? If so, are there good ways to combat it?\n\nThanks for any advice!\n",
          "author_fullname": "t2_7pimnskf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get Qwen 3 to stop asking terrible questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mg4lxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754177798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.&lt;/p&gt;\n\n&lt;p&gt;If I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, both sides are the same.  (E.g., &amp;quot;Are you helpless or do you need my help?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, they&amp;#39;re so unbalanced it becomes a Mitch Hedberg-style question. (E.g., &amp;quot;Have you ever tried sugar or PCP?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, a very open-ended question is presented as either/or. (E.g., &amp;quot;Is your favorite CSS color value #ff73c1 or #2141af?&amp;quot; like those are the only two options.)&lt;/p&gt;\n\n&lt;p&gt;I have found myself utterly unable to affect this behavior at all through the system prompt. I&amp;#39;ve tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with &amp;quot;Don&amp;#39;t...&amp;quot; instructions) I&amp;#39;ve tried prompting it not to use &amp;quot;either/or&amp;quot; questions, &amp;quot;A or B?&amp;quot; questions, questions that limit the user&amp;#39;s options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.&lt;/p&gt;\n\n&lt;p&gt;And if I bring it up in chat, I get Qwen3&amp;#39;s usual long obsequious apology (&amp;quot;You&amp;#39;re absolutely right, I&amp;#39;m sorry, I made assumptions and didn&amp;#39;t respect your blah blah blah... I&amp;#39;ll be sure to blah blah blah...&amp;quot;) and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird &amp;quot;shell-shocked&amp;quot; mode where it starts writing responses with three words per line that read like it&amp;#39;s a frustrated beat poet.&lt;/p&gt;\n\n&lt;p&gt;Have other people run into this? If so, are there good ways to combat it?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg4lxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TastesLikeOwlbear",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754177798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don't see an official way and don't want to use LM Studio. What options do I have here? \n\n============================\n\nSecond, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.\n\nIs there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people running an MLX-compatible OpenAI API server locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg26g0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754171064,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don&amp;#39;t see an official way and don&amp;#39;t want to use LM Studio. What options do I have here? &lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;Second, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.&lt;/p&gt;\n\n&lt;p&gt;Is there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg26g0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754171064,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry if this is a basic question, but I seem to be really struggling :/\n\nConsider a typical, text-in text-out use case. If I'm using an offline model API via e.g. REST, how can I incorporate tool  use? Is \"tool use\" some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?\n\nIs there some basic, no-nonsense code or tutorial to get a concrete idea?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is \"tool use\", exactly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrq3v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a basic question, but I seem to be really struggling :/&lt;/p&gt;\n\n&lt;p&gt;Consider a typical, text-in text-out use case. If I&amp;#39;m using an offline model API via e.g. REST, how can I incorporate tool  use? Is &amp;quot;tool use&amp;quot; some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?&lt;/p&gt;\n\n&lt;p&gt;Is there some basic, no-nonsense code or tutorial to get a concrete idea?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfrq3v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754144468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dhanishtha-2.0-preview can now tool call. \n\nUpdated Model link:- [https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825](https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825)  \nAPI and Chat page :- [https://helpingai.co](https://helpingai.co)\n\n",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool calling is now supported on World's first Intermediate Reasoning model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn7pv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754130292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt;\n\n&lt;p&gt;Updated Model link:- &lt;a href=\"https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825\"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br/&gt;\nAPI and Chat page :- &lt;a href=\"https://helpingai.co\"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?auto=webp&amp;s=26ef07178b4b9394a1c2c1b0dfc81de665a07de8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6528fc720427c7c1ed30f20ddd332d01526c4f8f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bffe53726d3c274e1c28ce6856f9194b9aca7fd2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=53fe26fae2e2b849fb6ac025e7f99b8738728cd0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14d7ff05a119f24e15918fc09489c78087b00a3e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e635b759cb0f70c7db7a222e92c287229ba6fb41",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a3feb1fc2bfd7f88481601ee7cdf04a9ff9b5ed",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfn7pv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754130292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys,\n\nI am actually running GLM-4.5-Air with vllm (4x3090) and even if it's quite early I'm quite impressed the model isn't \"lost\" and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.\n\nWhat is your experience actually? Any tip is wellcome\n\nFor info, I use [AWQ](https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ) with the latest (nightly) version of vllm with following cmd:\n\n`vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx`\n\nThen [claude-code-router](https://github.com/musistudio/claude-code-router) with following config:\n\n`{`\n\n`\"LOG\": true,`\n\n`\"Providers\": [`\n\n`{`\n\n`\"name\": \"openai\",`\n\n`\"api_base_url\": \"http://localhost:8123/v1/chat/completions\",`\n\n`\"api_key\": \"xxxx\",`\n\n`\"models\": [\"cpatonn/GLM-4.5-Air-AWQ\"]`\n\n`}`\n\n`],`\n\n`\"Router\": {`\n\n`\"default\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"background\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"think\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContext\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContextThreshold\": 64000,`\n\n`\"webSearch\": \"openai,cpatonn/GLM-4.5-Air-AWQ\"`\n\n`}`\n\n`}`",
          "author_fullname": "t2_udr659irv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Experience with GLM-4.5-Air + claude code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzzt4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754166182,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754165375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am actually running GLM-4.5-Air with vllm (4x3090) and even if it&amp;#39;s quite early I&amp;#39;m quite impressed the model isn&amp;#39;t &amp;quot;lost&amp;quot; and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.&lt;/p&gt;\n\n&lt;p&gt;What is your experience actually? Any tip is wellcome&lt;/p&gt;\n\n&lt;p&gt;For info, I use &lt;a href=\"https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ\"&gt;AWQ&lt;/a&gt; with the latest (nightly) version of vllm with following cmd:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Then &lt;a href=\"https://github.com/musistudio/claude-code-router\"&gt;claude-code-router&lt;/a&gt; with following config:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;LOG&amp;quot;: true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Providers&amp;quot;: [&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;name&amp;quot;: &amp;quot;openai&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_base_url&amp;quot;: &amp;quot;http://localhost:8123/v1/chat/completions&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_key&amp;quot;: &amp;quot;xxxx&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;models&amp;quot;: [&amp;quot;cpatonn/GLM-4.5-Air-AWQ&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;],&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Router&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;default&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;background&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;think&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContext&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContextThreshold&amp;quot;: 64000,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;webSearch&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?auto=webp&amp;s=1b31f315329427daf17c78d7a27488cb4a4815b1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44ceba9879a0b6cc35b31f44bfe9d55af546e830",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2b1cffdeb754044dca444f9940bf1f77e3c1130",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acf9e48a54c3292dcada1d3027879b052535da57",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4127f2600eb35191a3b12e7e09dc0154803bb2d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b52fefc4e7d9396d19c12d4f2875b5d7cd950a56",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9fb7f9f2f97040afaa6980f1c6f3ed5ece7f4490",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfzzt4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Leflakk",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754165375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).\n\n# Autonomous Research Mode\n\nIn this mode, the application automates research tasks for you.\n\n* **Process**: You start by giving it a research question or a topic.\n* **Action**: The AI then searches for information in your uploaded documents or on the web.\n* **Output**: Based on what it finds, the AI generates organized notes and then writes a full research report.\n\nThis mode is useful when you need to quickly gather information on a topic or create a first draft of a document.\n\n# AI-Assisted Writing Mode\n\nThis mode provides help from an AI while you are writing.\n\n* **Interface**: It consists of a markdown text editor next to an AI chat window.\n* **Workflow**: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.\n* **Function**: The AI provides the information you request in the chat window, which you can then use in the document you are writing.\n\nThis mode allows you to get research help without needing to leave your writing environment.\n\n# Document Management\n\nThe application is built around a document management system.\n\n* **Functionality**: You can upload your documents (currently only PDFs) and group them into \"folders.\"\n* **Purpose**: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.",
          "author_fullname": "t2_281myw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wfye5hh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=263452376481ede5f5ee30b3d1fa8d06b92acb2f"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6985e77e3d428df5e2fae627ebdaccf43636351"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36eabba987901a939cb1cba3fa220c59244f5980"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54358ea98fe46953da35a682104b2d28c041974a"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fce8a0da83bbecc8646448132fc88235fc0cc6f"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41259159b0776715b20abd50ed3f735ab55ec141"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=defebde57be1e79d81fad254f827317795a27064"
              },
              "id": "wfye5hh28hgf1"
            },
            "vdsr7ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdacf777510be4c1c1e2cb42159066c1880e6dab"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c356b2b93449bd2d60ae1d426ad98a061cd4282"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae0474fcb61f8fc562cb20c1d53c97e12decaa3c"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32dbcb0b4526b601435315f9f9ad55eac8e9e9f1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54e5b3229d01b08c97d666d27a878563ef0471ae"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e566592ef69e058c675213f1b88982c9f112e66b"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=6a56c58428afe63b1a658254915645c82408e1ce"
              },
              "id": "vdsr7ch28hgf1"
            },
            "4tvg8ah28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=388d6a0156beacf091be4cabfc6a12ef9b2df0e0"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab7ae93af73319fe2da856256040e6bb81700659"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36457622c836252465883bf076811b25d1ad38a2"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60ee85a9f73c317a448f5f7a4a39ee239c5eed29"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6a9af626a4376d663d8cf2d5ce76f802eb0d173"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=726f4a6fca4e608a57b66118d9f5186788c5d1b8"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=ace0180e9725fd088236b603c03e703e138e1af1"
              },
              "id": "4tvg8ah28hgf1"
            },
            "tjfnu7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65253585bbce7fb9495344864b4dbf089fe4a866"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab36743c53f66ceaa0b69bd5950db8c4c34979cd"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1946f554204091e4ee226eecbc6b9b75297b047d"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb8016f74c1a4927f5217225504f99e78266bd6d"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6fff118076daf8093a4c9a0e956b59d667d8a17a"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5073f2ce72eef58984bea03e338686a824f604b1"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=16d847bb68c7a49b063970c2a53144217a358b64"
              },
              "id": "tjfnu7h28hgf1"
            },
            "ko10neh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33c335bff8fd3207fbe0ef4cb7e045a625ad3790"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4245d285c8e6e167c8aa5f8a8196f6f0f79234b0"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ec4b568dd2d51940205e41d3a1168db82a30974"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcb8c81cb1083135981d3576d1e94c4d7061ef1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3f2a7685b2f5d071bd47a85f2da0b70360fa499"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=379b494f484f127cc25b662a1a5f9a3753470d94"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=de810a3b9c7f9df343cc5c34d57dd4752a8b191d"
              },
              "id": "ko10neh28hgf1"
            },
            "xpft85h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b16877882ed90f113b1d8753896a42c5c1cb57"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d6125a00674b9020cb681a99893ad47b47e7913"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cb747f3b0be9eca90601bb0dd0e4f364c7ebc2b"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd1b23bc33d392f682dd70713ef0e6770cafd430"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=43a97bc0e6dad1aeeb47843446cd709c83f052db"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3927f27a929df1bb9fa672c58a3616cffaa32c5d"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=a8d61da6b389396eadf3f74a5e84046b8a859996"
              },
              "id": "xpft85h28hgf1"
            },
            "n8f3d7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0099db17c53d752e1f52434bb4a16edfe724a7d1"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7b068877d833bcda5b55ab2245068d9b2e0148e"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e3c098055fc230578e3b0d4cf171776999a25ae"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=124fc23c5296c23dc74d6a6c55ce26db75d2d137"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d4bec4c048c58090daaad2dfcaf34e92f941cb"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4be81c901606198bf5e6e6c8097ebe64e6a6d39c"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=e915bfa4f37d301877e87b2951c78762e6a8ea17"
              },
              "id": "n8f3d7h28hgf1"
            },
            "9gzp5ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b4696764dfc847eef669961f79b0866d42f5d9"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=168ad0f4c7c2dbc7f867cf7085cff2fc27e24bc3"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d89d57702d0ba973c5772c39da3b51e2e6b5da5"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f80ab136ac496f933eb22700bd7e0c0cfabe288"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=978a98d62c65f1d83a8464507a025600df74f898"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5124a214a0cad85638935b440b2b58a3587824c6"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=f958ce1ae3061d6b0c94c35b4636b3aa5e8260d7"
              },
              "id": "9gzp5ch28hgf1"
            },
            "otdkteh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=055054f2a3a59c53fa172087439b7f25945578f3"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63baa369d4fd7095a2c702fb261aebb398248ee5"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=993e59ad6c2ab02adfae15d2fbd66ec726a4bd13"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e5d52a002ea01c6b7d0bfa10e537c7d7969f822"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49dcb5b218991d6cfc81b7dea2cdd7b72a730add"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64546f9c94b5f46090ccd646df96e7d3c4477044"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=2f6b24cf0547caddaa5f756f6cd6778238257d37"
              },
              "id": "otdkteh28hgf1"
            }
          },
          "name": "t3_1mf92r1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#c7b594",
          "ups": 233,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "Deep Research Draft",
                "media_id": "wfye5hh28hgf1",
                "id": 719101200
              },
              {
                "caption": "Writing Draft with chat pulling data from your documents as well as the internet",
                "media_id": "ko10neh28hgf1",
                "id": 719101201
              },
              {
                "caption": "Write in markdown",
                "media_id": "vdsr7ch28hgf1",
                "id": 719101202
              },
              {
                "caption": "Make document folders to use with your research/writing projects",
                "media_id": "9gzp5ch28hgf1",
                "id": 719101203
              },
              {
                "caption": "Manage documents",
                "media_id": "4tvg8ah28hgf1",
                "id": 719101204
              },
              {
                "caption": "Deep dive into the Deep Researcher outputs like notes prepped from your sources",
                "media_id": "n8f3d7h28hgf1",
                "id": 719101205
              },
              {
                "caption": "Comprehensive research flow with iterative action/reflection loops",
                "media_id": "otdkteh28hgf1",
                "id": 719101206
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "tjfnu7h28hgf1",
                "id": 719101207
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "xpft85h28hgf1",
                "id": 719101208
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 233,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754084338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt;\n\n&lt;h1&gt;Autonomous Research Mode&lt;/h1&gt;\n\n&lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt;\n\n&lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt;\n\n&lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt;\n\n&lt;h1&gt;Document Management&lt;/h1&gt;\n\n&lt;p&gt;The application is built around a document management system.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mf92r1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf92r1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedonihilistic",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mf92r1",
          "subreddit_subscribers": 509052,
          "created_utc": 1754084338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).\n\nAiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?\n\n\n16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).\n\n24 GB (go for size - Radeon 7900 - can get new for 800-1000)\n\n24 GB (3090 - refurbished around 1000, new closet to 1500)\n\nPrimary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?\n\nWhat would you do, and why? Or - where can I read up to better understand my options? Threads like this (https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/) make me think offloading to CPU/RAM doesn't need to absolutely throttle throughput when running locally...",
          "author_fullname": "t2_2orcezep",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guide for GPU Purchase for Local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1cg5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754168901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).&lt;/p&gt;\n\n&lt;p&gt;Aiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?&lt;/p&gt;\n\n&lt;p&gt;16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).&lt;/p&gt;\n\n&lt;p&gt;24 GB (go for size - Radeon 7900 - can get new for 800-1000)&lt;/p&gt;\n\n&lt;p&gt;24 GB (3090 - refurbished around 1000, new closet to 1500)&lt;/p&gt;\n\n&lt;p&gt;Primary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?&lt;/p&gt;\n\n&lt;p&gt;What would you do, and why? Or - where can I read up to better understand my options? Threads like this (&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/&lt;/a&gt;) make me think offloading to CPU/RAM doesn&amp;#39;t need to absolutely throttle throughput when running locally...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg1cg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanManPanther",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754168901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nHeads up to anyone considering Cerebras. This is my conclusion of today's top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.\n\nThe marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn't mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.",
          "author_fullname": "t2_czmwiot",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cerebras Pro Coder Deceptive Limits",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeazc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 106,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 106,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heads up to anyone considering Cerebras. This is my conclusion of today&amp;#39;s top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.&lt;/p&gt;\n\n&lt;p&gt;The marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn&amp;#39;t mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfeazc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snipsthekittycat",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754098861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The \"Leaked\" 120B OpenAI Model Is Trained In FP4\n",
          "author_fullname": "t2_9zkdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The “Leaked” 120 B OpenAI Model is not Trained in FP4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3tm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 403,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 403,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/D9ovYsMF-MnoR2CRSeSS8Yh_VU6bc2G4S1R5gIz7WiE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754071895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?auto=webp&amp;s=af4bbedc766a4ee5a39037f2ab17d7b5501cd231",
                  "width": 1290,
                  "height": 906
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=926137a58fce6f1ef8bee443ff019ae18b337863",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e501be0974ffc2db9bd5cda1678b62434d903b5",
                    "width": 216,
                    "height": 151
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f48501cc5bd5fb8e0b44c8d1575f9a4f16b061b",
                    "width": 320,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06",
                    "width": 640,
                    "height": 449
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=002c3d3feadb04f86a25d1efd1608fcdeb907ef0",
                    "width": 960,
                    "height": 674
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1b63bb57ddbe4e7a32c5af91d1f065caf81082f",
                    "width": 1080,
                    "height": 758
                  }
                ],
                "variants": {},
                "id": "xtQ4De0a5tVgoxoyHlb2WUymp023nocly_no7XupZ6k"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf3tm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "badbutt21",
          "discussion_type": null,
          "num_comments": 88,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/",
          "stickied": false,
          "url": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "subreddit_subscribers": 509052,
          "created_utc": 1754071895,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nEvery app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.\n\nHere's what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.\n\nKnow any other ?\n\nThanks",
          "author_fullname": "t2_cbs8s4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Easily installable GUI for ML-powered audio transcription on AMD GPU ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3g2e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Every app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.&lt;/p&gt;\n\n&lt;p&gt;Know any other ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg3g2e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaKi_87",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754174523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m using Qwen 3 14B right now but haven’t checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? ",
          "author_fullname": "t2_4r17bvqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM that fits with 12GB VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfv3b0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m using Qwen 3 14B right now but haven’t checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfv3b0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tthane50",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754152915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hg：https://huggingface.co/ScienceOne-AI/S1-Base-671B",
          "author_fullname": "t2_44shmmed9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China report the finetune deepseek scientific model 40.44% on HLE",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8pdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 206,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 206,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UTfHZTk3AkcFKdrIagp_47SxVLFGVmG24S66FaWQdXU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754083417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hg：&lt;a href=\"https://huggingface.co/ScienceOne-AI/S1-Base-671B\"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?auto=webp&amp;s=06740559a06a04129cbe49eca027a0862fd98c3b",
                  "width": 1927,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71982e38120d9120577e53d8fabd9588d8007e4b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b276889bf7a37bf56f4c27a131188e43815865c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc9bc3ff25403be175e671f34f1139f244fb6d61",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19933808cef3cec6dce268be3e9d5d269f435579",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd174de9a930fac1ee5ef6231c6a4a6d70fd332",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f6e3196a3a315fef89102429c2a9e280f5ad273",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "l_Dz9A7On8qHXdUNNBTW3zJ0Gj8IkayixkIAvRolxWc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf8pdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Afraid_Hall_2971",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/",
          "stickied": false,
          "url": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "subreddit_subscribers": 509052,
          "created_utc": 1754083417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! So I've been playing around with everyone's baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.\n\nShoutout to u/Skatardude10 and u/farkinga\n\nSo everyone knows it's a great idea to offload some/all tensors to RAM with these models if you can't fit them all. But from what I gathered, if you offload them using \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\", the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.\n\nWhat is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).\n\nThis is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).\n\nThe test is simple \"write me a sophisticated web scraper\". I run it once, then regenerate it to compare (I don't quite understand draft model context, noob, again).\n\n|~~With Qwen3 4b draft model\\*~~|No draft model|\n|:-|:-|\n|~~Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s~~|Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s|\n|~~Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s~~|Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s|\n\n*edit: tried* u/AliNT77\\*'s tip: set draft model's cache to Q8 Q8 and you'll have a higher acceptance rate with the smaller mode, allowing you to go up with main model's context and gain some speed.\\*\n\n\\* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):\n\n* XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)\n* DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s\n* Unsloth Qwen3 0.6B  - 33%, 19t/s\n* **Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s**\n* Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.\n\nWhat was the acceptance rate for 4B you're gonna ask... 67%.\n\nWhy do this instead of trying to offload some layers and try to gain performance this way? I don't know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)\n\nNow this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I'm unemployed, but I'm not THAT unemployed.\n\nQuestions:\n\n1. 1.7b vs 4b draft model. This obvs needs more testing and longer context, but I'm assuming that 4b will perform better than 1.7b with more complex code.\n2. What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?\n3. Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.\n4. I've got a Ryzen CPU. It's massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I'm assuming this is normal and there are no optimisations for AMD cpus?\n5. Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.\n6. Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don't these exist out in the wild?\n7. How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it's localllama so I dare not ask about bigger applications)\n\nWell, if you read all of this, here's your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp; caches, but I am not done yet. This doesn't crash the system, any other combo does. So if you've got more than 12gb vram, you might get away with more context.\n\nStart with: LLAMA\\_SET\\_ROWS=1  \n\\--model \"(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4\\_K\\_XL.gguf\"  \n\\--model-draft \"(full path)/Qwen3-4B-Q8\\_0.gguf\"  \n\\--override-tensor \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\" (yet to test this, but it can now be replaced with --cpu-moe)  \n\\--flash-attn  \n~~--ctx-size 192000~~  \n\\--ctx-size 262144 --cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--threads -1  \n\\--n-gpu-layers 99  \n\\--n-gpu-layers-draft 99  \n~~--ctx-size-draft 1024 --cache-type-k-draft q4\\_0 --cache-type-v-draft q4\\_0~~  \n\\--ctx-size-draft 24567 --cache-type-v-draft q8\\_0 --cache-type-v-draft q8\\_0\n\nor you can do for more speed (30t/s)/accuracy, but less context.  \n\\--ctx-size 131072 --cache-type-k q8\\_0 --cache-type-v q8\\_0  \n\\--ctx-size-draft 24576 --cache-type-k-draft q8\\_0 --cache-type-v-draft q8\\_0  \n\\--batch-size 1024 --ubatch-size 1024\n\nThese settings get you to 11197MiB /  12227MiB vram on the gpu.",
          "author_fullname": "t2_wxxnd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 30B A3b --override-tensor + Qwen3 4b draft = &lt;3 (22 vs 14 t/s)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpgae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754147599,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! So I&amp;#39;ve been playing around with everyone&amp;#39;s baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.&lt;/p&gt;\n\n&lt;p&gt;Shoutout to &lt;a href=\"/u/Skatardude10\"&gt;u/Skatardude10&lt;/a&gt; and &lt;a href=\"/u/farkinga\"&gt;u/farkinga&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So everyone knows it&amp;#39;s a great idea to offload some/all tensors to RAM with these models if you can&amp;#39;t fit them all. But from what I gathered, if you offload them using &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot;, the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.&lt;/p&gt;\n\n&lt;p&gt;What is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).&lt;/p&gt;\n\n&lt;p&gt;This is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).&lt;/p&gt;\n\n&lt;p&gt;The test is simple &amp;quot;write me a sophisticated web scraper&amp;quot;. I run it once, then regenerate it to compare (I don&amp;#39;t quite understand draft model context, noob, again).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;del&gt;With Qwen3 4b draft model*&lt;/del&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;No draft model&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;em&gt;edit: tried&lt;/em&gt; &lt;a href=\"/u/AliNT77\"&gt;u/AliNT77&lt;/a&gt;*&amp;#39;s tip: set draft model&amp;#39;s cache to Q8 Q8 and you&amp;#39;ll have a higher acceptance rate with the smaller mode, allowing you to go up with main model&amp;#39;s context and gain some speed.*&lt;/p&gt;\n\n&lt;p&gt;* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)&lt;/li&gt;\n&lt;li&gt;DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 0.6B  - 33%, 19t/s&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What was the acceptance rate for 4B you&amp;#39;re gonna ask... 67%.&lt;/p&gt;\n\n&lt;p&gt;Why do this instead of trying to offload some layers and try to gain performance this way? I don&amp;#39;t know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)&lt;/p&gt;\n\n&lt;p&gt;Now this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I&amp;#39;m unemployed, but I&amp;#39;m not THAT unemployed.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;1.7b vs 4b draft model. This obvs needs more testing and longer context, but I&amp;#39;m assuming that 4b will perform better than 1.7b with more complex code.&lt;/li&gt;\n&lt;li&gt;What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?&lt;/li&gt;\n&lt;li&gt;Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve got a Ryzen CPU. It&amp;#39;s massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I&amp;#39;m assuming this is normal and there are no optimisations for AMD cpus?&lt;/li&gt;\n&lt;li&gt;Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.&lt;/li&gt;\n&lt;li&gt;Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don&amp;#39;t these exist out in the wild?&lt;/li&gt;\n&lt;li&gt;How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it&amp;#39;s localllama so I dare not ask about bigger applications)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Well, if you read all of this, here&amp;#39;s your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp;amp; caches, but I am not done yet. This doesn&amp;#39;t crash the system, any other combo does. So if you&amp;#39;ve got more than 12gb vram, you might get away with more context.&lt;/p&gt;\n\n&lt;p&gt;Start with: LLAMA_SET_ROWS=1&lt;br/&gt;\n--model &amp;quot;(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf&amp;quot;&lt;br/&gt;\n--model-draft &amp;quot;(full path)/Qwen3-4B-Q8_0.gguf&amp;quot;&lt;br/&gt;\n--override-tensor &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot; (yet to test this, but it can now be replaced with --cpu-moe)&lt;br/&gt;\n--flash-attn&lt;br/&gt;\n&lt;del&gt;--ctx-size 192000&lt;/del&gt;&lt;br/&gt;\n--ctx-size 262144 --cache-type-k q4_0 --cache-type-v q4_0&lt;br/&gt;\n--threads -1&lt;br/&gt;\n--n-gpu-layers 99&lt;br/&gt;\n--n-gpu-layers-draft 99&lt;br/&gt;\n&lt;del&gt;--ctx-size-draft 1024 --cache-type-k-draft q4_0 --cache-type-v-draft q4_0&lt;/del&gt;&lt;br/&gt;\n--ctx-size-draft 24567 --cache-type-v-draft q8_0 --cache-type-v-draft q8_0&lt;/p&gt;\n\n&lt;p&gt;or you can do for more speed (30t/s)/accuracy, but less context.&lt;br/&gt;\n--ctx-size 131072 --cache-type-k q8_0 --cache-type-v q8_0&lt;br/&gt;\n--ctx-size-draft 24576 --cache-type-k-draft q8_0 --cache-type-v-draft q8_0&lt;br/&gt;\n--batch-size 1024 --ubatch-size 1024&lt;/p&gt;\n\n&lt;p&gt;These settings get you to 11197MiB /  12227MiB vram on the gpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfpgae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "igorwarzocha",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754138032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.\n\nHere are a few things I've focused on:\n\n* It can create and run files automatically from natural language.\n* I've tried to make it smart about executing tools like git, search, and bash commands.\n* It's designed to work with any Ollama model that supports function calling.\n* A big priority for me was to keep it 100% local to ensure privacy.\n\nIt's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.",
          "author_fullname": "t2_2lznz5yi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollamacode - Local AI assistant that can create, run and understand your codebase.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqr3o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=c955545b2920d3da4a8cd08357963d4b7cac5b74",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754141855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a project called OllamaCode, and I&amp;#39;d love to share it with you. It&amp;#39;s an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt;\n\n&lt;p&gt;Here are a few things I&amp;#39;ve focused on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s designed to work with any Ollama model that supports function calling.&lt;/li&gt;\n&lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s still in the very early days, and there&amp;#39;s a lot I still want to improve. It&amp;#39;s been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/tooyipjee/ollamacode",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?auto=webp&amp;s=0d661e9993d3a8339a8a04e91dab3d495a1068d8",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad56c7eac7e674170e1bce20ac9f2fbb89067d4f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4264d87bb5a2e08e33aa5bd378354efc3de28a29",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbbbbe9284ddae09fc99ccbdd7bbad23851be4ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc443c649d71e676f2ac536126f73b6f681be48d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bf93455ce16aa988227f0c0581aef56a3355348",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0dd9f689127fdc2e0c3b1dd1996536119e75b475",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqr3o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud-Consideration-2",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/",
          "stickied": false,
          "url": "https://github.com/tooyipjee/ollamacode",
          "subreddit_subscribers": 509052,
          "created_utc": 1754141855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt's agent mode, but haven't found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?",
          "author_fullname": "t2_w61xk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Closest Local Version of OpenAI's Agent Mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg24nd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754170932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt&amp;#39;s agent mode, but haven&amp;#39;t found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg24nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RabbitEater2",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754170932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing an OpenAI proxy solution for Claude-Code\n\n[https://github.com/ziozzang/claude2openai-proxy](https://github.com/ziozzang/claude2openai-proxy)\n\nAdvantages:\n\n1. In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.\n2. You can start using Claude Code with any specified model, whether it’s internal or external.\n3. You can also debug if needed.\n4. Anyway, at least from my tests, it works very well. The only issue is the model itself.\n\n\\----\n\nWhile using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn’t find a solid solution that worked well for multiple users. So, based on [https://github.com/1rgs/claude-code-proxy ](https://github.com/1rgs/claude-code-proxy), I built a proxy tailored for my use. Since it converts between different protocols, “gateway” might actually be a more fitting term.\n\nAnyway, here are the features:\n\nFull support for Claude Code.\n\n* On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.\n* Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.\n* Authentication is done via the ANTHROPIC\\_API\\_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.\n\nBelow is an example of setting up the server and actually using it from a client:\n\n    ANTHROPIC_BASE_URL=http://localhost:8082 \\\n    ANTHROPIC_API_KEY=sk-openapi-auth-token \\\n    ANTHROPIC_MODEL=\"openrouter/horizon-beta\" \\\n    ANTHROPIC_SMALL_FAST_MODEL=\"openrouter/horizon-beta\" \\\n    claude  \n\nTo be honest, I made this to test the openrouter/horizon-beta model. :)\n\nThe pipeline works great: Claude Code -(Claude API)-&gt; my modified proxy server -(OpenAI API)-&gt; openrouter/horizon-beta.\n\nBy the way, you can find what I built at [https://github.com/ziozzang/claude2openai-proxy ](https://github.com/ziozzang/claude2openai-proxy). I use it by building it into a container.\n\nTo be honest,\n\n* horizon-alpha doesn’t seem to handle Claude Code’s prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)\n* horizon-beta handles them quite well.\n* However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don’t feel ideal for automated workflows.\n\nPS.\n\nThe whole reason this started was because of Claude Code’s usage limits. LoL...",
          "author_fullname": "t2_5409gkc6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gateway/Proxy for Claude-Code to OpenAI API compatible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuu40",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754153859,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing an OpenAI proxy solution for Claude-Code&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Advantages:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.&lt;/li&gt;\n&lt;li&gt;You can start using Claude Code with any specified model, whether it’s internal or external.&lt;/li&gt;\n&lt;li&gt;You can also debug if needed.&lt;/li&gt;\n&lt;li&gt;Anyway, at least from my tests, it works very well. The only issue is the model itself.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;While using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn’t find a solid solution that worked well for multiple users. So, based on &lt;a href=\"https://github.com/1rgs/claude-code-proxy\"&gt;https://github.com/1rgs/claude-code-proxy &lt;/a&gt;, I built a proxy tailored for my use. Since it converts between different protocols, “gateway” might actually be a more fitting term.&lt;/p&gt;\n\n&lt;p&gt;Anyway, here are the features:&lt;/p&gt;\n\n&lt;p&gt;Full support for Claude Code.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.&lt;/li&gt;\n&lt;li&gt;Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.&lt;/li&gt;\n&lt;li&gt;Authentication is done via the ANTHROPIC_API_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Below is an example of setting up the server and actually using it from a client:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ANTHROPIC_BASE_URL=http://localhost:8082 \\\nANTHROPIC_API_KEY=sk-openapi-auth-token \\\nANTHROPIC_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nANTHROPIC_SMALL_FAST_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nclaude  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To be honest, I made this to test the openrouter/horizon-beta model. :)&lt;/p&gt;\n\n&lt;p&gt;The pipeline works great: Claude Code -(Claude API)-&amp;gt; my modified proxy server -(OpenAI API)-&amp;gt; openrouter/horizon-beta.&lt;/p&gt;\n\n&lt;p&gt;By the way, you can find what I built at &lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy &lt;/a&gt;. I use it by building it into a container.&lt;/p&gt;\n\n&lt;p&gt;To be honest,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;horizon-alpha doesn’t seem to handle Claude Code’s prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)&lt;/li&gt;\n&lt;li&gt;horizon-beta handles them quite well.&lt;/li&gt;\n&lt;li&gt;However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don’t feel ideal for automated workflows.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PS.&lt;/p&gt;\n\n&lt;p&gt;The whole reason this started was because of Claude Code’s usage limits. LoL...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?auto=webp&amp;s=42f669382e1c8928b5e1d529da5526d93801fc0d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e43bc658beb09686e12af7368ba5a1c7c7d40071",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41d704239a298e8b8e9ef761cbf0504c8c77ccaf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fed85d51213a23fb5ee442d3c4a5ad79bfcf800",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d1364de265c02e79206344fc84f7451660adc50",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5975df8d3cb9a0c0f9fe7ad4afa6ddfa17a56a05",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cbd06ed3b8ba41d88914fbda7d1b7707aee647f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfuu40",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ziozzang0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754152276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.",
          "author_fullname": "t2_1tcpn4d5tw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small LLM in german",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfldxj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122940,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfldxj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ghulaschsuppe",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754122940,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tried [https://huggingface.co/MetaStoneTec/XBai-o4](https://huggingface.co/MetaStoneTec/XBai-o4) ? Big if true -\n\n\\&gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance \n\nHave not tried it myself, downloading atm from [https://huggingface.co/mradermacher/XBai-o4-GGUF](https://huggingface.co/mradermacher/XBai-o4-GGUF)",
          "author_fullname": "t2_4rlver1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MetaStoneTec/XBai-o4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk3y2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt; ? Big if true -&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini&amp;#39;s performance &lt;/p&gt;\n\n&lt;p&gt;Have not tried it myself, downloading atm from &lt;a href=\"https://huggingface.co/mradermacher/XBai-o4-GGUF\"&gt;https://huggingface.co/mradermacher/XBai-o4-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?auto=webp&amp;s=21ea26fb14e3912f80713c95ddb95b37cd316f66",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f69ac45b09b4ccb4576fc51eb90d2b847eace7ec",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a115ce28ec07456cec2ed3598f59bf288c35b1a9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=02748428a3ab0ac779c13dffcf54fbcc054b5691",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed975e60e78ac76aed0294ed900e215343eefe17",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64a929e4331b92d45b82827765bb555c57573df7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24081c4d549b93ebfca1a8d44ef5f08c619cebf8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfk3y2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ljosif",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754118021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.\n\nI also prefer to use CLI only, because I want everything to eventually be for a robot I'm working on (because of this I don't really want a UI interface). For some I haven't fully tested only the CLI, and for some I've tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.\n\nI will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here's my personal \"rankings\".\n\n* **Bark/Coqui TTS -**\n   * **The Good:** The emotions are next level... kinda. At least they have it, is the main thing. What I've done is create a custom Llama model, that knows when to send a \\[laughs\\], \\[sighs\\], etc. that's appropriate, given the conversation. The custom ollama model is pretty good at this (if you're curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.\n   * **The Bad:** It's pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also \"trails off\", if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I'm using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn't sometimes even follow the cloned voice. However, when it does, it's somewhat decent. But given how it often does not, it's not really too usable.\n* **F5 TTS -**\n   * **The Good:** Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I'd say it's better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that's close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn't trail off. It will finish speaking until the text from my custom ollama model is done being spoken.\n   * **The Bad:** As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I'd say it's about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can't do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn't sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It's pretty good, but not perfect.\n* **Orpheus TTS**\n   * **The Good:** This one can also do laughing, yawning, etc. and it's decent at it. But not as good as Coqui/Bark. Although it's still better than what most offer, since it has the ability at all. There's a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn't trail off.\n   * **The Bad:** This one is a pain to set up, at least if you try to go the normal route, via CLI. I've only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I'd say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn't get picked up, and it just says \"yawn\", instead. Coqui didn't really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).\n* **Kokoro TTS**\n   * **The Good:** Man, the UI is blazing FAST. If I had to guess about \\~ 1 second or so. And that's using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn't perfect, it's probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn't trail off and reads all the speech too, which is nice.\n   * **The Bad:** It sounds a little bland. Some of the models, even if they don't have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn't distinct between exclamation, or questions, much. It's not terrible, but sounds like an average Speech to Text, that you'd find on an average book reader, for example. Also doesn't offer native voice cloning, that I'm aware of at least, but I could be wrong.",
          "author_fullname": "t2_4guqxmy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS Model Comparisons: My Personal Rankings (So far) of TTS Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjn88",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp;amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.&lt;/p&gt;\n\n&lt;p&gt;I also prefer to use CLI only, because I want everything to eventually be for a robot I&amp;#39;m working on (because of this I don&amp;#39;t really want a UI interface). For some I haven&amp;#39;t fully tested only the CLI, and for some I&amp;#39;ve tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.&lt;/p&gt;\n\n&lt;p&gt;I will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here&amp;#39;s my personal &amp;quot;rankings&amp;quot;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Bark/Coqui TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; The emotions are next level... kinda. At least they have it, is the main thing. What I&amp;#39;ve done is create a custom Llama model, that knows when to send a [laughs], [sighs], etc. that&amp;#39;s appropriate, given the conversation. The custom ollama model is pretty good at this (if you&amp;#39;re curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It&amp;#39;s pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also &amp;quot;trails off&amp;quot;, if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I&amp;#39;m using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn&amp;#39;t sometimes even follow the cloned voice. However, when it does, it&amp;#39;s somewhat decent. But given how it often does not, it&amp;#39;s not really too usable.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;F5 TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I&amp;#39;d say it&amp;#39;s better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that&amp;#39;s close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn&amp;#39;t trail off. It will finish speaking until the text from my custom ollama model is done being spoken.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I&amp;#39;d say it&amp;#39;s about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can&amp;#39;t do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn&amp;#39;t sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It&amp;#39;s pretty good, but not perfect.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orpheus TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; This one can also do laughing, yawning, etc. and it&amp;#39;s decent at it. But not as good as Coqui/Bark. Although it&amp;#39;s still better than what most offer, since it has the ability at all. There&amp;#39;s a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn&amp;#39;t trail off.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; This one is a pain to set up, at least if you try to go the normal route, via CLI. I&amp;#39;ve only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I&amp;#39;d say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn&amp;#39;t get picked up, and it just says &amp;quot;yawn&amp;quot;, instead. Coqui didn&amp;#39;t really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kokoro TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; Man, the UI is blazing FAST. If I had to guess about ~ 1 second or so. And that&amp;#39;s using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn&amp;#39;t perfect, it&amp;#39;s probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn&amp;#39;t trail off and reads all the speech too, which is nice.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; It sounds a little bland. Some of the models, even if they don&amp;#39;t have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn&amp;#39;t distinct between exclamation, or questions, much. It&amp;#39;s not terrible, but sounds like an average Speech to Text, that you&amp;#39;d find on an average book reader, for example. Also doesn&amp;#39;t offer native voice cloning, that I&amp;#39;m aware of at least, but I could be wrong.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfjn88",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iKontact",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754116331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named [UniTok](https://arxiv.org/abs/2502.20321) and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.\n\nThey say their method is better but i am not sure, when i tried their code it didn't give high quality construction results but they said that they didn't train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don't know.\n\nWhat is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)",
          "author_fullname": "t2_1urjd1hc7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How the best image generation models work from the inside ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0ur7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167616,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named &lt;a href=\"https://arxiv.org/abs/2502.20321\"&gt;UniTok&lt;/a&gt; and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.&lt;/p&gt;\n\n&lt;p&gt;They say their method is better but i am not sure, when i tried their code it didn&amp;#39;t give high quality construction results but they said that they didn&amp;#39;t train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;What is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0ur7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Severe-Awareness829",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754167616,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Rank|Model|Score|95% CI|Votes|Company|License|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|gemini 2.5 pro|1474|±8|7,178|Goog||\n|1|qwen3 235b a22b instruct 2507|1464|±18|1,089|Alibaba|Apache|\n|2|o3 2025 04 16|1445|±7|9,877|Closed AI||\n|2|grok 4 2502|1442|±10|4,063|xAI||\n|2|qwen3 235b a22b thinking 2507|1442|±20|917|Alibaba|Apache|\n|2|grok 3 preview 02 24|1439|±7|7,588|xAI||\n|3|deepseek r1 0528|1436|±9|4,851|DeepSeek|MIT|\n\nStyle control removed.\nhttps://lmarena.ai/leaderboard/text/coding",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alibaba not doing to bad at coding according to lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0sbe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167443,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Rank&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Score&lt;/th&gt;\n&lt;th align=\"left\"&gt;95% CI&lt;/th&gt;\n&lt;th align=\"left\"&gt;Votes&lt;/th&gt;\n&lt;th align=\"left\"&gt;Company&lt;/th&gt;\n&lt;th align=\"left\"&gt;License&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;gemini 2.5 pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;1474&lt;/td&gt;\n&lt;td align=\"left\"&gt;±8&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,178&lt;/td&gt;\n&lt;td align=\"left\"&gt;Goog&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1464&lt;/td&gt;\n&lt;td align=\"left\"&gt;±18&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,089&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;o3 2025 04 16&lt;/td&gt;\n&lt;td align=\"left\"&gt;1445&lt;/td&gt;\n&lt;td align=\"left\"&gt;±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;9,877&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 4 2502&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;±10&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,063&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;±20&lt;/td&gt;\n&lt;td align=\"left\"&gt;917&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 3 preview 02 24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1439&lt;/td&gt;\n&lt;td align=\"left\"&gt;±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,588&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;deepseek r1 0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;1436&lt;/td&gt;\n&lt;td align=\"left\"&gt;±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,851&lt;/td&gt;\n&lt;td align=\"left\"&gt;DeepSeek&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Style control removed.\n&lt;a href=\"https://lmarena.ai/leaderboard/text/coding\"&gt;https://lmarena.ai/leaderboard/text/coding&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0sbe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754167443,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!",
          "author_fullname": "t2_kfjfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfbw8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 96,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 96,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=140&amp;height=76&amp;crop=140:76,smart&amp;auto=webp&amp;s=178741032ad68bb72212a1f0482ccf59165855d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754091780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "medium.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just because you are hosting locally, doesn&amp;#39;t mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?auto=webp&amp;s=db6f154eef502137479106eb0ee5e3497a2c8a5d",
                  "width": 1024,
                  "height": 559
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fc30b818f499ebfea16a1a44bc05f5b89c31100",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8f36d4f360255f41bf1da0fda787ab734a46fbd",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad1e44aa12420eb39ba559c3fdbad4ad0407e0b",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24b8b3213823bb044c73076e1852d1957545a17f",
                    "width": 640,
                    "height": 349
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cb31249007f2a139fccd496d749e3a58bdc7c0d",
                    "width": 960,
                    "height": 524
                  }
                ],
                "variants": {},
                "id": "1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfbw8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JAlbrethsen",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/",
          "stickied": false,
          "url": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "subreddit_subscribers": 509052,
          "created_utc": 1754091780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Beta seems really solid from early testing, not a magnitude better than what SOTA's offer but still impressive",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Alpha vs Horizon Beta",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgwyu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756775957%2CMzQwYzEwZTdiMTYwNmI3M2Q5NTRkNDhhZGYxY2VkZjdhMTc0OTc1NWY2N2MyYTY4ZmIzMzBiYmM1M2VjYThiMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756775957%2CMWU1YWI3NGRmYzcyNzA4ZDI3NzM0NzMyMWY0MGFiMWRmYWQ2NzVmNjA3MTk5YTYwMTNkN2Q4NWM5ZjQ2OWU3MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=140&amp;height=84&amp;crop=140:84,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=dd582f91752e7fd7a72f3be30d66c628b1cabf42",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754106912,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Beta seems really solid from early testing, not a magnitude better than what SOTA&amp;#39;s offer but still impressive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/dg8cy7ia4jgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?format=pjpg&amp;auto=webp&amp;s=30b632f2cdd42a3234c72ef686841eed57c63aec",
                  "width": 3600,
                  "height": 2178
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d42e84556d577f46b8f2cc66b308d63c6ba3aa39",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0a342e02b97eb15e7ae8d55711f5dc21ed403177",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1c923808071843a8b872aa37e16f734fbae495b3",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbb1ef3633d6485f7f54280916bb8b15600f3044",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1f330f77b0817cdba51f914a124707d5bffe8a1",
                    "width": 960,
                    "height": 580
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9a786dc189b4a8425437a29d560c6d7825e64a0",
                    "width": 1080,
                    "height": 653
                  }
                ],
                "variants": {},
                "id": "d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfgwyu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/",
          "stickied": false,
          "url": "https://v.redd.it/dg8cy7ia4jgf1",
          "subreddit_subscribers": 509052,
          "created_utc": 1754106912,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756775957%2CMzQwYzEwZTdiMTYwNmI3M2Q5NTRkNDhhZGYxY2VkZjdhMTc0OTc1NWY2N2MyYTY4ZmIzMzBiYmM1M2VjYThiMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756775957%2CMWU1YWI3NGRmYzcyNzA4ZDI3NzM0NzMyMWY0MGFiMWRmYWQ2NzVmNjA3MTk5YTYwMTNkN2Q4NWM5ZjQ2OWU3MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.\n\nBuilt using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It's ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.\n\n[https://github.com/dokasto/Saidia](https://github.com/dokasto/Saidia)",
          "author_fullname": "t2_11mwsd0p41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Saidia: Offline-First AI Assistant for Educators in low-connectivity regions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn2xf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754161230,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754129775,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.&lt;/p&gt;\n\n&lt;p&gt;Built using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It&amp;#39;s ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dokasto/Saidia\"&gt;https://github.com/dokasto/Saidia&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?auto=webp&amp;s=5141edfe493867e3dafdd151010b3f5040912cc0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e949d6258248c551dcfd9cb47f1304f4151400c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3484da8634402f8ed7774d2e4f8c4669168d53",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7c82a3e8557eeca0f83fc2e477928d5f3031a9f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=493dbfda76c0eab29abee992f4eb1d96ff6d93fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92c32a097682bd7a3113fbc01e20ac3a07fd673",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e412f9a5dfdc2c2cf419a9494a9e06f0757f95",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfn2xf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dokasto_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754129775,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT's throttled free-tier, and Claude 4 Sonnet",
          "author_fullname": "t2_65zz9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Four Models, One Prompt: Who Writes the Best Instructions for AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwec7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754156214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "selfenrichment.hashnode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT&amp;#39;s throttled free-tier, and Claude 4 Sonnet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfwec7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertotomas",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwec7/four_models_one_prompt_who_writes_the_best/",
          "stickied": false,
          "url": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "subreddit_subscribers": 509052,
          "created_utc": 1754156214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just launched GridLLM (https://github.com/GridLLM/GridLLM), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. \n\nWould love any feedback on the project!\n",
          "author_fullname": "t2_g5wvmwck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-Source Project for Distributed Inference Management",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzg8h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754163962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just launched GridLLM (&lt;a href=\"https://github.com/GridLLM/GridLLM\"&gt;https://github.com/GridLLM/GridLLM&lt;/a&gt;), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. &lt;/p&gt;\n\n&lt;p&gt;Would love any feedback on the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?auto=webp&amp;s=5262c21723d990876d1321b058c80d37499608f4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a422ad10d8ba96d4fcb866b6ade60210133fea91",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92141835b6803ca57fbce4c55c4e89ef337c1d98",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc5e385866a70deb5e03659dfedb836c11a84a0c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00d881194968dc37ee1e6d0224b4065c46e497f3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f6e74f447093226b82fd0f39651b6d915ce7b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fe17371d5951c9213d509050b853dbafa2832f8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzg8h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Choice_Nature9658",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754163962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So many versions! I saw something about how the DWQ versions are best, but then obviously MLX \\*seems\\* like it would be best? And what quantization version?",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "64GB M1 Max, which GLM-4.5-Air?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg44ya",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176467,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So many versions! I saw something about how the DWQ versions are best, but then obviously MLX *seems* like it would be best? And what quantization version?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg44ya",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754176467,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? \n\n- Apple m4 pro chip\n- 14 core CPU, 20 core GPU\n- 48GB unified memory. \n- One TB SSD storage \n\nEventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.",
          "author_fullname": "t2_1ook1izxzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this set up sufficient?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg40u1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Apple m4 pro chip&lt;/li&gt;\n&lt;li&gt;14 core CPU, 20 core GPU&lt;/li&gt;\n&lt;li&gt;48GB unified memory. &lt;/li&gt;\n&lt;li&gt;One TB SSD storage &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Eventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg40u1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wild-Muffin9190",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754176144,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: \n\n- Roo Code with [Unsloth Qwen3 Coder 30B Q8](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)\n- [llama-swap](https://github.com/mostlygeek/llama-swap) with new Activity page with real time updates. \n- [VibeCities MCP server](https://github.com/mostlygeek/vibecities) for hosting the pages\n- Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. \n\nHere's my llama-swap config: \n\n```\nmacros:\n  \"qwen3-coder-server\": |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full\n\nmodels:\n  \"Q3-30B-CODER-3090\":\n    env:\n      - \"CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10\"\n    name: \"Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)\"\n    description: \"Q8_K_XL, 180K context, 2x3090\"\n    filters:\n      # enforce recommended params for model\n      strip_params: \"temperature, top_k, top_p, repeat_penalty\"\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```\n\nRoo code MCP settings: \n\n```\n{\n  \"mcpServers\": {\n    \"vibecities\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"http://10.0.1.173:8888/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-secure-api-key\"\n      },\n      \"alwaysAllow\": [\n        \"page_list\",\n        \"page_set\",\n        \"page_get\"\n      ],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n\n\n",
          "author_fullname": "t2_11gh93nhos",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "All local Roo Code and qwen3 coder 30B Q8",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfariy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756775957%2CZDhmM2M4MzZmYWU3YWJmOTAzYzVmZGFlNmMxYWFhOGZhNzgwZmY3MDMzODM4NTgyZDM5ODQ5OThiOGYzZDkzZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756775957%2CNjAxNzY3ZWE4ZjRkZmJmOWMxMDFkOWE5OGE2NjJlYTMyZmRlODg1MGZkMmVjZmY4ZWRiY2JjZGFjMzQ1MjdmOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=96e2057bb1f1ef1e9f3beb1d9a28a9ccd4dcaa6b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754088672,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Roo Code with &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;Unsloth Qwen3 Coder 30B Q8&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/llama-swap\"&gt;llama-swap&lt;/a&gt; with new Activity page with real time updates. &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/vibecities\"&gt;VibeCities MCP server&lt;/a&gt; for hosting the pages&lt;/li&gt;\n&lt;li&gt;Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s my llama-swap config: &lt;/p&gt;\n\n&lt;p&gt;```\nmacros:\n  &amp;quot;qwen3-coder-server&amp;quot;: |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full&lt;/p&gt;\n\n&lt;p&gt;models:\n  &amp;quot;Q3-30B-CODER-3090&amp;quot;:\n    env:\n      - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;\n    name: &amp;quot;Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)&amp;quot;\n    description: &amp;quot;Q8_K_XL, 180K context, 2x3090&amp;quot;\n    filters:\n      # enforce recommended params for model\n      strip_params: &amp;quot;temperature, top_k, top_p, repeat_penalty&amp;quot;\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```&lt;/p&gt;\n\n&lt;p&gt;Roo code MCP settings: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;vibecities&amp;quot;: {\n      &amp;quot;type&amp;quot;: &amp;quot;streamable-http&amp;quot;,\n      &amp;quot;url&amp;quot;: &amp;quot;http://10.0.1.173:8888/mcp&amp;quot;,\n      &amp;quot;headers&amp;quot;: {\n        &amp;quot;X-API-Key&amp;quot;: &amp;quot;your-secure-api-key&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;page_list&amp;quot;,\n        &amp;quot;page_set&amp;quot;,\n        &amp;quot;page_get&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g5aj1csfjhgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?format=pjpg&amp;auto=webp&amp;s=a799694f3cd2a8d09be3eac7cc9981be88d234a1",
                  "width": 1920,
                  "height": 1197
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7b56a58c7f6f027ee7357cad95a460ff999afeea",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5adfd5aba83e12c59bf4648d145f4ab40fd5648e",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4be2e0e3ac56fa7c11be8c2c58c9a02a90039429",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c11415fad487d232adecf2767cc5b4b8ac2ab42f",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fd1f305092aa1774fdb5bb8d64c33f4d4acc5781",
                    "width": 960,
                    "height": 598
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0c4665784dbdfe56aa44706a6b06cd7fb427df9",
                    "width": 1080,
                    "height": 673
                  }
                ],
                "variants": {},
                "id": "OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfariy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Statement-0001",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/",
          "stickied": false,
          "url": "https://v.redd.it/g5aj1csfjhgf1",
          "subreddit_subscribers": 509052,
          "created_utc": 1754088672,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756775957%2CZDhmM2M4MzZmYWU3YWJmOTAzYzVmZGFlNmMxYWFhOGZhNzgwZmY3MDMzODM4NTgyZDM5ODQ5OThiOGYzZDkzZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756775957%2CNjAxNzY3ZWE4ZjRkZmJmOWMxMDFkOWE5OGE2NjJlYTMyZmRlODg1MGZkMmVjZmY4ZWRiY2JjZGFjMzQ1MjdmOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with Qwen3:30b-a3b-instruct-2507-q8\\_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I'm really impressed with the speed!  \n\nIn light conversation (I tested with general knowledge questions and everyday scenarios), I'm achieving up to 34 tokens/s, which is \\*significantly\\* faster than other models I've tested (all Q4 except for qwen3):\n\n* Qwen3 (30B): \\~34 tokens/s\n* Qwen2.5 (32B): \\~10 tokens/s\n* Gemma3 (27B): \\~10 tokens/s\n* Llama3 (70B): 4-5 tokens/s\n\nHowever, I'm also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.\n\nMy first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What's your experience been like with speed and accuracy of Qwen3?",
          "author_fullname": "t2_tlzk7zie",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 (30B) with Ollama: Blazing Fast, but accuracy concerns",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfl6bo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Ollama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with Qwen3:30b-a3b-instruct-2507-q8_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I&amp;#39;m really impressed with the speed!  &lt;/p&gt;\n\n&lt;p&gt;In light conversation (I tested with general knowledge questions and everyday scenarios), I&amp;#39;m achieving up to 34 tokens/s, which is *significantly* faster than other models I&amp;#39;ve tested (all Q4 except for qwen3):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 (30B): ~34 tokens/s&lt;/li&gt;\n&lt;li&gt;Qwen2.5 (32B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Gemma3 (27B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Llama3 (70B): 4-5 tokens/s&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, I&amp;#39;m also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.&lt;/p&gt;\n\n&lt;p&gt;My first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What&amp;#39;s your experience been like with speed and accuracy of Qwen3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Ollama",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfl6bo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gerhardmpl",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754122088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_xq83l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta - new openai open source model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfda7s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=e6bc83d22983565c851331294c37a95f480af7fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754095791,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/openrouter/horizon-beta",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?auto=webp&amp;s=76311bc0d854d91946fad4dd34c15d2aabd68203",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f17bb8ad3532cb9e5aee2735555aab1785143fb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fde130f24de4941de6382c0b47920888676cb02",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e2b3b64a4ebeaaef6a2c94effc7c44b3d4bf9e5",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d1e79479fdaa990ea889c0b392a6ab4a884ffc4",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5298d732c100973951f754378b36e657d827055b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7a4a4538b90aad59b31dde3f164c87c0a08175",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfda7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "popsumbong",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/",
          "stickied": false,
          "url": "https://openrouter.ai/openrouter/horizon-beta",
          "subreddit_subscribers": 509052,
          "created_utc": 1754095791,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?\n\nClean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.\n\nSo you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, *Can-Ai-Code*, twice.\n\nFine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you've just unlocked new nightmares: Did you accidentally make your \"hard\" tests easier than your \"easy\" ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems \"make sense\"?\n\nYou solve that with clever statistical rigor, only to discover configuration explosion hell. You'd like to test different prompting templates and sampling parameters, but that's 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.\n\nYou're now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?\n\nThat's when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that's only 20% above random chance. Your \"75% accurate\" multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different \"guess rates\"?\n\nFinally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That's 80K tokens wasted for one data point but with no useful answers.  You're overflowing your KV caches while the confidence intervals laugh at you.\n\nAfter drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.\n\n# ReasonScape treats language models as information processing systems, not text completion black boxes.\n\nIt generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.\n\n[C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.](https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1)\n\nThe initial C2 dataset represents \\~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you're on a PC - this application has too much going on to be mobile friendly!\n\n[C2 Explorer](https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d)\n\nI built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn't just another benchmark. It's a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.\n\n[C2 Leaderboard \\(Static snapshot - the Interactive is much nicer!\\)](https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d)\n\nThe ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have **8 additional tasks** to bring up, and lots more reasoning models I'd like to evaluate but my 2xRTX3090 only have so much to give.\n\nThanks for reading this far! &lt;3\n\nLinks:\n\n* [ReasonScape Homepage](https://reasonscape.com/)\n* [ReasonScape Leaderboard - C2](https://reasonscape.com/c2/leaderboard)\n* [ReasonScape Explorer - C2](https://reasonscape.com/c2/explorer) (note: PC required, not mobile-friendly)\n* [ReasonScape GitHub](https://github.com/the-crypt-keeper/reasonscape)\n* [ReasonScape System Architecture](https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture)",
          "author_fullname": "t2_30i1a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ahuh87m4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9b289044767660d3c8e2d034ccd1c1b1902f538"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e8a1fc6a4649edbc5bfde934cedfd13c3fce90c"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c9c7d1e879f15dbfdea396948cc5826d3a32b67"
                },
                {
                  "y": 372,
                  "x": 640,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4a45011cfb5e75257e82a1018ba38ca3849d833"
                },
                {
                  "y": 558,
                  "x": 960,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3046d2f091a49910ecb192cd251d2fa91bbf04a"
                },
                {
                  "y": 628,
                  "x": 1080,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=053460c5647cc12262626da5af12623413fee0ff"
                }
              ],
              "s": {
                "y": 717,
                "x": 1233,
                "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d"
              },
              "id": "4ahuh87m4ggf1"
            },
            "rn7r2k3t4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d505276a5f0dc04e59903c6abc89821d0f8f99b"
                },
                {
                  "y": 146,
                  "x": 216,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d747b1a0f8fd1a1097161d95e21ccf0503560deb"
                },
                {
                  "y": 216,
                  "x": 320,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9a6dd29394e5fb1992ae084b12e8749a9d84824"
                },
                {
                  "y": 433,
                  "x": 640,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=720bb40aec42b94acce8d40bc234aa44e5b4c208"
                },
                {
                  "y": 650,
                  "x": 960,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e355ab07ff4b2410135911c2070a7cb42b1b8221"
                },
                {
                  "y": 732,
                  "x": 1080,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=668705727d06f0b0d4c5c5867747868786f17635"
                }
              ],
              "s": {
                "y": 812,
                "x": 1198,
                "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d"
              },
              "id": "rn7r2k3t4ggf1"
            },
            "vsoidu4e4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=871d7d827171f900b2598bcca602b93414c369f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a4c54e76149d09d301df5cbd0f5c388e82bb54d"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=317f1298cb62e84fc0ac688a8a4b2143d1d2fc8a"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=769ba43ab175981d7853c0c0620df46e4f20be04"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e16a01628210ccf57e6c94fa2d7d58110870a99"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49835bd796714f4bcb033b2b80c71bc4b6e37e82"
                }
              ],
              "s": {
                "y": 3150,
                "x": 1280,
                "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1"
              },
              "id": "vsoidu4e4ggf1"
            }
          },
          "name": "t3_1mf3nw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#c7b594",
          "subreddit_type": "public",
          "ups": 146,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 146,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-4v8QT3_SA4NBTuWsWHLP1NxBvsUSLBCUXILi1-L8H8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754071528,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?&lt;/p&gt;\n\n&lt;p&gt;Clean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.&lt;/p&gt;\n\n&lt;p&gt;So you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, &lt;em&gt;Can-Ai-Code&lt;/em&gt;, twice.&lt;/p&gt;\n\n&lt;p&gt;Fine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you&amp;#39;ve just unlocked new nightmares: Did you accidentally make your &amp;quot;hard&amp;quot; tests easier than your &amp;quot;easy&amp;quot; ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems &amp;quot;make sense&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;You solve that with clever statistical rigor, only to discover configuration explosion hell. You&amp;#39;d like to test different prompting templates and sampling parameters, but that&amp;#39;s 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that&amp;#39;s only 20% above random chance. Your &amp;quot;75% accurate&amp;quot; multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different &amp;quot;guess rates&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;Finally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That&amp;#39;s 80K tokens wasted for one data point but with no useful answers.  You&amp;#39;re overflowing your KV caches while the confidence intervals laugh at you.&lt;/p&gt;\n\n&lt;p&gt;After drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.&lt;/p&gt;\n\n&lt;h1&gt;ReasonScape treats language models as information processing systems, not text completion black boxes.&lt;/h1&gt;\n\n&lt;p&gt;It generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d29809860b081384d998a428bc75faeba16cedc1\"&gt;C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The initial C2 dataset represents ~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you&amp;#39;re on a PC - this application has too much going on to be mobile friendly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d\"&gt;C2 Explorer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn&amp;#39;t just another benchmark. It&amp;#39;s a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d\"&gt;C2 Leaderboard (Static snapshot - the Interactive is much nicer!)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have &lt;strong&gt;8 additional tasks&lt;/strong&gt; to bring up, and lots more reasoning models I&amp;#39;d like to evaluate but my 2xRTX3090 only have so much to give.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading this far! &amp;lt;3&lt;/p&gt;\n\n&lt;p&gt;Links:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/\"&gt;ReasonScape Homepage&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/leaderboard\"&gt;ReasonScape Leaderboard - C2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/explorer\"&gt;ReasonScape Explorer - C2&lt;/a&gt; (note: PC required, not mobile-friendly)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape\"&gt;ReasonScape GitHub&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture\"&gt;ReasonScape System Architecture&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf3nw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kryptkpr",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754071528,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dphk4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 is the top open weights model on lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0qlf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 187,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 187,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754064880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/lmarena_ai/status/1951308670375174457",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf0qlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tarruda",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/",
          "stickied": false,
          "url": "https://x.com/lmarena_ai/status/1951308670375174457",
          "subreddit_subscribers": 509052,
          "created_utc": 1754064880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nWhen our radiology department rejected another batch of low-resolution X-rays because they couldn't see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.\n\nOn that day in the hospital basement, I decided somehow to fix this problem that's now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.\n\nThe Problem I Was Really Solving\n\nAs a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.\n\nWhen my friend completed research on X-ray super resolution using [GANs](https://www.youtube.com/watch?v=BVWn8JfeekY), I saw an opportunity to turn this research paper into a practical solution.\n\nThe existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.\n\nThe breaking point wasn't just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.\n\nThe Technical Breakthrough\n\nI built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.\n\nThe Core Technology (from the research):\n\nPatch-based processing that preserves fine-grained structural details\n\nGenerative adversarial network trained on 3,000 clinical X-ray images\n\nMoving average filters with random kernel sizes (1-40 pixels) for robustness\n\nFour-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses\n\nWhat I Built Around It:\n\nSimple drag-and-drop web interface for uploading X-ray DICOM files\n\nCloud processing pipeline using AWS GPU instances\n\nHIPAA-compliant storage and transmission\n\nIntegration with existing PACS (Picture Archiving and Communication Systems)\n\nThe first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.\n\nThe Growth Numbers (Early Traction)\n\nIn first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)\n\nCurrent Unit Economics:\n\nMRR: $1,450\n\nCloud compute costs: $520/month (AWS GPU instances)\n\nInfrastructure &amp; compliance: $180/month\n\nGross profit: $750/month (52% margin)\n\nAverage customer acquisition cost: $95 (mostly referrals and medical imaging forums).\n\nWhat I actually understood is\n\nBuilding on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.\n\nMedical software is about trust and usability. Radiologists don't want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.\n\nEarly traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.\n\nHospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.\n\nTechnical Reality\n\nUnlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.\n\nThe discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.\n\nWe validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.\n\nThe Real Lesson\n\nThis business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.\n\nThe opportunity wasn't in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.",
          "author_fullname": "t2_1tts756whd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I Built Medical AI by Solving the Radiation Dose Problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzu3d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754164960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When our radiology department rejected another batch of low-resolution X-rays because they couldn&amp;#39;t see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.&lt;/p&gt;\n\n&lt;p&gt;On that day in the hospital basement, I decided somehow to fix this problem that&amp;#39;s now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.&lt;/p&gt;\n\n&lt;p&gt;The Problem I Was Really Solving&lt;/p&gt;\n\n&lt;p&gt;As a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.&lt;/p&gt;\n\n&lt;p&gt;When my friend completed research on X-ray super resolution using &lt;a href=\"https://www.youtube.com/watch?v=BVWn8JfeekY\"&gt;GANs&lt;/a&gt;, I saw an opportunity to turn this research paper into a practical solution.&lt;/p&gt;\n\n&lt;p&gt;The existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.&lt;/p&gt;\n\n&lt;p&gt;The breaking point wasn&amp;#39;t just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The Technical Breakthrough&lt;/p&gt;\n\n&lt;p&gt;I built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.&lt;/p&gt;\n\n&lt;p&gt;The Core Technology (from the research):&lt;/p&gt;\n\n&lt;p&gt;Patch-based processing that preserves fine-grained structural details&lt;/p&gt;\n\n&lt;p&gt;Generative adversarial network trained on 3,000 clinical X-ray images&lt;/p&gt;\n\n&lt;p&gt;Moving average filters with random kernel sizes (1-40 pixels) for robustness&lt;/p&gt;\n\n&lt;p&gt;Four-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses&lt;/p&gt;\n\n&lt;p&gt;What I Built Around It:&lt;/p&gt;\n\n&lt;p&gt;Simple drag-and-drop web interface for uploading X-ray DICOM files&lt;/p&gt;\n\n&lt;p&gt;Cloud processing pipeline using AWS GPU instances&lt;/p&gt;\n\n&lt;p&gt;HIPAA-compliant storage and transmission&lt;/p&gt;\n\n&lt;p&gt;Integration with existing PACS (Picture Archiving and Communication Systems)&lt;/p&gt;\n\n&lt;p&gt;The first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.&lt;/p&gt;\n\n&lt;p&gt;The Growth Numbers (Early Traction)&lt;/p&gt;\n\n&lt;p&gt;In first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)&lt;/p&gt;\n\n&lt;p&gt;Current Unit Economics:&lt;/p&gt;\n\n&lt;p&gt;MRR: $1,450&lt;/p&gt;\n\n&lt;p&gt;Cloud compute costs: $520/month (AWS GPU instances)&lt;/p&gt;\n\n&lt;p&gt;Infrastructure &amp;amp; compliance: $180/month&lt;/p&gt;\n\n&lt;p&gt;Gross profit: $750/month (52% margin)&lt;/p&gt;\n\n&lt;p&gt;Average customer acquisition cost: $95 (mostly referrals and medical imaging forums).&lt;/p&gt;\n\n&lt;p&gt;What I actually understood is&lt;/p&gt;\n\n&lt;p&gt;Building on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.&lt;/p&gt;\n\n&lt;p&gt;Medical software is about trust and usability. Radiologists don&amp;#39;t want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.&lt;/p&gt;\n\n&lt;p&gt;Early traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.&lt;/p&gt;\n\n&lt;p&gt;Hospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.&lt;/p&gt;\n\n&lt;p&gt;Technical Reality&lt;/p&gt;\n\n&lt;p&gt;Unlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.&lt;/p&gt;\n\n&lt;p&gt;The discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.&lt;/p&gt;\n\n&lt;p&gt;We validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.&lt;/p&gt;\n\n&lt;p&gt;The Real Lesson&lt;/p&gt;\n\n&lt;p&gt;This business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The opportunity wasn&amp;#39;t in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?auto=webp&amp;s=ddcf97793b8059e1ac6f8e5f42c0fb141ac1f06b",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecfe2ffa2015cbfbedc8dfb6cbcd4cf0d32f0bee",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d64b11c760cf9d689a103b871b38eb77933c0ed",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=085b7fe67421b9ba136cbbcbf7988771acb578f2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfzu3d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Perception-9919",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754164960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.\n\nI also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it's on the leaderboard. Glm was also much better at tool calls, it basically never failed.",
          "author_fullname": "t2_58t8ty6v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is bad at tool call while glm-4.5 is surprisingly good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8la7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 63,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 63,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754083147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.&lt;/p&gt;\n\n&lt;p&gt;I also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it&amp;#39;s on the leaderboard. Glm was also much better at tool calls, it basically never failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf8la7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BoJackHorseMan53",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754083147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it’s just my system",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox tts on amd",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfz1k2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754162909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it’s just my system&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfz1k2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754162909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, it’s been a while but I’m happy to announce another major update for my app **EasyWhisperUI**, now with **live transcriptions**!\n\nIt features full cross-platform GPU acceleration:\n\n* **Vulkan** on Windows (Intel, AMD, or NVIDIA)\n* **Metal** on macOS (Apple silicon)\n\n**New features!**\n\n1. **GPU-accelerated Live Transcriptions** • Transcribe speech in real time using your default mic (user request)\n2. **Output Cleanup** • Automatically removes repeated segments from live transcriptions\n3. **Open in Notepad Checkbox** • New option to disable automatic opening in Notepad after transcription (user request)\n4. Various bug fixes and code improvements.\n\n**Other key features**\n\n5. **Batch File Processing** • Drag &amp; drop multiple files — EasyWhisperUI will queue and transcribe them automatically (user request)  \n6. **CPU-Only Toggle** • Option to disable GPU acceleration and run fully on CPU (user request)  \n7. **Modern UI** • Acrylic background on Windows, clean layout and spacing improvements  \n8. **macOS Support** • EasyWhisperUI works on macOS thanks to a community contribution  \n9. **Installer Included** • Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click\n\nThere are a lot more features — check out the GitHub for more info:\n\n🔗 **GitHub:** [https://github.com/mehtabmahir/easy-whisper-ui](https://github.com/mehtabmahir/easy-whisper-ui)\n\nLet me know what you think or if you have any suggestions!",
          "author_fullname": "t2_epvxz7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EasyWhisperUI – GPU accelerated Open Source Whisper UI for Windows &amp; macOS now with Live Transcriptions!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffjjj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754103866,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754102625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, it’s been a while but I’m happy to announce another major update for my app &lt;strong&gt;EasyWhisperUI&lt;/strong&gt;, now with &lt;strong&gt;live transcriptions&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;It features full cross-platform GPU acceleration:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; on Windows (Intel, AMD, or NVIDIA)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Metal&lt;/strong&gt; on macOS (Apple silicon)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;New features!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;GPU-accelerated Live Transcriptions&lt;/strong&gt; • Transcribe speech in real time using your default mic (user request)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output Cleanup&lt;/strong&gt; • Automatically removes repeated segments from live transcriptions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Open in Notepad Checkbox&lt;/strong&gt; • New option to disable automatic opening in Notepad after transcription (user request)&lt;/li&gt;\n&lt;li&gt;Various bug fixes and code improvements.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Other key features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Batch File Processing&lt;/strong&gt; • Drag &amp;amp; drop multiple files — EasyWhisperUI will queue and transcribe them automatically (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU-Only Toggle&lt;/strong&gt; • Option to disable GPU acceleration and run fully on CPU (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modern UI&lt;/strong&gt; • Acrylic background on Windows, clean layout and spacing improvements&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;macOS Support&lt;/strong&gt; • EasyWhisperUI works on macOS thanks to a community contribution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Installer Included&lt;/strong&gt; • Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are a lot more features — check out the GitHub for more info:&lt;/p&gt;\n\n&lt;p&gt;🔗 &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/mehtabmahir/easy-whisper-ui\"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think or if you have any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?auto=webp&amp;s=8b1a4fb535e00a6da0368011b0e6cec45497c5d7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5822df3220f343ebd2934400d0f59847247da8d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=823cd6c140e6f8b93ef72f68e9d0b425f56243f3",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e4b350fed949d338ba9708cf08854a1b4237e89",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccc578f650ebc7544f2e8cfe89996ac4b84f501f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d4fb11dd5292586b4d7444d6d35ffe279e98ba1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2237e0f4ab1ae9e5ec2baa35b902b491ffb5d35",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mffjjj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehtabmahir",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754102625,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are local llms fast and powerful enough to do analysis on movies in real time?\n\nSay you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?\n\nIf not today, then when will it be possible to do that?",
          "author_fullname": "t2_26u5g058",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LocalLLM for movies",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy924",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are local llms fast and powerful enough to do analysis on movies in real time?&lt;/p&gt;\n\n&lt;p&gt;Say you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?&lt;/p&gt;\n\n&lt;p&gt;If not today, then when will it be possible to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfy924",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImaginaryRea1ity",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754160893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Je vais être franc :**\n\nJe souffre de multiples pathologies (cinq maladies chroniques dont une auto-immune persistante et sévère). Les deux seuls modèles qui m'ont aidé avec succès à trouver des pistes d'amélioration sont Deepseek R1 et Perplexity Pro. Personnellement, lors de mes interactions avec Horizon Beta, je me suis senti à la fois mal à l'aise et plein d'espoir en raison de l'impression qu'une sorte de pouvoir caché ou restreint se cache derrière cela, car certaines des réponses étaient tout simplement stupéfiantes.\n\nAvant de continuer, je ne suis pas venu ici pour jouer à la victime, et je n'ai besoin d'aucune empathie, merci.\n\nBref, j'ai une liste de pistes spécifiques à ma situation que j'ai élaborées après plusieurs tentatives, car je ne dispose que d'une fenêtre de temps quotidienne qui me permet d'avoir un peu d'énergie et de clarté d'esprit pour, dans un premier temps, comprendre comment organiser mentalement mes besoins, les exprimer, et enfin les structurer et les prioriser.\n\n**Voici 5 de mes modèles si cela peut aider quelqu'un :**\n\n&gt;[ https://pastebin.com/KXenEf7Q ](https://pastebin.com/KXenEf7Q)\n\n**Prudence !**\n\n&gt;*Je ne fournirai aucun commentaire ou information supplémentaire sur ma santé, mes traitements, etc.*\n\n&gt;*N'essayez pas l'automédication, parlez à votre cheval, bla bla, ne faites pas l'imbécile sans avis médical HUMAIN !*\n\n**Retour au sujet :**\n\nCe qui m’a pris environ deux mois pour avoir un semblant d’espoir thérapeutique semble désormais ridicule par rapport aux résultats d’Horizon Beta. J'utilise principalement TTS pour éviter de lire les réponses des modèles, \"surtout les longues\", parce que parfois c'est épuisant et putain d'ennuyeux, alors j'écoute ! Et quand on a l'habitude d'écouter les réponses du LLM, il y a des schémas qui ne m'échappent pas.\n\n**Deux possibilités :**\n\n\\*C'est pur OpenAI\n\nou\n\n* C'est un modèle distillé GptXYZ\n\n**Ce qui m'amène à confirmer mon ressenti pour la première possibilité :**\n\nLa théorie selon laquelle les modèles Quasar et Horizon sur OpenRouter sont de mystérieux prototypes OpenAI est basée sur plusieurs indices et spéculations.\n\nQuand j'ai découvert Quasar-Alpha pour la première fois, en avril 2025 sur OpenRouter, il était décrit comme un modèle préliminaire avec une fenêtre contextuelle de 1 million de tokens, optimisé pour l'encodage, et des capacités multimodales, bla bla...\n\nRien d'excitant pour moi à l'époque.\n\nEnsuite, il y avait des \"preuves\" suggérant une connexion à OpenAI : le style de sortie utilise des caractères Unicode spécifiques pour la ponctuation, un trait observé dans les modèles OpenAI  :\n\n[ https://news.ycombinator.com/item?id=43640166 ](https://news.ycombinator.com/item?id=43640166)\n\nD'autres spéculent sur une origine Google en raison du nom « Quasar » évoquant des objets cosmiques, semblables aux Gémeaux.\n\nEnsuite Horizon Alpha et Beta, apparus plus récemment, sont vus comme une suite potentielle.\n\nCertaines spéculations le lient directement à OpenAI, peut-être en tant que version open source ou précurseur de GPT-5, en raison de benchmarks dominants et d'un style de sortie rappelant OpenAI:\n\n[ https://ossels.ai/openai-horizon-alpha-gpt-5-model/ ](https://ossels.ai/openai-horizon-alpha-gpt-5-model/)\n\nLes utilisateurs ont signalé qu'il code des applications entières en une seule requête, renforçant ainsi l'idée d'un prototype avancé.\n\n**J'ai essayé moi-même avec cette invite et il m'a fallu 5 échanges pour obtenir un résultat correct :**\n\nhttps://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;format=png&amp;auto=webp&amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d\n\n**Voici l'invite Neo Tetris HTML :**\n\n    Créez un fichier HTML autonome qui fournit un Tetris néo-rétro complet avec les spécifications ci-dessous. Pas de bibliothèques externes, pas de liens CDN : intégrez tout (polices, CSS, JS, sons le cas échéant) via la base 64 ou le code en ligne.\n    Gameplay de base • Classique 7 tétrominos, rotation SRS, pièce de maintien, aperçu de la pièce fantôme. • Suppression de lignes avec explosion de particules : lorsqu'une ligne est terminée, générez une explosion de particules lumineuses qui volent vers l'extérieur et s'estompent. • « Onde de choc d'impact » à chaque verrouillage : un anneau de néon concentrique qui s'étend à partir de la base de la pièce et s'estompe. • Score, progression de niveau, 3 vies (vies perdues lorsque les blocs atteignent le sommet), file d'attente des 3 pièces suivantes.\n    Contrôles • Bureau Flèches gauche/droite : déplacer Flèche vers le bas : soft-drop (accélérer) – afficher la trace de mouvement / la séquence de vitesse derrière la pièce qui tombe Flèche vers le haut OU Espace : tourner dans le sens des aiguilles d'une montre Entrée : démarrer le jeu / confirmer dans les menus • Smartphone (toucher) Glisser vers la gauche/droite : déplacer Glisser vers le bas : soft-drop (avec la même trace) Appuyez sur les moitiés gauche/droite de l'écran : faire pivoter • Mettre en pause la superposition sur « P ».\n    Visuels et animations • Effet d'écran CRT : superposez un cadre en verre incurvé subtil, des lignes de balayage, une légère distorsion RVB et une vignette qui reste pendant la lecture. • Palette néon harmonieuse sur fond indigo profond (cyan, magenta, violet électrique, citron vert). • Écran titre avant le jeu : – Texte centré \"PRESS ENTER\" clignotant avec lueur. – Derrière lui, un défilé sans fin de chaque tétromino tournant lentement en mode filaire 3D, avec des particules de filament traçant leurs bords et s'éloignant. – Petit bloc affichant des astuces de contrôle : Gauche/Droite – Déplacer | Vers le bas – Chute de vitesse | Haut/Espace – Rotation | Entrée – Démarrer • Tableau de scores persistant (top 5) stocké dans localStorage, affiché sur l'écran titre et l'écran de fin de partie.\n    Adaptatif • Disposition fluide : le plateau de jeu s'adapte automatiquement aux fenêtres d'affichage des ordinateurs de bureau et mobiles tout en préservant les proportions. • Zones tactiles dimensionnées pour les pouces.\n    Technique • Rendu Canvas 2D, rendu d'image : pixellisé pour des blocs nets. • Toutes les images clés, CSS et JavaScript dans les balises &lt;style&gt; et &lt;script&gt;. • Aucune demande de réseau externe.\n    Important : les pièces ne doivent pas tomber trop rapidement. Commencez lentement et augmentez progressivement la vitesse à mesure que les jalons de score sont atteints, avec un niveau maximum de 100.\n    Renvoie uniquement le code HTML complet.\n\n**Spéculons :**\n\nQuant à la spéculation selon laquelle le prochain modèle ou étape serait la « singularité », elle trouve son origine dans les déclarations de Sam Altman, qui a publié un article en juin 2025 intitulé « La douce singularité » ( [ https://blog.samaltman.com/the-gentle-singularity ](https://blog.samaltman.com/the-gentle-singularity) ), affirmant que l'humanité a dépassé l'horizon des événements vers la superintelligence, avec des agents d'IA capables d'un véritable travail cognitif comme dès 2025 et des idées innovantes en 2026.\n\nAltman décrit un processus d'auto-amélioration récursive, dans lequel l'IA assiste la recherche sur l'IA, conduisant à une abondance d'intelligence et d'énergie :\n\n[ https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/ ](https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/)\n\nLes prédictions des experts, basées sur 8 590 estimations, placent l’AGI vers 2060 avec une probabilité de 50 %, mais des voix comme Ray Kurzweil visent 2029.\n\nMais OpenAI a également résolu les problèmes de niveau or de l'OMI avec un modèle expérimental général, soutenant l'idée d'une progression rapide vers la singularité :\n\n[ https://www.reddit.com/r/singularity/comments/1m3qutl/openai\\_achieved\\_imo\\_gold\\_with\\_experimental/ ](https://www.reddit.com/r/singularity/comments/1m3qutl/openai_achieved_imo_gold_with_experimental/)",
          "author_fullname": "t2_ti5m9mpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quasar, Horizon, \"Singularity?\", Diseases &amp; Theory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "ywuejlr1apgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=301aefc74b41ee6ff2109d4b74c5bb037e7776c4"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5700d44e832bceef8fecc9f79c9b6d121798cc4"
                },
                {
                  "y": 177,
                  "x": 320,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1dceb4e138080ddbb94a9f5797b1ebd4e95a2086"
                },
                {
                  "y": 354,
                  "x": 640,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdea74a602d24154843aa58afeb7d7b7c05d0fbf"
                },
                {
                  "y": 532,
                  "x": 960,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7dfcbd21e17c90eb183099ce1e76638f537f89ec"
                },
                {
                  "y": 598,
                  "x": 1080,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=220ec113997c299dcc44858c06358f79cbf8c4b5"
                }
              ],
              "s": {
                "y": 900,
                "x": 1624,
                "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;format=png&amp;auto=webp&amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d"
              },
              "id": "ywuejlr1apgf1"
            }
          },
          "name": "t3_1mg5zcx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1VZ-7dUfibZEb-GUiH0w3fY_1ECf6GJPMzUsVAS0HXc.jpg",
          "edited": 1754182043,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Je vais être franc :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Je souffre de multiples pathologies (cinq maladies chroniques dont une auto-immune persistante et sévère). Les deux seuls modèles qui m&amp;#39;ont aidé avec succès à trouver des pistes d&amp;#39;amélioration sont Deepseek R1 et Perplexity Pro. Personnellement, lors de mes interactions avec Horizon Beta, je me suis senti à la fois mal à l&amp;#39;aise et plein d&amp;#39;espoir en raison de l&amp;#39;impression qu&amp;#39;une sorte de pouvoir caché ou restreint se cache derrière cela, car certaines des réponses étaient tout simplement stupéfiantes.&lt;/p&gt;\n\n&lt;p&gt;Avant de continuer, je ne suis pas venu ici pour jouer à la victime, et je n&amp;#39;ai besoin d&amp;#39;aucune empathie, merci.&lt;/p&gt;\n\n&lt;p&gt;Bref, j&amp;#39;ai une liste de pistes spécifiques à ma situation que j&amp;#39;ai élaborées après plusieurs tentatives, car je ne dispose que d&amp;#39;une fenêtre de temps quotidienne qui me permet d&amp;#39;avoir un peu d&amp;#39;énergie et de clarté d&amp;#39;esprit pour, dans un premier temps, comprendre comment organiser mentalement mes besoins, les exprimer, et enfin les structurer et les prioriser.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Voici 5 de mes modèles si cela peut aider quelqu&amp;#39;un :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://pastebin.com/KXenEf7Q\"&gt; https://pastebin.com/KXenEf7Q &lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Prudence !&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Je ne fournirai aucun commentaire ou information supplémentaire sur ma santé, mes traitements, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;N&amp;#39;essayez pas l&amp;#39;automédication, parlez à votre cheval, bla bla, ne faites pas l&amp;#39;imbécile sans avis médical HUMAIN !&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Retour au sujet :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Ce qui m’a pris environ deux mois pour avoir un semblant d’espoir thérapeutique semble désormais ridicule par rapport aux résultats d’Horizon Beta. J&amp;#39;utilise principalement TTS pour éviter de lire les réponses des modèles, &amp;quot;surtout les longues&amp;quot;, parce que parfois c&amp;#39;est épuisant et putain d&amp;#39;ennuyeux, alors j&amp;#39;écoute ! Et quand on a l&amp;#39;habitude d&amp;#39;écouter les réponses du LLM, il y a des schémas qui ne m&amp;#39;échappent pas.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Deux possibilités :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;*C&amp;#39;est pur OpenAI&lt;/p&gt;\n\n&lt;p&gt;ou&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;C&amp;#39;est un modèle distillé GptXYZ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Ce qui m&amp;#39;amène à confirmer mon ressenti pour la première possibilité :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;La théorie selon laquelle les modèles Quasar et Horizon sur OpenRouter sont de mystérieux prototypes OpenAI est basée sur plusieurs indices et spéculations.&lt;/p&gt;\n\n&lt;p&gt;Quand j&amp;#39;ai découvert Quasar-Alpha pour la première fois, en avril 2025 sur OpenRouter, il était décrit comme un modèle préliminaire avec une fenêtre contextuelle de 1 million de tokens, optimisé pour l&amp;#39;encodage, et des capacités multimodales, bla bla...&lt;/p&gt;\n\n&lt;p&gt;Rien d&amp;#39;excitant pour moi à l&amp;#39;époque.&lt;/p&gt;\n\n&lt;p&gt;Ensuite, il y avait des &amp;quot;preuves&amp;quot; suggérant une connexion à OpenAI : le style de sortie utilise des caractères Unicode spécifiques pour la ponctuation, un trait observé dans les modèles OpenAI  :&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://news.ycombinator.com/item?id=43640166\"&gt; https://news.ycombinator.com/item?id=43640166 &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;D&amp;#39;autres spéculent sur une origine Google en raison du nom « Quasar » évoquant des objets cosmiques, semblables aux Gémeaux.&lt;/p&gt;\n\n&lt;p&gt;Ensuite Horizon Alpha et Beta, apparus plus récemment, sont vus comme une suite potentielle.&lt;/p&gt;\n\n&lt;p&gt;Certaines spéculations le lient directement à OpenAI, peut-être en tant que version open source ou précurseur de GPT-5, en raison de benchmarks dominants et d&amp;#39;un style de sortie rappelant OpenAI:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ossels.ai/openai-horizon-alpha-gpt-5-model/\"&gt; https://ossels.ai/openai-horizon-alpha-gpt-5-model/ &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Les utilisateurs ont signalé qu&amp;#39;il code des applications entières en une seule requête, renforçant ainsi l&amp;#39;idée d&amp;#39;un prototype avancé.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;J&amp;#39;ai essayé moi-même avec cette invite et il m&amp;#39;a fallu 5 échanges pour obtenir un résultat correct :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d\"&gt;https://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Voici l&amp;#39;invite Neo Tetris HTML :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Créez un fichier HTML autonome qui fournit un Tetris néo-rétro complet avec les spécifications ci-dessous. Pas de bibliothèques externes, pas de liens CDN : intégrez tout (polices, CSS, JS, sons le cas échéant) via la base 64 ou le code en ligne.\nGameplay de base • Classique 7 tétrominos, rotation SRS, pièce de maintien, aperçu de la pièce fantôme. • Suppression de lignes avec explosion de particules : lorsqu&amp;#39;une ligne est terminée, générez une explosion de particules lumineuses qui volent vers l&amp;#39;extérieur et s&amp;#39;estompent. • « Onde de choc d&amp;#39;impact » à chaque verrouillage : un anneau de néon concentrique qui s&amp;#39;étend à partir de la base de la pièce et s&amp;#39;estompe. • Score, progression de niveau, 3 vies (vies perdues lorsque les blocs atteignent le sommet), file d&amp;#39;attente des 3 pièces suivantes.\nContrôles • Bureau Flèches gauche/droite : déplacer Flèche vers le bas : soft-drop (accélérer) – afficher la trace de mouvement / la séquence de vitesse derrière la pièce qui tombe Flèche vers le haut OU Espace : tourner dans le sens des aiguilles d&amp;#39;une montre Entrée : démarrer le jeu / confirmer dans les menus • Smartphone (toucher) Glisser vers la gauche/droite : déplacer Glisser vers le bas : soft-drop (avec la même trace) Appuyez sur les moitiés gauche/droite de l&amp;#39;écran : faire pivoter • Mettre en pause la superposition sur « P ».\nVisuels et animations • Effet d&amp;#39;écran CRT : superposez un cadre en verre incurvé subtil, des lignes de balayage, une légère distorsion RVB et une vignette qui reste pendant la lecture. • Palette néon harmonieuse sur fond indigo profond (cyan, magenta, violet électrique, citron vert). • Écran titre avant le jeu : – Texte centré &amp;quot;PRESS ENTER&amp;quot; clignotant avec lueur. – Derrière lui, un défilé sans fin de chaque tétromino tournant lentement en mode filaire 3D, avec des particules de filament traçant leurs bords et s&amp;#39;éloignant. – Petit bloc affichant des astuces de contrôle : Gauche/Droite – Déplacer | Vers le bas – Chute de vitesse | Haut/Espace – Rotation | Entrée – Démarrer • Tableau de scores persistant (top 5) stocké dans localStorage, affiché sur l&amp;#39;écran titre et l&amp;#39;écran de fin de partie.\nAdaptatif • Disposition fluide : le plateau de jeu s&amp;#39;adapte automatiquement aux fenêtres d&amp;#39;affichage des ordinateurs de bureau et mobiles tout en préservant les proportions. • Zones tactiles dimensionnées pour les pouces.\nTechnique • Rendu Canvas 2D, rendu d&amp;#39;image : pixellisé pour des blocs nets. • Toutes les images clés, CSS et JavaScript dans les balises &amp;lt;style&amp;gt; et &amp;lt;script&amp;gt;. • Aucune demande de réseau externe.\nImportant : les pièces ne doivent pas tomber trop rapidement. Commencez lentement et augmentez progressivement la vitesse à mesure que les jalons de score sont atteints, avec un niveau maximum de 100.\nRenvoie uniquement le code HTML complet.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Spéculons :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Quant à la spéculation selon laquelle le prochain modèle ou étape serait la « singularité », elle trouve son origine dans les déclarations de Sam Altman, qui a publié un article en juin 2025 intitulé « La douce singularité » ( &lt;a href=\"https://blog.samaltman.com/the-gentle-singularity\"&gt; https://blog.samaltman.com/the-gentle-singularity &lt;/a&gt; ), affirmant que l&amp;#39;humanité a dépassé l&amp;#39;horizon des événements vers la superintelligence, avec des agents d&amp;#39;IA capables d&amp;#39;un véritable travail cognitif comme dès 2025 et des idées innovantes en 2026.&lt;/p&gt;\n\n&lt;p&gt;Altman décrit un processus d&amp;#39;auto-amélioration récursive, dans lequel l&amp;#39;IA assiste la recherche sur l&amp;#39;IA, conduisant à une abondance d&amp;#39;intelligence et d&amp;#39;énergie :&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/\"&gt; https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/ &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Les prédictions des experts, basées sur 8 590 estimations, placent l’AGI vers 2060 avec une probabilité de 50 %, mais des voix comme Ray Kurzweil visent 2029.&lt;/p&gt;\n\n&lt;p&gt;Mais OpenAI a également résolu les problèmes de niveau or de l&amp;#39;OMI avec un modèle expérimental général, soutenant l&amp;#39;idée d&amp;#39;une progression rapide vers la singularité :&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/singularity/comments/1m3qutl/openai_achieved_imo_gold_with_experimental/\"&gt; https://www.reddit.com/r/singularity/comments/1m3qutl/openai_achieved_imo_gold_with_experimental/ &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5zcx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Ninja7526",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5zcx/quasar_horizon_singularity_diseases_theory/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5zcx/quasar_horizon_singularity_diseases_theory/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754181849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The person who \"leaked\" this model is from the openai (HF) organization \n\nSo as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "The OpenAI Open weight model might be 120B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9dqwiep9ucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c6a3fca8afcd98702c0b7d846a7e160d71c2ba"
                },
                {
                  "y": 257,
                  "x": 216,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daf71cd9fc68c069a95fe74761dcc142bc617a8b"
                },
                {
                  "y": 381,
                  "x": 320,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=436cbfa27d8543f73db2371953a82aac39ecda85"
                },
                {
                  "y": 763,
                  "x": 640,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8299ea44db486e3a2fbec74c3d0bd88466708866"
                },
                {
                  "y": 1144,
                  "x": 960,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ff8e80fe3dec58ee22a247bfdab51b53bcac09ca"
                },
                {
                  "y": 1288,
                  "x": 1080,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4d90e7d7d325eaa34c7621c43ce177895ff0383"
                }
              ],
              "s": {
                "y": 1288,
                "x": 1080,
                "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b3767f5800ed4a78ec866eaa707e8dbe0151f82f"
              },
              "id": "9dqwiep9ucgf1"
            },
            "w9h7ftgaucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 187,
                  "x": 108,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65c73f27d3ec2b6268ed676ac73a20d96cc01864"
                },
                {
                  "y": 375,
                  "x": 216,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7776f97e1b1b9b42cd96de35d6b38c2bdf1948d"
                },
                {
                  "y": 555,
                  "x": 320,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17560f484f4fc6684187f9b4e3214d2bb480250d"
                },
                {
                  "y": 1111,
                  "x": 640,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=64632b927b61af650b70b9b6e2e93befcc39ee18"
                },
                {
                  "y": 1667,
                  "x": 960,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bd9619e6620d4a4c7a2fbdf4077d3780cf0b105"
                }
              ],
              "s": {
                "y": 1873,
                "x": 1078,
                "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=1078&amp;format=png&amp;auto=webp&amp;s=dd26649ae5edf99eedfd8378f94c7fdf3aeb6ae9"
              },
              "id": "w9h7ftgaucgf1"
            },
            "s1j5io2aucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=066bdffe86c232ba5ff6c7d20d8dde94884c7f6e"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3ab77eae4021c3e0b9af7ed66501883f4211b90"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61d00c48c6b5fb9736c6d3503205a847c8146f46"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de6f7067561af43d54b56fbf6902277437c1fa94"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=062e6b2cc5f7e404a5f33fca5b18a6e367b0f99b"
                },
                {
                  "y": 942,
                  "x": 1080,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e518857b677661c538f1b99548e5ca304d3b186"
                }
              ],
              "s": {
                "y": 942,
                "x": 1080,
                "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b86914c6db57a17c4bde148a77b9a789f692e93f"
              },
              "id": "s1j5io2aucgf1"
            }
          },
          "name": "t3_1mepeqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 712,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "9dqwiep9ucgf1",
                "id": 718647288
              },
              {
                "caption": "",
                "media_id": "s1j5io2aucgf1",
                "id": 718647289
              },
              {
                "caption": "",
                "media_id": "w9h7ftgaucgf1",
                "id": 718647290
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 712,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754030862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt;\n\n&lt;p&gt;So as expected, it&amp;#39;s not gonna be something you can easily run locally, it won&amp;#39;t hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mepeqh",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepeqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 157,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mepeqh",
          "subreddit_subscribers": 509052,
          "created_utc": 1754030862,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?\n\nI initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?\n\nBut then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?\n\nSo both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?",
          "author_fullname": "t2_kjhuip53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG or prompt engineering",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfor6n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754135853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?&lt;/p&gt;\n\n&lt;p&gt;But then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?&lt;/p&gt;\n\n&lt;p&gt;So both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfor6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SignatureHuman8057",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754135853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.\n\nI gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it\n\n[https://github.com/mmemoo/dis-cal](https://github.com/mmemoo/dis-cal)  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. \n\nThanks in advance!",
          "author_fullname": "t2_cgzigjpd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a opensource CAL-AI alternative using ollama which runs completely locally and for is fully free.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrec0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754143606,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.&lt;/p&gt;\n\n&lt;p&gt;I gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmemoo/dis-cal\"&gt;https://github.com/mmemoo/dis-cal&lt;/a&gt;  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?auto=webp&amp;s=7740e51b065f73baef20acb9661652efbd31473e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43d0c8bafbb43faea860f8f86e81107782da6aa3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8b2d47057fd20576416383a2c09e2a524b760d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b75736c0b2fa807b69315eb351d4357ec156394",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=78079915dfeceeab89fd37ed8f707b7cfbc64ce5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cabc737e4eaac8c82320118a5fb4890d5545856f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57c3313f0eff8bcc3ad6c3af276b7a5cb4a42f83",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfrec0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehmetflix_",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754143606,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna use this model for DMing a dnd game as well as using it to write stories. I’d like it to be abliterated if possible.\n\nI’ve been looking at using Gemma 3 27B, and I do like its writing style, but I’m concerned about its ability to handle long context lengths.\n\nSo far I haven’t had that problem but that’s only because I’ve been running it with low context lengths, since I’m using it on my gaming pc right now.\n\nI’m in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.\n\nThanks in advance ",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best creative writing + long context model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfifhh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna use this model for DMing a dnd game as well as using it to write stories. I’d like it to be abliterated if possible.&lt;/p&gt;\n\n&lt;p&gt;I’ve been looking at using Gemma 3 27B, and I do like its writing style, but I’m concerned about its ability to handle long context lengths.&lt;/p&gt;\n\n&lt;p&gt;So far I haven’t had that problem but that’s only because I’ve been running it with low context lengths, since I’m using it on my gaming pc right now.&lt;/p&gt;\n\n&lt;p&gt;I’m in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfifhh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754111899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi All,\n\nI'm quite new to local AI models, and started today by playing with Chatterbox TTS on my Mac Studio M4 (using the apple silicon version on Hugging Face). Also, hopefully this is the right reddit - I see other posts regarding Chatterbox here, so I guess it is!\n\nIt's actually working very nicely indeed, doing a conversion of a small piece of a book with a voice sample I provided.\n\nIt's taking a while though; \\~25 minutes to generate a 10 minute sample. The full book is likely to be 15-20 hours long, so we could be talking 50 hours for the full conversion.\n\nSo - I would like to see if there are services I might run the model on in the cloud - for example [RunPod.io](http://RunPod.io) or [Vast.ai](http://Vast.ai) are two that I have seen. But I'm not sure what the costs might end up being, and not really sure how to find out.\n\nCan anyone offer any guidance? Is it as simple as saying 50 hours x (hourly price for GPU)?\n\nThanks!",
          "author_fullname": "t2_15wm1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox TTS in cloud?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvk5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754154092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to local AI models, and started today by playing with Chatterbox TTS on my Mac Studio M4 (using the apple silicon version on Hugging Face). Also, hopefully this is the right reddit - I see other posts regarding Chatterbox here, so I guess it is!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s actually working very nicely indeed, doing a conversion of a small piece of a book with a voice sample I provided.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s taking a while though; ~25 minutes to generate a 10 minute sample. The full book is likely to be 15-20 hours long, so we could be talking 50 hours for the full conversion.&lt;/p&gt;\n\n&lt;p&gt;So - I would like to see if there are services I might run the model on in the cloud - for example &lt;a href=\"http://RunPod.io\"&gt;RunPod.io&lt;/a&gt; or &lt;a href=\"http://Vast.ai\"&gt;Vast.ai&lt;/a&gt; are two that I have seen. But I&amp;#39;m not sure what the costs might end up being, and not really sure how to find out.&lt;/p&gt;\n\n&lt;p&gt;Can anyone offer any guidance? Is it as simple as saying 50 hours x (hourly price for GPU)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                  "width": 2400,
                  "height": 1260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvk5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BrotherBrutha",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvk5h/chatterbox_tts_in_cloud/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvk5h/chatterbox_tts_in_cloud/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754154092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_48ezkeai",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI OS model info leaked - 120B &amp; 20B will be available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepz8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 479,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 479,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/koWApmUvRcfMpfhrqJD5WrepTEKpDhchruNu54mqVSQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754033016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/08m94pio0dgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?auto=webp&amp;s=6fb8f07eaa524714454f02be25da5a0c8bd501ec",
                  "width": 1052,
                  "height": 1588
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57a6c0b7f95c81ab1bf5553bdcd58df7e2e53602",
                    "width": 108,
                    "height": 163
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=846a6af2093db0113d0ac29622308c99f3409503",
                    "width": 216,
                    "height": 326
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a6414cea64697cd84845fbfa7d319abc63ee4be",
                    "width": 320,
                    "height": 483
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8e423b8c1c16726ef958bbd8725e985cc58bc68",
                    "width": 640,
                    "height": 966
                  },
                  {
                    "url": "https://preview.redd.it/08m94pio0dgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0995094fc1bd94332893e4e6b74528bc339b0123",
                    "width": 960,
                    "height": 1449
                  }
                ],
                "variants": {},
                "id": "IacMR9LsboNRW0VP4Zwz_MvZddRdCNqWrVrUgsEhFxY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepz8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShreckAndDonkey123",
          "discussion_type": null,
          "num_comments": 144,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/",
          "stickied": false,
          "url": "https://i.redd.it/08m94pio0dgf1.jpeg",
          "subreddit_subscribers": 509052,
          "created_utc": 1754033016,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to make an agent that get YouTube videos transcript but i keep having ip ban or a ban from requests to youtube-transcript-api, how to manage this?",
          "author_fullname": "t2_w2tkxer3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to avoid IP bans when using youtube-transcript-api to fetch YouTube video transcripts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfij9a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754112257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to make an agent that get YouTube videos transcript but i keep having ip ban or a ban from requests to youtube-transcript-api, how to manage this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfij9a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Anas_M1nt",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfij9a/how_to_avoid_ip_bans_when_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfij9a/how_to_avoid_ip_bans_when_using/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754112257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,  \nI'm a final-year CS student working on a project to build an **AI assistant for my university** using **RAG (Retrieval-Augmented Generation)** and possibly agentic tools down the line.\n\nThe chatbot will help students find answers to common university-related questions (like academic queries, admissions, etc.) and eventually perform light actions like form redirection, etc.\n\n# What I’m struggling with:\n\nI'm not exactly sure **what types of data** I should collect and prepare to make this assistant useful, accurate, and robust.\n\nI plan to use LangChain or LlamaIndex + a vector store, but I want to hear from folks with experience in this kind of thing:\n\n* **What kinds of data did you use for similar projects?**\n* How do you decide what to include or ignore?\n* Any tips for formatting / chunking / organizing it early on?\n\nAny help, advice, or even just a pointer in the right direction would be awesome.",
          "author_fullname": "t2_1ckf2evf4b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help: Building a University Assistant RAGbot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuz5w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754154614,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI&amp;#39;m a final-year CS student working on a project to build an &lt;strong&gt;AI assistant for my university&lt;/strong&gt; using &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt; and possibly agentic tools down the line.&lt;/p&gt;\n\n&lt;p&gt;The chatbot will help students find answers to common university-related questions (like academic queries, admissions, etc.) and eventually perform light actions like form redirection, etc.&lt;/p&gt;\n\n&lt;h1&gt;What I’m struggling with:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly sure &lt;strong&gt;what types of data&lt;/strong&gt; I should collect and prepare to make this assistant useful, accurate, and robust.&lt;/p&gt;\n\n&lt;p&gt;I plan to use LangChain or LlamaIndex + a vector store, but I want to hear from folks with experience in this kind of thing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;What kinds of data did you use for similar projects?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;How do you decide what to include or ignore?&lt;/li&gt;\n&lt;li&gt;Any tips for formatting / chunking / organizing it early on?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any help, advice, or even just a pointer in the right direction would be awesome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfuz5w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jarrarhaidery",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuz5w/need_help_building_a_university_assistant_ragbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuz5w/need_help_building_a_university_assistant_ragbot/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754152634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Disclaimer:\n\nNo actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Me lately... Anyone else can relate? 😎",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf4ihq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3BSxzZpbbsOE49m6Gzlq2vc_AKs8no1XsGYN2LTJOrg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754073451,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer:&lt;/p&gt;\n\n&lt;p&gt;No actual plushy pandas were hurt in the process of trying and failing to fit in a plastic box...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rqzixk49cggf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rqzixk49cggf1.gif?format=png8&amp;s=b762d39ddcffd472526fa05b0d769adcc9064fe7",
                  "width": 800,
                  "height": 453
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=4f274e8ff485a4989b9d127943de26befcdfb05d",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=0965b1a108af3d1709115e6fb86455125fa468ea",
                    "width": 216,
                    "height": 122
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=beb312a49051ec985b83d4c6031aee1db97d0956",
                    "width": 320,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=8163f8e2cdf42526b89cbf134b8c0bbe2c377aa2",
                    "width": 640,
                    "height": 362
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/rqzixk49cggf1.gif?s=e2f65b8b2302c9de598fe0355ffa07608973a7b7",
                      "width": 800,
                      "height": 453
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;crop=smart&amp;s=f83c49f5a04ebce5a16c8b856d87f5215d57d86d",
                        "width": 108,
                        "height": 61
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;crop=smart&amp;s=41ec44955588452eaeb561d8f4d6b51bdafdc817",
                        "width": 216,
                        "height": 122
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;crop=smart&amp;s=507d4f46da1e538fa1fe51f68da80696036c09aa",
                        "width": 320,
                        "height": 181
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;crop=smart&amp;s=3d8b06b0091af494d702ac39636bf603e600b301",
                        "width": 640,
                        "height": 362
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/rqzixk49cggf1.gif?format=mp4&amp;s=7f326be51b3cb7f3e2485d49898a5015d2bb1140",
                      "width": 800,
                      "height": 453
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=108&amp;format=mp4&amp;s=40d2c981c71f4d6d8f0c9a6bc711893b6edbebdc",
                        "width": 108,
                        "height": 61
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=216&amp;format=mp4&amp;s=585d088b1216ab7ef3b8d529198e1dc2849966e9",
                        "width": 216,
                        "height": 122
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=320&amp;format=mp4&amp;s=c445476bc95afa7c794b1c3af79421dcb8a4f570",
                        "width": 320,
                        "height": 181
                      },
                      {
                        "url": "https://preview.redd.it/rqzixk49cggf1.gif?width=640&amp;format=mp4&amp;s=c9b96f95344d2207fe0fd0ea23af8ea9f3f2e686",
                        "width": 640,
                        "height": 362
                      }
                    ]
                  }
                },
                "id": "A5U9B6UgGQoc_r5iq28_9pm0dir0rIEwOZlniIEu3DQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mf4ihq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf4ihq/me_lately_anyone_else_can_relate/",
          "stickied": false,
          "url": "https://i.redd.it/rqzixk49cggf1.gif",
          "subreddit_subscribers": 509052,
          "created_utc": 1754073451,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently doing the mcp course from huggingface, and I am planning to roll my own local agentic AI. Any idea what the BEST model I should use for RTX 4090? I know best is objective, so I am looking for two models, one for general purpose, and the other for coding. I will be building simple tools for personal use. For example, making a custom resume generator given job description etc. ",
          "author_fullname": "t2_1us1jsxdkh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model to use as agentic AI for RTX 4090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfubwt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151003,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently doing the mcp course from huggingface, and I am planning to roll my own local agentic AI. Any idea what the BEST model I should use for RTX 4090? I know best is objective, so I am looking for two models, one for general purpose, and the other for coding. I will be building simple tools for personal use. For example, making a custom resume generator given job description etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfubwt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flat_Chard_3763",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfubwt/best_model_to_use_as_agentic_ai_for_rtx_4090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfubwt/best_model_to_use_as_agentic_ai_for_rtx_4090/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754151003,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This set of experiments were conducted about half a year ago and we are suggested to share them to the community. Summary of the experiments\n\n(1) Lihua world dataset: conversation data, all texts\n\n(2) In previous studies, Graph RAG (and variants) showed advantages over \"naïve\" RAG.\n\n(3) Using OpenAI RAG API (File Search), the accuracy is substantially higher than graph RAG &amp; variants\n\n(4) Using the same embeddings, [https://chat.vecml.com](https://chat.vecml.com/) produces consistently better accuracies than OpenAI RAG API (File Search).\n\n(5) More interestingly, [https://chat.vecml.com/](https://chat.vecml.com/) is substantially (550x) faster than OpenAI RAG (File Search)\n\n(6) Additional experiments on different embeddings are also provided.\n\nNote that Lihua world dataset is purely text. In practice, the documents are in all sorts of formats: PDFs, OCR, Excel, HTML, DocX, PPTX, WPS, and more. [https://chat.vecml.com/](https://chat.vecml.com/) is able to handle documents of many different formats and is capable of dealing with multi-modal RAG.",
          "author_fullname": "t2_x7ezv9dmy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "OpenAI RAG API (File Search): an experimental study",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "snfxiwiotngf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecc1cf0a6bca5c4e3328477b7026c95b8f03eb70"
                },
                {
                  "y": 96,
                  "x": 216,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=914115ad10b3b2959058b4fd792112b6f9b05f24"
                },
                {
                  "y": 143,
                  "x": 320,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73850eb9a8677a5bea2d313bccdaa792685357ff"
                },
                {
                  "y": 286,
                  "x": 640,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=244d9f4891b0943eb383c852c0e469dbaf82ffc5"
                },
                {
                  "y": 429,
                  "x": 960,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cfa0e69bb3aa9572c84bf191e4bb5ef839f6f18"
                },
                {
                  "y": 483,
                  "x": 1080,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58d6f1634978dfaf2f93ccc8b49cacf69fc18ded"
                }
              ],
              "s": {
                "y": 598,
                "x": 1336,
                "u": "https://preview.redd.it/snfxiwiotngf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=69a448d93ef8f0a3be6f01eec30503520d18f053"
              },
              "id": "snfxiwiotngf1"
            },
            "kdrqcvintngf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 130,
                  "x": 108,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f4114f77943df8660e61b3e333ac3cd0aabebfd"
                },
                {
                  "y": 260,
                  "x": 216,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8274df2d3ccb3961cf1d9322752fd6967fd017ed"
                },
                {
                  "y": 386,
                  "x": 320,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20bec91b1fa2c9b44e0434ddd20d6eb31063312"
                },
                {
                  "y": 772,
                  "x": 640,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6177b1bb06a5fd33588da0d54116a9f1e75d1690"
                }
              ],
              "s": {
                "y": 881,
                "x": 730,
                "u": "https://preview.redd.it/kdrqcvintngf1.png?width=730&amp;format=png&amp;auto=webp&amp;s=9244cca0f81ba098aa6e1aaea3a59ac371ac8b0b"
              },
              "id": "kdrqcvintngf1"
            }
          },
          "name": "t3_1mfzezz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kdrqcvintngf1",
                "id": 719740416
              },
              {
                "media_id": "snfxiwiotngf1",
                "id": 719740417
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/fwt17fYbAFMoObf3HbNFT-k87BCWFCICFS0WC914Tx8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754163875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This set of experiments were conducted about half a year ago and we are suggested to share them to the community. Summary of the experiments&lt;/p&gt;\n\n&lt;p&gt;(1) Lihua world dataset: conversation data, all texts&lt;/p&gt;\n\n&lt;p&gt;(2) In previous studies, Graph RAG (and variants) showed advantages over &amp;quot;naïve&amp;quot; RAG.&lt;/p&gt;\n\n&lt;p&gt;(3) Using OpenAI RAG API (File Search), the accuracy is substantially higher than graph RAG &amp;amp; variants&lt;/p&gt;\n\n&lt;p&gt;(4) Using the same embeddings, &lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com&lt;/a&gt; produces consistently better accuracies than OpenAI RAG API (File Search).&lt;/p&gt;\n\n&lt;p&gt;(5) More interestingly, &lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com/&lt;/a&gt; is substantially (550x) faster than OpenAI RAG (File Search)&lt;/p&gt;\n\n&lt;p&gt;(6) Additional experiments on different embeddings are also provided.&lt;/p&gt;\n\n&lt;p&gt;Note that Lihua world dataset is purely text. In practice, the documents are in all sorts of formats: PDFs, OCR, Excel, HTML, DocX, PPTX, WPS, and more. &lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com/&lt;/a&gt; is able to handle documents of many different formats and is capable of dealing with multi-modal RAG.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mfzezz",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzezz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DueKitchen3102",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzezz/openai_rag_api_file_search_an_experimental_study/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mfzezz",
          "subreddit_subscribers": 509052,
          "created_utc": 1754163875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "GPU snapshotting is finally a thing! NVIDIA recently released their [CUDA checkpoint/restore API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html) and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.\n\nGPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!\n\nhttps://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;format=png&amp;auto=webp&amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137\n\nMore on how GPU snapshotting works plus additional benchmarks in this blog post: [https://modal.com/blog/gpu-mem-snapshots](https://modal.com/blog/gpu-mem-snapshots)",
          "author_fullname": "t2_9av3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cold start vLLM in 5 seconds with GPU snapshotting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "opb5odlb2hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 54,
                  "x": 108,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1a99ec3a648e3acbc71ef3233e807e5fbfc20dd"
                },
                {
                  "y": 108,
                  "x": 216,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a87d457cf006589e58141d5831e05fc39cc999c4"
                },
                {
                  "y": 161,
                  "x": 320,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=156b5387ff8776f0f27387621395be6a4f1bb15b"
                },
                {
                  "y": 322,
                  "x": 640,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c498b446dda08ce80241842d3232b30cf40b31ec"
                },
                {
                  "y": 483,
                  "x": 960,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eae84d78449fb87be8ed155a688b75c0cb5ea458"
                },
                {
                  "y": 543,
                  "x": 1080,
                  "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=505dd6a12adbdbe687a6467d0b0843adb780d82a"
                }
              ],
              "s": {
                "y": 1592,
                "x": 3162,
                "u": "https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;format=png&amp;auto=webp&amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137"
              },
              "id": "opb5odlb2hgf1"
            }
          },
          "name": "t3_1mf86rn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/9zGmAn5wtWE9ciYDNGxrwYmoTgYDYEOmBf7p4EOrZDY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754082162,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPU snapshotting is finally a thing! NVIDIA recently released their &lt;a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html\"&gt;CUDA checkpoint/restore API&lt;/a&gt; and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.&lt;/p&gt;\n\n&lt;p&gt;GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137\"&gt;https://preview.redd.it/opb5odlb2hgf1.png?width=3162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00995e770fa4d3ac454bd9b1f0df5296391fb137&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More on how GPU snapshotting works plus additional benchmarks in this blog post: &lt;a href=\"https://modal.com/blog/gpu-mem-snapshots\"&gt;https://modal.com/blog/gpu-mem-snapshots&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf86rn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crookedstairs",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf86rn/cold_start_vllm_in_5_seconds_with_gpu_snapshotting/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754082162,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm developing a product that uses AI, but it's entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? ",
          "author_fullname": "t2_1u3u0ca58k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dutch LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy5qs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m developing a product that uses AI, but it&amp;#39;s entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy5qs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Bath-7378",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754160656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I try to run all of my models at 32k context using llama.cpp, but it feels bad to be losing so much performance compared to launching with 2-4k context for short one-shot question prompts",
          "author_fullname": "t2_1j5x86i7wh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What context lengths do people actually run their models at?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfi8ly",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111252,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I try to run all of my models at 32k context using llama.cpp, but it feels bad to be losing so much performance compared to launching with 2-4k context for short one-shot question prompts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfi8ly",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OUT_OF_HOST_MEMORY",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfi8ly/what_context_lengths_do_people_actually_run_their/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfi8ly/what_context_lengths_do_people_actually_run_their/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754111252,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like to start self hosting models for my own usage. I have right now MacBook Pro m4 Pro 24Gb ram and it feels slow with larger models and very limited. Do you think it would be better to build some custom spec pc for this purpose running on Linux just to run LLMs? Or buy maxed out Mac Studio or Mac mini for this purpose\n\nMain usage would be coding and image generation if that would be possible. \n\nPs. I have sitting somewhere i7 12700K with 32Gb ram but without gpu ",
          "author_fullname": "t2_1pnbev48g0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting started into self hosting LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9cw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to start self hosting models for my own usage. I have right now MacBook Pro m4 Pro 24Gb ram and it feels slow with larger models and very limited. Do you think it would be better to build some custom spec pc for this purpose running on Linux just to run LLMs? Or buy maxed out Mac Studio or Mac mini for this purpose&lt;/p&gt;\n\n&lt;p&gt;Main usage would be coding and image generation if that would be possible. &lt;/p&gt;\n\n&lt;p&gt;Ps. I have sitting somewhere i7 12700K with 32Gb ram but without gpu &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfs9cw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ywful",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754145855,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just bought a new PC, it’s not primarily for AI but I wanna try out llms. I’m not too familiar about the different models, so I’d appreciate if someone could provide recommendations.\n\nPc specs: 5070 Ti 16gb + i7 14700 32 gb ddr5 6000 MHz.",
          "author_fullname": "t2_17ltlw5q9t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any Open source LLM’s better than free tier of ChatGPT(4o and 4o mini)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxdlg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a new PC, it’s not primarily for AI but I wanna try out llms. I’m not too familiar about the different models, so I’d appreciate if someone could provide recommendations.&lt;/p&gt;\n\n&lt;p&gt;Pc specs: 5070 Ti 16gb + i7 14700 32 gb ddr5 6000 MHz.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfxdlg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Championship7986",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxdlg/are_there_any_open_source_llms_better_than_free/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxdlg/are_there_any_open_source_llms_better_than_free/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754158656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, an enthusiast but no formal CS training background asking for help \n\nI am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that's the best we can get :) \n\nWhich is the best \"small size\" LLM for document question answering with European language - mostly specific medical jargon.\n\nI tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.\n\nI am running the app using ollama and langchain also any recommendations for alternatives are welcome :)\n",
          "author_fullname": "t2_1e27iwczfb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best &lt;2B open-source LLMs for European languages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnfrp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754131171,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, an enthusiast but no formal CS training background asking for help &lt;/p&gt;\n\n&lt;p&gt;I am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that&amp;#39;s the best we can get :) &lt;/p&gt;\n\n&lt;p&gt;Which is the best &amp;quot;small size&amp;quot; LLM for document question answering with European language - mostly specific medical jargon.&lt;/p&gt;\n\n&lt;p&gt;I tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.&lt;/p&gt;\n\n&lt;p&gt;I am running the app using ollama and langchain also any recommendations for alternatives are welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfnfrp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Material-Ad5426",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754131171,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently switched my local models to the new 30B-A3B 2507 models. However, when testing the instruct model, I noticed it hallucinates much more than previous Qwen models.\n\nI fed it a README file I wrote myself for summarization, so I know its contents well. The 2507 instruct model not only uses excessive emojis but also fabricates lots of information that isn’t in the file.\n\nI also tested the 2507 thinking and coder versions with the same README, prompt, and quantization level (q4). Both used zero emojis and showed no noticeable hallucinations.\n\nHas anyone else experienced similar issues with the 2507 instruct model?\n\n* I'm using llama.cpp + llama swap, and the \"best practice\" settings from the HF model card ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Serious hallucination issues of 30B-A3B Instruct 2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfiroj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754113093,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently switched my local models to the new 30B-A3B 2507 models. However, when testing the instruct model, I noticed it hallucinates much more than previous Qwen models.&lt;/p&gt;\n\n&lt;p&gt;I fed it a README file I wrote myself for summarization, so I know its contents well. The 2507 instruct model not only uses excessive emojis but also fabricates lots of information that isn’t in the file.&lt;/p&gt;\n\n&lt;p&gt;I also tested the 2507 thinking and coder versions with the same README, prompt, and quantization level (q4). Both used zero emojis and showed no noticeable hallucinations.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced similar issues with the 2507 instruct model?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;m using llama.cpp + llama swap, and the &amp;quot;best practice&amp;quot; settings from the HF model card &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfiroj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfiroj/serious_hallucination_issues_of_30ba3b_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfiroj/serious_hallucination_issues_of_30ba3b_instruct/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754113093,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.\n\nGreat at general knowledge, but also follows instructions very well.\n\nPlease let me know your experiences with it!",
          "author_fullname": "t2_d2gb9jhgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 30b a3b 2507 instruct as good as Gemma 3 27B!?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0i54",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754064349,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What an awesome model. Everything I throw at it I get comparable results to Gemma 3, but 4.5x faster.&lt;/p&gt;\n\n&lt;p&gt;Great at general knowledge, but also follows instructions very well.&lt;/p&gt;\n\n&lt;p&gt;Please let me know your experiences with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf0i54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hanthunius",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf0i54/qwen_30b_a3b_2507_instruct_as_good_as_gemma_3_27b/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754064349,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "One of the things I want to do with my local build is to make my home more efficient.   I'd like to be able to get data points from various sources and have them analyzed either for actionable changes or optimization.  Not sure how to get from here to there though.\n\nExample:\n\nGather data from \n- temp outside\n- temp inside\n- temp inside cooling ducts (only measured when the system is blowing)\n- electrical draw from the ac\n- commanded on off cycles \n- amount of sun in specific loacations\n\nThen figure out \n- hvac gets commanded on but take longer at this time to cool off the house\n- at those times, command ac at lower temps to mitigate the time loss\n- discover that sun load at specific times effects efficiency, shade the area. \n\nI feel like there are enough smart home sensors out there that a well tuned ai could crunch all the data and give some real insight.   Why go of daily averages when I can record actual data in almost real time?  Why guess at the type of things home owners and so called efficiency experts have done in the past?  \n\nSo the set up might be something like this:\n\n1 install smart features and sensors (that can communicate with 2)\n\n2 set up code script etc to record data from all sources\n\n3 have ai model that interprets data and spit back patterns and adjustments to make\n\n4 maybe have ai create new script to adjust settings in the smart home for optimal efficiency\n\n5 run daily or or weekly analysis and adjust the efficiency script.\n\n\nThis is just me thinking outlook as a starting point.  And its only one area of efficiency of several that this could play a noticeable impact\n",
          "author_fullname": "t2_clyuifd5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smart integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqzc8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754142487,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the things I want to do with my local build is to make my home more efficient.   I&amp;#39;d like to be able to get data points from various sources and have them analyzed either for actionable changes or optimization.  Not sure how to get from here to there though.&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;Gather data from \n- temp outside\n- temp inside\n- temp inside cooling ducts (only measured when the system is blowing)\n- electrical draw from the ac\n- commanded on off cycles \n- amount of sun in specific loacations&lt;/p&gt;\n\n&lt;p&gt;Then figure out \n- hvac gets commanded on but take longer at this time to cool off the house\n- at those times, command ac at lower temps to mitigate the time loss\n- discover that sun load at specific times effects efficiency, shade the area. &lt;/p&gt;\n\n&lt;p&gt;I feel like there are enough smart home sensors out there that a well tuned ai could crunch all the data and give some real insight.   Why go of daily averages when I can record actual data in almost real time?  Why guess at the type of things home owners and so called efficiency experts have done in the past?  &lt;/p&gt;\n\n&lt;p&gt;So the set up might be something like this:&lt;/p&gt;\n\n&lt;p&gt;1 install smart features and sensors (that can communicate with 2)&lt;/p&gt;\n\n&lt;p&gt;2 set up code script etc to record data from all sources&lt;/p&gt;\n\n&lt;p&gt;3 have ai model that interprets data and spit back patterns and adjustments to make&lt;/p&gt;\n\n&lt;p&gt;4 maybe have ai create new script to adjust settings in the smart home for optimal efficiency&lt;/p&gt;\n\n&lt;p&gt;5 run daily or or weekly analysis and adjust the efficiency script.&lt;/p&gt;\n\n&lt;p&gt;This is just me thinking outlook as a starting point.  And its only one area of efficiency of several that this could play a noticeable impact&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqzc8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JellyfishAutomatic25",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqzc8/smart_integration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqzc8/smart_integration/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754142487,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mentioned in the new, Qwen3 30B download announcement was that 480B's tool calling was fixed and it [needed to be re-downloaded](https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:~:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard)\n\nI'm just posting it so that no one misses it. I'm using LMStudio and it just showed as \"downloaded\". It didn't seem to know there was a change.\n\nEDIT: Yes, this only refers to the unsloth versions of 480B.  Thank you u/MikeRoz",
          "author_fullname": "t2_taorh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heads up to those that downloaded Qwen3 Coder 480B before yesterday",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mexa2g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 72,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 72,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754059918,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754056847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mentioned in the new, Qwen3 30B download announcement was that 480B&amp;#39;s tool calling was fixed and it &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/#:%7E:text=We%20also%20fixed%20tool%20calling%20for%20the%20480B%20and%20this%20model%20and%20fixed%2030B%20thinking%2C%20so%20please%20redownload%20the%20first%20shard\"&gt;needed to be re-downloaded&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just posting it so that no one misses it. I&amp;#39;m using LMStudio and it just showed as &amp;quot;downloaded&amp;quot;. It didn&amp;#39;t seem to know there was a change.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Yes, this only refers to the unsloth versions of 480B.  Thank you &lt;a href=\"/u/MikeRoz\"&gt;u/MikeRoz&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mexa2g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VegetaTheGrump",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mexa2g/heads_up_to_those_that_downloaded_qwen3_coder/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754056847,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Gemini is too long and detailed. Grok's format is weird. Perplexity doesn't search enough. Qwen takes years and writes an entire book.\n\nchatGPT does it perfectly. A double lengthed message with citations, well-written, searches through websites trying to find what it needs, reasoning through it. But it's limited.\n\nThx guys!\n",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best free good deep research LLM websites?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpnxi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Gemini is too long and detailed. Grok&amp;#39;s format is weird. Perplexity doesn&amp;#39;t search enough. Qwen takes years and writes an entire book.&lt;/p&gt;\n\n&lt;p&gt;chatGPT does it perfectly. A double lengthed message with citations, well-written, searches through websites trying to find what it needs, reasoning through it. But it&amp;#39;s limited.&lt;/p&gt;\n\n&lt;p&gt;Thx guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfpnxi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpnxi/best_free_good_deep_research_llm_websites/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpnxi/best_free_good_deep_research_llm_websites/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754138692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey! I’d like to set up my machine to work on my codebase while I’m AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.\n\nI’ve already tested a few tools — kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.\n\nAre there any more or less standardized setups for this kind of workflow?\n\n",
          "author_fullname": "t2_nt892",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the current go-to setup for a fully-local coding agent that continuously improves code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpn4a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I’d like to set up my machine to work on my codebase while I’m AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.&lt;/p&gt;\n\n&lt;p&gt;I’ve already tested a few tools — kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.&lt;/p&gt;\n\n&lt;p&gt;Are there any more or less standardized setups for this kind of workflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfpn4a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sasik520",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754138620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or is it unlimited on [chat.qwen.ai](http://chat.qwen.ai) ? ",
          "author_fullname": "t2_14c4jctjk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any limits on Deep Research mode on Qwen Chat?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxjd5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754159065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or is it unlimited on &lt;a href=\"http://chat.qwen.ai\"&gt;chat.qwen.ai&lt;/a&gt; ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfxjd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sostrene_Blue",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxjd5/is_there_any_limits_on_deep_research_mode_on_qwen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxjd5/is_there_any_limits_on_deep_research_mode_on_qwen/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754159065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)\n\nbonus: fix hunyuan\\_moe chat template",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for the upcoming hunyuan dense models has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0hou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=40b87c745f679bca8fe0aa91a0c19c36c7177277",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754064323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the source code, we see a link to Hunyuan-4B-Instruct, but I think we’ll see much larger models :)&lt;/p&gt;\n\n&lt;p&gt;bonus: fix hunyuan_moe chat template&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?auto=webp&amp;s=7d88566cdde4131f4d8262c975dfd9b55caf89f5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed015fcc9ee802baeb72ee117bf3077725576fed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b29546e91bdc6de331482f1a9525a141f86b6852",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a5913f23624227672a7032140b173c0f2635ae9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f81f256091726e20730924e97a225729e6c971ec",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7d6c5bdd7a760c5bf13d60c0766042314f38e1c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e0455df24a27f9bce1dd5b8080cbffc5f292d90",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "pRyHe6l3-qrKqD2qUrrGwmAgS3GhMUUKtd4TRQbJKGc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf0hou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf0hou/support_for_the_upcoming_hunyuan_dense_models_has/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 509052,
          "created_utc": 1754064323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.",
          "author_fullname": "t2_1nsamx8udx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-Air running on 64GB Mac Studio(M4)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mesi2s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/xg-TV-u5t8CZYga5vqS7H6cvSQQNm6mI1iKN_y1d0Xg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754042719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/87ng5bmisdgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/87ng5bmisdgf1.png?auto=webp&amp;s=278b1e2b20f96d9793b766eaf9a4662e0c4177fa",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2b027c72800b8bb18356e66239bf4a6fe201ecf",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d6e4b8a9f3ad7ba583fe5cae8620c382e5db0be",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=136800587978cd65bc334b35b13dfe4c311ab4c6",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54f42f44d09cb4df95a9f6ed8ad3cf70c2cc96bf",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8cde2cdaafe0d7e74429488394a890a17630d1a5",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/87ng5bmisdgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7ec880558fbdfda721507e10398b64ecc96a673",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "n4NTI1RG1xenOb1HsAwQ1LQYB7_slm74LFJZKuG7hJQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mesi2s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "riwritingreddit",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/",
          "stickied": false,
          "url": "https://i.redd.it/87ng5bmisdgf1.png",
          "subreddit_subscribers": 509052,
          "created_utc": 1754042719,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_j2lm1hb2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SVDQuant does INT4 quantization of text-to-image models without losing quality. Can't the same technique be used in LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf08e5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/N50eG8wUbPgLuCMeBevOa0eyQWBygGeeMCJcikzOQiM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754063762,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cq321qc1fgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?auto=webp&amp;s=55a8648bf4010ec76dde9382784506ff1f279b6b",
                  "width": 3293,
                  "height": 2120
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9b9b51c5d0f9c3b9de0cbaa0cdbb1c69e4ee263",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ffc5eae3e6f9fd9ac0ac50aea60adf88a4c90f0",
                    "width": 216,
                    "height": 139
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6e8b0e358cd85ccb5a6738bd9900251292d4eb9",
                    "width": 320,
                    "height": 206
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=162d6f061d05ab906b370e0b7ce08f4a7f85014d",
                    "width": 640,
                    "height": 412
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20c332654151a9cd9bcbb7f5f225cbb367a05536",
                    "width": 960,
                    "height": 618
                  },
                  {
                    "url": "https://preview.redd.it/0cq321qc1fgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5661bfba947547555164c9f84545ce932ae65377",
                    "width": 1080,
                    "height": 695
                  }
                ],
                "variants": {},
                "id": "D8iRezvmv7BRzfgNCmicLKUlcwGdQOUpQHIUZAPRPow"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mf08e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "we_are_mammals",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf08e5/svdquant_does_int4_quantization_of_texttoimage/",
          "stickied": false,
          "url": "https://i.redd.it/0cq321qc1fgf1.jpeg",
          "subreddit_subscribers": 509052,
          "created_utc": 1754063762,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. I'm a non native English writer, who could use some help with phrasing, [something like this](https://www.gingersoftware.com/products/sentence-rephraser), character and plot detail suggestions etc. Are there any good models that can help with that?\n\nI'm planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it's absolutely necessary. Please let me know if it is.",
          "author_fullname": "t2_169t2g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a local model that can help a non native writer with sentence phrasing and ideas.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfoh32",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m a non native English writer, who could use some help with phrasing, &lt;a href=\"https://www.gingersoftware.com/products/sentence-rephraser\"&gt;something like this&lt;/a&gt;, character and plot detail suggestions etc. Are there any good models that can help with that?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it&amp;#39;s absolutely necessary. Please let me know if it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfoh32",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "logicSnob",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754134900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?\n\nHere's the Dockerfile:\n\n&gt; FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate\n\nHere's the docker-compose:\n\n&gt; version: '3.8'\nservices:\n  infinity:\n    build: .\n    ports:\n      - \"7997:7997\"\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B\n\nStartup command was:\n\ndocker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch",
          "author_fullname": "t2_cocl8roo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Issues with michaelf34/infinity:latest-cpu + Qwen3-Embedding-8B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfofx5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134794,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the Dockerfile:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Here&amp;#39;s the docker-compose:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;version: &amp;#39;3.8&amp;#39;\nservices:\n  infinity:\n    build: .\n    ports:\n      - &amp;quot;7997:7997&amp;quot;\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Startup command was:&lt;/p&gt;\n\n&lt;p&gt;docker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfofx5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Patentsmatter",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754134794,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi r/LocalLLaMA,\n\nI'm exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.\n\nFor example, I could give it a high-level command like \"Save the document and close the application,\" and it would need to:\n\n1. Visually parse the screen to locate the \"Save\" button or menu item.\n2. Move the mouse cursor to that location.\n3. Perform a click.\n4. Then, locate the \"Close\" button and do the same.\n\nI'm trying to figure out the best stack for this using local models. My main questions are:\n\n* **Vision/Perception:** What's the current best approach for a model to \"see\" the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?\n* **Decision Making (LLM):** How would the LLM receive the visual information and output the decision (e.g., \"click button with text 'OK' at coordinates \\[x, y\\]\")? What kind of prompting or fine-tuning would be required?\n* **Action/Control:** What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like pyautogui the way to go, or are there more robust alternatives?\n* **Frameworks:** Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?\n\nI'm aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!\n\nThanks",
          "author_fullname": "t2_1t42bjvz6a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to build a local agent for Windows GUI automation (mouse control &amp; accurate button clicking)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfodac",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.&lt;/p&gt;\n\n&lt;p&gt;For example, I could give it a high-level command like &amp;quot;Save the document and close the application,&amp;quot; and it would need to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Visually parse the screen to locate the &amp;quot;Save&amp;quot; button or menu item.&lt;/li&gt;\n&lt;li&gt;Move the mouse cursor to that location.&lt;/li&gt;\n&lt;li&gt;Perform a click.&lt;/li&gt;\n&lt;li&gt;Then, locate the &amp;quot;Close&amp;quot; button and do the same.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out the best stack for this using local models. My main questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vision/Perception:&lt;/strong&gt; What&amp;#39;s the current best approach for a model to &amp;quot;see&amp;quot; the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Decision Making (LLM):&lt;/strong&gt; How would the LLM receive the visual information and output the decision (e.g., &amp;quot;click button with text &amp;#39;OK&amp;#39; at coordinates [x, y]&amp;quot;)? What kind of prompting or fine-tuning would be required?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action/Control:&lt;/strong&gt; What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like pyautogui the way to go, or are there more robust alternatives?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frameworks:&lt;/strong&gt; Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfodac",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xSNYPSx777",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754134546,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI’m working on a side project to build a virtual assistant that can do two main things:\n\n1. Answer questions based on a company’s internal docs (using RAG).\n2. Perform actions like “create an account,” “schedule a meeting,” or “find the nearest location.”\n\nI’d love some advice from folks who’ve built similar systems or explored this space. A few questions:\n\n- How would you store and access the internal data (both docs and structured info)?\n\n- What RAG setup works well in practice (vector store, retrieval strategy, etc)?\n\n- Would you use a separate intent classifier to route between info-lookup vs action execution?\n\n- For tasks, do agent frameworks like LangGraph or AutoGen make sense?\n\n- Have frameworks like ReAct/MRKL been useful in real-world projects?\n\n- When is fine-tuning or LoRA worth the effort vs just RAG + good prompting?\n\n- Any tips or lessons learned on overall architecture or scaling?\n\nNot looking for someone to design it for me, just hoping to hear what’s worked (or not) in your experience. Cheers!",
          "author_fullname": "t2_m4wc3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scalable LLM Virtual Assistant – Looking for Architecture Tips",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mft55c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754148045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I’m working on a side project to build a virtual assistant that can do two main things:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Answer questions based on a company’s internal docs (using RAG).&lt;/li&gt;\n&lt;li&gt;Perform actions like “create an account,” “schedule a meeting,” or “find the nearest location.”&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’d love some advice from folks who’ve built similar systems or explored this space. A few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How would you store and access the internal data (both docs and structured info)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What RAG setup works well in practice (vector store, retrieval strategy, etc)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Would you use a separate intent classifier to route between info-lookup vs action execution?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For tasks, do agent frameworks like LangGraph or AutoGen make sense?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have frameworks like ReAct/MRKL been useful in real-world projects?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;When is fine-tuning or LoRA worth the effort vs just RAG + good prompting?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any tips or lessons learned on overall architecture or scaling?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Not looking for someone to design it for me, just hoping to hear what’s worked (or not) in your experience. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mft55c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeadFinger",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mft55c/scalable_llm_virtual_assistant_looking_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mft55c/scalable_llm_virtual_assistant_looking_for/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754148045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing **DocStrange**, an open-source Python library that makes document data extraction easy.\n\n* **Universal Input**: PDFs, Images, Word docs, PowerPoint, Excel\n* **Multiple Outputs**: Clean Markdown, structured JSON, CSV tables, formatted HTML\n* **Smart Extraction**: Specify exact fields you want (e.g., \"invoice\\_number\", \"total\\_amount\")\n* **Schema Support**: Define JSON schemas for consistent structured output\n\n**Quick start:**\n\n    from docstrange import DocumentExtractor\n    \n    extractor = DocumentExtractor()\n    result = extractor.extract(\"research_paper.pdf\")\n    \n    # Get clean markdown for LLM training\n    markdown = result.extract_markdown()\n\n**CLI**\n\n    pip install docstrange\n    docstrange document.pdf --output json --extract-fields title author date\n\n**Data Processing Options**\n\n* **Cloud Mode**: Fast and free processing with minimal setup\n* **Local Mode**: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu\n\n**Links:**\n\n* PyPI: [https://pypi.org/project/docstrange/](https://pypi.org/project/docstrange/)",
          "author_fullname": "t2_1mv39a664r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DocStrange - Open Source Document Data Extractor",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mepr38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 178,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 178,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/n_8zXMrMBAhovAnFfx7BottIYrMkhK42uDJfBp4i85Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754032135,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing &lt;strong&gt;DocStrange&lt;/strong&gt;, an open-source Python library that makes document data extraction easy.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Universal Input&lt;/strong&gt;: PDFs, Images, Word docs, PowerPoint, Excel&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multiple Outputs&lt;/strong&gt;: Clean Markdown, structured JSON, CSV tables, formatted HTML&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Smart Extraction&lt;/strong&gt;: Specify exact fields you want (e.g., &amp;quot;invoice_number&amp;quot;, &amp;quot;total_amount&amp;quot;)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Schema Support&lt;/strong&gt;: Define JSON schemas for consistent structured output&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from docstrange import DocumentExtractor\n\nextractor = DocumentExtractor()\nresult = extractor.extract(&amp;quot;research_paper.pdf&amp;quot;)\n\n# Get clean markdown for LLM training\nmarkdown = result.extract_markdown()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install docstrange\ndocstrange document.pdf --output json --extract-fields title author date\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Data Processing Options&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cloud Mode&lt;/strong&gt;: Fast and free processing with minimal setup&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Local Mode&lt;/strong&gt;: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PyPI: &lt;a href=\"https://pypi.org/project/docstrange/\"&gt;https://pypi.org/project/docstrange/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vghke2r1ycgf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vghke2r1ycgf1.gif?format=png8&amp;s=0646017ef4eefc01cddecd722d9f8f2d50380882",
                  "width": 1138,
                  "height": 717
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=fc63503d84bbe4a224856c2516262be363da4ac8",
                    "width": 108,
                    "height": 68
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=3c6a452195c5261d81d558756ca975edc42b2fae",
                    "width": 216,
                    "height": 136
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=5175141f8d36c2b53aecbc5cb9f68c8f3fdc88ef",
                    "width": 320,
                    "height": 201
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=b028677410eada3f069136c8e6c1b8c48c06f285",
                    "width": 640,
                    "height": 403
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=83ef66bfaa9d844d3079c6f8d7f7ec2c933079bb",
                    "width": 960,
                    "height": 604
                  },
                  {
                    "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=e9c266f359e28d38390413a4c7f732940b9da31e",
                    "width": 1080,
                    "height": 680
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/vghke2r1ycgf1.gif?s=5ddee0d3d4b59db883ad7324e63d6d44c4c6edc1",
                      "width": 1138,
                      "height": 717
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;crop=smart&amp;s=f965200e3ad373df4b75762cdb1f8901fa898b66",
                        "width": 108,
                        "height": 68
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;crop=smart&amp;s=2ede88d7ce8de83577c6d4caba7ac71eadd2671c",
                        "width": 216,
                        "height": 136
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;crop=smart&amp;s=85aaad5e77f8fa02ca6dcd40563302da8c506a58",
                        "width": 320,
                        "height": 201
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;crop=smart&amp;s=12643bc505cd05a85286b55a7fff556b82b4872a",
                        "width": 640,
                        "height": 403
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;crop=smart&amp;s=0a82ceb7382adf023c53606bed72529202ab9f69",
                        "width": 960,
                        "height": 604
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;crop=smart&amp;s=2786689a41ab5170679f3efd992a64e464d981e2",
                        "width": 1080,
                        "height": 680
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/vghke2r1ycgf1.gif?format=mp4&amp;s=7043fb757ff13b2cf394629a8043afef1a0a0995",
                      "width": 1138,
                      "height": 717
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=108&amp;format=mp4&amp;s=50bc5d05247661d61d18415388e20a7fd4866caf",
                        "width": 108,
                        "height": 68
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=216&amp;format=mp4&amp;s=d334100ea224720cfcaa53b23a6a1fd993d24945",
                        "width": 216,
                        "height": 136
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=320&amp;format=mp4&amp;s=f8eed5e239635d7c160661180140118e60510641",
                        "width": 320,
                        "height": 201
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;format=mp4&amp;s=23de0d42f6d1959233bf55d4f8f865fb4f03146e",
                        "width": 640,
                        "height": 403
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=960&amp;format=mp4&amp;s=f9a9a9f47cf0d08c3603a0b57a5250634e9d42ea",
                        "width": 960,
                        "height": 604
                      },
                      {
                        "url": "https://preview.redd.it/vghke2r1ycgf1.gif?width=1080&amp;format=mp4&amp;s=340f0a49b5bca4ea0f0ee8982400c578d1646297",
                        "width": 1080,
                        "height": 680
                      }
                    ]
                  }
                },
                "id": "rhbuyimmJ2s8b0MEkSp5-3yQopRWq3kKuTIAyO2Dmsk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mepr38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LostAmbassador6872",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/",
          "stickied": false,
          "url": "https://i.redd.it/vghke2r1ycgf1.gif",
          "subreddit_subscribers": 509052,
          "created_utc": 1754032135,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sup guys. I've been using the voyage 3 lg as an embedding model for the longest time and because an embedding model can't be switched and you need to fill the vector database from scratch, I didn't switch even after the release of great OS models.  \nRecently I've been thinking of switching to either qwen 3 0.6b, 4b or 8b.  \nCan anyone tell me if in terms of performance voyage 3 lg beats these 3?  \nDon't worry about the pricing. Since the documents are already ingested using voyage 3 lg, the cost has already been paid, if I switch I do need to do that process all over again.\n\nThanks in advance.",
          "author_fullname": "t2_6f9r5p4z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Embedding models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk4hx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118068,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sup guys. I&amp;#39;ve been using the voyage 3 lg as an embedding model for the longest time and because an embedding model can&amp;#39;t be switched and you need to fill the vector database from scratch, I didn&amp;#39;t switch even after the release of great OS models.&lt;br/&gt;\nRecently I&amp;#39;ve been thinking of switching to either qwen 3 0.6b, 4b or 8b.&lt;br/&gt;\nCan anyone tell me if in terms of performance voyage 3 lg beats these 3?&lt;br/&gt;\nDon&amp;#39;t worry about the pricing. Since the documents are already ingested using voyage 3 lg, the cost has already been paid, if I switch I do need to do that process all over again.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfk4hx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackkksparx",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk4hx/embedding_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk4hx/embedding_models/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754118068,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI am a medical student and had begun to spend a significant amount of time creating a clinic notebook using Notion. Problem is, I essentially have to take all the text from every pdf and PowerPoint, paste it into notion, reformat (this takes forever) only to be able to have the text searchable because it can only embed documents. Not search them.\n\nI had been reading about LLM which would essentially allow me to create a master file, upload the hundreds if not thousands of documents of medical information, and then use AI to search my documents and retrieve the info specified in the prompt.\n\nI’m just not sure if this is something I can do through ChatGPT, Claude, or using llama. Trying to become more educated in this. \n\nAny insight? Thoughts?\n\nThanks for your time.",
          "author_fullname": "t2_66d4kglu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Med school and LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhv2c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754110002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am a medical student and had begun to spend a significant amount of time creating a clinic notebook using Notion. Problem is, I essentially have to take all the text from every pdf and PowerPoint, paste it into notion, reformat (this takes forever) only to be able to have the text searchable because it can only embed documents. Not search them.&lt;/p&gt;\n\n&lt;p&gt;I had been reading about LLM which would essentially allow me to create a master file, upload the hundreds if not thousands of documents of medical information, and then use AI to search my documents and retrieve the info specified in the prompt.&lt;/p&gt;\n\n&lt;p&gt;I’m just not sure if this is something I can do through ChatGPT, Claude, or using llama. Trying to become more educated in this. &lt;/p&gt;\n\n&lt;p&gt;Any insight? Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfhv2c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IndubitablyPreMed",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhv2c/med_school_and_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhv2c/med_school_and_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754110002,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "just curious on what the community thinks how these models compare in real world use cases. I have tried glm 4.5 quite a lot and would say im pretty impressed by it. I haven't tried K2 or qwen3 coder that much yet so for now im biased towards glm 4.5\n\n  \nas now benchmarks basically mean nothing, im curious what everyone here thinks of their coding abilities according to their personal experiences",
          "author_fullname": "t2_qatlsiyq4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3 coder vs glm 4.5 vs kimi k2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf955w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754084502,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just curious on what the community thinks how these models compare in real world use cases. I have tried glm 4.5 quite a lot and would say im pretty impressed by it. I haven&amp;#39;t tried K2 or qwen3 coder that much yet so for now im biased towards glm 4.5&lt;/p&gt;\n\n&lt;p&gt;as now benchmarks basically mean nothing, im curious what everyone here thinks of their coding abilities according to their personal experiences&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf955w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "YourAverageDev_",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf955w/qwen3_coder_vs_glm_45_vs_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf955w/qwen3_coder_vs_glm_45_vs_kimi_k2/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754084502,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A quick heads up for anyone playing with the little [HuggingFaceTB/SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) model that was released a few weeks ago with llama.cpp.\n\nSmolLM3-3B supports toggling thinking mode using `/think` or `/no_think` in a system prompt, but it relies on Jinja template features that weren't available in llama.cpp's jinja processor until very recently (merged yesterday: [b56683eb](https://github.com/BradHutchings/Mmojo-Server/commit/b56683eb659d6d39138bd90b27cb258a21b7aa5c)).\n\nSo to get system-prompt `/think` and `/no_think` working, you need to be running the current master version of llama.cpp (until the next official release). I believe some Qwen3 templates might also be affected, so keep that in mind if you're using those.\n\n(And since it relies on the jinja template, if you want to be able to enable/disable thinking from the system prompt remember to pass `--jinja` to llama-cli and llama-server. Otherwise it will use a fallback template with no system prompt and no thinking.)\n\nAdditionally, I ran into a frustrating issue while using the llama-server with the built-in web client where SmolLM3-3B would stop thinking after a few messages even with thinking enabled. It turns out the model needs to see the `&lt;think&gt;&lt;/think&gt;` tags in previous messages or it will stop thinking. The llama web client, by default, has an option enabled that strips those tags.\n\nTo fix this, go to your web client settings -&gt; Reasoning and disable \"Exclude thought process when sending requests to API (Recommended for DeepSeek-R1)\".\n\nFinally, to have the web client correctly show the \"thinking\" section (that you can click to expand/collapse), you need to pass the `--reasoning-format none` option to llama-server. Example invocation:\n\n    ./llama-server --jinja -ngl 99 --temp 0.6 --reasoning-format none -c 64000 -fa -m ~/llama/models/smollm3-3b/SmolLM3-Q8_0.gguf",
          "author_fullname": "t2_38xkk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting SmolLM3-3B's /think and /no_think to work with llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeipz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099505,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A quick heads up for anyone playing with the little &lt;a href=\"https://huggingface.co/HuggingFaceTB/SmolLM3-3B\"&gt;HuggingFaceTB/SmolLM3-3B&lt;/a&gt; model that was released a few weeks ago with llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;SmolLM3-3B supports toggling thinking mode using &lt;code&gt;/think&lt;/code&gt; or &lt;code&gt;/no_think&lt;/code&gt; in a system prompt, but it relies on Jinja template features that weren&amp;#39;t available in llama.cpp&amp;#39;s jinja processor until very recently (merged yesterday: &lt;a href=\"https://github.com/BradHutchings/Mmojo-Server/commit/b56683eb659d6d39138bd90b27cb258a21b7aa5c\"&gt;b56683eb&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;So to get system-prompt &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; working, you need to be running the current master version of llama.cpp (until the next official release). I believe some Qwen3 templates might also be affected, so keep that in mind if you&amp;#39;re using those.&lt;/p&gt;\n\n&lt;p&gt;(And since it relies on the jinja template, if you want to be able to enable/disable thinking from the system prompt remember to pass &lt;code&gt;--jinja&lt;/code&gt; to llama-cli and llama-server. Otherwise it will use a fallback template with no system prompt and no thinking.)&lt;/p&gt;\n\n&lt;p&gt;Additionally, I ran into a frustrating issue while using the llama-server with the built-in web client where SmolLM3-3B would stop thinking after a few messages even with thinking enabled. It turns out the model needs to see the &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags in previous messages or it will stop thinking. The llama web client, by default, has an option enabled that strips those tags.&lt;/p&gt;\n\n&lt;p&gt;To fix this, go to your web client settings -&amp;gt; Reasoning and disable &amp;quot;Exclude thought process when sending requests to API (Recommended for DeepSeek-R1)&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Finally, to have the web client correctly show the &amp;quot;thinking&amp;quot; section (that you can click to expand/collapse), you need to pass the &lt;code&gt;--reasoning-format none&lt;/code&gt; option to llama-server. Example invocation:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server --jinja -ngl 99 --temp 0.6 --reasoning-format none -c 64000 -fa -m ~/llama/models/smollm3-3b/SmolLM3-Q8_0.gguf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?auto=webp&amp;s=049d11d6074271bc2869bc098ea0e6349fad17b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=210d7e5aa5feb8c1ba4995490dffc2c6390e22d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72ddf9689e0962d05c6c024b4b695cb9794bb6ca",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=981e7a5684252e695178248adea004772a6db062",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5512ea26289819cd2f0dfd1f239935cece32dfcf",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=434567689943a3b73e96f753c8a3258d47c9a766",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ff25afc08fd9bbaf1816eb558f97ca00295655f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfeipz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cristoper",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeipz/getting_smollm33bs_think_and_no_think_to_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeipz/getting_smollm33bs_think_and_no_think_to_work/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754099505,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just wanted to share my results running Llama-4-Scout-17B-16E-Instruct-GGUF:Q4\\_K\\_S on my Ryzen AI Max + 395 using llama.cpp with Vulkan backend and the Lemonade server. I’m getting a solid 20 tokens/second with 60 GB of GPU memory in use. ",
          "author_fullname": "t2_1rpxg2806v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S running at 20 tk/s on Ryzen AI Max + 395 with llama.cpp Vulkan + Lemonade server (60GB GPU memory)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf6gaa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/zf13w9taqggf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/zf13w9taqggf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zf13w9taqggf1/DASHPlaylist.mpd?a=1756775957%2CNTkyNmEyNTljYWVjZjg3ODIzYjY3NGI1ZjE1MjMyYjhlZTg3ZWM3YTBiNDM3OGIxY2FkMmU2MDI2ODAyOWY5Mw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 33,
              "hls_url": "https://v.redd.it/zf13w9taqggf1/HLSPlaylist.m3u8?a=1756775957%2CMzE1NWZhZTgzMGNmMmM0NDNlN2E2YjEwZTk2NzcyNzY2ZGVkNzAwOTg2YzM1Y2Y4YjVmYjRlOTRiNjY2YmU5MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ad7991508d2b549531e751e830361ac79a4409b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754077979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to share my results running Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S on my Ryzen AI Max + 395 using llama.cpp with Vulkan backend and the Lemonade server. I’m getting a solid 20 tokens/second with 60 GB of GPU memory in use. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/zf13w9taqggf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?format=pjpg&amp;auto=webp&amp;s=de129663feb9bbf95e122a859d728b746c9defbf",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9150f6d4ace971cda6add553081a915ea75aff7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0815afd00caf616bfa60e82c3f16ddc27fce172e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc775410ecfa429f782ad107df358093728a3ee5",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2de42a5c72bba5762d5890300e1ca483a478e69b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8985c8a18d1e7b326211843c594c2ffef6d43740",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06a9c35365c0dc196bcf55cc7cc2519c5f5e8e01",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "anl1eG84dGFxZ2dmMUED0vbVDpHB_6J3h9pq2feZQo01Xw2lEninALLqCef8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf6gaa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ShamanFlamingoFR",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf6gaa/llama4scout17b16einstructggufq4_k_s_running_at_20/",
          "stickied": false,
          "url": "https://v.redd.it/zf13w9taqggf1",
          "subreddit_subscribers": 509052,
          "created_utc": 1754077979,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/zf13w9taqggf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/zf13w9taqggf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zf13w9taqggf1/DASHPlaylist.mpd?a=1756775957%2CNTkyNmEyNTljYWVjZjg3ODIzYjY3NGI1ZjE1MjMyYjhlZTg3ZWM3YTBiNDM3OGIxY2FkMmU2MDI2ODAyOWY5Mw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 33,
              "hls_url": "https://v.redd.it/zf13w9taqggf1/HLSPlaylist.m3u8?a=1756775957%2CMzE1NWZhZTgzMGNmMmM0NDNlN2E2YjEwZTk2NzcyNzY2ZGVkNzAwOTg2YzM1Y2Y4YjVmYjRlOTRiNjY2YmU5MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'll need to use free tools. I am looking to make a video with this content. How do I do that? What tools should I use? How do i format this information to be processed by an AI? \n\n\\[Begin\\]\n\n The Globe wants you to believe everything opposite of physics:  \n\n1)  Heliocentrism teaches large bodies of liquid water curves into a ball. Physics says water lays flat and always seeks it's level, (thanks to the physics Law - Hydrostatic Equilibrium)\n\n2) Heliocentrism teaches we have a Big Bang Creation Story where everything spontaneously evolved from nothingness to what we have today.  Physics shows us this idea violates the 1st law of thermodynamics. \n\n3) Heliocentrism tells us Gravity is mass attracting mass. Physics shows us gas which is physical matter with mass does Not obey any silly idea of gravity. Gas always expands due to entropy to fill an available volume until equalization occurs. (Thanks to the 2nd law of thermodynamics)  \n\n4) Heliocentrism also teaches Gravity is Einstein's Gravitational Accretion where gas coalesces on itself. - (That violates the 2nd law of thermodynamics.) - \n\n5) Heliocentrism teaches gas forms a sphere in a vacuum. (what you call atmosphere) Again,  Gas always expands due to entropy to fill an available volume until equalization occurs. It cannot form a sphere in a vacuum Ever! (Again thanks to the 2nd Law of Thermodynamics.)\n\nI just gave you 5 examples (or to the untrained in science and physics, Paradoxes) how the Globe Story is purposefully deceptive because it doesn't align with actual physics and science facts.\n\nYou can learn these physics laws yourself with a study of thermodynamics at Khan Academy: The Laws of Thermodynamics and The Behavior of Gas at Chem Libre Text. \n\nNow if you think I'm Wrong then Demonstrate the claims! - You see your explanation is only good if you can Back it with Actual Physics Demonstrations. Demonstrate gas forming a sphere in a vacuum that then Fails to fully expand due to entropy until equalization occurs. (what you call Atmosphere) - Demonstrate large standing bodies of water Failing to seek their own level, Failing to lay flat and Failing to lay Horizontal. - These things Cannot be done thanks to the 2nd law of thermodynamics and hydrostatic equilibrium.\n\nLiquid water covers 70 % of Earths surface. Physics properties of liquid water (Hydrostatic Equilibrium) show it always seeks it's own level, lays flat and horizontal. Nothing, that is 70% of anything that seeks it's own level, lays flat and horizontal can Ever Be a Sphere! That's an Impossible Ratio! - Your Earth Curvature is Impossible in Physics and in Math!  \n\\[End\\]\n\nHow do make that video?  I don't know anything about AI but it uses something they choose to call prompts. That doesn't help me. ",
          "author_fullname": "t2_v7hacf3s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get this information into an AI to make a video?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy6vo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160738,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll need to use free tools. I am looking to make a video with this content. How do I do that? What tools should I use? How do i format this information to be processed by an AI? &lt;/p&gt;\n\n&lt;p&gt;[Begin]&lt;/p&gt;\n\n&lt;p&gt;The Globe wants you to believe everything opposite of physics:  &lt;/p&gt;\n\n&lt;p&gt;1)  Heliocentrism teaches large bodies of liquid water curves into a ball. Physics says water lays flat and always seeks it&amp;#39;s level, (thanks to the physics Law - Hydrostatic Equilibrium)&lt;/p&gt;\n\n&lt;p&gt;2) Heliocentrism teaches we have a Big Bang Creation Story where everything spontaneously evolved from nothingness to what we have today.  Physics shows us this idea violates the 1st law of thermodynamics. &lt;/p&gt;\n\n&lt;p&gt;3) Heliocentrism tells us Gravity is mass attracting mass. Physics shows us gas which is physical matter with mass does Not obey any silly idea of gravity. Gas always expands due to entropy to fill an available volume until equalization occurs. (Thanks to the 2nd law of thermodynamics)  &lt;/p&gt;\n\n&lt;p&gt;4) Heliocentrism also teaches Gravity is Einstein&amp;#39;s Gravitational Accretion where gas coalesces on itself. - (That violates the 2nd law of thermodynamics.) - &lt;/p&gt;\n\n&lt;p&gt;5) Heliocentrism teaches gas forms a sphere in a vacuum. (what you call atmosphere) Again,  Gas always expands due to entropy to fill an available volume until equalization occurs. It cannot form a sphere in a vacuum Ever! (Again thanks to the 2nd Law of Thermodynamics.)&lt;/p&gt;\n\n&lt;p&gt;I just gave you 5 examples (or to the untrained in science and physics, Paradoxes) how the Globe Story is purposefully deceptive because it doesn&amp;#39;t align with actual physics and science facts.&lt;/p&gt;\n\n&lt;p&gt;You can learn these physics laws yourself with a study of thermodynamics at Khan Academy: The Laws of Thermodynamics and The Behavior of Gas at Chem Libre Text. &lt;/p&gt;\n\n&lt;p&gt;Now if you think I&amp;#39;m Wrong then Demonstrate the claims! - You see your explanation is only good if you can Back it with Actual Physics Demonstrations. Demonstrate gas forming a sphere in a vacuum that then Fails to fully expand due to entropy until equalization occurs. (what you call Atmosphere) - Demonstrate large standing bodies of water Failing to seek their own level, Failing to lay flat and Failing to lay Horizontal. - These things Cannot be done thanks to the 2nd law of thermodynamics and hydrostatic equilibrium.&lt;/p&gt;\n\n&lt;p&gt;Liquid water covers 70 % of Earths surface. Physics properties of liquid water (Hydrostatic Equilibrium) show it always seeks it&amp;#39;s own level, lays flat and horizontal. Nothing, that is 70% of anything that seeks it&amp;#39;s own level, lays flat and horizontal can Ever Be a Sphere! That&amp;#39;s an Impossible Ratio! - Your Earth Curvature is Impossible in Physics and in Math!&lt;br/&gt;\n[End]&lt;/p&gt;\n\n&lt;p&gt;How do make that video?  I don&amp;#39;t know anything about AI but it uses something they choose to call prompts. That doesn&amp;#39;t help me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy6vo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DivergentDroid1",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy6vo/how_do_i_get_this_information_into_an_ai_to_make/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy6vo/how_do_i_get_this_information_into_an_ai_to_make/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754160738,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Basically title, ideally something that can process both text, images, and documents/sheets of data, as smart as possible, and as lean as possible. \n\nMy initial research led me to Phi-4, Gemma 3, and Mistral Small 3.1, but considering how fast this space progresses, I think they have probably been outdated a few gens ago. So what wouldyou suggest for a complete newb to help set-up for free for farmers? Ideally something that is good enough that even if things progress substantially it would be enough to cover basic needs I have described, and depending on the local set-up, could operate without internet and either in low-complexity low-power device, or a higher-end \"gaming\" pc?",
          "author_fullname": "t2_5ci2w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm researching some OS &amp; Local LLMs that can be useful for farmers, either in high-end PCs and in raspberry pi. Suggestions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfu41i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754150462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically title, ideally something that can process both text, images, and documents/sheets of data, as smart as possible, and as lean as possible. &lt;/p&gt;\n\n&lt;p&gt;My initial research led me to Phi-4, Gemma 3, and Mistral Small 3.1, but considering how fast this space progresses, I think they have probably been outdated a few gens ago. So what wouldyou suggest for a complete newb to help set-up for free for farmers? Ideally something that is good enough that even if things progress substantially it would be enough to cover basic needs I have described, and depending on the local set-up, could operate without internet and either in low-complexity low-power device, or a higher-end &amp;quot;gaming&amp;quot; pc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfu41i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hjras",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfu41i/im_researching_some_os_local_llms_that_can_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfu41i/im_researching_some_os_local_llms_that_can_be/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754150462,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the current recommended local LLM inference HW (**local, always-on inference box)** for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.  \nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.\n\n**What are the Hardware Options?:**\n\n* **Apple M3/M4 Ultra**\n* **AMD AI Max+ 395**\n* NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?\n\nWhat’s the most **practical prosumer option**?  \nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  \n\n\nI'm favoring the Strix Halo, but I think I'll be limited to Gemma 27B with maybe another model loaded at best.",
          "author_fullname": "t2_l0ba7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "24/7 local HW buying guide 2025-H2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj6fq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114575,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the current recommended local LLM inference HW (&lt;strong&gt;local, always-on inference box)&lt;/strong&gt; for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.&lt;br/&gt;\nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are the Hardware Options?:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Apple M3/M4 Ultra&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AMD AI Max+ 395&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What’s the most &lt;strong&gt;practical prosumer option&lt;/strong&gt;?&lt;br/&gt;\nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m favoring the Strix Halo, but I think I&amp;#39;ll be limited to Gemma 27B with maybe another model loaded at best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj6fq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xraybies",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754114575,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does the current llama.cpp binaries release support Blackwell GPU in Windows? I just got the card and not sure how to move forward.\n\nDo I need to recompile the binaries for Windows ? Please share your experience. Much appreciated.",
          "author_fullname": "t2_1vxjobhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Blackwell (RTX 5090 / RTX 6000 Pro) support in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgqb0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754106336,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does the current llama.cpp binaries release support Blackwell GPU in Windows? I just got the card and not sure how to move forward.&lt;/p&gt;\n\n&lt;p&gt;Do I need to recompile the binaries for Windows ? Please share your experience. Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfgqb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud_Structure4664",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgqb0/blackwell_rtx_5090_rtx_6000_pro_support_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfgqb0/blackwell_rtx_5090_rtx_6000_pro_support_in/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754106336,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}