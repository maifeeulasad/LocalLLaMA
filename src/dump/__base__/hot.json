{
  "kind": "Listing",
  "data": {
    "after": "t3_1m6skm6",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;&gt;&gt; Qwen3-Coder is here! ✅\n\nWe’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀\n\nAlongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! ",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is here!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qdet",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 565,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 565,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lAVBKeQpbXFJZ84JgZVPph8kD3MjUQeFX9TO1gsVqgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3-Coder is here! ✅&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀&lt;/p&gt;\n\n&lt;p&gt;Alongside the model, we&amp;#39;re also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?auto=webp&amp;s=e161efd029b20a9bcbbc26db043c320a38b26d7f",
                  "width": 2048,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0e7ce793e40e6f9057df0ac4084bef74851aa3c",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f4add6c188177de3f1a921fce3e9fdcd751975",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d3ed6788720878c02cad2db45833f254f311864",
                    "width": 320,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b",
                    "width": 640,
                    "height": 367
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca8024659f203e8b748284b077d47d512488b3",
                    "width": 960,
                    "height": 550
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348beac8c62000f3a203a6467f098a1c8a696369",
                    "width": 1080,
                    "height": 619
                  }
                ],
                "variants": {},
                "id": "kx6kRcRUBkO_mMM0khkM5jTQgMXazrrYG6wlH3UPCCs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qdet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 102,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/",
          "stickied": false,
          "url": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218847,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Available in https://chat.qwen.ai",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3- Coder 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mew9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 457,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 457,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/e6gFp_J-Dv7QIFguXfhuN4U3lDC6MMgny7SMuBnt9pI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Available in &lt;a href=\"https://chat.qwen.ai\"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?auto=webp&amp;s=a57681c6848dc38714b9bea86a26c30bed7d4d42",
                  "width": 1036,
                  "height": 695
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4a02434a648980c01b1a76032aa8e02027937c6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be152167170e73dd02f7850c4f9bb67cf143ec4a",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c86d3d06fd820523c1470692b2726d59dbaf6d3",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d63431a252098393997b8c247ff6a0a80b67f78",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "52S4zww-hEGiuCbEDUlQZAn66M2iCNb-181uTVxpyGY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6mew9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 142,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/",
          "stickied": false,
          "url": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753209850,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could this be Deepseek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 33,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6lf9s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 250,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 250,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAXw-XuvIZ9mRKeenbrWXREbY65LvO1BDwwwlpUBowY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753207666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qzkjkgegugef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qzkjkgegugef1.png?auto=webp&amp;s=982da5cfa0575f138ae47f73b6eddafc3a141895",
                  "width": 822,
                  "height": 197
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=917acda1e7d58dd2b0c466213686f858f3d1d90f",
                    "width": 108,
                    "height": 25
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac14c427611c1eca254c8bb52ac34a30bf33d9f1",
                    "width": 216,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8524670d8b150c1ad50fa23613a644190b14608f",
                    "width": 320,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e224ff9a214f929b3917304102fe92d67371e639",
                    "width": 640,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "LX0EdZ_oilMQBPfRRihja4cnZPDhv01xC24KCslp3qQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6lf9s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/",
          "stickied": false,
          "url": "https://i.redd.it/qzkjkgegugef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753207666,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone brace up for qwen !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nxh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 177,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 177,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vP7s1FThQpvmySmVJXMTU3-8PcS1dzgy5zKouaE_2IM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mn8auem2bhef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mn8auem2bhef1.png?auto=webp&amp;s=f8d9250eb919b06b9873df5541dfb4181c23ecb3",
                  "width": 1080,
                  "height": 938
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f18ccc22bd1429048af2d71903a4986f10f4370",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e9b0375cba8b59a1f2ff6a059540b35b2e80af5",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd33b6c0d9557d99a79e31264f8c7962a467e6de",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=855c907a55cf3f70afe582932d52350878ef5e68",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=727a28b13a2a4b6feeeb2646b4c5ef5d4feba605",
                    "width": 960,
                    "height": 833
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cdd53e8429b6be0b40bf16e28d255d818f7b04a",
                    "width": 1080,
                    "height": 938
                  }
                ],
                "variants": {},
                "id": "rMAWLMOw9tEiFwd35Iv66C0AmNRfGhg4PeoHVtVBYI4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6nxh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/mn8auem2bhef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753213236,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\n\nToday, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.\n\n",
          "author_fullname": "t2_e7q9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 coder will be in multiple sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qnpq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 127,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 127,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Today, we&amp;#39;re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we&amp;#39;re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qnpq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dinesh2609",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219525,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct](https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct)\n\n hyperolic already has it\n\n",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mlbk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753210248,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct\"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;hyperolic already has it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mlbk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753210248,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.\n\nVery strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.\n\nCreds [The Feature Crew](https://www.youtube.com/@TheFeatureCrew) for the original idea.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Web Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ny2q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 142,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755825166%2CMzA0OTZmZGFlZTk4N2Y2MGM2MmZlZjUxMDI1MDc2NmMxYmQ5OGM1NGU1NzA0NjVhMjJjN2FhZmQ4ZTUwZWI2Yw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755825166%2CZGIwN2E4ZDU5MGFhNzYxNWZmNGY1ODNiOWQ0YmI0MDA1ODI0ODllZjU1ODU5MDc0OGJmMTM5NzMyMTQ2YzA4Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 142,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=127481f43d7a622f7d4c23a977a165102347dc33",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt;\n\n&lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt;\n\n&lt;p&gt;Creds &lt;a href=\"https://www.youtube.com/@TheFeatureCrew\"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ob9yhvcjahef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?format=pjpg&amp;auto=webp&amp;s=9da74680d1673a7d5086bef35987945fda2390f7",
                  "width": 3024,
                  "height": 1964
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=69ec82c87ea25ca0cf09c32e6e2e65fd1ebe0353",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7e4214f7585ef0bf76db563e79ceb7b7b73df5",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=da1a4545269003e5a3164b1074444b181b803a22",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ee471871b0892b000bd102b783d4c1fea31bbdf2",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b638262bf1c7530fbee91a29e1a5465444ef5500",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a910147ce667ac1bb6fed7eb4d08a092f3974fe",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ny2q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/",
          "stickied": false,
          "url": "https://v.redd.it/ob9yhvcjahef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753213272,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755825166%2CMzA0OTZmZGFlZTk4N2Y2MGM2MmZlZjUxMDI1MDc2NmMxYmQ5OGM1NGU1NzA0NjVhMjJjN2FhZmQ4ZTUwZWI2Yw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755825166%2CZGIwN2E4ZDU5MGFhNzYxNWZmNGY1ODNiOWQ0YmI0MDA1ODI0ODllZjU1ODU5MDc0OGJmMTM5NzMyMTQ2YzA4Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen out here releasing models like it’s a Costco sample table",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qixu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/S7tH6DgPEGKSzcu1dZlyahFSv5skqhzjfdd4AVfWVi4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219204,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5eb8n31sshef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5eb8n31sshef1.png?auto=webp&amp;s=c1694040f87c60dc765d805ee64b6518e3bd108b",
                  "width": 722,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47116ec0e7ef90202d820540f88598c3cfd0a160",
                    "width": 108,
                    "height": 154
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3d8c4cb9e760fdd971704cc87722923b6445146",
                    "width": 216,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a14963b84a65b1261a6b4b6b451fce1c2285102",
                    "width": 320,
                    "height": 457
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f24e0235850da677693988507655dde73bf8e60",
                    "width": 640,
                    "height": 914
                  }
                ],
                "variants": {},
                "id": "yWOKVm4sVIve_VHC2bO92aBIp15Yh_tuiWnp6wkEtnE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m6qixu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/",
          "stickied": false,
          "url": "https://i.redd.it/5eb8n31sshef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219204,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qc8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#ab96c2",
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qc8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218772,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6vcmk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is imminent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6medy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 88,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 88,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mmNhnm_QiKDvZ8nOArY9M-gXEHPij6ccQfZ3Z4a4vrs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209818,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mruaiodv0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mruaiodv0hef1.png?auto=webp&amp;s=be215118da4d1c5ae5fc739c077ba4bbf8354f1a",
                  "width": 501,
                  "height": 251
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49a20e04a28093446580d2909236b45d1e2f568e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=772268126ecad9399aa5fb8ad3dc61fa7a8e5af0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809",
                    "width": 320,
                    "height": 160
                  }
                ],
                "variants": {},
                "id": "gxF1-bhuks7kobb2JTcsN29raeY4IvwO_eL--8kAZ38"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6medy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dudensen",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/",
          "stickied": false,
          "url": "https://i.redd.it/mruaiodv0hef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753209818,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1M token context length\n\nNo model weights yet, but Qwen3-Coder is already available for testing on [Qwen Chat](https://chat.qwen.ai)",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Available on chat.qwen.ai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mfic",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bs1O5LLiPqNQ-8leTmrD3PNczGaZCiiN00b8eacOHzE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1M token context length&lt;/p&gt;\n\n&lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href=\"https://chat.qwen.ai\"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8xj4raow0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8xj4raow0hef1.png?auto=webp&amp;s=6c9d8670b9960f64e78150ca2039fc5471464158",
                  "width": 450,
                  "height": 150
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67fbf003dc0b0cdf945b3ac5069eddaeeaf26ed5",
                    "width": 108,
                    "height": 36
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ab53d55f78ada2bc730c345d2b92f3cbd18dad6",
                    "width": 216,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83",
                    "width": 320,
                    "height": 106
                  }
                ],
                "variants": {},
                "id": "jj-Sn3KQKbzD6M9iYIRnKA0q_gXhkpIWDEoj36IQyis"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mfic",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/",
          "stickied": false,
          "url": "https://i.redd.it/8xj4raow0hef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753209889,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "It's here guys and qwen nailed it !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4aoalqp6thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebd63c731638a1db5036229c80b9ef7c6e9824fd"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4ac724a3670cd9c86d3d9eed68ced783891cd55"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27543fd2099856fe749e4022dd8e44bdf2a203ec"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c67265c054dccbb5299415ace1ae413e53f4ba40"
                },
                {
                  "y": 550,
                  "x": 960,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9060138fd041f795e2cf4d5c898b3e767230dbfb"
                },
                {
                  "y": 619,
                  "x": 1080,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aea2f77c1907ccf952d971c3b5b2331c1f6aba9"
                }
              ],
              "s": {
                "y": 1837,
                "x": 3202,
                "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=3202&amp;format=pjpg&amp;auto=webp&amp;s=8e55f7d4fbd65d488aa6606016ce609320a82186"
              },
              "id": "4aoalqp6thef1"
            },
            "mloztw07thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1da39fa93c261ee74d697606209312d155d2610"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08ecfdf3e13689eb562525b1962e0901e009181c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=873f78f3d4bc2c3fc9641a129d06524b3f3f4951"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=468a855136721f0ffae9fb7beacce6df6030447b"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b695450503944ff02456a685ccc317281f8bf8f7"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0ed9b87aa6525e3c700e7835ab55f9fe859d7aa"
                }
              ],
              "s": {
                "y": 1715,
                "x": 3306,
                "u": "https://preview.redd.it/mloztw07thef1.jpg?width=3306&amp;format=pjpg&amp;auto=webp&amp;s=a4886e1284e4cebe0e8558f5cf664cfc4c36b481"
              },
              "id": "mloztw07thef1"
            }
          },
          "name": "t3_1m6qkse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 39,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "4aoalqp6thef1",
                "id": 711818629
              },
              {
                "caption": "",
                "media_id": "mloztw07thef1",
                "id": 711818630
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6qkse",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qkse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6qkse",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6rsym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6acd0a4d30c117c56e597d84c1ebb5cedb6e4e00",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753222281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/QwenLM/qwen-code",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?auto=webp&amp;s=ea430b9854a08b70e3dd0972ad9e4758c7fc266d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb193a50d7978c33be16ebec135a318dc6943ea1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6992a1a70171bd4f98508b22498e5ac88cdc45df",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5465c1e679bafd45447bd81f6753867f296ffb49",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a9eb8de1aed9882d7841738230f2ecef892f334f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a21f51e6f25035a3147bda1057127718b3b29129",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6rsym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/",
          "stickied": false,
          "url": "https://github.com/QwenLM/qwen-code",
          "subreddit_subscribers": 502981,
          "created_utc": 1753222281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ikawrakow/ik\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp)\n\nFriendly reminder to back up all the things!",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The ik_llama.cpp repository is back! \\o/",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6cfzi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 183,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 183,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?auto=webp&amp;s=7c74a86a8d22a1d2e90ce704f456a5a36cf050e7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9057a5a31598407ca7946c278de43e70cf0c9ed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=533c07c0d65f89514a6ba54ce5f1c6649e969c77",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9699bd0efa2b26a1c034cdb0fe8abc1317589b6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=355e07ff2e46e3a253b40e25c06644c7282af5b2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c7a47a3cb456bb05dfd53a716bae5ef6addff5e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fbd5e6235d1a60da5c17ef35a7bd40a655c4d80",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6cfzi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753186412,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "See https://x.com/makingAGI/status/1947286324735856747",
          "author_fullname": "t2_1nt1n3y6xj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here who has been able to reproduce their results yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6orbr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XcNs-lUbqrAcvyj8WfRxfyYgGorJ8nCrbsxZweyByLc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753215098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See &lt;a href=\"https://x.com/makingAGI/status/1947286324735856747\"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/cfffg12fghef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/cfffg12fghef1.jpeg?auto=webp&amp;s=25f023da9eda3ae6d327e173ef9c7cba8f89880c",
                  "width": 948,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad8d41aa9654515fde6f4b396a86ebf1ad4b0687",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d65ac6156085250ea9aa09ddb48e1e0ca0d499b3",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69d24ed7a665a31761d01a39290d42a299442408",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5",
                    "width": 640,
                    "height": 591
                  }
                ],
                "variants": {},
                "id": "eXDcYXmDs3JXlXLxg7VAJanyEvhS_GKAlZPpe8O1v6Y"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6orbr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Original_Log_9899",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/",
          "stickied": false,
          "url": "https://i.redd.it/cfffg12fghef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753215098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3\\_K\\_L is runnable (though slow) for anybody with similar PC specs and produces something usable!\n\nI used LM Studio for loading the model:\n\n* Context: 4096 (default)\n* GPU Offload: 18 / 94\n* CPU Thread Pool: 16\n* ... all else default besides ...\n* Flash Attention: On\n\nWhen loaded, it used up 23.3GB of VRAM and \\~80GB of RAM.\n\nBasic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token",
          "author_fullname": "t2_8l0jj9jq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ct7u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 151,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755825166%2CZjgwNjQyNDNiMGU0Y2U0MTZkZDgxMjY1MmM1YTdhMDM3MDI3NDQ5ZTVkYWIxN2YxYTZhZDc1NmI1ZWZlNjRkZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755825166%2CNWRjNjNkZmYyZGMyMzU1ZWJlOWNmZWI4NDk5MmNhZTI5NzNiZDY4MTk0ZjlhNzQ2YmNmMTY5MGMwYWViOTEyMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 151,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=448f64b1e900a0ecbdc8a71bf39468b788eff73b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753187462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday&amp;#39;s release! I&amp;#39;ll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt;\n\n&lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096 (default)&lt;/li&gt;\n&lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool: 16&lt;/li&gt;\n&lt;li&gt;... all else default besides ...&lt;/li&gt;\n&lt;li&gt;Flash Attention: On&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;Basic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/1x5u9hrp5fef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?format=pjpg&amp;auto=webp&amp;s=58a925e2785f62712af69dad90636ab48df32160",
                  "width": 480,
                  "height": 698
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=74909995fdb7a4a31d72b707fc5a6406503d7c48",
                    "width": 108,
                    "height": 157
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8bc0a3ee9ac60bd5820c383ea906cb68b5b9d1d",
                    "width": 216,
                    "height": 314
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4b70a04846a7db43f33c721283fb6e592dd8d570",
                    "width": 320,
                    "height": 465
                  }
                ],
                "variants": {},
                "id": "MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ct7u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aidanjustsayin",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/",
          "stickied": false,
          "url": "https://v.redd.it/1x5u9hrp5fef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753187462,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755825166%2CZjgwNjQyNDNiMGU0Y2U0MTZkZDgxMjY1MmM1YTdhMDM3MDI3NDQ5ZTVkYWIxN2YxYTZhZDc1NmI1ZWZlNjRkZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755825166%2CNWRjNjNkZmYyZGMyMzU1ZWJlOWNmZWI4NDk5MmNhZTI5NzNiZDY4MTk0ZjlhNzQ2YmNmMTY5MGMwYWViOTEyMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unsloth quants already starting to roll out for Qwen3-Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6u0gt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=df2ca14ad32406cbfd2154f6392b11b3062c0b80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753227930,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?auto=webp&amp;s=c6a55f1fe010145ae8782e1593f28ec04aee30a9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ca2157367c76507911bd02cc27f2bd77fdeb58f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a1260c12ba170ca3258b3d164bd71b26d3fd637",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82c9ef33075d79d05f812d774d3d9963a2ca93c2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=651f424884542b7c34073b3bc62c0fc1b199eaae",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=287c77e1abcef6dc2ad9cac5e8a8d70a85c3f900",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6671827d2f6adf5ce554df23310df3e1e4805228",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u0gt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/",
          "stickied": false,
          "url": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "subreddit_subscribers": 502981,
          "created_utc": 1753227930,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Strix Halo \"Ryzen AI MAX\" APUs Come To DIY PC Builders With New MoDT \"Mini-ITX\" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6bddm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 114,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 114,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=140&amp;height=81&amp;crop=140:81,smart&amp;auto=webp&amp;s=00290105ce815b049672399f0e7e28e9d1afcbc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753183102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?auto=webp&amp;s=10c3b72c0b82b9a62677b9306104bb21064031ab",
                  "width": 2471,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=097bbe6e55a92f58db20c497b5cd55b71c248bb0",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20e37d29ee5af324410ac9397017c9ae2f497b28",
                    "width": 216,
                    "height": 125
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9a76caea9352dd47f6185e29c67bab1c2374a02",
                    "width": 320,
                    "height": 186
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01",
                    "width": 640,
                    "height": 372
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92e328c881739b5b57de5c1f24a60a877d78657",
                    "width": 960,
                    "height": 559
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61c8a758226b77b202d02e702bc26f1ac195c306",
                    "width": 1080,
                    "height": 629
                  }
                ],
                "variants": {},
                "id": "wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6bddm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/",
          "stickied": false,
          "url": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753183102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is available on OpenRouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6u3kd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9352d9aaa19f84f05307725c60c6280cb5ce4153",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753228157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/qwen/qwen3-coder",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?auto=webp&amp;s=8a80c032c084b7af008f30d36302aa3e2b303841",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=81bbe5b26b024567de7a02963aa1047661c30d21",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31982c88f4e8f0ba3e19de3cb4fe1aecc0737271",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4de872e3b3e085cf7e3edcad2410dce6e017ff0c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f4da7fa00b2fee69899af4df9a137f3645df9e7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b233723391e8a2e702cc58632dd60a7a281a8fbd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d4e480d1b5455587e7a48ddb01c62b2f0bdbd5a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u3kd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/",
          "stickied": false,
          "url": "https://openrouter.ai/qwen/qwen3-coder",
          "subreddit_subscribers": 502981,
          "created_utc": 1753228157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MegaTTS 3 voice cloning is here!\n\nFor context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.\n\nRecently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: [https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT](https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT) with quite promising results.\n\nI reuploaded the weights to Hugging Face: [https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning](https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning)\n\nAnd put up a quick Gradio demo to try it out: [https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning](https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning)\n\nOverall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!\n\nh/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MegaTTS 3 Voice Cloning is Here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m641zg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 359,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 359,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca44b1060cf304798e39247090bed7e9f195130b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753156417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt;\n\n&lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt;\n\n&lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href=\"https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT\"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt;\n\n&lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href=\"https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning\"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href=\"https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning\"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt;\n\n&lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?auto=webp&amp;s=0e8f184606f9f3e558a6971b8dfbfc9a3f0d1af8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8ad97c6cb72e96abaf27c1cc2565dda7970c68",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85f1ec201fac1de1a714a3b74b2040ea838d357f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=623e3d06175018d3caaaf85d7742c402b0f0a84d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13bd3c86a79666218395f17439b714df6a5fc52c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08124b488d263f78d5cebc4fffc2a8bd5fa5f05b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3c4e8f40ef1fb69f68df6601b917c02f65c89ad",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m641zg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "subreddit_subscribers": 502981,
          "created_utc": 1753156417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.\n\n# The problem: too many options\n\nI've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.\n\nHere's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (`&lt;think&gt;...&lt;/think&gt;`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\\_p, etc), and much more.\n\n# How a focus on usability turned into over 2000 test cases\n\nI wanted things to \"just work\" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.\n\nTo make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.\n\nThe solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.\n\n# Wait, doesn't that cost a lot of money and take forever?\n\n**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.\n\nOur blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).\n\n# The Result\n\nThe end result is that it's much easier to rapidly evaluate AI models and methods. It includes\n\n* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.\n* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).\n\nHowever, you're in control. You can always override any suggestion.\n\n# Next Step: A Giant Ollama Server\n\nI can run a decent sampling of our Ollama tests locally, but I lack the \\~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!\n\n# How to Find the Best Model for Your Task with Kiln\n\nAll of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.\n\nKiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.\n\nTo get started, check out the tool or our guides:\n\n* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)\n* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)\n* [Kiln Discord](https://getkiln.ai/discord)\n* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)\n\nI'm happy to answer questions if anyone wants to dive deeper on specific aspects!",
          "author_fullname": "t2_slbscky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote 2000 LLM test cases so you don't have to",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6gq8e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753199300,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753197164,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.&lt;/p&gt;\n\n&lt;h1&gt;The problem: too many options&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;ve been building &lt;a href=\"https://github.com/kiln-ai/kiln\"&gt;Kiln AI&lt;/a&gt;: an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn&amp;#39;t easy. If evaluating an additional model is painful, you&amp;#39;re less likely to do it, which makes you less likely to find the best way to run your AI workload.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (&lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt;), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top_p, etc), and much more.&lt;/p&gt;\n\n&lt;h1&gt;How a focus on usability turned into over 2000 test cases&lt;/h1&gt;\n\n&lt;p&gt;I wanted things to &amp;quot;just work&amp;quot; as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.&lt;/p&gt;\n\n&lt;p&gt;To make it easy to use, we needed reasonable defaults for every major model. That&amp;#39;s no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.&lt;/p&gt;\n\n&lt;p&gt;The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.&lt;/p&gt;\n\n&lt;h1&gt;Wait, doesn&amp;#39;t that cost a lot of money and take forever?&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Yes it does!&lt;/strong&gt; Each time we run these tests, we&amp;#39;re making thousands of LLM calls against a wide variety of providers. There&amp;#39;s no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn&amp;#39;t an option.&lt;/p&gt;\n\n&lt;p&gt;Our blog has some details on the &lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\"&gt;Python pytest setup we used to make this manageable&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h1&gt;The Result&lt;/h1&gt;\n\n&lt;p&gt;The end result is that it&amp;#39;s much easier to rapidly evaluate AI models and methods. It includes&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.&lt;/li&gt;\n&lt;li&gt;Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, you&amp;#39;re in control. You can always override any suggestion.&lt;/p&gt;\n\n&lt;h1&gt;Next Step: A Giant Ollama Server&lt;/h1&gt;\n\n&lt;p&gt;I can run a decent sampling of our Ollama tests locally, but I lack the ~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I&amp;#39;d love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!&lt;/p&gt;\n\n&lt;h1&gt;How to Find the Best Model for Your Task with Kiln&lt;/h1&gt;\n\n&lt;p&gt;All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.&lt;/p&gt;\n\n&lt;p&gt;Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.&lt;/p&gt;\n\n&lt;p&gt;To get started, check out the tool or our guides:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/\"&gt;Kiln AI on Github - over 3900 stars&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://docs.getkiln.ai/docs/quickstart\"&gt;Quickstart Guide&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/discord\"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\"&gt;Blog post with more details on our LLM testing (more detailed version of above)&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?auto=webp&amp;s=23e4ff0dbe2d03ff352aea774053e4e9cdb80d20",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9815f077288b33817e75895d23e661f1193778",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df51b519d6d99631039f2563f587d4f7fb7f337",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=584735f7b916c00d422195a7ea012563d4e134db",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ceb01849b330103f92aaf6b1331cd97e415c722",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0594f7e041119a136f22914764b2a128e73d5ff",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=415b728bd16022b553cb45cb75a1a8fee65a2e5b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6gq8e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "davernow",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753197164,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/lechmazur/bazaar](https://github.com/lechmazur/bazaar)\n\nEach LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.\n\nFour market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.\n\nKey Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a \"truthful\" baseline (bid your exact value).\n\nAll agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.\n\nBAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.\n\nWith chat enabled, LLMs form illegal cartels.\n\n",
          "author_fullname": "t2_p2tr0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "A new LLM benchmark for markets, supply chains, and trading: BAZAAR. Agents must understand supply, demand, and risk, and learn to bid strategically.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 130,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "krskqj5kxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52842a9e07595d9c9c3601a2dea21160adf1de8a"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51debf7127ddb9eb252f98403727de4ea71e3b26"
                },
                {
                  "y": 220,
                  "x": 320,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc37da5f71f16ac60f4021f12058eb6f7d1d1ffa"
                },
                {
                  "y": 440,
                  "x": 640,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd7e2d668330dc48f70489a82adc44bb6ac8b783"
                },
                {
                  "y": 660,
                  "x": 960,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20373c287598ec9c9ec1b3675c4e6bfc60416e91"
                },
                {
                  "y": 742,
                  "x": 1080,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b83f6051676e0cda84cceb93c1ed1814a2480891"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1600,
                "u": "https://preview.redd.it/krskqj5kxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=4ff785353e78862c3e5aa1051c1ed608279d4ce2"
              },
              "id": "krskqj5kxgef1"
            },
            "sm28w3jkxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a106a7c31b839e001fb11d95974dd872e057d3ea"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=abdee405d15714aab50ee9bac28da807c9c5dce5"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ee13743536f18f83ee647bad91676e134d8b73"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=68347db5d6b681079896f10816cf011e6ea016f7"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba2a511f53896c7fcf10c27c5dd3474ac86cebaa"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b99c23960f58a2a10376f2b66638477cbb0759e"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1760,
                "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=1760&amp;format=png&amp;auto=webp&amp;s=0dd57e157ff15bab79010c7c0f70eb09a62fd728"
              },
              "id": "sm28w3jkxgef1"
            },
            "1lxnjuhjxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=72e24d5b1d0e1dd0daa8f02a9b2f774f306babb2"
                },
                {
                  "y": 201,
                  "x": 216,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1def4cd9fb7732ec050a5cd207c21d461eec1206"
                },
                {
                  "y": 298,
                  "x": 320,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c280e397d6012029315b0e383ea58668cf027866"
                },
                {
                  "y": 597,
                  "x": 640,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=760be0af867d8774678164fab47b6cc0171286f3"
                },
                {
                  "y": 896,
                  "x": 960,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17061c04c488808ca4b62703ccc6ec12c844d024"
                },
                {
                  "y": 1008,
                  "x": 1080,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15167538cbae4dd9f2ff0fa9f7ac8aac5a4a2975"
                }
              ],
              "s": {
                "y": 1400,
                "x": 1500,
                "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=249b3b40abc90da4b77f7d32efeb03508565afde"
              },
              "id": "1lxnjuhjxgef1"
            },
            "hpjd1p6ixgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6293b2a19de7d9ba0bbaab4f4dcb0149ebed7f9e"
                },
                {
                  "y": 201,
                  "x": 216,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa5d9cccc426c744502b2fcb6d56ec099fafef8a"
                },
                {
                  "y": 298,
                  "x": 320,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fba153c47c966a618a5ab2e86458569dc3546eb"
                },
                {
                  "y": 597,
                  "x": 640,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d92f53373a034c556e7508e70078158b5109615"
                },
                {
                  "y": 896,
                  "x": 960,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee327f3041a80dee0a0cc5ba5125fb752a8e87df"
                },
                {
                  "y": 1008,
                  "x": 1080,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a74df25ea42c2f72fd616e87306c704f3e0ed510"
                }
              ],
              "s": {
                "y": 1400,
                "x": 1500,
                "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=8e0c2e04bb81b4cafd539e93483917338b2a7cbf"
              },
              "id": "hpjd1p6ixgef1"
            },
            "halpj9alxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71af75f43565b17c222ee28e6536557bf0c3b66b"
                },
                {
                  "y": 316,
                  "x": 216,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf6edfa44cc6dd4c0857a1d46e9e8430115b516"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b38103d8019a9abc83c8d4e322eac7e3f93e149"
                },
                {
                  "y": 938,
                  "x": 640,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=803ba0808b04d93186cedc3fd4412004029ce523"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4403712c39c9d5c921f773419ac84c9c967acdab"
                },
                {
                  "y": 1584,
                  "x": 1080,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b02503a0234e0e29e9cc7cbda79516bc27d5ce8d"
                }
              ],
              "s": {
                "y": 2200,
                "x": 1500,
                "u": "https://preview.redd.it/halpj9alxgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=36b00f321d44dbba4e8b0c7041f074a668f9f5cc"
              },
              "id": "halpj9alxgef1"
            },
            "kbimogvjxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=21566b4eaa8e0018afdbcde3998ab0c87bec3864"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2e518755ea98beae9b0afc7a74e41272fec936a"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a101a0df12dcd241bd826dbfacefcce95f5a07d"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=158915b11d94f85eb7c4d52f3d5a5666038e173d"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c11c0e086b3dff1a9c401a1b5d827e8decdba3d2"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01e4040beb9b0c46060f90a7ef583b60fd18db8e"
                }
              ],
              "s": {
                "y": 1000,
                "x": 1600,
                "u": "https://preview.redd.it/kbimogvjxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=f440c7f305ed6913c05879610375ba51cf28b48a"
              },
              "id": "kbimogvjxgef1"
            },
            "n6pbexvixgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c138d9032ae311f137755885cc0fcc0f6238e39"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33900c58d0c49879f33d0b02cf46e500ecc981a"
                },
                {
                  "y": 220,
                  "x": 320,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5abe3365b02062b47ea098312f8e91aaa9b1c020"
                },
                {
                  "y": 440,
                  "x": 640,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d65f8cad0c9583d4dba040e27503305982260bd"
                },
                {
                  "y": 660,
                  "x": 960,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2870cf430de5b5aee66e3c2a277bac99eb35522"
                },
                {
                  "y": 742,
                  "x": 1080,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2bb320350260da19bfcd63aad9e05c223ff7c949"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1600,
                "u": "https://preview.redd.it/n6pbexvixgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=ab9cb5daeba91d7a5dcc8702e8682e511b2c39c4"
              },
              "id": "n6pbexvixgef1"
            },
            "ynn314skxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 101,
                  "x": 108,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1999d49a70e9400e739c2a68cfaa8117e059ceb7"
                },
                {
                  "y": 202,
                  "x": 216,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c0408ab7d16d8a0ffa87e03021c2e84a7cd0f47"
                },
                {
                  "y": 300,
                  "x": 320,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d09aa8b184cc4a2b2acb4c7d41278907884d79d8"
                },
                {
                  "y": 600,
                  "x": 640,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2bd12e78edee4049b2ea5f39bd728943a1f72cf"
                },
                {
                  "y": 900,
                  "x": 960,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7d2d61c313d3bcf2e9dbab634c3444733306afb"
                },
                {
                  "y": 1012,
                  "x": 1080,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8384cbde378a8494e262b0397b8c7e0f21c606f5"
                }
              ],
              "s": {
                "y": 1500,
                "x": 1600,
                "u": "https://preview.redd.it/ynn314skxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=70915d68ffb1eb141632310b69caba6aa08dc162"
              },
              "id": "ynn314skxgef1"
            }
          },
          "name": "t3_1m6m0f7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 22,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "hpjd1p6ixgef1",
                "id": 711713756
              },
              {
                "media_id": "n6pbexvixgef1",
                "id": 711713757
              },
              {
                "media_id": "1lxnjuhjxgef1",
                "id": 711713758
              },
              {
                "media_id": "kbimogvjxgef1",
                "id": 711713759
              },
              {
                "media_id": "krskqj5kxgef1",
                "id": 711713760
              },
              {
                "media_id": "sm28w3jkxgef1",
                "id": 711713761
              },
              {
                "media_id": "ynn314skxgef1",
                "id": 711713762
              },
              {
                "media_id": "halpj9alxgef1",
                "id": 711713763
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EFJzlTp5mgKfQzJuDpD-TjIuXrwPWnGGidBsBvFKiPg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753208973,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/lechmazur/bazaar\"&gt;https://github.com/lechmazur/bazaar&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.&lt;/p&gt;\n\n&lt;p&gt;Four market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.&lt;/p&gt;\n\n&lt;p&gt;Key Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a &amp;quot;truthful&amp;quot; baseline (bid your exact value).&lt;/p&gt;\n\n&lt;p&gt;All agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.&lt;/p&gt;\n\n&lt;p&gt;BAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.&lt;/p&gt;\n\n&lt;p&gt;With chat enabled, LLMs form illegal cartels.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6m0f7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6m0f7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zero0_one1",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6m0f7/a_new_llm_benchmark_for_markets_supply_chains_and/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6m0f7",
          "subreddit_subscribers": 502981,
          "created_utc": 1753208973,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A while back I posted some [Strix Halo LLM performance testing](https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/) benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).\n\nThe biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.\n\nThis is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp \n\nAll the full data and latest info is available in the Github repo: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench) but here are the topline stats below:\n\n# Strix Halo LLM Benchmark Results\n\nAll testing was done on pre-production [Framework Desktop](https://frame.work/desktop) systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)\n\nExact testing/system details are in the results folders, but roughly these are running:\n\n* Close to production BIOS/EC\n* Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1\n* Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels\n* Recent llama.cpp builds (eg b5863 from 2005-07-10)\n\nJust to get a ballpark on the hardware:\n\n* \\~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)\n* theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is *much* lower\n\n# Results\n\n# Prompt Processing (pp) Performance\n\nhttps://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan||998.0|46.5|4237|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|HIP|hipBLASLt|906.1|40.8|4720|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|HIP|hipBLASLt|878.2|37.2|5308|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|fa=1|604.8|66.3|17527|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|HIP|hipBLASLt|316.9|13.6|14638|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1|270.5|17.1|68785|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|HIP|hipBLASLt|264.1|17.2|59720|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|HIP rocWMMA||94.7|4.5|41522|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n\n# Text Generation (tg) Performance\n\nhttps://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|b=256|591.1|72.0|17377|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|Vulkan|fa=1|620.9|47.9|4463|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan|fa=1|1014.1|45.8|4219|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|Vulkan|fa=1|614.2|42.0|5333|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|Vulkan|fa=1 b=256|146.1|19.3|59917|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1 b=256|223.9|17.1|68608|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|Vulkan|fa=1|119.6|14.3|14540|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|Vulkan|fa=1|26.4|5.0|41456|\n\n# Testing Notes\n\nThe best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.\n\nThere's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).\n\nOne thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.\n\nUnlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).\n\nFor testing, the HIP backend, I highly recommend trying `ROCBLAS_USE_HIPBLASLT=1` as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) `HSA_OVERRIDE_GFX_VERSION=11.0.0` \\- in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔",
          "author_fullname": "t2_eztox",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "mjr2d31ujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80836e3346fcd0e6847fcb3f1d33c5f2ac3c12e3"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd34de136a42ad65ac9facd37455912c1a13410"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=675c17530bfdbbd8ae2830c3b694e07b5163a1b0"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdc9a2d7260d9fd3acbab23e12917b54e651493"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e051180cc70357e18ffe4725601fb9811ba1193"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75e938dac2457cc4191363a5cd8cbcaf777049ed"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"
              },
              "id": "mjr2d31ujeef1"
            },
            "7y0pdbqujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=493fe8a11d9ee1d7a485e1b7233ca7e945637599"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c43022989f8b24ef64216d0b17642e3f80013763"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7f683ec0155e0b8f196a71bccaf428b44550399"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb65c24153d597ae592f5012aabff2a638b88357"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f57d68c01ba672fba3a5062a2ec12ac45a621fac"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59b563c8992d2568941d7c73829b8f7ebc4f5585"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"
              },
              "id": "7y0pdbqujeef1"
            }
          },
          "name": "t3_1m6b151",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753182004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while back I posted some &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/\"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I&amp;#39;m back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt;\n\n&lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt;\n\n&lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt;\n\n&lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href=\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt;\n\n&lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt;\n\n&lt;p&gt;All testing was done on pre-production &lt;a href=\"https://frame.work/desktop\"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt;\n\n&lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Close to production BIOS/EC&lt;/li&gt;\n&lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt;\n&lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt;\n&lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt;\n&lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;998.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4237&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;906.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;40.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;878.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;5308&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;17527&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;316.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14638&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;270.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68785&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;264.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;59720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP rocWMMA&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;94.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;41522&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;591.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;72.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17377&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;620.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;47.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4463&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1014.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;45.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4219&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;5333&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;146.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;59917&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;223.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68608&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;119.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;14540&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26.4&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;41456&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Testing Notes&lt;/h1&gt;\n\n&lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#&amp;#39;s might be a bit much).&lt;/p&gt;\n\n&lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it&amp;#39;s now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt;\n\n&lt;p&gt;Unlike last time, I won&amp;#39;t be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt;\n\n&lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I&amp;#39;ve found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6b151",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "randomfoo2",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753182004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?",
          "author_fullname": "t2_10rx6s0f1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone tried Hierarchical Reasoning Models yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6ufm4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753229040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ufm4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackboulder33",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753229040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.\n\nThat said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)\n\nAI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.\n\nLike how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.\n\nIt’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.\n\nWhat are your thoughts on this?",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI should just be open-source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67zde",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753198579,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753170472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.&lt;/p&gt;\n\n&lt;p&gt;That said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)&lt;/p&gt;\n\n&lt;p&gt;AI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.&lt;/p&gt;\n\n&lt;p&gt;Like how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.&lt;/p&gt;\n\n&lt;p&gt;It’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67zde",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 83,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753170472,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We had a lot of posts about the updated [235b model](https://x.com/Alibaba_Qwen/status/1947344511988076547) and the [Unsloth quants](https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF). I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.\n\nRuns great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.\n\n[More details on the Ollama library page](https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl), performance benchmarks included.",
          "author_fullname": "t2_1gpif4cz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The LLM for M4 Max 128GB: Unsloth Qwen3-235B-A22B-Instruct-2507 Q3 K XL for Ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ocfd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/FGAC4_FgxMG-7e9w8V8PkDHHC0Spkue03KT-5Vo9mU4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753214166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We had a lot of posts about the updated &lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;235b model&lt;/a&gt; and the &lt;a href=\"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF\"&gt;Unsloth quants&lt;/a&gt;. I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.&lt;/p&gt;\n\n&lt;p&gt;Runs great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl\"&gt;More details on the Ollama library page&lt;/a&gt;, performance benchmarks included.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y3x24rxqchef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y3x24rxqchef1.png?auto=webp&amp;s=8871c292b16bce4a1a3ebad50bdc70a4755edfb1",
                  "width": 1119,
                  "height": 699
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e41bb7b82dd23ca399246b0ad273bfca55313312",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8196b3b26f6cebea121b85e8d13e70ccede750b",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=71b905987f840aecad2d4d33ebe4b67e00e55446",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c4dd5da6091ae77e58d63dfd95935c34e266d7e",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d2b6677488033099a15dec1df1c7088540544b7",
                    "width": 960,
                    "height": 599
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96a500d949b1758bb6f522dd4c3912c8e4c57f64",
                    "width": 1080,
                    "height": 674
                  }
                ],
                "variants": {},
                "id": "65hJ7hQzTtv2DTpl4kNAtfCBSaTl0aIGG0bqSxzPvcM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6ocfd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "waescher",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ocfd/the_llm_for_m4_max_128gb_unsloth/",
          "stickied": false,
          "url": "https://i.redd.it/y3x24rxqchef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753214166,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, i was scrolling on LM studio and always saw model like \"model_name_q4_k_m.gguf\" everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some \"dynamic quantization\" but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?\n\nthank by advance ",
          "author_fullname": "t2_jgegifux8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does the _K _S _M _L mean behind the quantization of a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6tbhm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226118,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i was scrolling on LM studio and always saw model like &amp;quot;model_name_q4_k_m.gguf&amp;quot; everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some &amp;quot;dynamic quantization&amp;quot; but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?&lt;/p&gt;\n\n&lt;p&gt;thank by advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6tbhm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hurtcraft01",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753226118,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.\n\nTo find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example. \n\nBut there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn't simply plan better meetings instead.\n\nWhat do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?",
          "author_fullname": "t2_4tnm5az4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Digital twins that attend meetings for you. Dystopia or soon reality?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6pw0o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wzygbrp0nhef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/wzygbrp0nhef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wzygbrp0nhef1/DASHPlaylist.mpd?a=1755825166%2CMDZlZWJkOTMzOGJjM2VjMWE2MmUwYTcyYjgzNGY3OGQ0MzA4MTlmNjAzOGU4OTlkM2IzZDFlOTUzNzgyZDBhZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 132,
              "hls_url": "https://v.redd.it/wzygbrp0nhef1/HLSPlaylist.m3u8?a=1755825166%2CZTA3OWM2ODViODUxZDM4YWQyODc1NTJmNDkzMjFjNjU1MjU1MzY3OTBhNWU3MDYxZDY4ZWI5YWJiODYwZThjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a69584394b29451108583c0345b3b4af510dbdc1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753217710,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.&lt;/p&gt;\n\n&lt;p&gt;To find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example. &lt;/p&gt;\n\n&lt;p&gt;But there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn&amp;#39;t simply plan better meetings instead.&lt;/p&gt;\n\n&lt;p&gt;What do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wzygbrp0nhef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?format=pjpg&amp;auto=webp&amp;s=364cf4d9aed4af7c6e67f0ad4201a809f25338cf",
                  "width": 1662,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3f0710b64e022e54027bd6589659498e18682ba",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98174bca9eb25e7cc20a591cc15f5ea33c8b5a11",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed0becb52791f8efb623b51e5eda057c3e3bde52",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7708460ab1cd8fdd958ab10e8b4301f392326d",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=91df4abe288ad6402853ff17541bfa618d570410",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7c1aaaa8918b2d9857a12c93d828793e424f997c",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6pw0o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DerErzfeind61",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6pw0o/digital_twins_that_attend_meetings_for_you/",
          "stickied": false,
          "url": "https://v.redd.it/wzygbrp0nhef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753217710,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wzygbrp0nhef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/wzygbrp0nhef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wzygbrp0nhef1/DASHPlaylist.mpd?a=1755825166%2CMDZlZWJkOTMzOGJjM2VjMWE2MmUwYTcyYjgzNGY3OGQ0MzA4MTlmNjAzOGU4OTlkM2IzZDFlOTUzNzgyZDBhZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 132,
              "hls_url": "https://v.redd.it/wzygbrp0nhef1/HLSPlaylist.m3u8?a=1755825166%2CZTA3OWM2ODViODUxZDM4YWQyODc1NTJmNDkzMjFjNjU1MjU1MzY3OTBhNWU3MDYxZDY4ZWI5YWJiODYwZThjMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a ***Private*** eval that has been updated for over a year by Zhihu user \"toyama nao\".  So qwen cannot be benchmaxxing on it because it is ***Private*** and the questions are being updated constantly.\n\nThe score of this 2507 update is amazing, especially since it's a non-reasoning model that ranks among other reasoning ones.\n\n[logic](https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9)\n\n[coding](https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc)\n\n\\*These 2 tables are OCR and translated by gemini, so it may contain small errors\n\nDo note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese\n\nSource:\n\n[Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873](Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873)",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Private Eval result of Qwen3-235B-A22B-Instruct-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 30,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "q1ld1vkvcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 17,
                  "x": 108,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6db0188556e8b5a6ad0b4e78e643d750f9c8eb3b"
                },
                {
                  "y": 34,
                  "x": 216,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=def0b755adbe43c0ef6c669ce96229bdf8642e8d"
                },
                {
                  "y": 51,
                  "x": 320,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3eabe87b380ad52a47b2c07b92750b4da2580788"
                },
                {
                  "y": 102,
                  "x": 640,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6df88708d1d58d4dfb207f2e3d4de7ff7ba44f34"
                },
                {
                  "y": 153,
                  "x": 960,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=52be3772b829b14a916bd0dba033163893df170e"
                },
                {
                  "y": 172,
                  "x": 1080,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83243c7b94f1c3bbb4c4965fc1926369f860f03a"
                }
              ],
              "s": {
                "y": 211,
                "x": 1319,
                "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc"
              },
              "id": "q1ld1vkvcdef1"
            },
            "s5t1rm4dcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 23,
                  "x": 108,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c77a3824658d5cc719a3b06a8dad382fb80e75b0"
                },
                {
                  "y": 46,
                  "x": 216,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cec128245e15150bcd1e79751e2e6d55996d697"
                },
                {
                  "y": 69,
                  "x": 320,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89eced78277fb9616c09bbe0f9fd5df078cbd54e"
                },
                {
                  "y": 139,
                  "x": 640,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=043d1bb82c725708bb35a58acd4d7c9504c7de3a"
                },
                {
                  "y": 208,
                  "x": 960,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=28844535a11fae5fff659cb0b949c1d842e4569e"
                }
              ],
              "s": {
                "y": 229,
                "x": 1054,
                "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9"
              },
              "id": "s5t1rm4dcdef1"
            }
          },
          "name": "t3_1m66qks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/z5Hij9VVgEz-n0a_TqfphvLzGcPNiRWrZnXSAwXHg_Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753165706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; eval that has been updated for over a year by Zhihu user &amp;quot;toyama nao&amp;quot;.  So qwen cannot be benchmaxxing on it because it is &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; and the questions are being updated constantly.&lt;/p&gt;\n\n&lt;p&gt;The score of this 2507 update is amazing, especially since it&amp;#39;s a non-reasoning model that ranks among other reasoning ones.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9\"&gt;logic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc\"&gt;coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;*These 2 tables are OCR and translated by gemini, so it may contain small errors&lt;/p&gt;\n\n&lt;p&gt;Do note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese&lt;/p&gt;\n\n&lt;p&gt;Source:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873\"&gt;Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m66qks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753165706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 Released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5owi8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 815,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 815,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sKHygFxyatHEivMjwhoU0rKccpX3n5vMlMuGtN0ebyc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118247,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?auto=webp&amp;s=2ca86b1c53db0d11a0c488d1d12c8c9cb55eaf20",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c0c862769016dce18130a1fb791dbf78757f922",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8596c55842b58dd3bf4190c4c47e309432ad77",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d51316624bbec96c8a0b28b2e3756e68ffadf98",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b66905b8a3f2f560c571babd372861e032c5ca94",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72cbdd4b47610ef9ddfdf989b7900703487934d6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DZ_J_yAfR8TLjLmR0s6ZMb4IqBdDowTQUhHZ335Z0r8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5owi8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 244,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/",
          "stickied": false,
          "url": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "subreddit_subscribers": 502981,
          "created_utc": 1753118247,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What options do we have for Qwen3 Coder, either local or cloud services?",
          "author_fullname": "t2_mxdkomgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the cheapest option for hosting llama cpp with Qwen Coder at Q8?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nvhs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753213772,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753213111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What options do we have for Qwen3 Coder, either local or cloud services?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6nvhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Available_Driver6406",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753213111,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!\\_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.\n\nRight now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:\n\n* Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?\n* Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?\n* How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?\n\nWould love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.",
          "author_fullname": "t2_1tltnwoxsz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people staging AI training datasets from NVMe → DDR5 → GPU VRAM for fine-tuning on RTX 5090s?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6vj8o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755825166%2CNzQ2Njk0Y2U4MGRmYTFiNzA4MjBkOTc0NDE3Njg3ZDNkYTUxY2YwMTEyMDMwNmEwZWY0NWZiYTYwZDFiNWY5Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755825166%2CNDVmMzIwNjQ3ZWM2NjI5YzdjMGQ1OWFhNTMyMDViZGUwYTY1NGJjZGMzMjVmMmJmZWQwNWI5ZjBlNDYzODA5NQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7fa7bed1c348998994fed16cd386547e5aac176b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753232111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.&lt;/p&gt;\n\n&lt;p&gt;Right now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?&lt;/li&gt;\n&lt;li&gt;Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?&lt;/li&gt;\n&lt;li&gt;How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/m3v13th5vief1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?format=pjpg&amp;auto=webp&amp;s=8a06bae144080a3451d0fb255b750bcab9e21c69",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=930f76891585f27565f3d929f2d1d4df9fbbe6f7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2cc4160f0d866354b83bad0ce200177193907cd",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=337db9c93858d2e6c9db6e22822d525e7600240d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1915548352adb0259f40c35397f4626912fc93d4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e2a49235d59469a5f29b50b3c42efc9cc7f4d39",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81350e82df349faf24b9ef86ecafb7b97303ebd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6vj8o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DJAI9LAB",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/",
          "stickied": false,
          "url": "https://v.redd.it/m3v13th5vief1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753232111,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755825166%2CNzQ2Njk0Y2U4MGRmYTFiNzA4MjBkOTc0NDE3Njg3ZDNkYTUxY2YwMTEyMDMwNmEwZWY0NWZiYTYwZDFiNWY5Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755825166%2CNDVmMzIwNjQ3ZWM2NjI5YzdjMGQ1OWFhNTMyMDViZGUwYTY1NGJjZGMzMjVmMmJmZWQwNWI5ZjBlNDYzODA5NQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.\n\nI've been looking at Kimi, but I've been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.\n\nWas wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.",
          "author_fullname": "t2_ijzb7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Epyc Qwen3 235B Q8 speed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6h67y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753198166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at Kimi, but I&amp;#39;ve been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.&lt;/p&gt;\n\n&lt;p&gt;Was wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6h67y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MidnightProgrammer",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753198166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m currently exploring multimodal LLMs — specifically models that can handle image input (like OCR, screenshot analysis, or general image understanding). I’m curious if anyone here has successfully deployed one of these models on a VPS.",
          "author_fullname": "t2_i9xt9vl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone here worked with LLMs that can read images? Were you able to deploy it on a VPS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6ucc0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753228806,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m currently exploring multimodal LLMs — specifically models that can handle image input (like OCR, screenshot analysis, or general image understanding). I’m curious if anyone here has successfully deployed one of these models on a VPS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ucc0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent-Cow4848",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ucc0/has_anyone_here_worked_with_llms_that_can_read/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ucc0/has_anyone_here_worked_with_llms_that_can_read/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753228806,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey!\n\nI’m looking for a study buddy (or a small group) to go through [Maxime Labonne’s “LLM From Scratch” course](https://github.com/mlabonne/llm-course) together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together\n\n# My plan:\n\n* **Set weekly goals** based on the course structure\n* **Meet once a week** (probably one evening over the weekend) for a **voice call** to review what we’ve learned, share insights, and help each other with anything confusing\n* Stay accountable and motivated through shared progress\n\nDrop a comment or DM me if you’re interested! Thank you",
          "author_fullname": "t2_bywk86ik",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for LLMs Study Buddy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6fvd5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753195208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;I’m looking for a study buddy (or a small group) to go through &lt;a href=\"https://github.com/mlabonne/llm-course\"&gt;Maxime Labonne’s “LLM From Scratch” course&lt;/a&gt; together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together&lt;/p&gt;\n\n&lt;h1&gt;My plan:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Set weekly goals&lt;/strong&gt; based on the course structure&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Meet once a week&lt;/strong&gt; (probably one evening over the weekend) for a &lt;strong&gt;voice call&lt;/strong&gt; to review what we’ve learned, share insights, and help each other with anything confusing&lt;/li&gt;\n&lt;li&gt;Stay accountable and motivated through shared progress&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Drop a comment or DM me if you’re interested! Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?auto=webp&amp;s=dbc698010f56afa71dd99dc709b0ca685c29aee4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3f3260a76cb9648a81e4ffd047ff8a749b3bc74",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21e3cfc0b6e73548c3f100a3b24c8e03c4cf7290",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=361d87092f3bd81a159645012629ae0fe171dc2e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c4bc7a322635ecbf6feaad42bf125031a8dec84",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=697fabf421f2c39c88cbed190d56d8d3653dd0e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2fea0b39998df7813abea97a6cbf5ead21679476",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6fvd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaiKawaii0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753195208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*\\*TL;DR\\*\\* Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (\\~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp; power, which I'm attempting to solve with bifurcation cards and a new PSU.  Looking for feedback!\n\nHi everyone,\n\nLately I've been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I'm seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?\n\n  \nIf its as good as some posts claim, then I'd be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I'm going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that's why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.\n\nCurrent system:  \n\\*   \\*\\*CPU:\\*\\* Ryzen 5 7600  \n\\*   \\*\\*RAM:\\*\\* 48GB DDR5 5200MHz  \n\\*   \\*\\*Motherboard:\\*\\* MSI Mortar AM5  \n\\*   \\*\\*SSD (Primary):\\*\\* 1TB SSD  \n\\*   \\*\\*SSD (Secondary):\\*\\* 2TB SSD  \n\\*   \\*\\*PSU:\\*\\* 850W  \n\\*   \\*\\*GPU(s):\\*\\* 2x AMD RX6800 \n\n  \nProspective system:  \n\\*   \\*\\*CPU:\\*\\* Ryzen 5 7600  \n\\*   \\*\\*RAM:\\*\\* 48GB DDR5 5200MHz  \n\\*   \\*\\*Motherboard:\\*\\* MSI Mortar AM5(with bifurcation enabled)  \n\\*   \\*\\*SSD (Primary):\\*\\* 1TB SSD  \n\\*   \\*\\*SSD (Secondary):\\*\\* 2TB SSD  \n\\*   \\*\\*GPUs (New):\\*\\* 5 x MI50 32GB ($130 each + $100 shipping = $750 total)  \n\\*   \\*\\*PSU (New):\\*\\* 1600W PSU - $200  \n\\*   \\*\\*Bifurcation Cards:\\*\\* Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)  \n\\*   \\*\\*Riser Cables:\\*\\* Four PCIe 4.0 8x Cables - $100 ($25 each)  \n\\*   \\*\\*Cooling Shrouds:\\*\\* DIY MI50 GPU Cooling Shrouds (DIY)\n\n\\*   \\*\\*Total Cost of New Hardware:\\*\\* $1,125\n\nWhich doesn't seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I've been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!",
          "author_fullname": "t2_3f9vjjno",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Considering 5xMI50 for Qwen 3 235b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6eggp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 70B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753191801,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;**TL;DR** Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp;amp; power, which I&amp;#39;m attempting to solve with bifurcation cards and a new PSU.  Looking for feedback!&lt;/p&gt;\n\n&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Lately I&amp;#39;ve been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I&amp;#39;m seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?&lt;/p&gt;\n\n&lt;p&gt;If its as good as some posts claim, then I&amp;#39;d be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I&amp;#39;m going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that&amp;#39;s why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.&lt;/p&gt;\n\n&lt;p&gt;Current system:&lt;br/&gt;\n*   **CPU:** Ryzen 5 7600&lt;br/&gt;\n*   **RAM:** 48GB DDR5 5200MHz&lt;br/&gt;\n*   **Motherboard:** MSI Mortar AM5&lt;br/&gt;\n*   **SSD (Primary):** 1TB SSD&lt;br/&gt;\n*   **SSD (Secondary):** 2TB SSD&lt;br/&gt;\n*   **PSU:** 850W&lt;br/&gt;\n*   **GPU(s):** 2x AMD RX6800 &lt;/p&gt;\n\n&lt;p&gt;Prospective system:&lt;br/&gt;\n*   **CPU:** Ryzen 5 7600&lt;br/&gt;\n*   **RAM:** 48GB DDR5 5200MHz&lt;br/&gt;\n*   **Motherboard:** MSI Mortar AM5(with bifurcation enabled)&lt;br/&gt;\n*   **SSD (Primary):** 1TB SSD&lt;br/&gt;\n*   **SSD (Secondary):** 2TB SSD&lt;br/&gt;\n*   **GPUs (New):** 5 x MI50 32GB ($130 each + $100 shipping = $750 total)&lt;br/&gt;\n*   **PSU (New):** 1600W PSU - $200&lt;br/&gt;\n*   **Bifurcation Cards:** Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)&lt;br/&gt;\n*   **Riser Cables:** Four PCIe 4.0 8x Cables - $100 ($25 each)&lt;br/&gt;\n*   **Cooling Shrouds:** DIY MI50 GPU Cooling Shrouds (DIY)&lt;/p&gt;\n\n&lt;p&gt;*   **Total Cost of New Hardware:** $1,125&lt;/p&gt;\n\n&lt;p&gt;Which doesn&amp;#39;t seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I&amp;#39;ve been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 70B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6eggp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PraxisOG",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753191801,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1947344511988076547\n\nNew Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ox8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 508,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 508,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BzOaH3m_YlAhztCv_hRYQh_Ms3ouqOY06ZKKT3zNke8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;New Qwen3-235B-A22B with thinking mode only –– no more hybrid reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?auto=webp&amp;s=d32732e5748b82ca37787c55ccd57f5d5f705318",
                  "width": 1186,
                  "height": 1038
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32d1f0ad8ac85f1518bb4e197d86320d03376d96",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf9ca6a58cad66052d5ca05375d82b3c7bf8f1cb",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c38bec3415e13d4801fadbc3fe0e9ec1df461dbf",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5242a889814e823acb6da0b1179758e2947ea2a7",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7330c9d38901abf96ac6373f03b5933dd9e99710",
                    "width": 960,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15a616880d08cfc896e48a40a70a2aa2315dc9e4",
                    "width": 1080,
                    "height": 945
                  }
                ],
                "variants": {},
                "id": "15L72ZL9LJC_7vzVz7gsW_m-zmkTuQofd74ZqjjUwaA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ox8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 91,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753118294,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running a 16-inch MacBook Pro with the new M4 Pro chip (48 GB unified RAM, 512 GB SSD). I’ve narrowed my local LLM experiments down to two heavy hitters:\n\nDeepSeek-Coder V3-Lite 33B for coding powerhouse \n\nQwen3-32B-Instruct-MoE for coding and reasoning all purpose \n\ni want your opinion how these two how these two feels in real world, for a person like me, i need it for writing python script , do some research, in VS we can use api in cline for execution and auto completion of the code without limit\n\nmy current setup\n\nmacOS 15.2 (Sonoma++)\nLM Studio 0.4.3 – MLX engine\nQwen3 GGUF Q4_K_M  — 18 GB\nDeepSeek-Coder Q4_K_M — 27 GB\nSwap disabled, running on mains (140 W)\n\nyour thoughts what are the other model we can try and test with limited hardware. thank you\n",
          "author_fullname": "t2_3nc1bpb5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M4 Pro Owners: I Want Your Biased Hot-Takes – DeepSeek-Coder V3-Lite 33B vs Qwen3-32B-Instruct-MoE on a 48 GB MacBook Pro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6tf9v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running a 16-inch MacBook Pro with the new M4 Pro chip (48 GB unified RAM, 512 GB SSD). I’ve narrowed my local LLM experiments down to two heavy hitters:&lt;/p&gt;\n\n&lt;p&gt;DeepSeek-Coder V3-Lite 33B for coding powerhouse &lt;/p&gt;\n\n&lt;p&gt;Qwen3-32B-Instruct-MoE for coding and reasoning all purpose &lt;/p&gt;\n\n&lt;p&gt;i want your opinion how these two how these two feels in real world, for a person like me, i need it for writing python script , do some research, in VS we can use api in cline for execution and auto completion of the code without limit&lt;/p&gt;\n\n&lt;p&gt;my current setup&lt;/p&gt;\n\n&lt;p&gt;macOS 15.2 (Sonoma++)\nLM Studio 0.4.3 – MLX engine\nQwen3 GGUF Q4_K_M  — 18 GB\nDeepSeek-Coder Q4_K_M — 27 GB\nSwap disabled, running on mains (140 W)&lt;/p&gt;\n\n&lt;p&gt;your thoughts what are the other model we can try and test with limited hardware. thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6tf9v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WestPush7",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tf9v/m4_pro_owners_i_want_your_biased_hottakes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tf9v/m4_pro_owners_i_want_your_biased_hottakes/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753226393,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI21 has just made Jamba 1.7 available on Kaggle:\n\n[https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7](https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7) \n\n* You can run and test the model without needing to install it locally\n* No need to harness setup, hardware and engineering knowledge via Hugging Face anymore\n* Now you can run sample tasks, benchmark against other models and share public notebooks with results\n\nPretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:\n\n* Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences\n* 256k context window - well-suited for long document summarization and memory-heavy chat agents\n* Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs\n\nWho is going to try it out? What use cases do you have in mind?",
          "author_fullname": "t2_1kwk178bd9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jamba 1.7 is now available on Kaggle",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6dco7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753202307,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753188954,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI21 has just made Jamba 1.7 available on Kaggle:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7\"&gt;https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7&lt;/a&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You can run and test the model without needing to install it locally&lt;/li&gt;\n&lt;li&gt;No need to harness setup, hardware and engineering knowledge via Hugging Face anymore&lt;/li&gt;\n&lt;li&gt;Now you can run sample tasks, benchmark against other models and share public notebooks with results&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Pretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences&lt;/li&gt;\n&lt;li&gt;256k context window - well-suited for long document summarization and memory-heavy chat agents&lt;/li&gt;\n&lt;li&gt;Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Who is going to try it out? What use cases do you have in mind?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6dco7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NullPointerJack",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753188954,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New qwen tested on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6172l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TLf5BqdXyD8b18S_CjlBuka8R6DaWW-Nnyc_DD4KFcw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753148000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9rynne03xbef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9rynne03xbef1.png?auto=webp&amp;s=4f7e2275d4e835b0f01387fc4e2f5de4682c92f8",
                  "width": 1520,
                  "height": 2266
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c72f32848579ff8381bbc07e00d52af73ccb790",
                    "width": 108,
                    "height": 161
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c05a3489ce34f1c0d3c48da6ac4fb493a3af2239",
                    "width": 216,
                    "height": 322
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7758dc0729434a0929fb46f9640d8ed72e9ba4f",
                    "width": 320,
                    "height": 477
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e",
                    "width": 640,
                    "height": 954
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1dd97abb7b7a97ba53a5a60797a4df72dbe1e9e",
                    "width": 960,
                    "height": 1431
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51749430f3b47afde5a4eee467854e441af14310",
                    "width": 1080,
                    "height": 1610
                  }
                ],
                "variants": {},
                "id": "OE4XOhVwW7bVZ94xo4IF074gf7GQOtJgsoNbI-IyttA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6172l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/9rynne03xbef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753148000,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Throwback to 3 months ago: [https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg\\_a\\_unified\\_scalable\\_vector\\_graphics/](https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/)\n\nWeights: [https://huggingface.co/OmniSVG/OmniSVG](https://huggingface.co/OmniSVG/OmniSVG)\n\nHuggingFace demo: [https://huggingface.co/spaces/OmniSVG/OmniSVG-3B](https://huggingface.co/spaces/OmniSVG/OmniSVG-3B)\n\nGitHub: [https://github.com/OmniSVG/OmniSVG/](https://github.com/OmniSVG/OmniSVG/)",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OmniSVG weights released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m61u94",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753149834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwback to 3 months ago: &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Weights: &lt;a href=\"https://huggingface.co/OmniSVG/OmniSVG\"&gt;https://huggingface.co/OmniSVG/OmniSVG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;HuggingFace demo: &lt;a href=\"https://huggingface.co/spaces/OmniSVG/OmniSVG-3B\"&gt;https://huggingface.co/spaces/OmniSVG/OmniSVG-3B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/OmniSVG/OmniSVG/\"&gt;https://github.com/OmniSVG/OmniSVG/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?auto=webp&amp;s=879a29e047e8b5a9e7c3cc213f6732c60bc2a1a7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a72266ba63c0bc5f87d6bf4f1a9d21ca8a03fb2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8554240e67e2f71d1e81cbf7f1b701e59cb5fefd",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e1d031a2135bd08b701037085db4506b941ab6d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b70a2f5fdb810f142ac53ef2d47901cc0c789f95",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a856840edcf86155de1faff51118fe59e453e241",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=479fbae9e67984006acc087b84b30e16cca24f24",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m61u94",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753149834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Used A100 40GB just dropped below $2000, for those who care with caveat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m60ahf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 102,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 102,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753145404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It&amp;#39;ll be an interesting piece of localllama HW.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m60ahf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753145404,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!",
          "author_fullname": "t2_mxles3cs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about updating Llama 3.3-70B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ahsu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ahsu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Only_Emergencies",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753180144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone. I hope you're doing well. I'm sorry if this post is unrelated to the topic of large language models, but I haven't found any other community that focuses on open source AI in general. My question is, are there any open source models for Arabic audio enhancement? Basically, the use case is making good quality data for training Arabic text-to-speech models, since the current ones are either afflicted with bad licenses or they are not up to the task. Thanks for your answers.",
          "author_fullname": "t2_9xer9y5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Models for Arabic tts and audio enhancement?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nbb7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753211838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I hope you&amp;#39;re doing well. I&amp;#39;m sorry if this post is unrelated to the topic of large language models, but I haven&amp;#39;t found any other community that focuses on open source AI in general. My question is, are there any open source models for Arabic audio enhancement? Basically, the use case is making good quality data for training Arabic text-to-speech models, since the current ones are either afflicted with bad licenses or they are not up to the task. Thanks for your answers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6nbb7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Silver-Champion-4846",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nbb7/best_models_for_arabic_tts_and_audio_enhancement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6nbb7/best_models_for_arabic_tts_and_audio_enhancement/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753211838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I've hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I'm playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?\n\nAre there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.\n\nLong-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let's be honest, you don't decide to buy a Strad because you want to learn violin, and I'm not looking to drop $$$$ on a GPU if it's avoidable.\n\nUpgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.",
          "author_fullname": "t2_fs6q6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Entry GPU options - 5060 8GB enough to play with?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6knhw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753205961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I&amp;#39;ve hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I&amp;#39;m playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?&lt;/p&gt;\n\n&lt;p&gt;Are there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.&lt;/p&gt;\n\n&lt;p&gt;Long-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let&amp;#39;s be honest, you don&amp;#39;t decide to buy a Strad because you want to learn violin, and I&amp;#39;m not looking to drop $$$$ on a GPU if it&amp;#39;s avoidable.&lt;/p&gt;\n\n&lt;p&gt;Upgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6knhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drabbiticus",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753205961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/JustinLin610/status/1947281769134170147\n\nMaybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng [here](https://x.com/ChujieZheng/status/1947307034980089905).",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Imminent release from Qwen tonight",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n148",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 438,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 438,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DBvwe9bi2sUadVKTh4wZB-h_0n3lxygls9SlP-B36wg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/JustinLin610/status/1947281769134170147\"&gt;https://x.com/JustinLin610/status/1947281769134170147&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Maybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng &lt;a href=\"https://x.com/ChujieZheng/status/1947307034980089905\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/um0pwye549ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/um0pwye549ef1.png?auto=webp&amp;s=034e1360dd4d1a71075a1978e81cc176280c0940",
                  "width": 570,
                  "height": 284
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac602ae1dcb08fc594a97f2c504da7e053543395",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34209fd3279089bca920b8e313de5e6ea1d3d074",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3401860a4fccf34b2ae631236b2b714dafe0ec28",
                    "width": 320,
                    "height": 159
                  }
                ],
                "variants": {},
                "id": "XO8ytuncZItjhr3hlokHSgG-P5B6fKV-FMv68ZqKLXQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5n148",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 86,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/um0pwye549ef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753114102,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1tpuoj72sa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m65iga",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ObGRZZdEJPhNPxSGc-Hl-FLX_u-ODU9Q-A84Zj7Q5Z4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753161224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ifz3sua70def1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ifz3sua70def1.jpeg?auto=webp&amp;s=3c8da0138281644b0b232fb926afb72c94068d2d",
                  "width": 4037,
                  "height": 3077
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cb9bd9d6aa78574351fa9778ec9d0b129263457",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7c4b9ff9064655d107fe8aab0d33aa934050ef",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34ad0918b9b41221e6a4d64ec2a5686a6466206b",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784878f38d8fa43b398531435fad5ad46f80423f",
                    "width": 640,
                    "height": 487
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fba24ff9f318cff389422ef0a5e7d51301d79f6c",
                    "width": 960,
                    "height": 731
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=674e7d2f5ae18055ec6bf581e5ec73734512ba00",
                    "width": 1080,
                    "height": 823
                  }
                ],
                "variants": {},
                "id": "xdYmB0YCSsaGa7OmRBAUHmZvhAr6D2eweUUdkl-syTg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m65iga",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTrack_ai",
          "discussion_type": null,
          "num_comments": 71,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/",
          "stickied": false,
          "url": "https://i.redd.it/ifz3sua70def1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753161224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Seeking recommendations for Android LLM apps with GPU acceleration and customisation like promts.",
          "author_fullname": "t2_8pq43jfs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best android local llm apk with gpu acceleration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6q0oh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753218021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeking recommendations for Android LLM apps with GPU acceleration and customisation like promts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6q0oh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Moose-228",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6q0oh/best_android_local_llm_apk_with_gpu_acceleration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6q0oh/best_android_local_llm_apk_with_gpu_acceleration/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\n\nThis model showed up on my LinkedIn feed today. After listening to a few examples on their [website](https://www.boson.ai/technologies/voice), I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. \n\nListen to this [demo video](https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d), it will just enable so many use cases.\n\nI tried a few examples in their HF [playground](https://huggingface.co/spaces/smola/higgs_audio_v2), it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. \n\n",
          "author_fullname": "t2_6nwb1mbe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 45,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "rmmpgv36tief1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 35,
                  "x": 108,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b8a7927fef36a8dc9cafddd20ca7395324bb30"
                },
                {
                  "y": 70,
                  "x": 216,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa3bc0976c73f20478fe4c41ae0d81d56d9b5efa"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c779645fab14bc4ce1ecf7b4cca7ce06002977dd"
                },
                {
                  "y": 207,
                  "x": 640,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2890cda589e72ebb380d268d25b2f0c730e4153"
                },
                {
                  "y": 311,
                  "x": 960,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b671678ce82ca5a2abdd86ced5de4262f068a656"
                },
                {
                  "y": 350,
                  "x": 1080,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea15a827df9bb20ae059bea42e6b92ad1d59800b"
                }
              ],
              "s": {
                "y": 872,
                "x": 2686,
                "u": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"
              },
              "id": "rmmpgv36tief1"
            }
          },
          "name": "t3_1m6vbds",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href=\"https://www.boson.ai/technologies/voice\"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. &lt;/p&gt;\n\n&lt;p&gt;Listen to this &lt;a href=\"https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d\"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt;\n\n&lt;p&gt;I tried a few examples in their HF &lt;a href=\"https://huggingface.co/spaces/smola/higgs_audio_v2\"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6vbds",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Tap3484",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753231503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Cards like 3090, 4090, 5090 has very high electric consumption. Isn't it possible to make 24,32gb cards with like 5060 level electric consumption?",
          "author_fullname": "t2_d9gk5hdlt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "+24GB VRAM with low electric consumption",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hzf0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753200005,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cards like 3090, 4090, 5090 has very high electric consumption. Isn&amp;#39;t it possible to make 24,32gb cards with like 5060 level electric consumption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6hzf0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "narca_hakan",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753200005,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to set up a homelab. I’ve got 2 NVIDIA Quadro RTX 6000’s laying around that I was given a few years back. I don’t have any server equipment yet, but I’m gonna buy a rack, PSU, server motherboard, Processor, RAM, and storage enclaves to set up my first homelab. \n\nI want to build an AI to help me with my job in Cybersecurity, I’d like to train it on big data sets like Stack Overflow and CVE.\n\nMy question is, are my GPU’s good enough for this task? What kind of CPU/S do I need to keep up? Ram capacity/speed recommendations?",
          "author_fullname": "t2_1cam2liip6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I own a few Quadro’s, can I build an AI with these?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6v9yq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to set up a homelab. I’ve got 2 NVIDIA Quadro RTX 6000’s laying around that I was given a few years back. I don’t have any server equipment yet, but I’m gonna buy a rack, PSU, server motherboard, Processor, RAM, and storage enclaves to set up my first homelab. &lt;/p&gt;\n\n&lt;p&gt;I want to build an AI to help me with my job in Cybersecurity, I’d like to train it on big data sets like Stack Overflow and CVE.&lt;/p&gt;\n\n&lt;p&gt;My question is, are my GPU’s good enough for this task? What kind of CPU/S do I need to keep up? Ram capacity/speed recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6v9yq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NetTechMan",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6v9yq/i_own_a_few_quadros_can_i_build_an_ai_with_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6v9yq/i_own_a_few_quadros_can_i_build_an_ai_with_these/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753231391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "have a project that uses the `deepseek-r1` model from `https://api.llama-api.com`. However, it seems Llama API has launched a new console. My email is not recognized in the new beta console, although I have an account and have added credit to it. \n\nThe old console links no longer work. Additionally, the DeepSeek models are not listed on the documentation page anymore (`https://llama.developer.meta.com/docs/models`).",
          "author_fullname": "t2_tqeqwc14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek not available at LLama API?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6pjpx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753216907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have a project that uses the &lt;code&gt;deepseek-r1&lt;/code&gt; model from &lt;code&gt;https://api.llama-api.com&lt;/code&gt;. However, it seems Llama API has launched a new console. My email is not recognized in the new beta console, although I have an account and have added credit to it. &lt;/p&gt;\n\n&lt;p&gt;The old console links no longer work. Additionally, the DeepSeek models are not listed on the documentation page anymore (&lt;code&gt;https://llama.developer.meta.com/docs/models&lt;/code&gt;).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6pjpx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AncientMayar",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6pjpx/deepseek_not_available_at_llama_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6pjpx/deepseek_not_available_at_llama_api/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753216907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks!\n\nThanks to this community, I pulled the trigger about a month ago to get a machine with a 3090. It's been a crazy month for me, and I've been coding local AI tools non-stop.\n\nI'm excited to share my favorite creation so far: **[agent-cli](https://github.com/basnijholt/agent-cli)**, a suite of tools that lets me interact with local models using system-wide hotkeys on my Mac.\n\n**What does it do?**\n\n*   **Hotkey-Powered Workflow:** I can transcribe audio, correct grammar, or have a voice-based conversation with my clipboard content without ever leaving my current application.\n*   **Transcription (`Cmd+Shift+R`):** Instantly transcribe my voice into the clipboard using a local Whisper model.\n*   **Autocorrect (`Cmd+Shift+A`):** Fix spelling and grammar on any copied text.\n*   **Voice Edit (`Cmd+Shift+V`):** I can copy some text, then use my voice to command an LLM to edit it, summarize it, or even answer a question based on it.\n\nThen it also has an interactive voice chat and one that is activated by a wake word.\n\n**It's 100% Local &amp; Private**\n\nThe whole stack is designed to run completely offline on your own machine:\n*   **LLM:** Works with any model via Ollama.\n*   **STT (Speech-to-Text):** Uses `wyoming-faster-whisper`.\n*   **TTS (Text-to-Speech):** Supports `wyoming-piper` and `Kokoro-FastAPI`.\n*   **Wake Word:** Integrates with `wyoming-openwakeword` for a hands-free assistant.\n\nI'd never recorded a video before, but I put together a short demo to make it easier to see how it all works in practice.\n\n- https://www.youtube.com/watch?v=7sBTCgttH48\n- https://github.com/basnijholt/agent-cli\n\nI'd love to get your feedback. Let me know what you think!\n",
          "author_fullname": "t2_yquzb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I stopped typing. Now I just use a hotkey. I built Agent-CLI to make it possible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6uq8q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Agent-CLI: Local AI Voice &amp; Text Tools on Your Desktop (macOS Demo)",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
              "author_name": "johnbaltis",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7sBTCgttH48/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@BasNij"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m6uq8q",
            "height": 200
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=8be70c16dc7a0e0e375b94ff98e0971b5cfbac91",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753229837,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\n\n&lt;p&gt;Thanks to this community, I pulled the trigger about a month ago to get a machine with a 3090. It&amp;#39;s been a crazy month for me, and I&amp;#39;ve been coding local AI tools non-stop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m excited to share my favorite creation so far: &lt;strong&gt;&lt;a href=\"https://github.com/basnijholt/agent-cli\"&gt;agent-cli&lt;/a&gt;&lt;/strong&gt;, a suite of tools that lets me interact with local models using system-wide hotkeys on my Mac.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  &lt;strong&gt;Hotkey-Powered Workflow:&lt;/strong&gt; I can transcribe audio, correct grammar, or have a voice-based conversation with my clipboard content without ever leaving my current application.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Transcription (&lt;code&gt;Cmd+Shift+R&lt;/code&gt;):&lt;/strong&gt; Instantly transcribe my voice into the clipboard using a local Whisper model.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Autocorrect (&lt;code&gt;Cmd+Shift+A&lt;/code&gt;):&lt;/strong&gt; Fix spelling and grammar on any copied text.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Voice Edit (&lt;code&gt;Cmd+Shift+V&lt;/code&gt;):&lt;/strong&gt; I can copy some text, then use my voice to command an LLM to edit it, summarize it, or even answer a question based on it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then it also has an interactive voice chat and one that is activated by a wake word.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;It&amp;#39;s 100% Local &amp;amp; Private&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The whole stack is designed to run completely offline on your own machine:\n*   &lt;strong&gt;LLM:&lt;/strong&gt; Works with any model via Ollama.\n*   &lt;strong&gt;STT (Speech-to-Text):&lt;/strong&gt; Uses &lt;code&gt;wyoming-faster-whisper&lt;/code&gt;.\n*   &lt;strong&gt;TTS (Text-to-Speech):&lt;/strong&gt; Supports &lt;code&gt;wyoming-piper&lt;/code&gt; and &lt;code&gt;Kokoro-FastAPI&lt;/code&gt;.\n*   &lt;strong&gt;Wake Word:&lt;/strong&gt; Integrates with &lt;code&gt;wyoming-openwakeword&lt;/code&gt; for a hands-free assistant.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d never recorded a video before, but I put together a short demo to make it easier to see how it all works in practice.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=7sBTCgttH48\"&gt;https://www.youtube.com/watch?v=7sBTCgttH48&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/basnijholt/agent-cli\"&gt;https://github.com/basnijholt/agent-cli&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love to get your feedback. Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=7sBTCgttH48",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?auto=webp&amp;s=b46176561bea253bdaeac9ab75bd82fbf837bb7f",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b60a9338c5518f795a38fedcae4b0e1233d18742",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec3a10104da04aff2c9950c5c90919a9bb1f40d4",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d14c7c0424b23b6f7f7f0c9a66d57e7440d29402",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m6uq8q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "basnijholt",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6uq8q/i_stopped_typing_now_i_just_use_a_hotkey_i_built/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=7sBTCgttH48",
          "subreddit_subscribers": 502981,
          "created_utc": 1753229837,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Agent-CLI: Local AI Voice &amp; Text Tools on Your Desktop (macOS Demo)",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
              "author_name": "johnbaltis",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7sBTCgttH48/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@BasNij"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vgnr5u5gg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m650ow",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/eOMW_Oov8IQoYGj2MLhSVhdZR_G_D49PS5lS7HSSsnk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753159541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?auto=webp&amp;s=bcac025445c3e19050b77650ec0313d164d2cb63",
                  "width": 696,
                  "height": 181
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93400e194109a5036a8e420d94408434e9409fa7",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=904c5d55682c555ac50948455d22e99da1ab864b",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae695aac40a41943b105be24e34d521e45b9b0b3",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2250dfec91bd4da5ce280844d385d4702942d1",
                    "width": 640,
                    "height": 166
                  }
                ],
                "variants": {},
                "id": "9dzCnXa9HN8TH6BqvCudoVpKpcesYzMwj4FmuXf8IdA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m650ow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JeffreySons_90",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/",
          "stickied": false,
          "url": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753159541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I'm curious to see if others find the same. PRs accepted and encouraged!",
          "author_fullname": "t2_5n8i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs against a sandbox airport to see if they can make the correct decisions in real time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m62vbw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=44392f6267e504eff05965daa4cd423000d27a80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753152819,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I&amp;#39;m curious to see if others find the same. PRs accepted and encouraged!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/jjasghar/ai-airport-simulation",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?auto=webp&amp;s=27c081b855fd81984a0fbd5a3cd8d041afd40a2a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e5cedfeb2acc17ed96c354aea24f51d83b107d8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f04243269cf381d5d666e8ad9cc1f63960e31ef9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4b0fca68877cd9584992c2a2b35a39c83a82f6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c17ae1c6715cde69de4cb21dd94c66e0f2a16d0b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04a1b374daef1cc2d36172a124f4af1e43e7a5c7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df058f8c50256c5ab0fda05e61968a5c791096db",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m62vbw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jjasghar",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/",
          "stickied": false,
          "url": "https://github.com/jjasghar/ai-airport-simulation",
          "subreddit_subscribers": 502981,
          "created_utc": 1753152819,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I'm a college student from India.\n\nSo i'm looking for a language model for code generation to run locally. I only have 16 GB of ram and iris xe gpu, so looking for some good opensource SLMs which can be decent enough. I could use something like llama.cpp given performance and latency would be decent(currently using a gguf version of mistral 7B-instruct and it's working fine) . Can also consider using raspberry pi if it'll be of any use",
          "author_fullname": "t2_7q8gvaa19",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best opensource SLM/ lightweight llm for code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6dvhi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753190318,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m a college student from India.&lt;/p&gt;\n\n&lt;p&gt;So i&amp;#39;m looking for a language model for code generation to run locally. I only have 16 GB of ram and iris xe gpu, so looking for some good opensource SLMs which can be decent enough. I could use something like llama.cpp given performance and latency would be decent(currently using a gguf version of mistral 7B-instruct and it&amp;#39;s working fine) . Can also consider using raspberry pi if it&amp;#39;ll be of any use&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6dvhi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RustinChole11",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6dvhi/best_opensource_slm_lightweight_llm_for_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6dvhi/best_opensource_slm_lightweight_llm_for_code/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753190318,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just updated LM Studio to 0.3.19, downloaded qwen/qwen3-235b-a22b-2507 Q3\\_K\\_L (the only one that fits on my 128GB Mac) and I'm getting a \"failed to send message\" error. I suspect it's the prompt template that's wrong. Can anyone here please post a working template for me to try?\n\nThank you!\n\n  \nEDIT: As suggested by [Minimum\\_Thought\\_x](https://www.reddit.com/user/Minimum_Thought_x/) the 3bit MLX version works! It doesn't show (at least at this moment) in the staff picks list for the model, but you can find it by using the search function.",
          "author_fullname": "t2_d2gb9jhgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Failed to Send Message\" from qwen/qwen3-235b-a22b-2507 Q3_K_L",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ldkd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753213776,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753207564,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just updated LM Studio to 0.3.19, downloaded qwen/qwen3-235b-a22b-2507 Q3_K_L (the only one that fits on my 128GB Mac) and I&amp;#39;m getting a &amp;quot;failed to send message&amp;quot; error. I suspect it&amp;#39;s the prompt template that&amp;#39;s wrong. Can anyone here please post a working template for me to try?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT: As suggested by &lt;a href=\"https://www.reddit.com/user/Minimum_Thought_x/\"&gt;Minimum_Thought_x&lt;/a&gt; the 3bit MLX version works! It doesn&amp;#39;t show (at least at this moment) in the staff picks list for the model, but you can find it by using the search function.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ldkd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hanthunius",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ldkd/failed_to_send_message_from_qwenqwen3235ba22b2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ldkd/failed_to_send_message_from_qwenqwen3235ba22b2507/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753207564,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there an active leaderboard for local models that ranks them by function calling capability?",
          "author_fullname": "t2_cxq4h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Leaderboard for function calling models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ht1r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753199602,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an active leaderboard for local models that ranks them by function calling capability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ht1r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tvmaly",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ht1r/leaderboard_for_function_calling_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ht1r/leaderboard_for_function_calling_models/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753199602,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "_A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him \"completely exhausted.\"_\n\nhttps://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/",
          "author_fullname": "t2_1gnii9bkc9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Exhausted man defeats AI model in world coding championship",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5r9ss",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753123500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/\"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?auto=webp&amp;s=01c6c5989382448ceacfb16d4716e1d43882c07d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c56d051b4b4e63e5c6627f8639b6bc541ebe7a70",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06c2478ecab47c428699645361b3f004e394ce98",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dcfa8df2a1e6cafbca016c61a6781fc1bd66b6e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=089271aa272fc9321b33a83d2db06a95c38b6ce9",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73adafa18daac7bc6b0c9c52f9fe6f72db681c41",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3d93ef0ea7ac95a421f4d7a2e7e1d2f2350aac9",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5r9ss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational_Sun_8813",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753123500,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "​Disclaimer: I made [hyprnote](https://hyprnote.com) \\- went trending in [here](https://www.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/) 3 months ago.\n\n**context:**\n\na lot of our users are using ollama at the moment and I thought why not make something for STT just like ollama. we are also getting more and more requests on the parakeet model so really looking into this right now.\n\n**research:**\n\nI haven't come across anything related to this. I found some projects using whisperX but haven't actually found one where you can just use different models like ollama.\n\n**owhisper:**\n\nI'm building an open-source alternative for granola ai. I want to make hyprnote self-hostable so people can play around with various stt and llms. thinking about making a unified proxy server that can be deployed and manages owhisper and custom llm endpoints - including ollama.\n\nCurious - if this existed, would you try it out? And what features would you want built in?",
          "author_fullname": "t2_j1t6g97wv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about \"owhisper\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hck1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753198571,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;​Disclaimer: I made &lt;a href=\"https://hyprnote.com\"&gt;hyprnote&lt;/a&gt; - went trending in &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/\"&gt;here&lt;/a&gt; 3 months ago.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;a lot of our users are using ollama at the moment and I thought why not make something for STT just like ollama. we are also getting more and more requests on the parakeet model so really looking into this right now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;research:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t come across anything related to this. I found some projects using whisperX but haven&amp;#39;t actually found one where you can just use different models like ollama.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;owhisper:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building an open-source alternative for granola ai. I want to make hyprnote self-hostable so people can play around with various stt and llms. thinking about making a unified proxy server that can be deployed and manages owhisper and custom llm endpoints - including ollama.&lt;/p&gt;\n\n&lt;p&gt;Curious - if this existed, would you try it out? And what features would you want built in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?auto=webp&amp;s=070858a1cfeec763c131615ac513538e6a19426b",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bee6065abfb7b73c45f622b4c1cc472253ace4e",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82ae4d019c19940aa95321c7be83b0edd546f2a0",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9757ca6f1422e8e8039213794d3c5ab9ab68d765",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=925c91e5f6ddcbcaa0e6039c50650c2027ac8c2b",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d7081fb329bed3002c26fa7002ce376573bb805",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=084bb2b0fc5cc7efd82866fa4b3992449215e74c",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6hck1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beerbellyman4vr",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hck1/thinking_about_owhisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hck1/thinking_about_owhisper/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753198571,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using RAG with open ai over a product description document which is rather technical. I chunked up sections of my document and then do hybrid search with weaviate. It does good but sometimes certain queries require retrieval from more than 1 sections and then it's 50/50. Will fine-tuning solve this? What model should I look into?",
          "author_fullname": "t2_vdwm0f4m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rag vs fine-tuning.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qb6p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753218703,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using RAG with open ai over a product description document which is rather technical. I chunked up sections of my document and then do hybrid search with weaviate. It does good but sometimes certain queries require retrieval from more than 1 sections and then it&amp;#39;s 50/50. Will fine-tuning solve this? What model should I look into?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6qb6p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Parking_Bluebird826",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qb6p/rag_vs_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6qb6p/rag_vs_finetuning/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218703,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)\n\nDidn't see that with the \"old\" 235b with /no\\_think \n\nIs that expected?",
          "author_fullname": "t2_joxwuyje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In Qwen3-235B-A22B-Instruct-2507-UD-Q4 (unsloth) I'm seeing some \"but wait\" and related ones (like kinda questioning and answering itself), were the model seems to \"think\" (even when is a non-thinking model and I haven't setup any system prompt), have you seen something similar?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69sb6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)&lt;/p&gt;\n\n&lt;p&gt;Didn&amp;#39;t see that with the &amp;quot;old&amp;quot; 235b with /no_think &lt;/p&gt;\n\n&lt;p&gt;Is that expected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m69sb6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "relmny",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Mind-Blowing](https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128)",
          "author_fullname": "t2_1tbloqabyg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "7by2astxg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c498a4606859ea7dc3344cea600a6287666c52bb"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a0f911df63c617cf62eddf44f25fd9d1526f4f9"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=18f18ad0a953d5f6cce28f16501265fc5f1b65fa"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a2c30752b5e9e36ee257e083921c23b7bd51214"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce6ecb7eb20a5950434de688e9e3e318d8dcf552"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b198e562025147aead0f6a4065ef96012102dc8e"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128"
              },
              "id": "7by2astxg9ef1"
            }
          },
          "name": "t3_1m5oz0h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 159,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 159,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sK-ChiNoLnwj2ggKTtNTJYTvWhnsGqdGF-BtGXWSrIM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128\"&gt;Mind-Blowing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oz0h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ken-senseii",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753118398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Doesn't it sound cool?\nSounds movie like",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Project: Print AI Replies on a Ticket Printer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6izt7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753202280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t it sound cool?\nSounds movie like&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6izt7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6izt7/project_print_ai_replies_on_a_ticket_printer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6izt7/project_print_ai_replies_on_a_ticket_printer/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753202280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. \n\nI've pulled the project back to make sure there are no differences between what's on the server and what's on my laptop, and my laptop works as expected. \n\nThe server doesn't have access to the outside world, but once everything is up and running, it shouldn't need it, right? \n\nAnyone have any ideas? I've never seen a model behave like this.",
          "author_fullname": "t2_243il8gu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "embedding model giving same embeddings regardless of input text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6oqxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753215074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pulled the project back to make sure there are no differences between what&amp;#39;s on the server and what&amp;#39;s on my laptop, and my laptop works as expected. &lt;/p&gt;\n\n&lt;p&gt;The server doesn&amp;#39;t have access to the outside world, but once everything is up and running, it shouldn&amp;#39;t need it, right? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas? I&amp;#39;ve never seen a model behave like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6oqxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "User1539",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753215074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello there,\n\nMy project to extract and collect the \"secret\" system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.\n\n**The idea is to see the advanced \"prompt architecture\" that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.**\n\nInstead of trying to reinvent the wheel, you can see exactly how they force models to \"think step-by-step\" in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.\n\nFor example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:\n\n    Knowledge cutoff: 2024-06\n    \n    You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n    \n    You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n    \n    You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n    \n    Your main goal is to follow the USER's instructions at each message, denoted by the &lt;user_query&gt; tag.\n    \n    &lt;communication&gt;\n    When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n    &lt;/communication&gt;\n\nI wrote a full article that does a deep dive into these patterns and also discusses the \"dual-use\" aspect of making these normally-hidden prompts public.\n\nI'm super curious: **How are you all structuring system prompts for your favorite models?**\n\n**Links:**\n\n* **The full article with more analysis:** [The Open Source Project That Became an Essential Library for Modern AI Engineering](https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------)[](https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------)\n\n* **The GitHub Repo (to grab the prompts):** [https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools)\n\nHope you find it useful!",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gwzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 378,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 378,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753099199,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753099002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it&amp;#39;s incredibly useful.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It&amp;#39;s a goldmine of ideas for crafting better system prompts.&lt;/p&gt;\n\n&lt;p&gt;For example, here&amp;#39;s a small snippet from the Cursor prompt that shows how they establish the AI&amp;#39;s role and capabilities right away:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06\n\nYou are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n\nYou are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n\nYou are an agent - please keep going until the user&amp;#39;s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n\nYour main goal is to follow the USER&amp;#39;s instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag.\n\n&amp;lt;communication&amp;gt;\nWhen using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n&amp;lt;/communication&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href=\"https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------\"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href=\"https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------\"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hope you find it useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5gwzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753099002,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_rkmud0isr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The reason why local models are better/necessary.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5iymb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 281,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 281,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_0QOwkUP9B0YXB-SAHakr_6UlxBhOQpPKlYLA0LCuiQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753104611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vdngpglhb8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vdngpglhb8ef1.png?auto=webp&amp;s=62e9d97048e91daee3582390075fb00d1887e202",
                  "width": 854,
                  "height": 504
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f4c8c8ea760457e111d37a839bbe4882b86b520",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=adc1595b273f56bd1537f9efa03ae0704717b281",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fca7b7544874c7613248f95246d1810ffb9ffb7b",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac",
                    "width": 640,
                    "height": 377
                  }
                ],
                "variants": {},
                "id": "f9sIdJ3aHKBFJd8WvU4XqZxgrL8gg-EBXrNrZSagJuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m5iymb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTshop_ai",
          "discussion_type": null,
          "num_comments": 143,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/",
          "stickied": false,
          "url": "https://i.redd.it/vdngpglhb8ef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753104611,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!\n\nAfter talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible. Today, we’re releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.\n\nThis model performs better than our last release, and we hope you’ll like it thanks to its strong overall abilities.\n\nQwen Chat: chat.qwen.ai — just start chatting with the default model, and feel free to use the search button!",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen released Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oxyp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 138,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 138,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3Sv34Z6mz9FmhoyS3JnMLxTuxuV0E_Efi7EvfiWKcOs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!&lt;/p&gt;\n\n&lt;p&gt;After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible. Today, we’re releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.&lt;/p&gt;\n\n&lt;p&gt;This model performs better than our last release, and we hope you’ll like it thanks to its strong overall abilities.&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: chat.qwen.ai — just start chatting with the default model, and feel free to use the search button!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?auto=webp&amp;s=10c54f96c9b0f8a2ead569e3e5e97915476224de",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6baea679e548efcaaac74cffb282ff70f159dd23",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df4fdf4fdf2d8f18aa169c3917aff6a59354480a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd168c86688eec1b15ed22e24f23dd96a0b2be9d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=13fd449c88365fae792fbacc8076a6e633ad74e2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e563d574e4328f00b0b38e4cd232ecc1c21ff8cb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc97776dcddebc962894dcf5ba459481257122c",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "fCmGXIZe3dEk6g_UwQiEUl1wK88M5Bv1vrt320AGj8o"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oxyp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753118337,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I got sglang running a few months ago with Qwen3 30B-A3B and its performance impressed me so much that there is no desire from me at this point to run 70B+ models because I can reach over 600tok/s with a single 3090 with it (8 inferences running in parallel, 150 or so for a single inference, 140tok/s with power limited to 250W)\n\nMy question I'd like to answer now is how much of a leap can I expect to see with 5090? I will be gaming and doing image/video generation with the 5090 as well if I get one, and I have no plans to sell my pair of 3090s (though it would be at profit so i could potentially do that to save money)\n\nHowever lately there's not a lot of time for games and besides all titles I play still do run fine on Ampere even though I have a 4K 240hz monitor so I was really trying to get a 5090 this year but I guess I just have a sour taste in my mouth about it all. Image generation is fine with 24GB but video in particular could benefit from more grunt. Still, it's not been a tier 1 hobby of mine so it's really kind of a side benefit. There are also other things i like to do aspirationally (tinker with algorithms in CUDA and so on) that it would be cool to have but two 3090s is already so incredibly far beyond what I need for that. \n\n5090 are poised to become possible to obtain soon it seems, so I want some more complete data. \n\nI'd like to see if someone with a 5090 running linux can test my docker image and tell me what inference performance you're able to get, to help me make this purchasing decision. \n\nHere is the dockerfile: [https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336](https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336)\n\n* I can provide a built docker image (it is 18GB though) if you have trouble building or running that dockerfile. the instructions are in a comment inside, and should work even if you are not familiar with docker or k8s. If we need to fall back to running the image though I'd like to troubleshoot with you a bit so that I can potentially improve my dockerfile.\n* if you want to actually human-readably view the output, I use a (dependencyless) python script that extracts out the streamed output tokens from that curl response, I provide it here: [https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b](https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b) What you would do is take the example curl command and add ` | python3 stream_parser.py`\n\nMy 600+ tok/s performance number is had on my 3090 by modifying the input curl request to put 8 separate messages into the curl request. Let me know if you're having trouble figuring out the syntax for that... My hope is a 5090 should have the arithmetic intensity that it probably wants 12 or even more to batch in parallel to get the highest possible throughput. I would be hoping for a 3 or 4x speedup compared to 3090 but I somehow doubt that will be the case for single inference but it may be the case with multiple inference (which on an efficient runtime like sglang seems to be able to extract compute performance while saturating mem bandwidth). From a theoretical point of view, 1.79TB/s over 936GB/s should yield a speedup of 96% for single inference. That's actually quite a bit better than I expected...\n\nNow if we can hit 3x or 4x total throughput going from 3090 to 5090 that will be a go for me and I'll gladly purchase one. If not... I dunno if I can justify the cost. If it only provides a 2x speed gain over a 3090, that means in terms of LLM heavy lifting it is only consolidating my two 3090s into one GPU, and gives only a mild efficiency win (two 3090s at 250W vs one 5090 at probably 400W, not much less, only saving 100W) and no performance win which would not be all that compelling. If 4x though, that would represent some serious consolidation factor. My gut is telling me to expect something like 3.3x speedup. Which I hope is enough to push me over the edge because I sure do want the shiny. I just gotta talk myself into it.\n\nIf you look at the docker logs (which in the way i tell you to launch it will be visible in the terminal) it will show the latest tok/s metric. \n\nThank you.",
          "author_fullname": "t2_iifi6ul2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "5090 batched inference performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hqi8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753200765,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753199447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got sglang running a few months ago with Qwen3 30B-A3B and its performance impressed me so much that there is no desire from me at this point to run 70B+ models because I can reach over 600tok/s with a single 3090 with it (8 inferences running in parallel, 150 or so for a single inference, 140tok/s with power limited to 250W)&lt;/p&gt;\n\n&lt;p&gt;My question I&amp;#39;d like to answer now is how much of a leap can I expect to see with 5090? I will be gaming and doing image/video generation with the 5090 as well if I get one, and I have no plans to sell my pair of 3090s (though it would be at profit so i could potentially do that to save money)&lt;/p&gt;\n\n&lt;p&gt;However lately there&amp;#39;s not a lot of time for games and besides all titles I play still do run fine on Ampere even though I have a 4K 240hz monitor so I was really trying to get a 5090 this year but I guess I just have a sour taste in my mouth about it all. Image generation is fine with 24GB but video in particular could benefit from more grunt. Still, it&amp;#39;s not been a tier 1 hobby of mine so it&amp;#39;s really kind of a side benefit. There are also other things i like to do aspirationally (tinker with algorithms in CUDA and so on) that it would be cool to have but two 3090s is already so incredibly far beyond what I need for that. &lt;/p&gt;\n\n&lt;p&gt;5090 are poised to become possible to obtain soon it seems, so I want some more complete data. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to see if someone with a 5090 running linux can test my docker image and tell me what inference performance you&amp;#39;re able to get, to help me make this purchasing decision. &lt;/p&gt;\n\n&lt;p&gt;Here is the dockerfile: &lt;a href=\"https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336\"&gt;https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I can provide a built docker image (it is 18GB though) if you have trouble building or running that dockerfile. the instructions are in a comment inside, and should work even if you are not familiar with docker or k8s. If we need to fall back to running the image though I&amp;#39;d like to troubleshoot with you a bit so that I can potentially improve my dockerfile.&lt;/li&gt;\n&lt;li&gt;if you want to actually human-readably view the output, I use a (dependencyless) python script that extracts out the streamed output tokens from that curl response, I provide it here: &lt;a href=\"https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b\"&gt;https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b&lt;/a&gt; What you would do is take the example curl command and add &lt;code&gt;| python3 stream_parser.py&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My 600+ tok/s performance number is had on my 3090 by modifying the input curl request to put 8 separate messages into the curl request. Let me know if you&amp;#39;re having trouble figuring out the syntax for that... My hope is a 5090 should have the arithmetic intensity that it probably wants 12 or even more to batch in parallel to get the highest possible throughput. I would be hoping for a 3 or 4x speedup compared to 3090 but I somehow doubt that will be the case for single inference but it may be the case with multiple inference (which on an efficient runtime like sglang seems to be able to extract compute performance while saturating mem bandwidth). From a theoretical point of view, 1.79TB/s over 936GB/s should yield a speedup of 96% for single inference. That&amp;#39;s actually quite a bit better than I expected...&lt;/p&gt;\n\n&lt;p&gt;Now if we can hit 3x or 4x total throughput going from 3090 to 5090 that will be a go for me and I&amp;#39;ll gladly purchase one. If not... I dunno if I can justify the cost. If it only provides a 2x speed gain over a 3090, that means in terms of LLM heavy lifting it is only consolidating my two 3090s into one GPU, and gives only a mild efficiency win (two 3090s at 250W vs one 5090 at probably 400W, not much less, only saving 100W) and no performance win which would not be all that compelling. If 4x though, that would represent some serious consolidation factor. My gut is telling me to expect something like 3.3x speedup. Which I hope is enough to push me over the edge because I sure do want the shiny. I just gotta talk myself into it.&lt;/p&gt;\n\n&lt;p&gt;If you look at the docker logs (which in the way i tell you to launch it will be visible in the terminal) it will show the latest tok/s metric. &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?auto=webp&amp;s=c7cbcc7517e2406e2326e7a1eb6bdb9022c27fda",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=796041decb8c1250cbc2f301331b72f7385b477d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e3562243f324d16bc6d9dd09adb1da4e0b100b5",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=564e5f4bb6808064a14eb3965a6911671c3c9807",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a4f79362039959fa37eab208ae001245ccfe6e3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=912f966e123e94e32e7975fe8aebac89450a6b98",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6hqi8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "michaelsoft__binbows",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753199447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company's RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.\n\n \n\nRequirements:\n\n* Multilingual (German &amp; English)\n* Max. 7B parameters\n* Preferably compatible with Sentence-Transformers\n* Open-source\n\n \n\nModels based on MTEB Retrieval performance:\n\n[http://mteb-leaderboard.hf.space/?benchmark\\_name=MTEB%28Multilingual%2C+v2%29](http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29)\n\n* Qwen Embedding 8B / 4B\n* SFR-Embedding-Mistral\n* E5-mistral-7b-instruct\n* Snowflake-arctic-embed-m-v2.0\n\n \n\nI also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:\n\n* BGE (all variants)\n* mE5\n* All-MiniLM-L6-v1.5\n* Text-Embedding-3-Large (often used as a baseline)\n\n \n\nWould love to hear your thoughts or experiences, especially if you've worked on similar multilingual or domain-specific retrieval systems!\n\n",
          "author_fullname": "t2_k6flr0wx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-Tuning Multilingual Embedding Models for Industrial RAG System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68elw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753172119,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company&amp;#39;s RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.&lt;/p&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Multilingual (German &amp;amp; English)&lt;/li&gt;\n&lt;li&gt;Max. 7B parameters&lt;/li&gt;\n&lt;li&gt;Preferably compatible with Sentence-Transformers&lt;/li&gt;\n&lt;li&gt;Open-source&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;Models based on MTEB Retrieval performance:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29\"&gt;http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen Embedding 8B / 4B&lt;/li&gt;\n&lt;li&gt;SFR-Embedding-Mistral&lt;/li&gt;\n&lt;li&gt;E5-mistral-7b-instruct&lt;/li&gt;\n&lt;li&gt;Snowflake-arctic-embed-m-v2.0&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;I also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BGE (all variants)&lt;/li&gt;\n&lt;li&gt;mE5&lt;/li&gt;\n&lt;li&gt;All-MiniLM-L6-v1.5&lt;/li&gt;\n&lt;li&gt;Text-Embedding-3-Large (often used as a baseline)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts or experiences, especially if you&amp;#39;ve worked on similar multilingual or domain-specific retrieval systems!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?auto=webp&amp;s=1c3659b9b728ad6e80974340870a81fcaca748a0",
                  "width": 2473,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e4403396b3271cb41e3343fd3daf2f432ae3c37",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afdae3f5636f14bbbbdb4a9c9476beaba37383bf",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcf9a9a3b91639d396787d20b125c83d45d2dd41",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb4fac65e86637fc3c5896156424954a65818fc",
                    "width": 640,
                    "height": 331
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1321b3e29e18141a9cf9837f0ec1dea4f236eabf",
                    "width": 960,
                    "height": 496
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=271a28cd1ea8c4260dcf2d7f9dcf3842182678c4",
                    "width": 1080,
                    "height": 558
                  }
                ],
                "variants": {},
                "id": "b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m68elw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Maddin187",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753172119,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 insane SimpleQA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qn1n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753122070,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qn1n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753122070,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What do you guys think about the idea of sharing tokens with your team or family? It feels a bit silly that my friend and I each have the $200 Cursor plan, but together we only use around $250 worth. I think it would be great if we could just have shared one plan 350 dollar plan instead. Do you feel the same way?",
          "author_fullname": "t2_sck77urj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Shared subscription/token with Team or family",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6kre5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753206199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys think about the idea of sharing tokens with your team or family? It feels a bit silly that my friend and I each have the $200 Cursor plan, but together we only use around $250 worth. I think it would be great if we could just have shared one plan 350 dollar plan instead. Do you feel the same way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6kre5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Refrigerator9508",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6kre5/shared_subscriptiontoken_with_team_or_family/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6kre5/shared_subscriptiontoken_with_team_or_family/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753206199,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running llama.cpp on Ubuntu 22.04 with ROCm 6.2. I cloned the repo and built it like this:\n\nHIPCXX=\"$(hipconfig -l)/clang\" HIP_PATH=\"$(hipconfig -R)\" \\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\n    &amp;&amp; cmake --build build --config Release -- -j 16\n\nThen I run the model:\n\n./build/bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF\n\nBut I’m only getting around 10 tokens/sec. When I check system usage:\n- GPU utilization is stuck at 1%\n- VRAM usage is 0\n- CPU is at 100%\n\nLooks like it’s not using the GPU at all.\nrocm-smi can list all 4 GPUs\nllama.cpp also able to list 4 GPU devices\nMachine is not plugged in into any monitor, just ssh remotely\n\nAnyone have experience running llama.cpp with ROCm or on multiple AMD GPUs? Any specific flags or build settings I might be missing?\n\n",
          "author_fullname": "t2_ip4e2mp8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama.cpp on ROCm only running at 10 tokens/sec, GPU at 1% util. What am I missing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6khbt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753205572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running llama.cpp on Ubuntu 22.04 with ROCm 6.2. I cloned the repo and built it like this:&lt;/p&gt;\n\n&lt;p&gt;HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\n    &amp;amp;&amp;amp; cmake --build build --config Release -- -j 16&lt;/p&gt;\n\n&lt;p&gt;Then I run the model:&lt;/p&gt;\n\n&lt;p&gt;./build/bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF&lt;/p&gt;\n\n&lt;p&gt;But I’m only getting around 10 tokens/sec. When I check system usage:\n- GPU utilization is stuck at 1%\n- VRAM usage is 0\n- CPU is at 100%&lt;/p&gt;\n\n&lt;p&gt;Looks like it’s not using the GPU at all.\nrocm-smi can list all 4 GPUs\nllama.cpp also able to list 4 GPU devices\nMachine is not plugged in into any monitor, just ssh remotely&lt;/p&gt;\n\n&lt;p&gt;Anyone have experience running llama.cpp with ROCm or on multiple AMD GPUs? Any specific flags or build settings I might be missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6khbt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Reasonable_Can_5793",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753205572,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TL;DR: This is a **massive** step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download—no command line or Docker needed. It's never been easier to start building your own local, privacy-first screen-watching agents!\n\n  \nHey r/LocalLLaMA !!\n\nI am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don't want to!!\n\n**What's new in this update:**\n\n* 🚀 **1-Click Desktop App:** The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.\n* 🔔 **Pushover &amp; Discord Notifications:** SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with **Pushover** or to your community with a **Discord** bot. **Email** stays being reliable!!\n* 🛠️ **Continuous Improvement:** My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.\n\nFor those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.\n\nI don't want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!\n\n**Check it out and let me know what you think:**\n\n[**https://github.com/Roy3838/Observer**](https://github.com/Roy3838/Observer)",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Observer Desktop App is Here! + Discord/Pushover Notifications!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5y9wj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753139895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: This is a &lt;strong&gt;massive&lt;/strong&gt; step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download—no command line or Docker needed. It&amp;#39;s never been easier to start building your own local, privacy-first screen-watching agents!&lt;/p&gt;\n\n&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !!&lt;/p&gt;\n\n&lt;p&gt;I am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don&amp;#39;t want to!!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What&amp;#39;s new in this update:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;🚀 &lt;strong&gt;1-Click Desktop App:&lt;/strong&gt; The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.&lt;/li&gt;\n&lt;li&gt;🔔 &lt;strong&gt;Pushover &amp;amp; Discord Notifications:&lt;/strong&gt; SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with &lt;strong&gt;Pushover&lt;/strong&gt; or to your community with a &lt;strong&gt;Discord&lt;/strong&gt; bot. &lt;strong&gt;Email&lt;/strong&gt; stays being reliable!!&lt;/li&gt;\n&lt;li&gt;🛠️ &lt;strong&gt;Continuous Improvement:&lt;/strong&gt; My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Check it out and let me know what you think:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Roy3838/Observer\"&gt;&lt;strong&gt;https://github.com/Roy3838/Observer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?auto=webp&amp;s=a9ed130dbe40bc283accf677a568089896baa4f1",
                  "width": 4030,
                  "height": 2260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2763f5b07d8000852738cc8bbf6420bc7a793d3e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b05a1b6a5908644048b0f050c15a00d2bc5d9ed",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3f92bd549dcbc4e5089662042619f32c668a07e",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ad9d2d4f08a9916f026e28e9a30fd6d1711d5d",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=adab929349280395b1958efc7ed9e9e58d447654",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f25dcc577bb8503c95962d2f130fa431008cd692",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5y9wj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753139895,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pbj0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68ff5489be2450ce2200e81da6540a1dd25ed70a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753119154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?auto=webp&amp;s=86c8fc358fab39d0103c80888dc78d172e254fd0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41efd73e0b1f2f6245cc18321de9593d2f691f2a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ad4906ae01be77390f3512429fd8d526cdbff6b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc710aa210ed6e6fd519830b6e1c20372358b8dc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a081e0d80600d61cab0535bcecc71bbb337e1d1f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47fe182351ca7738d227c54e9f94e68766055478",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5pbj0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5pbj0/qwenqwen3235ba22binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "subreddit_subscribers": 502981,
          "created_utc": 1753119154,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi community,\n\nI’m facing two issues:\n\n1. I want to correct Hindi text. I feel using llms  is overkill for this task. I came across the GRMR 2B model, but it only supports English. My text is in Hindi.\n\n2. I want to transliterate Hindi to Hinglish. Again, I believe LLMs are too heavy for this and often make mistakes. Is there any lightweight solution I can run on Colab—maybe on an T4, A100 or L4 GPU?\n\nFor example, I have text like:\n\"जी शुरू करते है\"\nand I want to convert it to:\n\"Ji shuru karte hai\"\n\nPlease help.",
          "author_fullname": "t2_1b2w3w6d73",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Help/Suggestion Wanted] Hindi to Hinglish and Spell correction",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6jdyz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753203170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community,&lt;/p&gt;\n\n&lt;p&gt;I’m facing two issues:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I want to correct Hindi text. I feel using llms  is overkill for this task. I came across the GRMR 2B model, but it only supports English. My text is in Hindi.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I want to transliterate Hindi to Hinglish. Again, I believe LLMs are too heavy for this and often make mistakes. Is there any lightweight solution I can run on Colab—maybe on an T4, A100 or L4 GPU?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For example, I have text like:\n&amp;quot;जी शुरू करते है&amp;quot;\nand I want to convert it to:\n&amp;quot;Ji shuru karte hai&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6jdyz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Grouchy-Pin9500",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6jdyz/helpsuggestion_wanted_hindi_to_hinglish_and_spell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6jdyz/helpsuggestion_wanted_hindi_to_hinglish_and_spell/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753203170,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Corporate deployment.  \n\nCurrently deployed with multi a6000 ada but I'd like to add more vram to support multiple larger models for full scale deployment.  \n\nConsidering mi300x x 4 to maximize vram per $. Any deployments that dont play nice on amd hardware (flux) would use existing a6000 ada stack.\n\nAny other options I should consider?\n\nBudget is flexible within reason.\n\n\n",
          "author_fullname": "t2_ze8yz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "~75k budget. Best bang for the buck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6j69n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753202690,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Corporate deployment.  &lt;/p&gt;\n\n&lt;p&gt;Currently deployed with multi a6000 ada but I&amp;#39;d like to add more vram to support multiple larger models for full scale deployment.  &lt;/p&gt;\n\n&lt;p&gt;Considering mi300x x 4 to maximize vram per $. Any deployments that dont play nice on amd hardware (flux) would use existing a6000 ada stack.&lt;/p&gt;\n\n&lt;p&gt;Any other options I should consider?&lt;/p&gt;\n\n&lt;p&gt;Budget is flexible within reason.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6j69n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bohdanowicz",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6j69n/75k_budget_best_bang_for_the_buck/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6j69n/75k_budget_best_bang_for_the_buck/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753202690,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(like deepseek-r1 1.5b)\nI just can't think of any simple straightforward examples of tasks they're useful / good enough for. And answers on the internet and from other LLMs are just too vague.\n\nWhat kind of task with what kind of prompt, system prompt, overall setup worth doing with it? \n",
          "author_fullname": "t2_16jnzh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the use cases for 1.5B model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6d6um",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753188511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(like deepseek-r1 1.5b)\nI just can&amp;#39;t think of any simple straightforward examples of tasks they&amp;#39;re useful / good enough for. And answers on the internet and from other LLMs are just too vague.&lt;/p&gt;\n\n&lt;p&gt;What kind of task with what kind of prompt, system prompt, overall setup worth doing with it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6d6um",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nathman999",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753188511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have wondered if you can get usable speeds on something like ERNIE-4.5-300B-A47B ~Q3 or Q4 on 2x 3090's, 128gb of DDR5 and what can't fit into RAM running on PCIE NVME's in raid 0. I'm sure it wouldn't be fast but I wonder if it could be usable.",
          "author_fullname": "t2_zws5yqyow",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would using PCIE NVME in raid 0 for swap work to run larger models that don't fit into RAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6akeo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have wondered if you can get usable speeds on something like ERNIE-4.5-300B-A47B ~Q3 or Q4 on 2x 3090&amp;#39;s, 128gb of DDR5 and what can&amp;#39;t fit into RAM running on PCIE NVME&amp;#39;s in raid 0. I&amp;#39;m sure it wouldn&amp;#39;t be fast but I wonder if it could be usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6akeo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Celery769",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6akeo/would_using_pcie_nvme_in_raid_0_for_swap_work_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6akeo/would_using_pcie_nvme_in_raid_0_for_swap_work_to/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753180396,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is a beast!  Both in performance and to run.   Ernie is much smaller and easier to run.  It's 47B active, so going to be a bit slower, however it performs quite well.  I would call it K2's little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer.  It took longer to also get support for it into llama.cpp  \nI have been testing it out and I really like it.   For general chat, (logically, scientific, mathematically), it's straight to the point, doesn't beat around the bush or hew and haw.  Great instruction following too, very precise and to the point.  I haven't heard much about it, and I know that many can't run it, but you should really consider it and add it to the mix.   Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven't experimented much with them, but there might even be better.  I'm running Q6 from unsloth. temp/top\\_p 0.8, top\\_k 50, min\\_p 0.01",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do not sleep on ERNIE-4.5-300B-A47B especially if you can't Kimi K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5p69p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is a beast!  Both in performance and to run.   Ernie is much smaller and easier to run.  It&amp;#39;s 47B active, so going to be a bit slower, however it performs quite well.  I would call it K2&amp;#39;s little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer.  It took longer to also get support for it into llama.cpp&lt;br/&gt;\nI have been testing it out and I really like it.   For general chat, (logically, scientific, mathematically), it&amp;#39;s straight to the point, doesn&amp;#39;t beat around the bush or hew and haw.  Great instruction following too, very precise and to the point.  I haven&amp;#39;t heard much about it, and I know that many can&amp;#39;t run it, but you should really consider it and add it to the mix.   Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven&amp;#39;t experimented much with them, but there might even be better.  I&amp;#39;m running Q6 from unsloth. temp/top_p 0.8, top_k 50, min_p 0.01&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5p69p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753118830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just designed the core architecture for a RAG agent. I’m testing the foundational decision:  \n**Is it smart to use Langchain or LlamaIndex for this kind of agentic system? Or am I better off going more lightweight or custom?**\n\nI’ve included a visual of the architecture in the post. Would love your feedback, especially if you’ve worked with or scaled these frameworks.\n\n# 🔧 What I’m Building\n\nThis is a **simpler agentic RAG system**, designed to be modular and scalable, but lean enough to move fast. It’s not just a question-answer bot but structured with foresight to evolve into a fully agentic system later.\n\n**Core Components:**\n\n* A **Session Manager** for planning, task decomposition, and execution flow\n* A **Vector Store** for context retrieval\n* A **RAG pipeline** for combining retrieval + generation\n* A **State &amp; Memory Unit** for session history, context tracking, and intermediate reasoning\n* A clean chat I/O interface\n\n# 🧱 Design Principles\n\n* **Modularity**: Every component is cleanly separated\n* **Progressive Architecture**: Built to scale into multi tool-using system\n* **Context Awareness**: Dynamic memory and reasoning path tracking\n* **Agentic Behavior**: Even in its early form, it plans, tracks, and self-updates\n\nWould love feedback on:\n\n* Whether Langchain or LlamaIndex make sense as the foundation here\n* Where others hit scaling or architectural limitations with these\n* How to avoid building into a box I’ll regret later\n\nIf this is the wrong move, I'd rather fix it now. Appreciate any insights.",
          "author_fullname": "t2_hpqb9e8q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Am I making a mistake building my RAG agent with Langchain or LlamaIndex?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 116,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6gwgl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/z4CEyTRNaRHdbhDKbWtTgySbGCfnpODod9OwS0FKfXc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753197551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just designed the core architecture for a RAG agent. I’m testing the foundational decision:&lt;br/&gt;\n&lt;strong&gt;Is it smart to use Langchain or LlamaIndex for this kind of agentic system? Or am I better off going more lightweight or custom?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve included a visual of the architecture in the post. Would love your feedback, especially if you’ve worked with or scaled these frameworks.&lt;/p&gt;\n\n&lt;h1&gt;🔧 What I’m Building&lt;/h1&gt;\n\n&lt;p&gt;This is a &lt;strong&gt;simpler agentic RAG system&lt;/strong&gt;, designed to be modular and scalable, but lean enough to move fast. It’s not just a question-answer bot but structured with foresight to evolve into a fully agentic system later.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Core Components:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A &lt;strong&gt;Session Manager&lt;/strong&gt; for planning, task decomposition, and execution flow&lt;/li&gt;\n&lt;li&gt;A &lt;strong&gt;Vector Store&lt;/strong&gt; for context retrieval&lt;/li&gt;\n&lt;li&gt;A &lt;strong&gt;RAG pipeline&lt;/strong&gt; for combining retrieval + generation&lt;/li&gt;\n&lt;li&gt;A &lt;strong&gt;State &amp;amp; Memory Unit&lt;/strong&gt; for session history, context tracking, and intermediate reasoning&lt;/li&gt;\n&lt;li&gt;A clean chat I/O interface&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🧱 Design Principles&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: Every component is cleanly separated&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Progressive Architecture&lt;/strong&gt;: Built to scale into multi tool-using system&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Context Awareness&lt;/strong&gt;: Dynamic memory and reasoning path tracking&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agentic Behavior&lt;/strong&gt;: Even in its early form, it plans, tracks, and self-updates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Whether Langchain or LlamaIndex make sense as the foundation here&lt;/li&gt;\n&lt;li&gt;Where others hit scaling or architectural limitations with these&lt;/li&gt;\n&lt;li&gt;How to avoid building into a box I’ll regret later&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If this is the wrong move, I&amp;#39;d rather fix it now. Appreciate any insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/zptnshw2yfef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/zptnshw2yfef1.png?auto=webp&amp;s=76e75b90a419c582aabc49fe139e7c91afe65a89",
                  "width": 847,
                  "height": 702
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ffe2b2f0146b5018418f2cf8a47aa964f7d008b",
                    "width": 108,
                    "height": 89
                  },
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1855fe9db743f1b2243d7403d41f5e2cce1d1e1",
                    "width": 216,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=24992bb3d858161a69debf7f29be67edbf56aa85",
                    "width": 320,
                    "height": 265
                  },
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=501fc562aaff34d23aba6f075a85b7873e823e86",
                    "width": 640,
                    "height": 530
                  }
                ],
                "variants": {},
                "id": "7ez5SdWmQCvcDpjSTikUtrexkFQ86FGFNJ_9HyyGiEQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6gwgl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "duke_x91",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6gwgl/am_i_making_a_mistake_building_my_rag_agent_with/",
          "stickied": false,
          "url": "https://i.redd.it/zptnshw2yfef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753197551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Inspired by the brain's hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.\n\nThough not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. 🌟\n\nhttps://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f\n\n📄Paper: [https://arxiv.org/abs/2506.21734](https://arxiv.org/abs/2506.21734)\n\n💻Code: [https://github.com/sapientinc/HRM](https://github.com/sapientinc/HRM)  \n",
          "author_fullname": "t2_dkj51uv0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[New Architecture] Hierarchical Reasoning Model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uslhwa2nh8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 26,
                  "x": 108,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ab6ae97e133c2649b92b0c8c0ef857c3d79bcc2"
                },
                {
                  "y": 53,
                  "x": 216,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bc3c540c30cdf2b3b12433e760b60c3ceef936c"
                },
                {
                  "y": 79,
                  "x": 320,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=056aa9e724c8d47937b0e044af74a221fc2a9886"
                },
                {
                  "y": 158,
                  "x": 640,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a710e5fa6141d56c1a472f9ac1de521e98a86a5"
                },
                {
                  "y": 237,
                  "x": 960,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ccd9b2f1992c3c691f918fd9309433d1e043fe2"
                },
                {
                  "y": 267,
                  "x": 1080,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc4733d2af3bd39a37a4c49868c5e8afeca4ff94"
                }
              ],
              "s": {
                "y": 501,
                "x": 2026,
                "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f"
              },
              "id": "uslhwa2nh8ef1"
            }
          },
          "name": "t3_1m5jr1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 82,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 82,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/jTPeTr-ZhJfvZ_xmMKlbONUjqH188dSuwhEIvPibXE8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753106572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by the brain&amp;#39;s hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.&lt;/p&gt;\n\n&lt;p&gt;Though not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. 🌟&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f\"&gt;https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;📄Paper: &lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;💻Code: &lt;a href=\"https://github.com/sapientinc/HRM\"&gt;https://github.com/sapientinc/HRM&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5jr1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "imonenext",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753106572,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of \"ultra high bandwidth memory\", but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it's going to let an on device assistant use a lot more context.",
          "author_fullname": "t2_lkljr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fmlp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 145,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 145,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=140&amp;height=102&amp;crop=140:102,smart&amp;auto=webp&amp;s=f46b99f9160b02acaab35b0793c2419a717f902a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753094763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "cnx-software.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of &amp;quot;ultra high bandwidth memory&amp;quot;, but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it&amp;#39;s going to let an on device assistant use a lot more context.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?auto=webp&amp;s=a3357893385aa57e61c85776502b465fe73661a4",
                  "width": 1184,
                  "height": 868
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d2a2efbd9333bbc8e7495d96c37dc9a67f94f7",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4320f14e39faee0bd0ec022e73a0260326f90d4",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=36df6ea44afc6e837f534a02a66625d799b64a88",
                    "width": 320,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ec21866ca44554bfe2c9ada5521c937f241afc3",
                    "width": 640,
                    "height": 469
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f1fc5136f191685dd65bd8cc69f5a732d025a2",
                    "width": 960,
                    "height": 703
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24e96efb97efa4c62ad46fa709863ab37c9c8266",
                    "width": 1080,
                    "height": 791
                  }
                ],
                "variants": {},
                "id": "p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5fmlp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PmMeForPCBuilds",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/",
          "stickied": false,
          "url": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
          "subreddit_subscribers": 502981,
          "created_utc": 1753094763,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_5n4jepc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Truly open LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69th7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=fec374170d117a6b9e9aef82babffa1e8039217a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753177663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "shchegrikovich.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://shchegrikovich.substack.com/p/truly-open-llms",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?auto=webp&amp;s=0647966f28fc4ee54cd9d415fd52ce6c38f79c03",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e50454aae6b161b0cafb9fcfd612d0809f5c73d9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a50eebcb744a514321bedd747dcc0dd7361cde1f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0658f86a17beb93a12b50a6781f7ecf2fecfd7ab",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3c812bd7d02ce4e898102f57a7b12c1898b9464",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f91de7eecda3e724888794a8b6e13c1ee3592d38",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a229cc66113465a58966fe1a9f7a47cc196ba9ad",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m69th7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoodSamaritan333",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69th7/truly_open_llms/",
          "stickied": false,
          "url": "https://shchegrikovich.substack.com/p/truly-open-llms",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m experimenting with multiple MCP servers and trying to understand how others are managing them across different AI tools like Claude Desktop, GPTs, Gemini clients, etc.\n\nDo you manually add them in each config file?\n\nAre you using any centralized tool or dashboard to start/stop/edit MCP servers?\n\nAny best practices or tooling you recommend?\n\n👉 I’m currently building a lightweight desktop tool that aims to solve this — centralized MCP management, multi-client compatibility, and better UX for non-technical users.\n\nWould love to hear how you currently do it — and what you’d want in a tool like this. Would anyone be interested in testing the beta later on?\n\nThanks in advance!",
          "author_fullname": "t2_9f7exri8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🧠 How are you managing MCP servers across different AI apps (Claude, GPTs, Gemini etc.)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69qs3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m experimenting with multiple MCP servers and trying to understand how others are managing them across different AI tools like Claude Desktop, GPTs, Gemini clients, etc.&lt;/p&gt;\n\n&lt;p&gt;Do you manually add them in each config file?&lt;/p&gt;\n\n&lt;p&gt;Are you using any centralized tool or dashboard to start/stop/edit MCP servers?&lt;/p&gt;\n\n&lt;p&gt;Any best practices or tooling you recommend?&lt;/p&gt;\n\n&lt;p&gt;👉 I’m currently building a lightweight desktop tool that aims to solve this — centralized MCP management, multi-client compatibility, and better UX for non-technical users.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how you currently do it — and what you’d want in a tool like this. Would anyone be interested in testing the beta later on?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m69qs3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hihurmuz",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69qs3/how_are_you_managing_mcp_servers_across_different/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69qs3/how_are_you_managing_mcp_servers_across_different/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.",
          "author_fullname": "t2_y0abrfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI 395+ 64GB vs 128GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5s6d1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753125511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5s6d1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cfogrady",
          "discussion_type": null,
          "num_comments": 87,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753125511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yo so am new to this and i want to run a local llm that answers questions using my custom dataset which is basically some financial data .\nI created a Q&amp;A dataset and an instruction based data set and my llm refuses to use them \nIve finetuned my llm using TorchTune \nAnd also tried Litgpt \nIts a llama 3.2 3B instruct model .\n\nAlso if theres a way to use a RAG instead or if there's a model that can retrieve info from pdf and Excel spreadsheets would be awesome, thanks 👍",
          "author_fullname": "t2_5ecncz67",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to apply a custom dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69m60",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753176910,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yo so am new to this and i want to run a local llm that answers questions using my custom dataset which is basically some financial data .\nI created a Q&amp;amp;A dataset and an instruction based data set and my llm refuses to use them \nIve finetuned my llm using TorchTune \nAnd also tried Litgpt \nIts a llama 3.2 3B instruct model .&lt;/p&gt;\n\n&lt;p&gt;Also if theres a way to use a RAG instead or if there&amp;#39;s a model that can retrieve info from pdf and Excel spreadsheets would be awesome, thanks 👍&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m69m60",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oG17DoGe",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69m60/how_to_apply_a_custom_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69m60/how_to_apply_a_custom_dataset/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753176910,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oyf5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68ff5489be2450ce2200e81da6540a1dd25ed70a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118362,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?auto=webp&amp;s=86c8fc358fab39d0103c80888dc78d172e254fd0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41efd73e0b1f2f6245cc18321de9593d2f691f2a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ad4906ae01be77390f3512429fd8d526cdbff6b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc710aa210ed6e6fd519830b6e1c20372358b8dc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a081e0d80600d61cab0535bcecc71bbb337e1d1f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47fe182351ca7738d227c54e9f94e68766055478",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oyf5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oyf5/qwenqwen3235ba22binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "subreddit_subscribers": 502981,
          "created_utc": 1753118362,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen a few people asking whether [GPUStack](https://github.com/gpustack/gpustack) is essentially a multi-node version of Ollama. I’ve used both, and here’s a breakdown for anyone curious.\n\n**Short answer:** GPUStack is *not just* Ollama with clustering — it's a more general-purpose, production-ready LLM service platform with multi-backend support, hybrid GPU/OS compatibility, and cluster management features.\n\n# Core Differences\n\n|Feature|Ollama|GPUStack|\n|:-|:-|:-|\n|Single-node use|✅ Yes|✅ Yes|\n|Multi-node cluster|❌|✅ Supports distributed + heterogeneous cluster|\n|Model formats|GGUF only|GGUF (llama-box), Safetensors (vLLM), Ascend (MindIE), Audio (vox-box)|\n|Inference backends|llama.cpp|llama-box, vLLM, MindIE, vox-box|\n|OpenAI-compatible API|✅|✅ Full API compatibility (/v1, /v1-openai)|\n|Deployment methods|CLI only|Script / Docker / pip (Linux, Windows, macOS)|\n|Cluster management UI|❌|✅ Web UI with GPU/worker/model status|\n|Model recovery/failover|❌|✅ Auto recovery + compatibility checks|\n|Use in Dify / RAGFlow|Partial|✅ Fully integrated|\n\n# Who is GPUStack for?\n\nIf you:\n\n* Have multiple PCs or GPU servers\n* Want to centrally manage model serving\n* Need both GGUF and safetensors support\n* Run LLMs in production with monitoring, load balancing, or distributed inference\n\n...then it’s worth checking out.\n\n# Installation (Linux)\n\n    bashCopyEditcurl -sfL https://get.gpustack.ai | sh -s -\n    \n\nDocker (recommended):\n\n    bashCopyEditdocker run -d --name gpustack \\\n      --restart=unless-stopped \\\n      --gpus all \\\n      --network=host \\\n      --ipc=host \\\n      -v gpustack-data:/var/lib/gpustack \\\n      gpustack/gpustack\n    \n\nThen add workers with:\n\n    bashCopyEditgpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n    \n\nGitHub: [https://github.com/gpustack/gpustack](https://github.com/gpustack/gpustack)  \nDocs: [https://docs.gpustack.ai](https://docs.gpustack.ai)\n\nLet me know if you’re running a local LLM cluster — curious what stacks others are using.",
          "author_fullname": "t2_uexbcvjr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is GPUStack the Cluster Version of Ollama? Comparison + Alternatives",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67a12",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753167757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a few people asking whether &lt;a href=\"https://github.com/gpustack/gpustack\"&gt;GPUStack&lt;/a&gt; is essentially a multi-node version of Ollama. I’ve used both, and here’s a breakdown for anyone curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Short answer:&lt;/strong&gt; GPUStack is &lt;em&gt;not just&lt;/em&gt; Ollama with clustering — it&amp;#39;s a more general-purpose, production-ready LLM service platform with multi-backend support, hybrid GPU/OS compatibility, and cluster management features.&lt;/p&gt;\n\n&lt;h1&gt;Core Differences&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Feature&lt;/th&gt;\n&lt;th align=\"left\"&gt;Ollama&lt;/th&gt;\n&lt;th align=\"left\"&gt;GPUStack&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Single-node use&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Yes&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Yes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Multi-node cluster&lt;/td&gt;\n&lt;td align=\"left\"&gt;❌&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Supports distributed + heterogeneous cluster&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Model formats&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF only&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF (llama-box), Safetensors (vLLM), Ascend (MindIE), Audio (vox-box)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Inference backends&lt;/td&gt;\n&lt;td align=\"left\"&gt;llama.cpp&lt;/td&gt;\n&lt;td align=\"left\"&gt;llama-box, vLLM, MindIE, vox-box&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenAI-compatible API&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Full API compatibility (/v1, /v1-openai)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Deployment methods&lt;/td&gt;\n&lt;td align=\"left\"&gt;CLI only&lt;/td&gt;\n&lt;td align=\"left\"&gt;Script / Docker / pip (Linux, Windows, macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Cluster management UI&lt;/td&gt;\n&lt;td align=\"left\"&gt;❌&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Web UI with GPU/worker/model status&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Model recovery/failover&lt;/td&gt;\n&lt;td align=\"left\"&gt;❌&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Auto recovery + compatibility checks&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Use in Dify / RAGFlow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Partial&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Fully integrated&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Who is GPUStack for?&lt;/h1&gt;\n\n&lt;p&gt;If you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have multiple PCs or GPU servers&lt;/li&gt;\n&lt;li&gt;Want to centrally manage model serving&lt;/li&gt;\n&lt;li&gt;Need both GGUF and safetensors support&lt;/li&gt;\n&lt;li&gt;Run LLMs in production with monitoring, load balancing, or distributed inference&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...then it’s worth checking out.&lt;/p&gt;\n\n&lt;h1&gt;Installation (Linux)&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditcurl -sfL https://get.gpustack.ai | sh -s -\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docker (recommended):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditdocker run -d --name gpustack \\\n  --restart=unless-stopped \\\n  --gpus all \\\n  --network=host \\\n  --ipc=host \\\n  -v gpustack-data:/var/lib/gpustack \\\n  gpustack/gpustack\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then add workers with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditgpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/gpustack/gpustack\"&gt;https://github.com/gpustack/gpustack&lt;/a&gt;&lt;br/&gt;\nDocs: &lt;a href=\"https://docs.gpustack.ai\"&gt;https://docs.gpustack.ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you’re running a local LLM cluster — curious what stacks others are using.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?auto=webp&amp;s=36e5d5b346040aad01c2d98a3687e53ff43f1e74",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30f5ae864bd715df1fff1d9fca1b871646878411",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d5352bbba2dab73ce92606dbbfae7b8cf522031",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7162311c2cc0d517c59c8ab11f9a534c3bda43df",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea0e488446c26927d44f14cd2b94edbfedb63b72",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e99b36307d9c32c12637550f6a5bfaf4c4ab483",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=874fc061c61ab7b83973e9ef244a3dafd88075ba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67a12",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Issac_jo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67a12/is_gpustack_the_cluster_version_of_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67a12/is_gpustack_the_cluster_version_of_ollama/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753167757,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that’s 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon.\nThis is probably the last open source project we will see from the author for  a while, but looks very very interesting.",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DMOSpeech 2: 2x faster + higher-quality F5-TTS from the author of StyleTTS 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mzxt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6f166dfac0527db4273e73fd28eabe37b909084c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that’s 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon.\nThis is probably the last open source project we will see from the author for  a while, but looks very very interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/yl4579/DMOSpeech2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?auto=webp&amp;s=e79caffc600940aacae822bf3d08a2972d97a9b3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a782ebf4637087ab602e003b76d529f0b2b9b0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4076be77f5945db6c69125ad169cbbf7a337377f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a24fc93b1713526b9869d4f3049face873e33bb1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=757c96b9786749c821edc73c85d3500a7f6d30fc",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49a731ff6d20f17d0c91af2ed14936af2a706c37",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2eb69e689a57d5034d4de02201511409b9c8993a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5mzxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mzxt/dmospeech_2_2x_faster_higherquality_f5tts_from/",
          "stickied": false,
          "url": "https://github.com/yl4579/DMOSpeech2",
          "subreddit_subscribers": 502981,
          "created_utc": 1753114028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_169jzqdxe5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3-3B training logs and intermediate checkpoints",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 94,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5m1et",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4ybrjEuV5RvXq5bR2OKH4U7Osu4se3imdtVZT91_5hA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753111884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/fcyltq1nx8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/fcyltq1nx8ef1.png?auto=webp&amp;s=561e48b7a9066f5e3a65cb4df61d4bcee1d3623d",
                  "width": 2384,
                  "height": 1608
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7312a97ee7ebc5986c39d4b65b8d2c7104ed72bb",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06e6cd1192b19cd320fac196b58a7f384f175290",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76c1665297b63d147f74996bd94da849649689ae",
                    "width": 320,
                    "height": 215
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=992660889d501145b2a21434e2e7b0563e643863",
                    "width": 640,
                    "height": 431
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f360d2d0886afbde91f505a0640db28051fde90",
                    "width": 960,
                    "height": 647
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8251eb5dab9d71321a09bb3fd6b9e6fc2388a343",
                    "width": 1080,
                    "height": 728
                  }
                ],
                "variants": {},
                "id": "U-FI27gby_ykbgKbgdzrNHEru7LMZIbTK_iFSM3XWRs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5m1et",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eliebakk",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5m1et/smollm33b_training_logs_and_intermediate/",
          "stickied": false,
          "url": "https://i.redd.it/fcyltq1nx8ef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753111884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nFirstly, I’m not a developer, so forgive me if I don’t ask as clearly as others, I hope this makes sense.\n\nI'm trying to get Chatterbox TTS ( local AI voice tool with Gradio UI) working on my **Windows 11** machine using **Conda** and a local **Python 3.11.3 environment**. I’ve installed the app and interface successfully, but I’m stuck with import errors and GPU not being used. Here’s the key info:\n\n* **GPU:** RTX 4060 (8GB), CUDA 12.7 installed\n* **Python:** 3.11.3 (inside Conda)\n* **PyTorch:** Installed via pip/conda (tried both), but errors persist\n* **TorchAudio:** Likely not aligned with correct PyTorch/CUDA version\n* **Gradio UI:** Loads, but model doesn't run (import error)\n\nThe critical error:\n\nlua\n\nCopyEdit\n\nImportError: DLL load failed while importing \\_C: The specified module could not be found.\n\nI understand this might be due to mismatched **PyTorch / CUDA / TorchAudio** versions — but the **CUDA 12.7 runtime** doesn't show up on most PyTorch install tables (latest listed is 12.1).\n\n**Questions:**\n\n1. **Can I safely use a PyTorch build meant for CUDA 12.1 if I have 12.7 installed?**\n2. **Which PyTorch + TorchAudio versions are guaranteed to work together (and with Chatterbox) under CUDA 12.7?**\n3. Is there a known *minimal install combo* that just works?\n4. Should I downgrade CUDA to 12.1, or can I work with what I have?\n\nI’m not a developer, so detailed explanations or clear steps would be hugely appreciated. Thanks in advance!",
          "author_fullname": "t2_94iwvjrz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox CUDA and PyTorch problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6cfou",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Firstly, I’m not a developer, so forgive me if I don’t ask as clearly as others, I hope this makes sense.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get Chatterbox TTS ( local AI voice tool with Gradio UI) working on my &lt;strong&gt;Windows 11&lt;/strong&gt; machine using &lt;strong&gt;Conda&lt;/strong&gt; and a local &lt;strong&gt;Python 3.11.3 environment&lt;/strong&gt;. I’ve installed the app and interface successfully, but I’m stuck with import errors and GPU not being used. Here’s the key info:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4060 (8GB), CUDA 12.7 installed&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Python:&lt;/strong&gt; 3.11.3 (inside Conda)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt; Installed via pip/conda (tried both), but errors persist&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;TorchAudio:&lt;/strong&gt; Likely not aligned with correct PyTorch/CUDA version&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gradio UI:&lt;/strong&gt; Loads, but model doesn&amp;#39;t run (import error)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The critical error:&lt;/p&gt;\n\n&lt;p&gt;lua&lt;/p&gt;\n\n&lt;p&gt;CopyEdit&lt;/p&gt;\n\n&lt;p&gt;ImportError: DLL load failed while importing _C: The specified module could not be found.&lt;/p&gt;\n\n&lt;p&gt;I understand this might be due to mismatched &lt;strong&gt;PyTorch / CUDA / TorchAudio&lt;/strong&gt; versions — but the &lt;strong&gt;CUDA 12.7 runtime&lt;/strong&gt; doesn&amp;#39;t show up on most PyTorch install tables (latest listed is 12.1).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Can I safely use a PyTorch build meant for CUDA 12.1 if I have 12.7 installed?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Which PyTorch + TorchAudio versions are guaranteed to work together (and with Chatterbox) under CUDA 12.7?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Is there a known &lt;em&gt;minimal install combo&lt;/em&gt; that just works?&lt;/li&gt;\n&lt;li&gt;Should I downgrade CUDA to 12.1, or can I work with what I have?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’m not a developer, so detailed explanations or clear steps would be hugely appreciated. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6cfou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kevin-she",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfou/chatterbox_cuda_and_pytorch_problem/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfou/chatterbox_cuda_and_pytorch_problem/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753186390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1ipy2mlwcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fcdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 114,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 114,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=140&amp;height=111&amp;crop=140:111,smart&amp;auto=webp&amp;s=afd01d73738739fc5f4d0d7e20210d85f5c6a7af",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753093764,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techpowerup.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?auto=webp&amp;s=1ab5566fa5cfc48759389d6459ab423c6a6d93ae",
                  "width": 1024,
                  "height": 815
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac4eb006eed458f45d94d2092f2b1d96e9111985",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e28f32683b490d6cd796b37d8e18e66ba9e8cc5",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d69b2795cdbb216ae0415feac75bbe4d91eed6ce",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b8ad27cd8415b4fcbb76f37f1c26b02e1e7a72d",
                    "width": 640,
                    "height": 509
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=da909b3e1331d3458fe14a0c9335539ce72f2955",
                    "width": 960,
                    "height": 764
                  }
                ],
                "variants": {},
                "id": "bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5fcdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OwnWitness2836",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/",
          "stickied": false,
          "url": "https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters",
          "subreddit_subscribers": 502981,
          "created_utc": 1753093764,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI am making a tool that needs to analyze a conversation (non-English) between two people. The conversation is provided to me in audio format. I am currently using OpenAI Whisper to transcribe and feed the transcription to ChatGPT-4o model through the API for analysis.\n\nSo far, it's doing a fair job. Sometimes, though, reading the transcription, I find it hard to figure out which speaker is speaking what. I have to listen to the audio to figure it out. I am wondering if ChatGPT-4o would also sometimes find it hard to follow the conversation from the transcription. I think that adding a speaker diarization step might make the transcription easier to understand and analyze.\n\nI am looking for Speaker Diarization tools that I can use. I have tried using pyannote speaker-diarization-3.1, but I find it does not work very well. What are some other options that I can look at?",
          "author_fullname": "t2_1pot8iygav",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What Speaker Diarization tools should I look into?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6741z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753167129,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am making a tool that needs to analyze a conversation (non-English) between two people. The conversation is provided to me in audio format. I am currently using OpenAI Whisper to transcribe and feed the transcription to ChatGPT-4o model through the API for analysis.&lt;/p&gt;\n\n&lt;p&gt;So far, it&amp;#39;s doing a fair job. Sometimes, though, reading the transcription, I find it hard to figure out which speaker is speaking what. I have to listen to the audio to figure it out. I am wondering if ChatGPT-4o would also sometimes find it hard to follow the conversation from the transcription. I think that adding a speaker diarization step might make the transcription easier to understand and analyze.&lt;/p&gt;\n\n&lt;p&gt;I am looking for Speaker Diarization tools that I can use. I have tried using pyannote speaker-diarization-3.1, but I find it does not work very well. What are some other options that I can look at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6741z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Chemical_Gas3710",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6741z/what_speaker_diarization_tools_should_i_look_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6741z/what_speaker_diarization_tools_should_i_look_into/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753167129,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been seeing loads of developers here going on about how LLM integraded IDE's like Windsurf and Cursor totally changed their coding. Of course, I was interested and wanted to give it a go. Spoke to work about it, and the boss just said \"no way dude\" GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data gets transferred to the US, too risky, blah blah. So no Cursor and Windsurf for me.\n\nHonestly, I get it. Not mad at my company they're just doing their job and don't want to get fined But man, still sucks. We are still stuck in legacy workflows because every new AI tool is geared for US devs first. Feels like being left behind not because the tech exists, but because we simply can't utilize it. And sure, I do understand the GDPR thing is big deal and that there is a chanche PII and API keys included in the code by accident. But still… it sucks.\n\nDoes anyone else get stuck with this? Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What are other EU devs/teams doing? Self-hosting? Or just keeping to old tools?",
          "author_fullname": "t2_sck77urj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EU is being left behinde and it sucks!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n6lq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.59,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753114446,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been seeing loads of developers here going on about how LLM integraded IDE&amp;#39;s like Windsurf and Cursor totally changed their coding. Of course, I was interested and wanted to give it a go. Spoke to work about it, and the boss just said &amp;quot;no way dude&amp;quot; GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data gets transferred to the US, too risky, blah blah. So no Cursor and Windsurf for me.&lt;/p&gt;\n\n&lt;p&gt;Honestly, I get it. Not mad at my company they&amp;#39;re just doing their job and don&amp;#39;t want to get fined But man, still sucks. We are still stuck in legacy workflows because every new AI tool is geared for US devs first. Feels like being left behind not because the tech exists, but because we simply can&amp;#39;t utilize it. And sure, I do understand the GDPR thing is big deal and that there is a chanche PII and API keys included in the code by accident. But still… it sucks.&lt;/p&gt;\n\n&lt;p&gt;Does anyone else get stuck with this? Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What are other EU devs/teams doing? Self-hosting? Or just keeping to old tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5n6lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Refrigerator9508",
          "discussion_type": null,
          "num_comments": 150,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753114446,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)\n\nJust wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)\n\nSo type out your prompt like this: \n\n* \\[Action\\] \\[UI type or page\\] \\[Framework(s)\\] \\[Key features\\] \\[Style (optional)\\]\n*  Examples:\n\n   * `Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.`\n   * `Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.`\n   * `Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.`\n   * `Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.`\n\nIf it is within the context, then you can additionally add edits.\n\nHere's a prompt template:\n\n* `Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]`\n\nAdditional things that are supported -&gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.\n\nRemember, its only an 8B model!\n\nWe are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. \n\nMake sure to join the community for more support. (Link in Huggingface!)",
          "author_fullname": "t2_7mx42xse",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UIGEN-X 8B supports React Headless, Flutter, React Native, Static Site Generators, Tauri, Vue, Gradio/Python, Tailwind, and prompt-based design. GGUF/GPTQ/MLX Available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kzzqul8nf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1469bca066e6dc8c4de5bbfbf70f8cb8da95d3f"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=690e827a5f003cd998910dbf524e31b73ec2db01"
                },
                {
                  "y": 211,
                  "x": 320,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4891ca95bed5b4dde677b103efacb0d0eb83c604"
                },
                {
                  "y": 422,
                  "x": 640,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=099f2fc6e3b5972b2c6aef17adae108361623908"
                },
                {
                  "y": 633,
                  "x": 960,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5482ce8142b6ea19c74d2fc590aff9b0f1a98db"
                },
                {
                  "y": 712,
                  "x": 1080,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42be0fc4e87a423ce4553e87cd0e02e8cc7403f1"
                }
              ],
              "s": {
                "y": 1251,
                "x": 1896,
                "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=1896&amp;format=png&amp;auto=webp&amp;s=720ab948b90f645d72464f3b1284749a8404c6ad"
              },
              "id": "kzzqul8nf8ef1"
            },
            "iv8y70kfg8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a45bdc2372e5c9d718e3c14b37cce4647d522ac"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=832e91e8046f6ec8fd954d5ea4f31ddaba93091e"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5194344fca943ac8a92d397bdd2d4229ac50941"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbeff775b694d92aadda1695d0d25455717df09f"
                },
                {
                  "y": 640,
                  "x": 960,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b61e1958cc6204508a5a54eb6faa52028d7b5bd3"
                },
                {
                  "y": 720,
                  "x": 1080,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9c323a72a30535b3747f536878ce0741be12614"
                }
              ],
              "s": {
                "y": 1271,
                "x": 1906,
                "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=e4168909aeb7910a1decb0e675e95dfc99172c97"
              },
              "id": "iv8y70kfg8ef1"
            },
            "s968gxs0g8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=409ea33391501bf0e1b3668cd6701584e2d6328a"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f3fe42af5e89f5bfb82327d8bdca78b9f676aa6"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afe29e0fd00f03e24733ba7561bc1fd1dce58aba"
                },
                {
                  "y": 427,
                  "x": 640,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee72596c92bb36c8533edc20100db23fbf59feb9"
                },
                {
                  "y": 641,
                  "x": 960,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=40291f8d7edfd5b2938a1017f5f7003cea6eea64"
                },
                {
                  "y": 721,
                  "x": 1080,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fcf187a13cb3e659ee2e4349b2239f6691f238ce"
                }
              ],
              "s": {
                "y": 1274,
                "x": 1907,
                "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=03c2b53b770f0e0199f946f45413e52f38101e43"
              },
              "id": "s968gxs0g8ef1"
            },
            "bkgben5hn8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebe469a22e8a696f6ade5d03df0afb99319ee40a"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=397c229507fa92cd0b2b700ecccd364b481a5d64"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acd20ec90f369e40b727066d598fc2b7edca4200"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=381cf7575766e29c2cee139dcdf1749c60b7b5a3"
                },
                {
                  "y": 639,
                  "x": 960,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2cf511ff6f043cc035e7a29da943acaecc33bd8"
                },
                {
                  "y": 719,
                  "x": 1080,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de8973571c93536552a02bae099f00622832e8b6"
                }
              ],
              "s": {
                "y": 1272,
                "x": 1909,
                "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=0ce0953efdfe2dc0a0167b401829ee4e78834594"
              },
              "id": "bkgben5hn8ef1"
            },
            "baogvk7lf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=303f5769de618aedc68d5befd55c0f23ad537998"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de639214f79d2d4e168910bdf7db8b2172d9de5b"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebc6b8b5b37c7d198ab01c45349ebaef3165a921"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fc9d239d6ad5c911867d958b15710cb6644d4b2"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3248842e9756c380dc1254d6ab7f23dba5a99e27"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb1d84003b1279f8c8656aa93a1111beacc131a8"
                }
              ],
              "s": {
                "y": 1249,
                "x": 1902,
                "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=d3b3d33655c346bf5bc703fa7512b6076b23c02b"
              },
              "id": "baogvk7lf8ef1"
            },
            "qlq88ceum8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=11392437c6eae7c3cb80093689973f601ba47f75"
                },
                {
                  "y": 147,
                  "x": 216,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8385ab7c59df8821a90902570bf798a646e92c0"
                },
                {
                  "y": 218,
                  "x": 320,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9015853f431160e764d4d2c07353f81ab3bdaf1"
                },
                {
                  "y": 436,
                  "x": 640,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f9b1c5463b34aa1d4a5064888091f446d588916"
                },
                {
                  "y": 654,
                  "x": 960,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58c5783739adb38ad8cf28b7f105be92b48bcccf"
                },
                {
                  "y": 736,
                  "x": 1080,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fca853f214192cc056b4a0d81f3bb2fa50ab2da3"
                }
              ],
              "s": {
                "y": 1276,
                "x": 1872,
                "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=1872&amp;format=png&amp;auto=webp&amp;s=8f2da76ecd06758cb6411dcaac6e07a89f684c92"
              },
              "id": "qlq88ceum8ef1"
            },
            "ec0ih2x5g8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9379f1e63a89f5367dfbfd32b9a8df532304055"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f434f85c9033511251c7f06b8c15761163442aa"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b033b91dc8c3d9b67a9812dc157b2f858852c15"
                },
                {
                  "y": 425,
                  "x": 640,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e7cfb3e5b4418180e7274c15d302f84d9d64e6cc"
                },
                {
                  "y": 637,
                  "x": 960,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f2d7171901ffd3b3fa481153f1df1277152d588"
                },
                {
                  "y": 717,
                  "x": 1080,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b07979b7f543d84d9c4384c6e77b8b911837df8"
                }
              ],
              "s": {
                "y": 1279,
                "x": 1925,
                "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=1925&amp;format=png&amp;auto=webp&amp;s=d8d041b099fe8e0d77604f0e37a8c0f85418ecd3"
              },
              "id": "ec0ih2x5g8ef1"
            },
            "9xa2d1o5f8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b935462ab2f8008aab03cd92672934b99a6d50f"
                },
                {
                  "y": 145,
                  "x": 216,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5bfb820526679232d933594d6a207edf0c3c3bd"
                },
                {
                  "y": 215,
                  "x": 320,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=30f7ea1e4bf5a5150b76e4b873193c82577deefd"
                },
                {
                  "y": 431,
                  "x": 640,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd6665da47568cba2ae3fbe0c12342303e7361f7"
                },
                {
                  "y": 646,
                  "x": 960,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=acbf35cb0b854faffc8500847045ddee546974e7"
                },
                {
                  "y": 727,
                  "x": 1080,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f802e6268270bd0257fd64d6a5f12a8d0775acfd"
                }
              ],
              "s": {
                "y": 1283,
                "x": 1904,
                "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=1904&amp;format=png&amp;auto=webp&amp;s=86d181cfb01012f731f45e4236be0ef691cd7c7a"
              },
              "id": "9xa2d1o5f8ef1"
            },
            "4cgtiz1pf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9476e3d32a2b4ebddc1d44940be795ca69c505de"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a012cb536e4088a6adcc6773c02216ff905ea974"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b1ae7259c182874929c4e41c2cd1db5089064c5"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb9b8a221009889a7ab5351b684233583bcb0cc"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ba7b4a8e8efa0432dab30568ad1477468ea5974"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77a0c9435849b1b3f806b87ef5f2fa6998fc6dff"
                }
              ],
              "s": {
                "y": 1253,
                "x": 1907,
                "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=daafca14c7e5ee623c552997eb11226cbf2767df"
              },
              "id": "4cgtiz1pf8ef1"
            },
            "eltdxfn8i8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ceb464884fcc4e8ea8f5673a530fca637a4ea00d"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4a99d61fb402903985f9c6be85f75cb987a9afb"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b2b59e21adf2de70f35de42b0b13badb2b7e1dc"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b5a7db457131ab9e6827d842044325ab24d3afb"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a4594f5796a9d0c3d0573956810621801f7651e"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c52767489ca42b8349b5bc5683443a466d6c3b2e"
                }
              ],
              "s": {
                "y": 1253,
                "x": 1907,
                "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=8c2d9c0743ca5a01676db6ac6e4d7b30a0911c27"
              },
              "id": "eltdxfn8i8ef1"
            },
            "ca3mb75rf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf1a822b143343c8c2f1c4a91351751e74e37014"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cad9a7097453af844b2072df10473d63407168f"
                },
                {
                  "y": 209,
                  "x": 320,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=816bdd87a87ae2ef4a2d2205f7b9e29f38489c91"
                },
                {
                  "y": 419,
                  "x": 640,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d0905850f6ee9cdef78d85c64fea4ac9e5f1916"
                },
                {
                  "y": 628,
                  "x": 960,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=db14232380210a75035e339554d958dbf35b7d3a"
                },
                {
                  "y": 707,
                  "x": 1080,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57e792572da9152faf1d00aa981a55b8306a3b78"
                }
              ],
              "s": {
                "y": 1248,
                "x": 1906,
                "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=d74d7bdeda2dd3fe454bc67dbc6906f9c203ce20"
              },
              "id": "ca3mb75rf8ef1"
            },
            "09e7cb48f8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7deeff37c4941bdf28a815588390a855ed862bd0"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=625263ce51714f18f7788cf0ad81624820b84b23"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3974e35855f594aed564ee278aaced85fd3c1769"
                },
                {
                  "y": 425,
                  "x": 640,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe5e13cb68b69fc904ce5dc9c280c1cf92a18e8"
                },
                {
                  "y": 638,
                  "x": 960,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d0f7e62ef3919918eaa711708200e1f4a6e545e"
                },
                {
                  "y": 718,
                  "x": 1080,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a39d1d3270cb5f31192d2137b97dfd051e6b2b"
                }
              ],
              "s": {
                "y": 1266,
                "x": 1902,
                "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=ec11ef011d0468b0f6f6bfc269287e472701bffe"
              },
              "id": "09e7cb48f8ef1"
            }
          },
          "name": "t3_1m5lgtr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 32,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "qlq88ceum8ef1",
                "id": 710842483
              },
              {
                "media_id": "baogvk7lf8ef1",
                "id": 710842484
              },
              {
                "media_id": "kzzqul8nf8ef1",
                "id": 710842485
              },
              {
                "media_id": "4cgtiz1pf8ef1",
                "id": 710842486
              },
              {
                "media_id": "ca3mb75rf8ef1",
                "id": 710842487
              },
              {
                "media_id": "9xa2d1o5f8ef1",
                "id": 710842488
              },
              {
                "media_id": "09e7cb48f8ef1",
                "id": 710842489
              },
              {
                "media_id": "s968gxs0g8ef1",
                "id": 710842490
              },
              {
                "media_id": "ec0ih2x5g8ef1",
                "id": 710842491
              },
              {
                "media_id": "iv8y70kfg8ef1",
                "id": 710842492
              },
              {
                "media_id": "eltdxfn8i8ef1",
                "id": 710842493
              },
              {
                "media_id": "bkgben5hn8ef1",
                "id": 710842494
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jkGttzBpvCgJdojYediLI8339DpfTnhALhUrjGFYPnA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753110583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)&lt;/p&gt;\n\n&lt;p&gt;So type out your prompt like this: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;[Action] [UI type or page] [Framework(s)] [Key features] [Style (optional)]&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If it is within the context, then you can additionally add edits.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a prompt template:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Additional things that are supported -&amp;gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.&lt;/p&gt;\n\n&lt;p&gt;Remember, its only an 8B model!&lt;/p&gt;\n\n&lt;p&gt;We are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. &lt;/p&gt;\n\n&lt;p&gt;Make sure to join the community for more support. (Link in Huggingface!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m5lgtr",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lgtr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "United-Rush4073",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lgtr/uigenx_8b_supports_react_headless_flutter_react/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m5lgtr",
          "subreddit_subscribers": 502981,
          "created_utc": 1753110583,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html)",
          "author_fullname": "t2_84qpb9rt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Interesting new blog post from Lemonade team",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5q35o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753120844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html\"&gt;https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?auto=webp&amp;s=370ef3c67fc9a466fb921e399215ca76e255bdd8",
                  "width": 1435,
                  "height": 645
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d6ab69b2057c1ff74bd4fad9be640864917203c",
                    "width": 108,
                    "height": 48
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc432f40f27f0a2f68d597fb271cdc9efc951403",
                    "width": 216,
                    "height": 97
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39d76385ecda761edb0e6a264b0e3efd78c0cd1c",
                    "width": 320,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ae8bc998e1523bb4c01bb0d57ef5cea220343f",
                    "width": 640,
                    "height": 287
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1e801ab47a7cb7d218797288798cd8f7d4e41fd",
                    "width": 960,
                    "height": 431
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d14e3dd033ec146ebe70b9e6b9135c50e7a9136",
                    "width": 1080,
                    "height": 485
                  }
                ],
                "variants": {},
                "id": "tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5q35o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Smooth-Screen4148",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753120844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any tiny spellcheck models for English which are good? What do you guys use?",
          "author_fullname": "t2_9kgt3ez",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you guys use for Spellcheck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68yvl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753174363,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any tiny spellcheck models for English which are good? What do you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m68yvl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CaptTechno",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68yvl/what_do_you_guys_use_for_spellcheck/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68yvl/what_do_you_guys_use_for_spellcheck/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753174363,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running a research project analysing hospital incident reports (answering structured questions based on them); we do have permission to use identifiable data but the PDFs I've been sent have been redacted and whichever software they've used has turned a lot of the text into an image. To add excitement, a lot of the text is in columns that flow across pages (ie you need to read the left of page 1,2 then the right of page 1,2)\n\nCan anyone recommend a local model capable of handling this? Our research machine has an A6000 (48Gb) and 128Gb RAM; speed isn't a massive issue. I don't mind if the workflow is PDF to text and then run a text model, or if a vision model could do the whole thing.\n\nThanks!",
          "author_fullname": "t2_pc1zg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model to process image-of-text PDFs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68tse",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753173792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a research project analysing hospital incident reports (answering structured questions based on them); we do have permission to use identifiable data but the PDFs I&amp;#39;ve been sent have been redacted and whichever software they&amp;#39;ve used has turned a lot of the text into an image. To add excitement, a lot of the text is in columns that flow across pages (ie you need to read the left of page 1,2 then the right of page 1,2)&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a local model capable of handling this? Our research machine has an A6000 (48Gb) and 128Gb RAM; speed isn&amp;#39;t a massive issue. I don&amp;#39;t mind if the workflow is PDF to text and then run a text model, or if a vision model could do the whole thing.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m68tse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thigger",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68tse/model_to_process_imageoftext_pdfs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68tse/model_to_process_imageoftext_pdfs/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753173792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't get the obsession with llama.cpp. It's completely unusable for any real work. The token generation speed collapses as soon as you add any meaningful context, and the prompt processing is painfully slow. With these fatal flaws, what is anyone actually using this for besides running toy demos? It's fundamentally broken for any serious application.",
          "author_fullname": "t2_qhk9kpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama.cpp is unusable for real work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6skm6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753224220,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t get the obsession with llama.cpp. It&amp;#39;s completely unusable for any real work. The token generation speed collapses as soon as you add any meaningful context, and the prompt processing is painfully slow. With these fatal flaws, what is anyone actually using this for besides running toy demos? It&amp;#39;s fundamentally broken for any serious application.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6skm6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "d00m_sayer",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753224220,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}