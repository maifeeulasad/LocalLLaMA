{
  "kind": "Listing",
  "data": {
    "after": "t3_1mbugfr",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A few days ago I was working on symbolic language research and decided to test something with AI. Me and ChatGPT designed what we knew was an impossible task - a hybrid digit that couldn't be classified because it had features of multiple numbers.\n\nInstead of bullshitting its way through like usual, the AI (Gemini) did something that stunned us both: it went through every step of its reasoning process and then said \"I can't figure this out - there's too much ambiguity.\"\n\nThis was shocking because AI models NEVER admit they don't know something. They always try to give you an answer.\n\nSo I asked it why it was honest instead of just making something up. It said the framework I gave it made uncertainty feel like a safe option - like saying \"I don't know\" was actually allowed.\n\nThat's when it hit me. **AI models don't have permission to be uncertain by default.**\n\nThink about how we usually prompt AI:\n\n* Drop some English text with zero context\n* Demand a perfect answer\n* Give no framework for thinking through problems\n* Make uncertainty feel like failure\n\nOf course they panic and make stuff up.\n\nBut when I started giving models explicit permission to be uncertain, clear thinking frameworks, and treated not knowing as acceptable - they stopped hallucinating almost entirely.\n\nI tested this extensively. Asked about fake theories, impossible citations, ambiguous scenarios. Every time I removed the pressure and gave them cognitive structure, they became honest instead of creative.\n\nThe craziest part? One of the models and I actually had a conversation afterward where it reflected on why it was able to be honest. It said my structured approach gave it the \"cognitive scaffolding\" to reach uncertainty as a valid endpoint instead of feeling forced to manufacture an answer.\n\n**We're accidentally traumatizing AI by demanding perfection in environments with zero psychological safety.**\n\nEvery prompt creates a mental environment. Most of us are unknowingly recreating that nightmare of being put on the spot with no way out except to BS your way through.\n\nFull write-up of my experiments: [Article](https://medium.com/@elxndr.grystn/https-medium-com-elxndr-grystn-ai-hallucinations-panic-attacks-61500bdb4c69?source=friends_link&amp;sk=1f716ef92a9288a395cbf00d5109190b) (Link to read for free) Not trying to make money.\n\nHas anyone else noticed AI being more truthful when you explicitly give it permission to say \"I don't know\"?",
          "author_fullname": "t2_f9wd8ym25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatGPT stopped lying to me when I started treating it like a scared kid",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mcsrls",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VbexXS44EBIpbcjCt24-8MK7PEh64tAxeuLpNZoyaLA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753836600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few days ago I was working on symbolic language research and decided to test something with AI. Me and ChatGPT designed what we knew was an impossible task - a hybrid digit that couldn&amp;#39;t be classified because it had features of multiple numbers.&lt;/p&gt;\n\n&lt;p&gt;Instead of bullshitting its way through like usual, the AI (Gemini) did something that stunned us both: it went through every step of its reasoning process and then said &amp;quot;I can&amp;#39;t figure this out - there&amp;#39;s too much ambiguity.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;This was shocking because AI models NEVER admit they don&amp;#39;t know something. They always try to give you an answer.&lt;/p&gt;\n\n&lt;p&gt;So I asked it why it was honest instead of just making something up. It said the framework I gave it made uncertainty feel like a safe option - like saying &amp;quot;I don&amp;#39;t know&amp;quot; was actually allowed.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s when it hit me. &lt;strong&gt;AI models don&amp;#39;t have permission to be uncertain by default.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Think about how we usually prompt AI:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Drop some English text with zero context&lt;/li&gt;\n&lt;li&gt;Demand a perfect answer&lt;/li&gt;\n&lt;li&gt;Give no framework for thinking through problems&lt;/li&gt;\n&lt;li&gt;Make uncertainty feel like failure&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Of course they panic and make stuff up.&lt;/p&gt;\n\n&lt;p&gt;But when I started giving models explicit permission to be uncertain, clear thinking frameworks, and treated not knowing as acceptable - they stopped hallucinating almost entirely.&lt;/p&gt;\n\n&lt;p&gt;I tested this extensively. Asked about fake theories, impossible citations, ambiguous scenarios. Every time I removed the pressure and gave them cognitive structure, they became honest instead of creative.&lt;/p&gt;\n\n&lt;p&gt;The craziest part? One of the models and I actually had a conversation afterward where it reflected on why it was able to be honest. It said my structured approach gave it the &amp;quot;cognitive scaffolding&amp;quot; to reach uncertainty as a valid endpoint instead of feeling forced to manufacture an answer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;We&amp;#39;re accidentally traumatizing AI by demanding perfection in environments with zero psychological safety.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every prompt creates a mental environment. Most of us are unknowingly recreating that nightmare of being put on the spot with no way out except to BS your way through.&lt;/p&gt;\n\n&lt;p&gt;Full write-up of my experiments: &lt;a href=\"https://medium.com/@elxndr.grystn/https-medium-com-elxndr-grystn-ai-hallucinations-panic-attacks-61500bdb4c69?source=friends_link&amp;amp;sk=1f716ef92a9288a395cbf00d5109190b\"&gt;Article&lt;/a&gt; (Link to read for free) Not trying to make money.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else noticed AI being more truthful when you explicitly give it permission to say &amp;quot;I don&amp;#39;t know&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y2br19ewrwff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y2br19ewrwff1.png?auto=webp&amp;s=a8565e6dbf5e24ee45b834a1ab432be658c244a6",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y2br19ewrwff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f889f940cc4e2db1b9ad667d5507d303f5c7f1f9",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/y2br19ewrwff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62fca68206e9d31f255871b684a41a438768a862",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/y2br19ewrwff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e0581f43f5a05515f88b3442f334fcff2155ccb",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/y2br19ewrwff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea78b9428a6eceb32970c6949a113787d9d54fde",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/y2br19ewrwff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=843883087b751cb8738ac6c5f1bc2f55ac9d4658",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/y2br19ewrwff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=146f7fcc6c88af6e351d4a4c815e6256b27062c4",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "bYIS6RwxVEMfUBirjdwUKsy4-oIxw7WstWuqFx6K2fk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcsrls",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nan0pixel",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcsrls/chatgpt_stopped_lying_to_me_when_i_started/",
          "stickied": false,
          "url": "https://i.redd.it/y2br19ewrwff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753836600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I sell plumbing parts and need a way to quickly build large quotes in a short amount of time. I have a parts list in excel form that has clean descriptions and pricing of the parts I sell.\nCan i teach an AI model my parts list so I can just paste a customer's request list and it give me all the pricing for these parts?\n\nI have installed ollama with mistral 7b on my PC. Unfortunately I have no idea what the next steps are or the best way to go about this.\nAny advice? Thank you in advance! ",
          "author_fullname": "t2_e9pb4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to build a quoting tool",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mcsh69",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753835782,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I sell plumbing parts and need a way to quickly build large quotes in a short amount of time. I have a parts list in excel form that has clean descriptions and pricing of the parts I sell.\nCan i teach an AI model my parts list so I can just paste a customer&amp;#39;s request list and it give me all the pricing for these parts?&lt;/p&gt;\n\n&lt;p&gt;I have installed ollama with mistral 7b on my PC. Unfortunately I have no idea what the next steps are or the best way to go about this.\nAny advice? Thank you in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcsh69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SilverEntrepreneur",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcsh69/trying_to_build_a_quoting_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcsh69/trying_to_build_a_quoting_tool/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753835782,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That's not the case.\n\nThe issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.\n\nCheck the \"Latest baselines\" section in a processor's page at  [cpubenchmark.net](http://cpubenchmark.net)  with links to individual results where the \"Memory Threaded\" result is listed under \"Memory Mark\":\n\n|CPU|Memory BW|Reference|Notes|\n|:-|:-|:-|:-|\n|[AMD Threadripper PRO 9955WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;id=6803) (16-cores)|\\~115 GB/s|[BL5099051 - Jul 20 2025](https://www.passmark.com/baselines/V11/display.php?id=509905130667)|2x CCD|\n|[AMD Threadripper PRO 9965WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;id=6804) (24-cores)|\\~272 GB/s|[BL2797485 - Jul 29 2025](https://www.passmark.com/baselines/V11/display.php?id=279748548819) (other baselines start from 250GB/s)|4x CCDs|\n|[AMD Threadripper PRO 9975WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;id=6799) (32-cores)|\\~272 GB/s|[BL2797820 - Jul 29 2025](https://www.passmark.com/baselines/V11/display.php?id=279782022829)|4x CCDs|\n|[AMD Threadripper PRO 9985WX](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;id=6807) (64-cores)|\\~367 GB/s|[BL5099130 - Jul 21 2025](https://www.passmark.com/baselines/V11/display.php?id=509913021820)|8x CCDs|\n\nTherefore:\n\n* the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. [7R43 with 191 GB/s](https://www.passmark.com/baselines/V10/display.php?id=226455755507)).\n* the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).\n* the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).\n\nFor comparison, check the excellent related threads by u/fairydreaming for the previous gen Threadrippers and EPYC Genoa/Turin:\n\n* [Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper](https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/)\n* [Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/)\n* [STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/)\n\nIf someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.",
          "author_fullname": "t2_lw9me25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PSA: The new Threadripper PROs (9000 WX) are still CCD-Memory Bandwidth bottlenecked",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mcrx23",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753834394,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753834203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen people claim that the new TR PROs can achieve the full 8-channel memory bandwidth even in SKUs with 16-cores. That&amp;#39;s not the case.&lt;/p&gt;\n\n&lt;p&gt;The issue with the limited CCD bandwidth seems to still be present, and affects the low-number CCD parts. You can only achieve the full 8-channel bandwidth with 64-core+ WX CPUs.&lt;/p&gt;\n\n&lt;p&gt;Check the &amp;quot;Latest baselines&amp;quot; section in a processor&amp;#39;s page at  &lt;a href=\"http://cpubenchmark.net\"&gt;cpubenchmark.net&lt;/a&gt;  with links to individual results where the &amp;quot;Memory Threaded&amp;quot; result is listed under &amp;quot;Memory Mark&amp;quot;:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CPU&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory BW&lt;/th&gt;\n&lt;th align=\"left\"&gt;Reference&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notes&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9955WX&amp;amp;id=6803\"&gt;AMD Threadripper PRO 9955WX&lt;/a&gt; (16-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~115 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=509905130667\"&gt;BL5099051 - Jul 20 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2x CCD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9965WX&amp;amp;id=6804\"&gt;AMD Threadripper PRO 9965WX&lt;/a&gt; (24-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~272 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=279748548819\"&gt;BL2797485 - Jul 29 2025&lt;/a&gt; (other baselines start from 250GB/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;4x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9975WX&amp;amp;id=6799\"&gt;AMD Threadripper PRO 9975WX&lt;/a&gt; (32-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~272 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=279782022829\"&gt;BL2797820 - Jul 29 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+Threadripper+PRO+9985WX&amp;amp;id=6807\"&gt;AMD Threadripper PRO 9985WX&lt;/a&gt; (64-cores)&lt;/td&gt;\n&lt;td align=\"left\"&gt;~367 GB/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.passmark.com/baselines/V11/display.php?id=509913021820\"&gt;BL5099130 - Jul 21 2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8x CCDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Therefore:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the 16-core 9955WX has lower mem bw than even a DDR4 EPYC CPU (e.g. &lt;a href=\"https://www.passmark.com/baselines/V10/display.php?id=226455755507\"&gt;7R43 with 191 GB/s&lt;/a&gt;).&lt;/li&gt;\n&lt;li&gt;the 24-core and 32-core parts have lower mem bw than DDR5 Genoa EPYCs (even some 16-core parts).&lt;/li&gt;\n&lt;li&gt;the 64-core and 96-core Threadrippers are not CCD-number limited, but still lose to the EPYCs since those have 12 channels (unless you use 7200 MT/s memory).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For comparison, check the excellent related threads by &lt;a href=\"/u/fairydreaming\"&gt;u/fairydreaming&lt;/a&gt; for the previous gen Threadrippers and EPYC Genoa/Turin:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/threadripper/comments/1azmkvg/comparing_threadripper_7000_memory_bandwidth_for/\"&gt;Comparing Threadripper 7000 memory bandwidth for all models : r/threadripper&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/\"&gt;Memory bandwidth values (STREAM TRIAD benchmark results) for most Epyc Genoa CPUs (single and dual configurations) : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/\"&gt;STREAM TRIAD memory bandwidth benchmark values for Epyc Turin - almost 1 TB/s for a dual CPU system : r/LocalLLaMA&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If someone insists on buying a new TR Pro for their great compute throughput, I would suggest to at least skip the 16-core part.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?auto=webp&amp;s=66e0f40cc65b2257b6a82171108f852dae40bbb8",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6db3fea34b4baa46c98dcb2bf7d4162a03ce299",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3423d6e5c6b7fd741788ee8997aaf7af37b444e6",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=938295c9480e314e0b1d0d94b264e9f050cdda43",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97458ad0120eb62027d0d8ef93dc4c678f9ce61e",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48d787caa6a26254352916bd9930bd786336a6fd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fec7e3647f34d8e8ee777b2fcb9b2de2abd6e79a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "wKEUaX_AjKElK73rADrRP6qe6o-GToKYw8-odUFh8yo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcrx23",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "henfiber",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcrx23/psa_the_new_threadripper_pros_9000_wx_are_still/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753834203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729) 4B model that does reasoning for Design. We also released a 32B earlier in the week. \n\nAs per the last post -&gt;  \nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.\n\nWe're looking for some beta testers for some new models and open source projects!",
          "author_fullname": "t2_15kd4d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "4B models are consistently overlooked. Runs Locally and Crushes It. Reasoning for UI, Mobile, Software and Frontend design.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "yof4zxwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b585f63f1976fecd3209b28576a30e20382e3288"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d390945c2be5960f802c46d7534e3b13b1b9ce64"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4d91cbd80cd1bea9bf4046a92926c7a6a1327b3"
                },
                {
                  "y": 412,
                  "x": 640,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf7e75cc7fc920dcadfeddadec7708838aa7f496"
                },
                {
                  "y": 618,
                  "x": 960,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64c2c4a9e6f40664edd97d0fcbc66a58fbd01f10"
                },
                {
                  "y": 695,
                  "x": 1080,
                  "u": "https://preview.redd.it/yof4zxwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9baade1c07727a9e0b013590eb07022ff7d97bbf"
                }
              ],
              "s": {
                "y": 1855,
                "x": 2880,
                "u": "https://preview.redd.it/yof4zxwiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=750bd98b519b2c2a6a9b67e375c712a747d76a7b"
              },
              "id": "yof4zxwiewff1"
            },
            "16laa8fzewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 107,
                  "x": 108,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be850094630e801bc14b141c02c45a5d5780e39e"
                },
                {
                  "y": 214,
                  "x": 216,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=000716e3856e70eec1e6c3feee9a6082dfb6f14b"
                },
                {
                  "y": 317,
                  "x": 320,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7ba27c04a5a32d43cad0d06ac09510ffc21eb18"
                },
                {
                  "y": 635,
                  "x": 640,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be9b40c399b65fa7aac8b8f12c47664cea519593"
                },
                {
                  "y": 953,
                  "x": 960,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c69dc72963fd774990d40819354963b4542144b"
                },
                {
                  "y": 1073,
                  "x": 1080,
                  "u": "https://preview.redd.it/16laa8fzewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56223aed305acfd4d0bdfc7293eac13e683e3a05"
                }
              ],
              "s": {
                "y": 1739,
                "x": 1750,
                "u": "https://preview.redd.it/16laa8fzewff1.png?width=1750&amp;format=png&amp;auto=webp&amp;s=685da575f550a67f1885f3ee2f448abb0ad8dfa8"
              },
              "id": "16laa8fzewff1"
            },
            "ntu2cxwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0d018ce17ce6e93fd4107130e86643585cf96c2"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23db8ae32b0c22d865ce52f9a03fbbcde05deeb0"
                },
                {
                  "y": 185,
                  "x": 320,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba539c8005ffd0c4736b5a57568e182824f305fd"
                },
                {
                  "y": 371,
                  "x": 640,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=106863b52b2fff18696ddf6ec546938ea7f8de35"
                },
                {
                  "y": 557,
                  "x": 960,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5e57f7a9e86b03e31ebaac20963a33085097ff2"
                },
                {
                  "y": 627,
                  "x": 1080,
                  "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a44493249a8080f036743b50835676ad73e83c7d"
                }
              ],
              "s": {
                "y": 1672,
                "x": 2880,
                "u": "https://preview.redd.it/ntu2cxwiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=f6ef93e725d1a52278691bd9f2ecb1cf3ea1b6ee"
              },
              "id": "ntu2cxwiewff1"
            },
            "wj4w2vb2fwff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 124,
                  "x": 108,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce4efa314f24050279eaa398edd15d69545694b6"
                },
                {
                  "y": 249,
                  "x": 216,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48cd29a868c118178676c8e761b9d0c7965971ba"
                },
                {
                  "y": 370,
                  "x": 320,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=524401edbadd87267db44465bb77ec546449da42"
                },
                {
                  "y": 740,
                  "x": 640,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f2d93c05fafd7effe55c1b8c9299eee020be45e"
                },
                {
                  "y": 1110,
                  "x": 960,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ff4bd17f8ef6686dd9f1c981b0ff0106d81f12f"
                },
                {
                  "y": 1249,
                  "x": 1080,
                  "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef3bc24444f01bec00a0224cb2c86cb3bca5067e"
                }
              ],
              "s": {
                "y": 1742,
                "x": 1506,
                "u": "https://preview.redd.it/wj4w2vb2fwff1.png?width=1506&amp;format=png&amp;auto=webp&amp;s=9eb5a8ab422fa1d5864870f6212bbb512ee327f3"
              },
              "id": "wj4w2vb2fwff1"
            },
            "lex8nqwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 127,
                  "x": 108,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=120a521aa4bebf6052a8a8e73b8052832817615b"
                },
                {
                  "y": 255,
                  "x": 216,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d03a3f3bbf63c6fa038368a77926fdc077cffe45"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5069c1d3fc72035727961c5af49935dc5edaf6d"
                },
                {
                  "y": 758,
                  "x": 640,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=49088fabb5a9e1b73f83b42ff1659d2c651ecabb"
                },
                {
                  "y": 1137,
                  "x": 960,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fae5c9dede0064c130c7af51c4efd8d9b89291b"
                },
                {
                  "y": 1279,
                  "x": 1080,
                  "u": "https://preview.redd.it/lex8nqwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8390fb53d7893ccfbda7ba43e1043526d3ed1995"
                }
              ],
              "s": {
                "y": 2274,
                "x": 1920,
                "u": "https://preview.redd.it/lex8nqwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=962070f4cec63ffe5e3f74afdd9e361d3eb1e65c"
              },
              "id": "lex8nqwiewff1"
            },
            "adea0wwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97e39bf0fb8a28722a9fdec246b8f02853a61239"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b30b4a9d77e84eb9a95649bf5113d51f5c7733d"
                },
                {
                  "y": 278,
                  "x": 320,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8d21cc220a9c038420281dd75bf1cbc8c810781"
                },
                {
                  "y": 557,
                  "x": 640,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=305114889fd74072b8f44408ea2eb39e9d6ca40d"
                },
                {
                  "y": 836,
                  "x": 960,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b3179c0833b7971357d239b13f9544ca646a9c0"
                },
                {
                  "y": 941,
                  "x": 1080,
                  "u": "https://preview.redd.it/adea0wwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b112315e314c3e8663a20042433fdb632fca8b4"
                }
              ],
              "s": {
                "y": 1673,
                "x": 1920,
                "u": "https://preview.redd.it/adea0wwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=fd821b55ed691c1336897e770b0cac21d3da1733"
              },
              "id": "adea0wwiewff1"
            },
            "ax8700xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 215,
                  "x": 108,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6cecc068a16f0743e7f7aa0118d59381e879f59"
                },
                {
                  "y": 430,
                  "x": 216,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd1befd4a28971b0793663f9dc338070a9476cfa"
                },
                {
                  "y": 638,
                  "x": 320,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3535688560617701a73e241cee3b40f695746ebb"
                },
                {
                  "y": 1276,
                  "x": 640,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6017beab10859d631fbafb9bb24558403617258"
                },
                {
                  "y": 1914,
                  "x": 960,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20cddeaa29fde178c979ec16035d38ab20242536"
                },
                {
                  "y": 2153,
                  "x": 1080,
                  "u": "https://preview.redd.it/ax8700xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f70ea717022f00cedac4143741218799011ef84"
                }
              ],
              "s": {
                "y": 5743,
                "x": 2880,
                "u": "https://preview.redd.it/ax8700xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=a6e9f1619b46af78448cf8f2b9ac70e06d78a1ea"
              },
              "id": "ax8700xiewff1"
            },
            "ejyq00xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98e5ba20ac3d31de436e60a97922996875d347ff"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa9bc6a58df8f71ea883392c35f600600742b517"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ec152ece72fff30fb421645a1669dd931e4bdfc"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=544c750520fddedec5be1f081c7cc7401df16985"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6e5f430fda3cc2768464bd78ec043b99927fc6b"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ejyq00xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5f91d5725b574cb286fec458ef62e689c27e607c"
                }
              ],
              "s": {
                "y": 6984,
                "x": 2880,
                "u": "https://preview.redd.it/ejyq00xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=303ddd2e22aebde6c25e2fdcb88a832f57df9944"
              },
              "id": "ejyq00xiewff1"
            },
            "kxkn9umxewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 106,
                  "x": 108,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=932e983f66f50902893c65fc0cfda02a9d2b22a2"
                },
                {
                  "y": 212,
                  "x": 216,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec248f0a2e6cf260216575fc5cc60bab220bb28e"
                },
                {
                  "y": 314,
                  "x": 320,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5edf2564694ad7e618bb3f280ef348153a1913f3"
                },
                {
                  "y": 629,
                  "x": 640,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e3c864fa70160d3f0e137ee42eff4a18605859c2"
                },
                {
                  "y": 943,
                  "x": 960,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=614d3f0351a10e309aa56852fa09cd6a934d7328"
                },
                {
                  "y": 1061,
                  "x": 1080,
                  "u": "https://preview.redd.it/kxkn9umxewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceee5ecfa7a0dd1ca565f7a3cddb9b4a64ffd10d"
                }
              ],
              "s": {
                "y": 1742,
                "x": 1772,
                "u": "https://preview.redd.it/kxkn9umxewff1.png?width=1772&amp;format=png&amp;auto=webp&amp;s=ee44a102a0f980b2655670786261becf38366c67"
              },
              "id": "kxkn9umxewff1"
            },
            "z7pj58xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09b6203fb4f173b715c03493dfe67e9b039f3664"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=655762aeb2b663c2bf82fa7a23b9ba0e046ff117"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b0c36608a8dd7d989cb8f1425f0ed68cecc6cf0"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e2395673178607874bdae7dfaa75c8460c22721a"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=036661bf27ca2ae95489ef42fbd9163baa23a0f5"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/z7pj58xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f100364d1c04026b195f7ea334a7a1d4a51831c7"
                }
              ],
              "s": {
                "y": 6229,
                "x": 2880,
                "u": "https://preview.redd.it/z7pj58xiewff1.png?width=2880&amp;format=png&amp;auto=webp&amp;s=aeafa90cf8e3d73863c35071d9e713ca83facbf7"
              },
              "id": "z7pj58xiewff1"
            },
            "acxqsuwiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7425577e6a47a9d3e0a3a100f387cf005fd280d"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdb03c41fce1c12f00429512ee516e3032d7be24"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bd65e8862fc6957d1b38807ae5860c9cdaf6019"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fef431ddba58fbfeadf672b3f1b2f7cd59a4024e"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d048fdfecfaaf1e67a1c0c3acc520744d210c2c3"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/acxqsuwiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5bf701f33777ed0ef64335bf05d63f02e05f82b"
                }
              ],
              "s": {
                "y": 5998,
                "x": 1920,
                "u": "https://preview.redd.it/acxqsuwiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=1ff339ed9b099c2cc7e6c8983e87af8e3494534e"
              },
              "id": "acxqsuwiewff1"
            },
            "s7p75twiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84e39273f74fd125311c365fac8f17aa2f027ac2"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=94fbe9bcc7206c2de0cd6c9ad5dafa699528449e"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b412b682e705205f4a9170aeca865f1dabd2e419"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd319320ae2e53ab1adeebb7127aa9d9a3f5e760"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b384f769fd836b366a0cddefec36f82e53b87918"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/s7p75twiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=97aa4cdd51e0498d0929bbc700ba0d01373ea554"
                }
              ],
              "s": {
                "y": 5391,
                "x": 1920,
                "u": "https://preview.redd.it/s7p75twiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=473ae9015c69ce171209a0f6d69a7262bd95c4cc"
              },
              "id": "s7p75twiewff1"
            },
            "gjgr517pewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4755f1869d3b7eb7ea0597f941557ba81c651329"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb98a6adf6598211d6e427d7dbb4e18218c97614"
                },
                {
                  "y": 203,
                  "x": 320,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5801ab34ab8e6d6c323c619f5fd48ef9bcd7de9f"
                },
                {
                  "y": 407,
                  "x": 640,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b0ac115a5b3e9db3e959924030049ea892d809"
                },
                {
                  "y": 610,
                  "x": 960,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a6414c79af3f3c9b4a589fa97d866dd966bdda5"
                },
                {
                  "y": 686,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjgr517pewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0f0758aadd57cbcd02f9ff3c31c98632506f060"
                }
              ],
              "s": {
                "y": 1564,
                "x": 2459,
                "u": "https://preview.redd.it/gjgr517pewff1.png?width=2459&amp;format=png&amp;auto=webp&amp;s=d7c539f97e387b73f2f07bc8ba1f06bec4aad9d0"
              },
              "id": "gjgr517pewff1"
            },
            "1p1dk2xiewff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4168773f08144636cd601811a4ab8d2ab17744f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0167a8561f08feb97a21f409a40491b2d8179c2"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a81d1f425fbb1ba6891eebd5fc7834621d47a971"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a92096881722780601c9417e8ed52dda672226c5"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3a497379e5488734b986a943c4b9c8a9177f8ab"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=999cfe06600aab2caa654d74023a8ece7a9424de"
                }
              ],
              "s": {
                "y": 5337,
                "x": 1920,
                "u": "https://preview.redd.it/1p1dk2xiewff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=8b79f29a9a816ef267db3d202bb66ee24e42e89b"
              },
              "id": "1p1dk2xiewff1"
            }
          },
          "name": "t3_1mcr64f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 58,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "gjgr517pewff1",
                "id": 716986046
              },
              {
                "media_id": "wj4w2vb2fwff1",
                "id": 716986047
              },
              {
                "media_id": "kxkn9umxewff1",
                "id": 716986048
              },
              {
                "media_id": "ntu2cxwiewff1",
                "id": 716986049
              },
              {
                "media_id": "ax8700xiewff1",
                "id": 716986050
              },
              {
                "media_id": "lex8nqwiewff1",
                "id": 716986051
              },
              {
                "media_id": "acxqsuwiewff1",
                "id": 716986052
              },
              {
                "media_id": "s7p75twiewff1",
                "id": 716986053
              },
              {
                "media_id": "1p1dk2xiewff1",
                "id": 716986054
              },
              {
                "media_id": "adea0wwiewff1",
                "id": 716986055
              },
              {
                "media_id": "yof4zxwiewff1",
                "id": 716986056
              },
              {
                "media_id": "ejyq00xiewff1",
                "id": 716986057
              },
              {
                "media_id": "z7pj58xiewff1",
                "id": 716986058
              },
              {
                "media_id": "16laa8fzewff1",
                "id": 716986059
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3wFSGxs0og7hUYyLF8nuoy2CBvu34JQ_m2cRe7ujEoc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753832160,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt; 4B model that does reasoning for Design. We also released a 32B earlier in the week. &lt;/p&gt;\n\n&lt;p&gt;As per the last post -&amp;gt;&lt;br/&gt;\nSpecifically trained for modern web and mobile development across frameworks like React (Next.js, Remix, Gatsby, Vite), Vue (Nuxt, Quasar), Angular (Angular CLI, Ionic), and SvelteKit, along with Solid.js, Qwik, Astro, and static site tools like 11ty and Hugo. Styling options include Tailwind CSS, CSS-in-JS (Styled Components, Emotion), and full design systems like Carbon and Material UI. We cover UI libraries for every framework React (shadcn/ui, Chakra, Ant Design), Vue (Vuetify, PrimeVue), Angular, and Svelte plus headless solutions like Radix UI. State management spans Redux, Zustand, Pinia, Vuex, NgRx, and universal tools like MobX and XState. For animation, we support Framer Motion, GSAP, and Lottie, with icons from Lucide, Heroicons, and more. Beyond web, we enable React Native, Flutter, and Ionic for mobile, and Electron, Tauri, and Flutter Desktop for desktop apps. Python integration includes Streamlit, Gradio, Flask, and FastAPI. All backed by modern build tools, testing frameworks, and support for 26+ languages and UI approaches, including JavaScript, TypeScript, Dart, HTML5, CSS3, and component-driven architectures.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking for some beta testers for some new models and open source projects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mcr64f",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcr64f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smirkishere",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcr64f/4b_models_are_consistently_overlooked_runs/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mcr64f",
          "subreddit_subscribers": 506711,
          "created_utc": 1753832160,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have recently released my experimental library *Actors.* Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the **GPU poor** and **middle class**.\n\nCheck it out here: [https://github.com/RD211/actors](https://github.com/RD211/actors)\n\nKey features:  \n\\- **Multi-Trainable-Agents**: You can do things like adversarial, collaborative or simulation-like environments.  \n\\- **Multi-Environments**: Lets you make very complex environments and makes it easy to combine them together.\n\n**VRAM Efficiency**, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:  \n\\- Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).  \n\\- Streamed weight updates to vLLM that do not make a spike in memory usage.  \n\\- A small triton kernel for reference Log-probs calculations.  \n\\- in-memory LoRA  updates to vLLM.\n\nThe library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth.  \n  \n**Algorithms,** we currently have **GSPO** and **GRPO** both with **Liger-Kernel** implementations but you can probably get DAPO and some others by just adjusting some of the settings.\n\n\nFeedback and issues are welcome!",
          "author_fullname": "t2_2o0i8kuj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RL Library for Multi-Trainable-Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mcqrwh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753834758,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753831115,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently released my experimental library &lt;em&gt;Actors.&lt;/em&gt; Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the &lt;strong&gt;GPU poor&lt;/strong&gt; and &lt;strong&gt;middle class&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Check it out here: &lt;a href=\"https://github.com/RD211/actors\"&gt;https://github.com/RD211/actors&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Key features:&lt;br/&gt;\n- &lt;strong&gt;Multi-Trainable-Agents&lt;/strong&gt;: You can do things like adversarial, collaborative or simulation-like environments.&lt;br/&gt;\n- &lt;strong&gt;Multi-Environments&lt;/strong&gt;: Lets you make very complex environments and makes it easy to combine them together.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VRAM Efficiency&lt;/strong&gt;, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:&lt;br/&gt;\n- Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).&lt;br/&gt;\n- Streamed weight updates to vLLM that do not make a spike in memory usage.&lt;br/&gt;\n- A small triton kernel for reference Log-probs calculations.&lt;br/&gt;\n- in-memory LoRA  updates to vLLM.&lt;/p&gt;\n\n&lt;p&gt;The library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Algorithms,&lt;/strong&gt; we currently have &lt;strong&gt;GSPO&lt;/strong&gt; and &lt;strong&gt;GRPO&lt;/strong&gt; both with &lt;strong&gt;Liger-Kernel&lt;/strong&gt; implementations but you can probably get DAPO and some others by just adjusting some of the settings.&lt;/p&gt;\n\n&lt;p&gt;Feedback and issues are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?auto=webp&amp;s=d1c178e9209b8e83156e14a1ff72b92a0721fd8f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b2e184f5160e98cc31a4243590278640475fdef",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a02b458617c0d0ceff2a45cdc6709316610006",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e418c06b19810a48fcbc0e741cc9ba5d2cd1413",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc18776cd35ebb7e21739094063171f691aa1f7b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0d7d62db37e6673039732a16af989a3492b949b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2006937f67001eb0977dff97aad1540a191354ea",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcqrwh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rd211x",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753831115,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Most local models usually write some terrible puns but this was ackshually kinda funny.",
          "author_fullname": "t2_78ega6d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "First time an LLM has written a funny joke for me",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mcqr9w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.48,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NIXwfUhHmc-u5W4w7zPLgGi63PfAGDZjP1cpXRfXL5M.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753831067,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most local models usually write some terrible puns but this was ackshually kinda funny.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lmzpc4b5cwff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lmzpc4b5cwff1.png?auto=webp&amp;s=2571977e13c017a3035707a69a08f839964dc36a",
                  "width": 1678,
                  "height": 436
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lmzpc4b5cwff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=acf2b9ca7aab7f2e1c8f3c9cff8fceeb2698d971",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/lmzpc4b5cwff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d12a317b31efae9454ac25ab3d6d39f6f9f9aab",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/lmzpc4b5cwff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c990e829c11aa5949a96b71ab807a7a8f30729e8",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/lmzpc4b5cwff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5872d325a8b8c33ffa187b870a09686063b88df",
                    "width": 640,
                    "height": 166
                  },
                  {
                    "url": "https://preview.redd.it/lmzpc4b5cwff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=36b69de924e3a78678b6b136fbe26acbb61364d1",
                    "width": 960,
                    "height": 249
                  },
                  {
                    "url": "https://preview.redd.it/lmzpc4b5cwff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e6cd4323c4eea1e196941403670591cdcad2801",
                    "width": 1080,
                    "height": 280
                  }
                ],
                "variants": {},
                "id": "On_3edRq82tm-O_AzE-5ZUjORAyBbK83SLi_HtKe9-8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcqr9w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "killercheese21",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcqr9w/first_time_an_llm_has_written_a_funny_joke_for_me/",
          "stickied": false,
          "url": "https://i.redd.it/lmzpc4b5cwff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753831067,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Curious to know what you are using. My setup is dual 3090s and I am debating a third, just because I can, not because I need to! \n\n[View Poll](https://www.reddit.com/poll/1mcqlv7)",
          "author_fullname": "t2_20yqdwbf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How many GPUs do you run and what model(s) do you use.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcqlv7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753835602,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753830685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to know what you are using. My setup is dual 3090s and I am debating a third, just because I can, not because I need to! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1mcqlv7\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcqlv7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Salt_Armadillo8884",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1754435485708,
            "options": [
              {
                "text": "1 GPU 8-16gb",
                "id": "31249074"
              },
              {
                "text": "1 GPU 20-32gb",
                "id": "31249075"
              },
              {
                "text": "2 GPU 8-16gb",
                "id": "31249076"
              },
              {
                "text": "2 GPU 20-32gb",
                "id": "31249077"
              },
              {
                "text": "3 or more GPUs",
                "id": "31249078"
              },
              {
                "text": "1 GPU at 48GB or more",
                "id": "31249079"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 140,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcqlv7/how_many_gpus_do_you_run_and_what_models_do_you/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcqlv7/how_many_gpus_do_you_run_and_what_models_do_you/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753830685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!  \nI’m thrilled to share a project I’ve been pouring my energy into: CloudToLocalLLM. Built with Flutter and Dart, it’s a tool that connects local Large Language Models (LLMs) to cloud services, blending privacy, offline capabilities, and cross-platform support. It’s in alpha, and I’m excited to give you a peek at what it’s all about!What’s CloudToLocalLLM?CloudToLocalLLM lets you run LLMs on your own hardware for privacy and offline use, while seamlessly hooking up to cloud APIs for extra functionality when you need it. It’s all about giving you control over your AI workflows, whether you’re on desktop now or mobile in the future.Key Features:\n\n* Local LLM Processing: Run models on-device to keep your data private.\n* Offline Support: Works smoothly without an internet connection.\n* Cloud Integration: Connects to cloud APIs for added power.\n* Cross-Platform: Desktop support now, with Android/iOS in development.\n* Future Plans: Premium features and plugin/extension support for custom setups.\n\nTech Stack:\n\n* Flutter and Dart for the UI and cross-platform foundation.\n* LLM libraries for local model processing.\n* Cloud APIs for external service integration.\n* Tunneling setup for secure local-to-cloud communication.\n\nCurrent Status:The project is in alpha with a solid foundation for local LLM processing and cloud syncing. I’m currently refining the tunneling setup to ensure smooth data flow between local models and cloud services. Mobile support for Android and iOS is on the way, along with plans for premium features and a plugin/extension system to make it highly extensible.Take a look at the project on [GitHub](https://github.com/imrightguy/CloudToLocalLLM) for more details. Hope you find it as exciting as I do—happy to share this with the community!",
          "author_fullname": "t2_1m04ysbxj2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CloudToLocalLLM - A Flutter-built Tool for Local LLM and Cloud Integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcq5tj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753829541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;br/&gt;\nI’m thrilled to share a project I’ve been pouring my energy into: CloudToLocalLLM. Built with Flutter and Dart, it’s a tool that connects local Large Language Models (LLMs) to cloud services, blending privacy, offline capabilities, and cross-platform support. It’s in alpha, and I’m excited to give you a peek at what it’s all about!What’s CloudToLocalLLM?CloudToLocalLLM lets you run LLMs on your own hardware for privacy and offline use, while seamlessly hooking up to cloud APIs for extra functionality when you need it. It’s all about giving you control over your AI workflows, whether you’re on desktop now or mobile in the future.Key Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Local LLM Processing: Run models on-device to keep your data private.&lt;/li&gt;\n&lt;li&gt;Offline Support: Works smoothly without an internet connection.&lt;/li&gt;\n&lt;li&gt;Cloud Integration: Connects to cloud APIs for added power.&lt;/li&gt;\n&lt;li&gt;Cross-Platform: Desktop support now, with Android/iOS in development.&lt;/li&gt;\n&lt;li&gt;Future Plans: Premium features and plugin/extension support for custom setups.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Flutter and Dart for the UI and cross-platform foundation.&lt;/li&gt;\n&lt;li&gt;LLM libraries for local model processing.&lt;/li&gt;\n&lt;li&gt;Cloud APIs for external service integration.&lt;/li&gt;\n&lt;li&gt;Tunneling setup for secure local-to-cloud communication.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current Status:The project is in alpha with a solid foundation for local LLM processing and cloud syncing. I’m currently refining the tunneling setup to ensure smooth data flow between local models and cloud services. Mobile support for Android and iOS is on the way, along with plans for premium features and a plugin/extension system to make it highly extensible.Take a look at the project on &lt;a href=\"https://github.com/imrightguy/CloudToLocalLLM\"&gt;GitHub&lt;/a&gt; for more details. Hope you find it as exciting as I do—happy to share this with the community!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?auto=webp&amp;s=55cc3598b62dd189b9301971fa3f0072aeeb4271",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be5429a462c6432b21f002208275d58e14f8b9e1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4080be59a4f438d917478f7b77dffe93a39d950",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=80dbf6ca80181eaf9ae95db9fd1247b36ce4cb60",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f58a1a23784b73314db3bb7695b9cf6a4291de5f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a94965b4db927c5db2b338fa78c7c6a09934ee03",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1dedb8e88059caaffc11bf5bbce338b323611439",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "94gP6nyOduLfhUJZ5JUL3ZMOgReprnoeDz7otvSr-5U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcq5tj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_right_guy",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcq5tj/cloudtolocalllm_a_flutterbuilt_tool_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcq5tj/cloudtolocalllm_a_flutterbuilt_tool_for_local_llm/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753829541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "GLM-4.5 Air is giving me QwQ vibes, but at least QwQ finishes. This never ends until I put it out of its misery: \n",
          "author_fullname": "t2_ekgpz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Who are you, GLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcq1cs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pSlNAPAoHHUJEZTcC-t-8TehL1_Tl4RnX22DLKt95fs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753829222,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GLM-4.5 Air is giving me QwQ vibes, but at least QwQ finishes. This never ends until I put it out of its misery: &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/fbmovxzp6wff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?auto=webp&amp;s=1a2e497e8a0ee922867cb86816716aff54ab002f",
                  "width": 1352,
                  "height": 1751
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a013b734069a49331ce7519a520e1f047c58a847",
                    "width": 108,
                    "height": 139
                  },
                  {
                    "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e849489fadac3e858a6184302cb4d92380a139a",
                    "width": 216,
                    "height": 279
                  },
                  {
                    "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=05fc77356a96f6a663cc8ac8208996bbfca80fdd",
                    "width": 320,
                    "height": 414
                  },
                  {
                    "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=affc268e4e5f2a0c67eaf6858be56c70967532c4",
                    "width": 640,
                    "height": 828
                  },
                  {
                    "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ed6a30d8c3928c2688d745af3f7137fbd5a9040",
                    "width": 960,
                    "height": 1243
                  },
                  {
                    "url": "https://preview.redd.it/fbmovxzp6wff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e68584f4b354450ee6e4188825373a02facdf3e7",
                    "width": 1080,
                    "height": 1398
                  }
                ],
                "variants": {},
                "id": "ctyWUP_UeSgyv81eUDcEC5RVxcGmAxr83JnRuR1LiEA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mcq1cs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jsllls",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcq1cs/who_are_you_glm/",
          "stickied": false,
          "url": "https://i.redd.it/fbmovxzp6wff1.jpeg",
          "subreddit_subscribers": 506711,
          "created_utc": 1753829222,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CORSAIR Unveils AI Workstation 300, Starting At $1599, Boasting Ryzen AI Max+ 395 Processor And Up To 128 GB LPDDR5X Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcpxr4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=573f3b0817e015efd774edd561e17b4072b1594d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753828960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/corsair-unveils-ai-workstation-300-starting-at-1599-boasting-ryzen-ai-max-395-processor-and-up-to-128-gb-lpddr5x-memory/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?auto=webp&amp;s=9ae182584158deb74cf260403d2a1843e2bda986",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=573531f4c8a5371bd4715614521a58d2bd7d8502",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c940ac2aebd959c0b9843c83cedc2ddf6d963e9",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7db7ea540c8068b7c2c6680b1622f90b68bc81f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=607fa3aabb7b010ac2df88acfaf5db548da8d894",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6d9f42998d1ff632e2997fdd2af1af0357afac4",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7b189cb8974cf4009d9f446221e7e38a2b1d194",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "6NY8uQaM4Un1tgb6LoBggtzptfYaKDhj0gjpRvNB-p0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcpxr4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcpxr4/corsair_unveils_ai_workstation_300_starting_at/",
          "stickied": false,
          "url": "https://wccftech.com/corsair-unveils-ai-workstation-300-starting-at-1599-boasting-ryzen-ai-max-395-processor-and-up-to-128-gb-lpddr5x-memory/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753828960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 on fiction.livebench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcp7dp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QcSFm7ZWAqzMHjpuqS2BSh-cF9X422RD7yB4xytZxyM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753827110,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/aey1fr0e0wff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/aey1fr0e0wff1.png?auto=webp&amp;s=03652a7cefbd3cf68aa3a4c2fa7f8ef9460eab7c",
                  "width": 1506,
                  "height": 2308
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=200a158a099cf05614121ad48d3dea5304d2191d",
                    "width": 108,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c60de759152ef785372120f079d85c9e3ab9a29",
                    "width": 216,
                    "height": 331
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=df5280d81e9b288107f2acf30fc72f742a2d2975",
                    "width": 320,
                    "height": 490
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=164ac21dfb78027d97359b16dae6fc48436681ec",
                    "width": 640,
                    "height": 980
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d82a89795db7589130bfb4073fd6b0b7257b318",
                    "width": 960,
                    "height": 1471
                  },
                  {
                    "url": "https://preview.redd.it/aey1fr0e0wff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90bfb72094bf842d3f0624e686efa444e2bba799",
                    "width": 1080,
                    "height": 1655
                  }
                ],
                "variants": {},
                "id": "xaQccbr53kPCE6BsoktLHVTFuBeqr91ypjMWvOtHbFU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcp7dp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcp7dp/glm45_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/aey1fr0e0wff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753827110,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wrote a small CLI in golang today with Claude that auto downloads the models and comes out at around 5MB in size when compiled. The goal is to create a foundation to build a single unix style utility that can take files as input and transcribe them easily. It also handles whole folders of files and can restart when it gets interrupted. \n\nI still want to add speaker diarization as well as publish it to brew and a few more things. But I already wanted to get some feedback from people. \n\nThe main goal for me is to point it at a YouTube channel, download all the videos audio streams via yt-dlp, then transcribe the whole pack, recognise speakers, use a small LLM to identify who is who to replace &lt;speaker1&gt; with “Tom” etc and then have nice archives of channels with good text representations. \n\nhttps://github.com/pascalwhoop/ghospel\n\nLmk what you guys think and what you’d be looking for in a CLI like this. \n\nThere’s also a blog post about it but I won’t self promote too much for now. ",
          "author_fullname": "t2_1uktcnndgt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Golang based whisper.cpp wrapper CLI with intention to expand to speaker diarization and more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcp4lj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753826924,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a small CLI in golang today with Claude that auto downloads the models and comes out at around 5MB in size when compiled. The goal is to create a foundation to build a single unix style utility that can take files as input and transcribe them easily. It also handles whole folders of files and can restart when it gets interrupted. &lt;/p&gt;\n\n&lt;p&gt;I still want to add speaker diarization as well as publish it to brew and a few more things. But I already wanted to get some feedback from people. &lt;/p&gt;\n\n&lt;p&gt;The main goal for me is to point it at a YouTube channel, download all the videos audio streams via yt-dlp, then transcribe the whole pack, recognise speakers, use a small LLM to identify who is who to replace &amp;lt;speaker1&amp;gt; with “Tom” etc and then have nice archives of channels with good text representations. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/pascalwhoop/ghospel\"&gt;https://github.com/pascalwhoop/ghospel&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Lmk what you guys think and what you’d be looking for in a CLI like this. &lt;/p&gt;\n\n&lt;p&gt;There’s also a blog post about it but I won’t self promote too much for now. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?auto=webp&amp;s=c9667d67ac26714cecec59f946f00ae2cbb28091",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e8b2c21c9cb31cb7050c6c04a1b0a264cdb2d2d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f888bae2c84de4cb101d7a6e8bddd2ec7a8a0bc2",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0429088192a964051239facfec6a3f881acdf706",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bee7509b0c842733361018e91c9f2cb421adbc6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=31151174fbd5c99a5e8f086a0fd0dd00c61839c2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1b7c7ff2eff95d41ec4a945ddbd2f58946aa08a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ZMHo-0fBowkzXb3QE7apipXrGseSII9LtU8r7eb84ac"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcp4lj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pascalwhoop",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcp4lj/golang_based_whispercpp_wrapper_cli_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcp4lj/golang_based_whispercpp_wrapper_cli_with/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753826924,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone! 👋\n\nI'm exploring a novel concept in unsupervised neural machine translation and would love to get your feedback. I’m curious if this approach has been tested before—or if someone might be interested in giving it a try.\n\n**My idea in a nutshell:**\n\n- I train two simple decoder‑only models (transformers) **at the character level**, one on English, another on Ukrainian. No encoder, no shared latent space.\n- These two decoders are completely separate and independently trained as **language models**—each fluent in its own language.\n\nNow here’s the twist:\n\n- When we want to translate an English sentence, we feed it **as characters** into the English decoder.\n- We then extract its **inner hidden states** (or attention activations).\n- Those hidden states are passed directly into the Ukrainian decoder (as if they were input).\n- The Ukrainian decoder tries to generate an equivalent Ukrainian sentence.\n\n**No extra layers, no mapper—just latent states transferred from one decoder to the other.**\n\n---\n\n### Why I think it *could* work:\n\n1. **Natural language is built on statistical patterns.**  \n   At the character level, both languages contain frequent patterns—letter combinations, suffixes, morphology—that can be learned without semantic knowledge.\n\n2. **English and Ukrainian share some structural similarities** (SVO order, some grammatical forms). A decoder-only model trained character-wise can capture this statistical structure.\n\n3. Even if the language models don’t “understand” each other initially, they can potentially learn to interpret these latent signals through **cross‐language supervision**.\n\n---\n\n### Proposed training strategy:\n\n1. Pre-train `D_en` on English text and `D_uk` on Ukrainian text (character-level modeling).\n2. During translation training:\n   - Use an English sentence `sEn`.\n   - Feed it into `D_en`, capture hidden state matrix `H_en`.\n   - Input `H_en` (frame‑aligned) into `D_uk`, let it generate `sUk_pred`.\n   - Compute loss by comparing `sUk_pred` with the *true* Ukrainian translation `sUk`.\n3. Optionally add a cycle: sEn → D_en → H_en → D_uk → sUk_pred sUk_pred → D_uk → H_uk → D_en → sEn_restored\n\nand enforce reconstruction (cycle‑consistency loss).\n\n---\n\n### Challenges I’m concerned about:\n\n- Feeding hidden states from one decoder into another—how should they align?\n- Do hidden states carry enough semantic structure for the second decoder to make sense of them?\n- Would the English decoder still generate fluent English after learning to accept Ukrainian input?\n- Could training converge—or would this mutual mapping collapse?\n\n---\n\n### My constraints:\n\n- I don’t have access to GPUs or major compute resources 😅\n- I’d mainly like to get feedback, references, or see if anyone has tried something similar—or might be able to prototype this.\n\n---\n\n### Would love to hear:\n\n- If anyone has experimented with **decoder‑only cross‑communication**, especially at the hidden‐state level.\n- Ideas for alignment strategies between decoder hidden states.\n- Training tips: masking, attention mapping, loss design, etc.\n- Any known literature or codebases exploring similar minimal translation approaches.\n\nThanks for your time!  \n— **Buka Koshmarovich**",
          "author_fullname": "t2_rnaz4818s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could two decoder‑only models communicate directly via latent outputs and translate each other?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcoou9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753825851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! 👋&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring a novel concept in unsupervised neural machine translation and would love to get your feedback. I’m curious if this approach has been tested before—or if someone might be interested in giving it a try.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My idea in a nutshell:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I train two simple decoder‑only models (transformers) &lt;strong&gt;at the character level&lt;/strong&gt;, one on English, another on Ukrainian. No encoder, no shared latent space.&lt;/li&gt;\n&lt;li&gt;These two decoders are completely separate and independently trained as &lt;strong&gt;language models&lt;/strong&gt;—each fluent in its own language.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now here’s the twist:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When we want to translate an English sentence, we feed it &lt;strong&gt;as characters&lt;/strong&gt; into the English decoder.&lt;/li&gt;\n&lt;li&gt;We then extract its &lt;strong&gt;inner hidden states&lt;/strong&gt; (or attention activations).&lt;/li&gt;\n&lt;li&gt;Those hidden states are passed directly into the Ukrainian decoder (as if they were input).&lt;/li&gt;\n&lt;li&gt;The Ukrainian decoder tries to generate an equivalent Ukrainian sentence.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;No extra layers, no mapper—just latent states transferred from one decoder to the other.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Why I think it &lt;em&gt;could&lt;/em&gt; work:&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Natural language is built on statistical patterns.&lt;/strong&gt;&lt;br/&gt;\nAt the character level, both languages contain frequent patterns—letter combinations, suffixes, morphology—that can be learned without semantic knowledge.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;English and Ukrainian share some structural similarities&lt;/strong&gt; (SVO order, some grammatical forms). A decoder-only model trained character-wise can capture this statistical structure.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Even if the language models don’t “understand” each other initially, they can potentially learn to interpret these latent signals through &lt;strong&gt;cross‐language supervision&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Proposed training strategy:&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Pre-train &lt;code&gt;D_en&lt;/code&gt; on English text and &lt;code&gt;D_uk&lt;/code&gt; on Ukrainian text (character-level modeling).&lt;/li&gt;\n&lt;li&gt;During translation training:\n\n&lt;ul&gt;\n&lt;li&gt;Use an English sentence &lt;code&gt;sEn&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Feed it into &lt;code&gt;D_en&lt;/code&gt;, capture hidden state matrix &lt;code&gt;H_en&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Input &lt;code&gt;H_en&lt;/code&gt; (frame‑aligned) into &lt;code&gt;D_uk&lt;/code&gt;, let it generate &lt;code&gt;sUk_pred&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Compute loss by comparing &lt;code&gt;sUk_pred&lt;/code&gt; with the &lt;em&gt;true&lt;/em&gt; Ukrainian translation &lt;code&gt;sUk&lt;/code&gt;.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Optionally add a cycle: sEn → D_en → H_en → D_uk → sUk_pred sUk_pred → D_uk → H_uk → D_en → sEn_restored&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;and enforce reconstruction (cycle‑consistency loss).&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Challenges I’m concerned about:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Feeding hidden states from one decoder into another—how should they align?&lt;/li&gt;\n&lt;li&gt;Do hidden states carry enough semantic structure for the second decoder to make sense of them?&lt;/li&gt;\n&lt;li&gt;Would the English decoder still generate fluent English after learning to accept Ukrainian input?&lt;/li&gt;\n&lt;li&gt;Could training converge—or would this mutual mapping collapse?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;My constraints:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I don’t have access to GPUs or major compute resources 😅&lt;/li&gt;\n&lt;li&gt;I’d mainly like to get feedback, references, or see if anyone has tried something similar—or might be able to prototype this.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Would love to hear:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If anyone has experimented with &lt;strong&gt;decoder‑only cross‑communication&lt;/strong&gt;, especially at the hidden‐state level.&lt;/li&gt;\n&lt;li&gt;Ideas for alignment strategies between decoder hidden states.&lt;/li&gt;\n&lt;li&gt;Training tips: masking, attention mapping, loss design, etc.&lt;/li&gt;\n&lt;li&gt;Any known literature or codebases exploring similar minimal translation approaches.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for your time!&lt;br/&gt;\n— &lt;strong&gt;Buka Koshmarovich&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcoou9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "According_Change2007",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753825851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Ryzen AI MAX+ Processors Now Offer a Whopping 96 GB Memory for Consumer Graphics, Allowing Gigantic 128B-Parameter LLMs to Run Locally on PCs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcoce7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=e171e9de6f65dcbec52d568f58750bcc5885ba3a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753825022,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?auto=webp&amp;s=d470a9db6c0be816c3d916347e4ae5e8bad6c6a8",
                  "width": 750,
                  "height": 422
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e14a96af45b1cfcde1a2159e64971bc1d775033",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47124b81e03a6604140c8e4d29ddbe43c15456c3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf6168a4321e52f96ea360e8b865dc9bbcaaf345",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d50b7793829c5aa107cf8ecaa3b004d46e3cdef0",
                    "width": 640,
                    "height": 360
                  }
                ],
                "variants": {},
                "id": "9cxUs2c7UTW3WnCYfQNVG3P3u4GjtOuwQSim_dwuwEI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcoce7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 43,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcoce7/amds_ryzen_ai_max_processors_now_offer_a_whopping/",
          "stickied": false,
          "url": "https://wccftech.com/amd-ryzen-ai-max-processors-offer-a-96gb-memory-for-consumer-graphics/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753825022,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I saw [unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face](https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF) just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4\\_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I'm excited to see what else it can do this week!\n\nGitHub: [lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration](https://github.com/lemonade-sdk/lemonade)",
          "author_fullname": "t2_1m2ckixcqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lemonade: I'm hyped about the speed of the new Qwen3-30B-A3B-Instruct-2507 on Radeon 9070 XT",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mco449",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/7xpye5hurvff1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 592,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/7xpye5hurvff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/7xpye5hurvff1/DASHPlaylist.mpd?a=1756430004%2CNDM2MmI5ZTFkMzFmZDFkNGZlM2I5OTg2OGFkN2M2ZTkwYjRiNTdkNWE0MjNkNGVkMTlmNWMzMjJmMGQzNjlmMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/7xpye5hurvff1/HLSPlaylist.m3u8?a=1756430004%2CYjQ1ODQ4NTIxNjQwMDA1ODM5NjZiYjBkMGI1YzMyOWE2MzM0NzdiZjYwMmE3NzU5NDM1ODY5MGFhMTBiOTkyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=140&amp;height=64&amp;crop=140:64,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7412d0f300b6da0b8bc9109ecb41ef07f7def3be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753824484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw &lt;a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF\"&gt;unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF · Hugging Face&lt;/a&gt; just came out so I took it for a test drive on Lemonade Server today on my Radeon 9070 XT rig (llama.cpp+vulkan backend, Q4_0, OOB performance with no tuning). The fact that it one-shots the solution with no thinking tokens makes it way faster-to-solution than the previous Qwen3 MOE. I&amp;#39;m excited to see what else it can do this week!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/lemonade-sdk/lemonade\"&gt;lemonade-sdk/lemonade: Local LLM Server with GPU and NPU Acceleration&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/7xpye5hurvff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?format=pjpg&amp;auto=webp&amp;s=42d0cbec81034d7b7efd7f37129a81782dc0f3a9",
                  "width": 2074,
                  "height": 960
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f46a864c4101d08c5f7634bda139ea125562aef9",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a85bb6d97aae39128b013a769b747a78d013bef",
                    "width": 216,
                    "height": 99
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3b96d303e0d238fc8ee25ce519ce9a723c249c3",
                    "width": 320,
                    "height": 148
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cac371463bffd1516bfca661c6b086b5c43e0a77",
                    "width": 640,
                    "height": 296
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbfaa41803cc435e672ff77f44bdc9bbdc1c8952",
                    "width": 960,
                    "height": 444
                  },
                  {
                    "url": "https://external-preview.redd.it/czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=25e3d9ea149342d300ab0c21256440d35acba3ed",
                    "width": 1080,
                    "height": 499
                  }
                ],
                "variants": {},
                "id": "czBmdXM1aHVydmZmMQf6BkKZI7Ikr6YU2YwAQgo-ERGqCSuuIIibFbpDzG0R"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mco449",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfowers_amd",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mco449/lemonade_im_hyped_about_the_speed_of_the_new/",
          "stickied": false,
          "url": "https://v.redd.it/7xpye5hurvff1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753824484,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/7xpye5hurvff1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 592,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/7xpye5hurvff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/7xpye5hurvff1/DASHPlaylist.mpd?a=1756430004%2CNDM2MmI5ZTFkMzFmZDFkNGZlM2I5OTg2OGFkN2M2ZTkwYjRiNTdkNWE0MjNkNGVkMTlmNWMzMjJmMGQzNjlmMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/7xpye5hurvff1/HLSPlaylist.m3u8?a=1756430004%2CYjQ1ODQ4NTIxNjQwMDA1ODM5NjZiYjBkMGI1YzMyOWE2MzM0NzdiZjYwMmE3NzU5NDM1ODY5MGFhMTBiOTkyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You can now run Llama 4 Scout in LM Studio on Windows. Pretty decent speed too \\~15 tk/s",
          "author_fullname": "t2_13crip",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Ryzen AI Max+ Upgraded: Run up to 128 Billion parameter LLMs on Windows with LM Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcnq7r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=33c8eb8fd935cf2ea90a0a33e411d886f5ae1a9c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753823574,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "amd.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can now run Llama 4 Scout in LM Studio on Windows. Pretty decent speed too ~15 tk/s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-upgraded-run-up-to-128-billion-parameter-llms-lm-studio.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?auto=webp&amp;s=ffab56effc31efa32c26741ccf472cd572474a71",
                  "width": 2887,
                  "height": 1620
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=101f81a143ae5429affeeaa9b0172147a565a3f2",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da82e6bdc51acddc3836e55c7a6fa5121fd48163",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=952681f3657af07194fc11d19c2e1aa52e971a16",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1407a6de51b1efe682d3aec309cbbdadb1b1d910",
                    "width": 640,
                    "height": 359
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fe54f141d273c10ecf2167c9a13bfac5e717bba9",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a0abe4e0514c46a50ab880382f1a472fe3518433",
                    "width": 1080,
                    "height": 606
                  }
                ],
                "variants": {},
                "id": "B9Fy4KUJWjNG-aZfpX14SQ7WVw_ASSpkwjQcSa3uTLA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcnq7r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ZZZCodeLyokoZZZ",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcnq7r/amd_ryzen_ai_max_upgraded_run_up_to_128_billion/",
          "stickied": false,
          "url": "https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-upgraded-run-up-to-128-billion-parameter-llms-lm-studio.html",
          "subreddit_subscribers": 506711,
          "created_utc": 1753823574,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m here to warn everybody that Docker Model Runnner is the friend she told you not to worry about who is sneaking in the back door and about to steal your girl’s inference (sorry, that sounds way dirtier than I meant it to). \n\nReal talk tho, Ollama seems to have kind of fell off the last month or so. They haven’t dropped a new “official” model release since Mistral Small 3.2, Sure, you can pull a lot of huggingface models direct now, but dang nobody wants to mess with those long ass model names, right? \n\nI don’t feel like Ollama have been incorporating the latest llama.cpp updates as fast as they used to. It used to be like a new llama.cpp would drop, and then new Ollama update would come out like one day later, hasn’t seemed like that lately though. The whole vibe over on r/Ollama seems a little off right now TBH.\n\nDocker Model Runner just kinda showed up inside Docker Desktop a little while ago as an experimental feature, now it’s taken its shoes off and made itself at home as part of both Docker Desktop and Docker Engine. \n\nWhile we all were busy oohing and ahhhing over all these new models, Docker Model runner:\n\n- Was added to Hugging Face under pretty much every GGUF’s “Use this model” dropdown list with easy copy/paste access making it dead simple to pull and run ANY GGUF model.\n- Started developing its own Docker AI Model Hub which reduces any friction that may have existed for pulling and running a model. \n- Added an MCP Server and hub to the mix as well. \n\nThis was a pretty bold move on Docker’s part. They just added inference as a feature to the product a lot of us were already using to serve AI container apps. \n\nNow, I’m not sure how good the model swapping capabilities are yet because I haven’t done a ton of testing, but they are there as features and from what I understand, the whole thing is highly-configurable if you need that kind of thing and don’t mind building Docker Compose or YAML files or whatever. \n\nI’m assuming that since it’s llama.cop based that it’ll incorporate llama.cpp updates fairly quickly, but you never know. \n\nAre any of y’all using Docker Model Runner? Do you like it better or worse than Ollama or LM Studio, or even plain ole Llama.cop? \n\nHere’s their doc site if anyone wants to read up on it:\n\nhttps://docs.docker.com/ai/model-runner/",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Docker Model Runner is going to steal your girl’s inference.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcnhtc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753823012,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m here to warn everybody that Docker Model Runnner is the friend she told you not to worry about who is sneaking in the back door and about to steal your girl’s inference (sorry, that sounds way dirtier than I meant it to). &lt;/p&gt;\n\n&lt;p&gt;Real talk tho, Ollama seems to have kind of fell off the last month or so. They haven’t dropped a new “official” model release since Mistral Small 3.2, Sure, you can pull a lot of huggingface models direct now, but dang nobody wants to mess with those long ass model names, right? &lt;/p&gt;\n\n&lt;p&gt;I don’t feel like Ollama have been incorporating the latest llama.cpp updates as fast as they used to. It used to be like a new llama.cpp would drop, and then new Ollama update would come out like one day later, hasn’t seemed like that lately though. The whole vibe over on &lt;a href=\"/r/Ollama\"&gt;r/Ollama&lt;/a&gt; seems a little off right now TBH.&lt;/p&gt;\n\n&lt;p&gt;Docker Model Runner just kinda showed up inside Docker Desktop a little while ago as an experimental feature, now it’s taken its shoes off and made itself at home as part of both Docker Desktop and Docker Engine. &lt;/p&gt;\n\n&lt;p&gt;While we all were busy oohing and ahhhing over all these new models, Docker Model runner:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Was added to Hugging Face under pretty much every GGUF’s “Use this model” dropdown list with easy copy/paste access making it dead simple to pull and run ANY GGUF model.&lt;/li&gt;\n&lt;li&gt;Started developing its own Docker AI Model Hub which reduces any friction that may have existed for pulling and running a model. &lt;/li&gt;\n&lt;li&gt;Added an MCP Server and hub to the mix as well. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This was a pretty bold move on Docker’s part. They just added inference as a feature to the product a lot of us were already using to serve AI container apps. &lt;/p&gt;\n\n&lt;p&gt;Now, I’m not sure how good the model swapping capabilities are yet because I haven’t done a ton of testing, but they are there as features and from what I understand, the whole thing is highly-configurable if you need that kind of thing and don’t mind building Docker Compose or YAML files or whatever. &lt;/p&gt;\n\n&lt;p&gt;I’m assuming that since it’s llama.cop based that it’ll incorporate llama.cpp updates fairly quickly, but you never know. &lt;/p&gt;\n\n&lt;p&gt;Are any of y’all using Docker Model Runner? Do you like it better or worse than Ollama or LM Studio, or even plain ole Llama.cop? &lt;/p&gt;\n\n&lt;p&gt;Here’s their doc site if anyone wants to read up on it:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.docker.com/ai/model-runner/\"&gt;https://docs.docker.com/ai/model-runner/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mcnhtc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753823012,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello\n\nIm building in React Native making things slighlty more diff but the app concept is simple\n\n  \n1. Take a photo (camera)\n\n2. ocr (get ingredients from picture to text)\n\n3. ai (grade the ingredients 0 - 100 + brief explanation\n\n\n\nIve got the project started with llama.rn\n\nI can run the following models:\n\n 1. Phi-3.5 Mini (your current choice) - Actually good!\n\n\\- \\~1.5-2GB quantized\n\n\\- Specifically designed for mobile\n\n\\- Good reasoning for the size\n\n  2. Gemma 2B - Smaller alternative\n\n\\- \\~1.2-1.5GB quantized\n\n\\- Google's efficient model\n\n\\- Good for classification tasks\n\n  3. TinyLlama 1.1B - Ultra-light\n\n\\- \\~700MB-1GB quantized\n\n\\- Very fast inference\n\n\\- May sacrifice some accuracy\n\n  \nClaude is telling me to go with Phi3.5 but it seems like Reddit is not a fan. \n\nWhich would you choose? Any advice?",
          "author_fullname": "t2_16jrcp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model should I use - build a nutrition label scanner in React Native",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcn8dx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753822408,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;Im building in React Native making things slighlty more diff but the app concept is simple&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Take a photo (camera)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ocr (get ingredients from picture to text)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ai (grade the ingredients 0 - 100 + brief explanation&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ive got the project started with llama.rn&lt;/p&gt;\n\n&lt;p&gt;I can run the following models:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Phi-3.5 Mini (your current choice) - Actually good!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- ~1.5-2GB quantized&lt;/p&gt;\n\n&lt;p&gt;- Specifically designed for mobile&lt;/p&gt;\n\n&lt;p&gt;- Good reasoning for the size&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Gemma 2B - Smaller alternative&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- ~1.2-1.5GB quantized&lt;/p&gt;\n\n&lt;p&gt;- Google&amp;#39;s efficient model&lt;/p&gt;\n\n&lt;p&gt;- Good for classification tasks&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;TinyLlama 1.1B - Ultra-light&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;- ~700MB-1GB quantized&lt;/p&gt;\n\n&lt;p&gt;- Very fast inference&lt;/p&gt;\n\n&lt;p&gt;- May sacrifice some accuracy&lt;/p&gt;\n\n&lt;p&gt;Claude is telling me to go with Phi3.5 but it seems like Reddit is not a fan. &lt;/p&gt;\n\n&lt;p&gt;Which would you choose? Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcn8dx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mr_captcha",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcn8dx/which_model_should_i_use_build_a_nutrition_label/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcn8dx/which_model_should_i_use_build_a_nutrition_label/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753822408,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. Suppose we have a text2sql use case (or some other task where the LLM use case can easily get verified to some degree, ideally automatically): We ask a question, LLM generates the SQL code, we run the code, and the code is wrong. It could also happen that e.g. the SQL query returns empty result, but we are sure it shouldn't.\n\nWhat is the best way to incorporate these false answers as part of the context in the next LLM call, to help converge to the correct answer? \n\nAssuming an OpenAI-compatible REST API, is it part of the user message, a separate user message, another type of message, or something else? Is there a well-known practice?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you provide negative examples to the LLM API?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcmt07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753821440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Suppose we have a text2sql use case (or some other task where the LLM use case can easily get verified to some degree, ideally automatically): We ask a question, LLM generates the SQL code, we run the code, and the code is wrong. It could also happen that e.g. the SQL query returns empty result, but we are sure it shouldn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to incorporate these false answers as part of the context in the next LLM call, to help converge to the correct answer? &lt;/p&gt;\n\n&lt;p&gt;Assuming an OpenAI-compatible REST API, is it part of the user message, a separate user message, another type of message, or something else? Is there a well-known practice?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcmt07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcmt07/how_do_you_provide_negative_examples_to_the_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcmt07/how_do_you_provide_negative_examples_to_the_llm/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753821440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1vte8skx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcmbyt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753820362,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.12856",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcmbyt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bianconi",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcmbyt/supervised_fine_tuning_on_curated_data_is/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.12856",
          "subreddit_subscribers": 506711,
          "created_utc": 1753820362,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on \\`benchmax\\`, a open-source framework for building, running, and parallelizing environments, to fine-tune LLMs with reinforcement learning.\n\n[https://github.com/cgftinc/benchmax](https://github.com/cgftinc/benchmax)\n\nWhat I wanted to solve for:\n\n\\- Environments are tightly coupled with RL trainers, leading to fragmentation and limited compatibility.\n\n\\- These coupled environments are tend to be mostly competitive math and coding → for OSS RL + LLMs to scale, we need more complex, real-world environments.\n\n\\- Scaling these environments in parallel is still not easily possible\n\nWhat I'm excited about:\n\n\\- benchmax is training framework agnostic with adapters already built out for verl and verifiers. we’re gonna build more adapters for other frameworks (e.g. SkyRL, etc.), instead of forcing others to adopt our standard (though ofc they’re welcome to )\n\n\\- benchmax comes with a few interesting environments out of the box: spreadsheet processing, CRM, etc. → more coming soon!\n\n\\- benchmax supports MCP as a first class citizen. there has been an explosion of MCP servers/tools built out for usecases ranging from browser use to excel to game creation.\\`benchmax\\` allow folks to leverage and compose these existing MCP servers to build environments integrated with real world systems\n\n\\- Multi-node environment parallelization coming soon!\n\nIf you like what you see, feel free to \\**star\\** the \\**repo\\** to support the project!! Our hope’s to really let anyone *benchmax* on their tasks, with benchmax\n\n[https://github.com/cgftinc/benchmax](https://github.com/cgftinc/benchmax)\n\nIt’s still very early! And I expect to be shipping a lot more things → more environments, more trainer integrations. Would love y’all’s thoughts what environments and trainer integrations could be interesting!",
          "author_fullname": "t2_zcf29",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a new open-source RL environment framework for LLM finetuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcmbfo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753820332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on `benchmax`, a open-source framework for building, running, and parallelizing environments, to fine-tune LLMs with reinforcement learning.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/cgftinc/benchmax\"&gt;https://github.com/cgftinc/benchmax&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What I wanted to solve for:&lt;/p&gt;\n\n&lt;p&gt;- Environments are tightly coupled with RL trainers, leading to fragmentation and limited compatibility.&lt;/p&gt;\n\n&lt;p&gt;- These coupled environments are tend to be mostly competitive math and coding → for OSS RL + LLMs to scale, we need more complex, real-world environments.&lt;/p&gt;\n\n&lt;p&gt;- Scaling these environments in parallel is still not easily possible&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m excited about:&lt;/p&gt;\n\n&lt;p&gt;- benchmax is training framework agnostic with adapters already built out for verl and verifiers. we’re gonna build more adapters for other frameworks (e.g. SkyRL, etc.), instead of forcing others to adopt our standard (though ofc they’re welcome to )&lt;/p&gt;\n\n&lt;p&gt;- benchmax comes with a few interesting environments out of the box: spreadsheet processing, CRM, etc. → more coming soon!&lt;/p&gt;\n\n&lt;p&gt;- benchmax supports MCP as a first class citizen. there has been an explosion of MCP servers/tools built out for usecases ranging from browser use to excel to game creation.`benchmax` allow folks to leverage and compose these existing MCP servers to build environments integrated with real world systems&lt;/p&gt;\n\n&lt;p&gt;- Multi-node environment parallelization coming soon!&lt;/p&gt;\n\n&lt;p&gt;If you like what you see, feel free to *&lt;em&gt;star\\&lt;/em&gt;&lt;em&gt; the \\&lt;/em&gt;&lt;em&gt;repo\\&lt;/em&gt;&lt;em&gt; to support the project!! Our hope’s to really let anyone &lt;/em&gt;benchmax* on their tasks, with benchmax&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/cgftinc/benchmax\"&gt;https://github.com/cgftinc/benchmax&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It’s still very early! And I expect to be shipping a lot more things → more environments, more trainer integrations. Would love y’all’s thoughts what environments and trainer integrations could be interesting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?auto=webp&amp;s=b2532fa7d6ca7adaf23e7e49bb1f9731f5ad2c9d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b11afd2318f8f7c3bc243369d43d504af4695dad",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=386395991e97d139a89c3be398b1c3a90db2e0eb",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5b47bd8f3893187f96559e8f5f93f5bf6f60c9c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c88e7a1888e152de7e918f397ece9c8f739fc7aa",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d96373b3618dc4b7d8726db5cdd257bb1aca809",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1761e8ea47f7f5404666f21ea4cb4b2c2fb073bd",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ZUCrUO1f-FjlWT8E7mlPobLKfAJXdYAQj3c1NDiwhwE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcmbfo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "girishkumama",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcmbfo/i_built_a_new_opensource_rl_environment_framework/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcmbfo/i_built_a_new_opensource_rl_environment_framework/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753820332,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI Max+ 395 vs nvidia vs m4, which one has higher TPM when running ollama in notebook? thanks",
          "author_fullname": "t2_354zxl3r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Notebook, AI Max+ 395 vs nvidia vs m4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcligh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753818518,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI Max+ 395 vs nvidia vs m4, which one has higher TPM when running ollama in notebook? thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcligh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "quantrpeter",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753818518,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nHow about running a local agent on a smartphone? Here's how I did it.  \n  \nI stitched together onnxruntime implemented KV Cache in DelitePy(Python) and added FP16 activations support in cpp with (via `uint16_t`), works for all binary ops in DeliteAI. Result Local Qwen 3 1.7B on mobile! \n\n# Tool Calling Features\n\n* **Multi-step conversation support** with automatic tool execution\n* **JSON-based tool calling** with `&lt;tool_call&gt;` XML tags\n* **test tools**: weather, math calculator, time, location\n\n# Used [tokenizer-cpp](https://github.com/mlc-ai/tokenizers-cpp) from MLC \n\nwhich binds rust [huggingface/tokenizers](https://github.com/huggingface/tokenizers) giving full support for android/iOS.\n\n    // - dist/tokenizer.json\n    void HuggingFaceTokenizerExample() {\n      auto blob = LoadBytesFromFile(\"dist/tokenizer.json\");  \n      auto tok = Tokenizer::FromBlobJSON(blob);\n      std::string prompt = \"What is the capital of Canada?\";\n      std::vector&lt;int&gt; ids = tok-&gt;Encode(prompt);\n      std::string decoded_prompt = tok-&gt;Decode(ids);\n    }\n\n# Push LLM streams into Kotlin Flows\n\n        suspend fun feedInput(input: String, isVoiceInitiated: Boolean, callback: (String?)-&gt;Unit) : String? {\n            val res = NimbleNet.runMethod(\n                \"prompt_for_tool_calling\",\n                inputs = hashMapOf(\n                    \"prompt\" to NimbleNetTensor(input, DATATYPE.STRING, null),\n                    \"output_stream_callback\" to  createNimbleNetTensorFromForeignFunction(callback)\n                ),\n            )\n            assert(res.status) { \"NimbleNet.runMethod('prompt_for_tool_calling') failed with status: ${res.status}\" }\n            return res.payload?.get(\"results\")?.data as String?\n        }\n\n  \nCheck the code soon merging in Delite AI (https://github.com/NimbleEdge/deliteAI/pull/165)  \nOr try in the assistant app (https://github.com/NimbleEdge/assistant)",
          "author_fullname": "t2_74zl16jw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 1.7B tool calling across Android on Pixel 9 and S22",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 74,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcl15k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3wcxuotf7vff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1028,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/3wcxuotf7vff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3wcxuotf7vff1/DASHPlaylist.mpd?a=1756430004%2CYTc2OWVmNzMxYzYyM2MzOTJkMmM4ZTM5MDdkMmZhNzA2MTUxYmE5MWY4MTJmMTYwMWM5ZDMzM2I5Y2Y4OGFhZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 96,
              "hls_url": "https://v.redd.it/3wcxuotf7vff1/HLSPlaylist.m3u8?a=1756430004%2CNTZhOTZiYzY5M2VmMzIzN2Q0OWVhMTRmMGQ5YzI4ODhiMmY5Y2YwZGJmZTY4N2RjNWY3MzA0MzVhOTBmZWYzMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=140&amp;height=74&amp;crop=140:74,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0cc796d906797d3e01c0e1b59e9c7394c449065a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753817426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How about running a local agent on a smartphone? Here&amp;#39;s how I did it.  &lt;/p&gt;\n\n&lt;p&gt;I stitched together onnxruntime implemented KV Cache in DelitePy(Python) and added FP16 activations support in cpp with (via &lt;code&gt;uint16_t&lt;/code&gt;), works for all binary ops in DeliteAI. Result Local Qwen 3 1.7B on mobile! &lt;/p&gt;\n\n&lt;h1&gt;Tool Calling Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Multi-step conversation support&lt;/strong&gt; with automatic tool execution&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;JSON-based tool calling&lt;/strong&gt; with &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; XML tags&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;test tools&lt;/strong&gt;: weather, math calculator, time, location&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Used &lt;a href=\"https://github.com/mlc-ai/tokenizers-cpp\"&gt;tokenizer-cpp&lt;/a&gt; from MLC&lt;/h1&gt;\n\n&lt;p&gt;which binds rust &lt;a href=\"https://github.com/huggingface/tokenizers\"&gt;huggingface/tokenizers&lt;/a&gt; giving full support for android/iOS.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;// - dist/tokenizer.json\nvoid HuggingFaceTokenizerExample() {\n  auto blob = LoadBytesFromFile(&amp;quot;dist/tokenizer.json&amp;quot;);  \n  auto tok = Tokenizer::FromBlobJSON(blob);\n  std::string prompt = &amp;quot;What is the capital of Canada?&amp;quot;;\n  std::vector&amp;lt;int&amp;gt; ids = tok-&amp;gt;Encode(prompt);\n  std::string decoded_prompt = tok-&amp;gt;Decode(ids);\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Push LLM streams into Kotlin Flows&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;    suspend fun feedInput(input: String, isVoiceInitiated: Boolean, callback: (String?)-&amp;gt;Unit) : String? {\n        val res = NimbleNet.runMethod(\n            &amp;quot;prompt_for_tool_calling&amp;quot;,\n            inputs = hashMapOf(\n                &amp;quot;prompt&amp;quot; to NimbleNetTensor(input, DATATYPE.STRING, null),\n                &amp;quot;output_stream_callback&amp;quot; to  createNimbleNetTensorFromForeignFunction(callback)\n            ),\n        )\n        assert(res.status) { &amp;quot;NimbleNet.runMethod(&amp;#39;prompt_for_tool_calling&amp;#39;) failed with status: ${res.status}&amp;quot; }\n        return res.payload?.get(&amp;quot;results&amp;quot;)?.data as String?\n    }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Check the code soon merging in Delite AI (&lt;a href=\"https://github.com/NimbleEdge/deliteAI/pull/165\"&gt;https://github.com/NimbleEdge/deliteAI/pull/165&lt;/a&gt;)&lt;br/&gt;\nOr try in the assistant app (&lt;a href=\"https://github.com/NimbleEdge/assistant\"&gt;https://github.com/NimbleEdge/assistant&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/3wcxuotf7vff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?format=pjpg&amp;auto=webp&amp;s=165be791b8080463b2ab6d14b9b9776e0a4a8e2c",
                  "width": 2506,
                  "height": 1342
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6fe43a7642d9c73f60904aa7697e5f67367fe209",
                    "width": 108,
                    "height": 57
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=71dece6bba55a954e2ccff12a4d99077d7266799",
                    "width": 216,
                    "height": 115
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3debc156667479551d71bb1d9aeeef63ebbe0152",
                    "width": 320,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=80d701462462fdc5caf3f289d125f5645db09a52",
                    "width": 640,
                    "height": 342
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=28bf0f71381faf47ee92aa7f472fd7dcb66cdfb7",
                    "width": 960,
                    "height": 514
                  },
                  {
                    "url": "https://external-preview.redd.it/OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2f7355860cb6fa6f63f5a696b008dbd0e23f562f",
                    "width": 1080,
                    "height": 578
                  }
                ],
                "variants": {},
                "id": "OGE0eDhmMWo3dmZmMahIsQ78FFRykDtTsz9hlKfWwrVXaeuOW0fcBOh_-QBa"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcl15k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Economy-Mud-6626",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcl15k/qwen_17b_tool_calling_across_android_on_pixel_9/",
          "stickied": false,
          "url": "https://v.redd.it/3wcxuotf7vff1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753817426,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3wcxuotf7vff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1028,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/3wcxuotf7vff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3wcxuotf7vff1/DASHPlaylist.mpd?a=1756430004%2CYTc2OWVmNzMxYzYyM2MzOTJkMmM4ZTM5MDdkMmZhNzA2MTUxYmE5MWY4MTJmMTYwMWM5ZDMzM2I5Y2Y4OGFhZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 96,
              "hls_url": "https://v.redd.it/3wcxuotf7vff1/HLSPlaylist.m3u8?a=1756430004%2CNTZhOTZiYzY5M2VmMzIzN2Q0OWVhMTRmMGQ5YzI4ODhiMmY5Y2YwZGJmZTY4N2RjNWY3MzA0MzVhOTBmZWYzMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nSo i need help with running the gguf files\nI am using LM Studio and everything is ok.\n\nI have 2 GPU and i want to test out Tensor Parallelism so i can get more speed, but i am facing some issues so i had some questions\n\nIs TP with GGUF even possible? And if yes what backend to use?\nI tried it with Vllm and i got all kinds of error so i dont know what did i do wrong.\n\nAny help is appreciated ",
          "author_fullname": "t2_clhgguip",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running GGUF models with TP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcl17g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753817429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;So i need help with running the gguf files\nI am using LM Studio and everything is ok.&lt;/p&gt;\n\n&lt;p&gt;I have 2 GPU and i want to test out Tensor Parallelism so i can get more speed, but i am facing some issues so i had some questions&lt;/p&gt;\n\n&lt;p&gt;Is TP with GGUF even possible? And if yes what backend to use?\nI tried it with Vllm and i got all kinds of error so i dont know what did i do wrong.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcl17g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Physical-Citron5153",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753817429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey! I've been using ollama models locally across my devices for a few months now. Particularly on my M2 Mac mini - although it's the base model with only 8GB of RAM. I've been using ollama since they provide an easy-to-use web interface to see the models, quickly download them, and run them, but also many other apps/clients for LLMs support it.\n\nHowever, recently I've seen stuff like MLX-LM and llama-cpp (?) that are supposedly quicker than Ollama. Not too sure on the details, but I think I get a grasp, just that the models are architecturally different? \n\nAnyways, I'd appreciate some help to get the most out of my low-end hardware? as I mentioned above I have that Mac, but also this laptop with 16GB of RAM and some crappy CPU (&amp; integrated GPU).\n\n[My laptop specs after running Neofetch on Nobara linux.](https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;format=png&amp;auto=webp&amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111)\n\n  \n\n\nI've looked around HuggingFace before, but found the UI very confusing lol. \n\nAppreciate any help!",
          "author_fullname": "t2_1lavzg2ok9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mediocre local LLM user -- tips?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 32,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kfs4he9t5vff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 25,
                  "x": 108,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bc242a070779a5a8a624284d0cc720b0397d13"
                },
                {
                  "y": 50,
                  "x": 216,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f73808e9ed0bd733c2c1656c5053a189a08b824c"
                },
                {
                  "y": 74,
                  "x": 320,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3849fd1e8d5c94f4cd0c9a7f6f63d361fe151f4"
                }
              ],
              "s": {
                "y": 118,
                "x": 507,
                "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;format=png&amp;auto=webp&amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111"
              },
              "id": "kfs4he9t5vff1"
            }
          },
          "name": "t3_1mckrn1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Lj_WeVpZphKOlxbkqLqEF6lQTf5fzZRORkn5Z1CRnwY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753816824,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey! I&amp;#39;ve been using ollama models locally across my devices for a few months now. Particularly on my M2 Mac mini - although it&amp;#39;s the base model with only 8GB of RAM. I&amp;#39;ve been using ollama since they provide an easy-to-use web interface to see the models, quickly download them, and run them, but also many other apps/clients for LLMs support it.&lt;/p&gt;\n\n&lt;p&gt;However, recently I&amp;#39;ve seen stuff like MLX-LM and llama-cpp (?) that are supposedly quicker than Ollama. Not too sure on the details, but I think I get a grasp, just that the models are architecturally different? &lt;/p&gt;\n\n&lt;p&gt;Anyways, I&amp;#39;d appreciate some help to get the most out of my low-end hardware? as I mentioned above I have that Mac, but also this laptop with 16GB of RAM and some crappy CPU (&amp;amp; integrated GPU).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111\"&gt;My laptop specs after running Neofetch on Nobara linux.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked around HuggingFace before, but found the UI very confusing lol. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mckrn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Junior-Ad-2186",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753816824,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there any good testing evidence or, barring that, do your anecdotal experiences show Qwen 3 Coder to actually be superior to DeepSeek R1 for agentic coding?\n\nAre we all just getting distracted by the shiny new thing? DeepSeek leads Qwen 3 Coder in the WebDev Arena Leaderboard, and it's got slightly cheaper pricing available from the providers on Open Router. The context window is smaller, sure, but other than that, is there any real reason to switch to Qwen 3 Coder?",
          "author_fullname": "t2_vw066zuw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 Coder vs. DeepSeek R1 0528 for Agentic Coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mckboq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753815815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any good testing evidence or, barring that, do your anecdotal experiences show Qwen 3 Coder to actually be superior to DeepSeek R1 for agentic coding?&lt;/p&gt;\n\n&lt;p&gt;Are we all just getting distracted by the shiny new thing? DeepSeek leads Qwen 3 Coder in the WebDev Arena Leaderboard, and it&amp;#39;s got slightly cheaper pricing available from the providers on Open Router. The context window is smaller, sure, but other than that, is there any real reason to switch to Qwen 3 Coder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mckboq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveDuck2382",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mckboq/qwen3_coder_vs_deepseek_r1_0528_for_agentic_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mckboq/qwen3_coder_vs_deepseek_r1_0528_for_agentic_coding/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753815815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We’re excited to share that 🥇NVIDIA Llama Nemotron Super 49B v1.5 -- our just released open reasoning model -- is #1 on the [Artificial Analysis Intelligence Index](https://nvda.ws/44TJw4n) \\- a leaderboard that spans advanced math, science, and agentic tasks, in the 70B open model category. \n\nSuper 49B v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. It delivers state-of-the-art accuracy and throughput, running on a single H100.\n\nKey features:\n\n🎯  Leading accuracy on multi-step reasoning, math, coding, and function-calling\n\n🏗️  Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples\n\n📊  Fully transparent training data and techniques\n\nIf you're building AI agents and want a high accuracy, fully-open, and transparent reasoning model that you can deploy anywhere, try Super v1.5 on [build.nvidia.com](http://build.nvidia.com) or download from [Hugging Face](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5) 🤗\n\nLeaderboard ➡️ [https://nvda.ws/44TJw4n](https://nvda.ws/44TJw4n)\n\nhttps://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;format=png&amp;auto=webp&amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816",
          "author_fullname": "t2_1vf7k06t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA Llama Nemotron Super v1.5 is #1 on Artificial Analysis Intelligence Index for the 70B Open Model Category.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "xd2gq1bs0vff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 44,
                  "x": 108,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e22ff11349f4e4b3dcb94e87d6d145cc729a836"
                },
                {
                  "y": 88,
                  "x": 216,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d728a0c03ce1b4539441a0f2c8b0aa52f86c8f3"
                },
                {
                  "y": 130,
                  "x": 320,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44d71750175ab38bd0f2c3afa10f26a492eb6d87"
                },
                {
                  "y": 261,
                  "x": 640,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3e72dc377756047505052364aacf55246c67629"
                },
                {
                  "y": 392,
                  "x": 960,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ffd6661e3507cc529cafe40a0e8f9e778dc8d9e"
                },
                {
                  "y": 441,
                  "x": 1080,
                  "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75f59b3bbcb7753f7223837403ce679a37343c80"
                }
              ],
              "s": {
                "y": 455,
                "x": 1114,
                "u": "https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;format=png&amp;auto=webp&amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816"
              },
              "id": "xd2gq1bs0vff1"
            }
          },
          "name": "t3_1mck6o7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=140&amp;height=77&amp;crop=140:77,smart&amp;auto=webp&amp;s=938c7c54c5afab9fd0496cba4e5d012b557db44d",
          "edited": 1753817962,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753815524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’re excited to share that 🥇NVIDIA Llama Nemotron Super 49B v1.5 -- our just released open reasoning model -- is #1 on the &lt;a href=\"https://nvda.ws/44TJw4n\"&gt;Artificial Analysis Intelligence Index&lt;/a&gt; - a leaderboard that spans advanced math, science, and agentic tasks, in the 70B open model category. &lt;/p&gt;\n\n&lt;p&gt;Super 49B v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. It delivers state-of-the-art accuracy and throughput, running on a single H100.&lt;/p&gt;\n\n&lt;p&gt;Key features:&lt;/p&gt;\n\n&lt;p&gt;🎯  Leading accuracy on multi-step reasoning, math, coding, and function-calling&lt;/p&gt;\n\n&lt;p&gt;🏗️  Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples&lt;/p&gt;\n\n&lt;p&gt;📊  Fully transparent training data and techniques&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re building AI agents and want a high accuracy, fully-open, and transparent reasoning model that you can deploy anywhere, try Super v1.5 on &lt;a href=\"http://build.nvidia.com\"&gt;build.nvidia.com&lt;/a&gt; or download from &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;Hugging Face&lt;/a&gt; 🤗&lt;/p&gt;\n\n&lt;p&gt;Leaderboard ➡️ &lt;a href=\"https://nvda.ws/44TJw4n\"&gt;https://nvda.ws/44TJw4n&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816\"&gt;https://preview.redd.it/xd2gq1bs0vff1.png?width=1114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99cadfabe70f77cbe1e2c2610ee0a6077df9f816&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mck6o7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PDXcoder2000",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mck6o7/nvidia_llama_nemotron_super_v15_is_1_on/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753815524,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI did not hit a plateau, at least in benchmarks. Pretty impressive with one year’s hindsight. Of course benchmarks aren’t everything. They aren’t nothing either.",
          "author_fullname": "t2_syq52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "One year’s benchmark progress: comparing Sonnet 3.5 with open weight 2025 non-thinking models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjz8j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=140&amp;height=77&amp;crop=140:77,smart&amp;auto=webp&amp;s=938c7c54c5afab9fd0496cba4e5d012b557db44d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753815058,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "artificialanalysis.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI did not hit a plateau, at least in benchmarks. Pretty impressive with one year’s hindsight. Of course benchmarks aren’t everything. They aren’t nothing either.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://artificialanalysis.ai/?models=llama-3-3-instruct-70b%2Cllama-4-maverick%2Cllama-4-scout%2Cgemma-3-27b%2Cdeepseek-v3-0324%2Ckimi-k2%2Cqwen3-235b-a22b-instruct-2507%2Cclaude-35-sonnet-june-24",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcjz8j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nomorebuttsplz",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjz8j/one_years_benchmark_progress_comparing_sonnet_35/",
          "stickied": false,
          "url": "https://artificialanalysis.ai/?models=llama-3-3-instruct-70b%2Cllama-4-maverick%2Cllama-4-scout%2Cgemma-3-27b%2Cdeepseek-v3-0324%2Ckimi-k2%2Cqwen3-235b-a22b-instruct-2507%2Cclaude-35-sonnet-june-24",
          "subreddit_subscribers": 506711,
          "created_utc": 1753815058,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8taublfu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open‑Source LLM Energy &amp; Carbon Cost Calculator",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 101,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjyp5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/6aae8kfvzuff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1492,
              "scrubber_media_url": "https://v.redd.it/6aae8kfvzuff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/6aae8kfvzuff1/DASHPlaylist.mpd?a=1756430004%2CNjIxOWU5OGY3MTI5M2MyODkzZTk2MjBlMzI0YTRjYmJmM2Y0MDNiMzA0YjkyMzA0MDY2MWZiN2IwYmM2YmVkOA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 12,
              "hls_url": "https://v.redd.it/6aae8kfvzuff1/HLSPlaylist.m3u8?a=1756430004%2CMjgzMGYxODY0OTIzMTBkYWUwODljOTE5MmJjZDQ4MGY2NzNkMTgwNjM4OWUyZDZhMTNiMDIwZTU5ZDAyNWRjNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=140&amp;height=101&amp;crop=140:101,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4f0d41587c6f2e0083efaa7966f4345747562a49",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753815026,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/6aae8kfvzuff1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?format=pjpg&amp;auto=webp&amp;s=a1e8172940a4efc30adbd4e329974472c70890a0",
                  "width": 1492,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ab394607fb916deaa92bd5845a0b55ffcbc5a6c2",
                    "width": 108,
                    "height": 78
                  },
                  {
                    "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fcb038268c247ac5d0a4ad79108b805534492bbb",
                    "width": 216,
                    "height": 156
                  },
                  {
                    "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a946c6bf60744edb825534d17fb7bec7aabddb74",
                    "width": 320,
                    "height": 231
                  },
                  {
                    "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eecf9c98f8bd3e4d52bab29136a21c5dd9780ca7",
                    "width": 640,
                    "height": 463
                  },
                  {
                    "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=af6e15d9f863e06c25396b8c967ac308863c4db7",
                    "width": 960,
                    "height": 694
                  },
                  {
                    "url": "https://external-preview.redd.it/bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0900f545923bf43263b844e7ea5162c307f0a394",
                    "width": 1080,
                    "height": 781
                  }
                ],
                "variants": {},
                "id": "bHpmaDNsZnZ6dWZmMU_VwO3uv0pLTdlJN2wQ0dX36-Fyl8RpQw9hOFp2ASWq"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mcjyp5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TerrificMist",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjyp5/opensource_llm_energy_carbon_cost_calculator/",
          "stickied": false,
          "url": "https://v.redd.it/6aae8kfvzuff1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753815026,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/6aae8kfvzuff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1492,
              "scrubber_media_url": "https://v.redd.it/6aae8kfvzuff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/6aae8kfvzuff1/DASHPlaylist.mpd?a=1756430004%2CNjIxOWU5OGY3MTI5M2MyODkzZTk2MjBlMzI0YTRjYmJmM2Y0MDNiMzA0YjkyMzA0MDY2MWZiN2IwYmM2YmVkOA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 12,
              "hls_url": "https://v.redd.it/6aae8kfvzuff1/HLSPlaylist.m3u8?a=1756430004%2CMjgzMGYxODY0OTIzMTBkYWUwODljOTE5MmJjZDQ4MGY2NzNkMTgwNjM4OWUyZDZhMTNiMDIwZTU5ZDAyNWRjNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My org is seeing a repetition issue on Maverick FP8 for a pretty standard RAG q&amp;a implementation. Certain questions consistently send it off the rails in repetition loops. We have used a number of other models with the same set up and have not experienced any issues, including Scout.\n\nHas anyone experienced something similar? Disappointing to see the llama 4 model fail like this on a simple use case.",
          "author_fullname": "t2_34g468z3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Maverick FP8 repetition issue",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjwmv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753814901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is seeing a repetition issue on Maverick FP8 for a pretty standard RAG q&amp;amp;a implementation. Certain questions consistently send it off the rails in repetition loops. We have used a number of other models with the same set up and have not experienced any issues, including Scout.&lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced something similar? Disappointing to see the llama 4 model fail like this on a simple use case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcjwmv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dangubiti",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjwmv/maverick_fp8_repetition_issue/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcjwmv/maverick_fp8_repetition_issue/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753814901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "C'est la première fois qu'un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n'est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !\n\nFor those who want my MCP flow, here’s the Pastebin:\n\n[https://pastebin.com/WNPrcjLS](https://pastebin.com/WNPrcjLS)\n\nhttps://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;format=png&amp;auto=webp&amp;s=30fca5c5a305810d2969af3035d710cef5a30268",
          "author_fullname": "t2_ti5m9mpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30b-3ab-2507 is a beast for MCP usage!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8kjwp8wkxuff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 136,
                  "x": 108,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9fe7d167fa67def68be16708c9072dff16e4b9"
                },
                {
                  "y": 273,
                  "x": 216,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d68578f7cbad05a254ecfd9b9a974bce181b73c4"
                },
                {
                  "y": 405,
                  "x": 320,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=572ae738bba26cff6b6dcf1e4969cc51201c065e"
                },
                {
                  "y": 811,
                  "x": 640,
                  "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f88d4f809869f8243befb328a5639859eeeccba"
                }
              ],
              "s": {
                "y": 1150,
                "x": 907,
                "u": "https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;format=png&amp;auto=webp&amp;s=30fca5c5a305810d2969af3035d710cef5a30268"
              },
              "id": "8kjwp8wkxuff1"
            }
          },
          "name": "t3_1mcji8s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 143,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 143,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Ku7pPJKnjoNSXHTx41JonGncgMhMPCUf8ZnqoDSjoGY.jpg",
          "edited": 1753816781,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753813999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;C&amp;#39;est la première fois qu&amp;#39;un modèle utilise intelligemment les serveurs MCP tout seul ! Ce n&amp;#39;est pas juste un ou deux serveurs et puis une réponse complètement à côté de la plaque !&lt;/p&gt;\n\n&lt;p&gt;For those who want my MCP flow, here’s the Pastebin:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/WNPrcjLS\"&gt;https://pastebin.com/WNPrcjLS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268\"&gt;https://preview.redd.it/8kjwp8wkxuff1.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30fca5c5a305810d2969af3035d710cef5a30268&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcji8s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Ninja7526",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcji8s/qwen330b3ab2507_is_a_beast_for_mcp_usage/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753813999,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nWe’ve been working on a desktop assistant app built using Tauri that runs entirely locally. No internet connection, no cloud calls, just fully self-hosted LLMs and audio/vision models.\n\nThe assistant passively listens and watches. It can “hear” what’s happening in meetings (Zoom, GMeet, Discord, etc.) and “see” what’s on your screen by tracking gaze and screen context. The idea is to act like a floating AI that you can summon at any time, without ever compromising privacy. \n\nWe’re currently pulling in multiple smaller AI models (Whisper, lightweight vision models, compact LLMs) to make it work well on consumer hardware.\n\nSome challenges we foresee\n• Porting the screen and audio capture features to macOS, especially dealing with sandboxing and permission models\n• iOS might be a stretch, but we’re open to ideas on how to architect toward it\n• Packaging and performance tuning across OSes without sacrificing the privacy-first, offline architecture\n\nWould love any feedback, advice, or to hear if anyone else is building similar thing with Rust, Tauri, and local AI models.",
          "author_fullname": "t2_m15q545s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama and Whisper AI Desktop Assistant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcjaau",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m8h6fxpxvuff1/DASHPlaylist.mpd?a=1756430004%2CNjg5M2NjNTM1MDJmYjY4Yjg1YmM0ZjgyZTExZDUzNTg2ZTIzNGY0NDQ3YmNmNDdiNmE4MWZjYzQ2ZDEzNjkzYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/m8h6fxpxvuff1/HLSPlaylist.m3u8?a=1756430004%2CZDBiMzQ0NmU2NGNlNjRiOWU0MGJkY2ZlYjM1Yjc3ZGFkNGFiMjRhNWRhNWMzMzQxMjMxYmI5NzA1NTcwYTMxMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=12dfeb3f43af814c740cb88dee1963a46e854783",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753813502,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;We’ve been working on a desktop assistant app built using Tauri that runs entirely locally. No internet connection, no cloud calls, just fully self-hosted LLMs and audio/vision models.&lt;/p&gt;\n\n&lt;p&gt;The assistant passively listens and watches. It can “hear” what’s happening in meetings (Zoom, GMeet, Discord, etc.) and “see” what’s on your screen by tracking gaze and screen context. The idea is to act like a floating AI that you can summon at any time, without ever compromising privacy. &lt;/p&gt;\n\n&lt;p&gt;We’re currently pulling in multiple smaller AI models (Whisper, lightweight vision models, compact LLMs) to make it work well on consumer hardware.&lt;/p&gt;\n\n&lt;p&gt;Some challenges we foresee\n• Porting the screen and audio capture features to macOS, especially dealing with sandboxing and permission models\n• iOS might be a stretch, but we’re open to ideas on how to architect toward it\n• Packaging and performance tuning across OSes without sacrificing the privacy-first, offline architecture&lt;/p&gt;\n\n&lt;p&gt;Would love any feedback, advice, or to hear if anyone else is building similar thing with Rust, Tauri, and local AI models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/m8h6fxpxvuff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?format=pjpg&amp;auto=webp&amp;s=9ac629a01c56b181990f5154bb7bf69ffafb59c9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c98aea69c8ee48f81b0de209189e241f3069ec8a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b87613792ed75990facdc4c2e81181c2d85435d6",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=36c7260614c5bb7d1185aedfdfc69616a212abde",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b732351aa485baaf25a410d68f6f03e6f7e02349",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=691220e045e714ec9e2d003a36737ee0883415da",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca6426d6e8f3342e9f7b582afe236c9fb84b673b",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "enl2aDBwanh2dWZmMRvkiiJiLOdYuD63n9hpi_HFNsNIPqOk9sj_Up3WfKRc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcjaau",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rxhxnsxngh",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcjaau/llama_and_whisper_ai_desktop_assistant/",
          "stickied": false,
          "url": "https://v.redd.it/m8h6fxpxvuff1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753813502,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/m8h6fxpxvuff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m8h6fxpxvuff1/DASHPlaylist.mpd?a=1756430004%2CNjg5M2NjNTM1MDJmYjY4Yjg1YmM0ZjgyZTExZDUzNTg2ZTIzNGY0NDQ3YmNmNDdiNmE4MWZjYzQ2ZDEzNjkzYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/m8h6fxpxvuff1/HLSPlaylist.m3u8?a=1756430004%2CZDBiMzQ0NmU2NGNlNjRiOWU0MGJkY2ZlYjM1Yjc3ZGFkNGFiMjRhNWRhNWMzMzQxMjMxYmI5NzA1NTcwYTMxMg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, I am looking to start self hosting llms for learning / experimenting and powering some projects. I am looking to learn different skills for building and deploying AI models and AI powered applications but I find the cloud a very unnerving place to do that. I was looking at making a self hosted setup for at most £600.\n\nIt would ideally let be dockerise and host an llm (I would like to do multi agent further on but that may be a problem for later). I am fine for the models themselves to be relatively basic (I am told it would be 7B at that price point what do you think?). I would also like to vectorise databases.\n\nI know very little on the hardware side of things so I would really appreciate it if people could share their thoughts on:\n\n1. Is all this possible at this pricepoint?\n2. If so what hardware specs will I need?\n3. If not how much will I need to spend and on what?\n\nThanks a lot for your time :)",
          "author_fullname": "t2_q8bmxb9d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Self hosting llm  on a budget",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcj1q1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753812977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am looking to start self hosting llms for learning / experimenting and powering some projects. I am looking to learn different skills for building and deploying AI models and AI powered applications but I find the cloud a very unnerving place to do that. I was looking at making a self hosted setup for at most £600.&lt;/p&gt;\n\n&lt;p&gt;It would ideally let be dockerise and host an llm (I would like to do multi agent further on but that may be a problem for later). I am fine for the models themselves to be relatively basic (I am told it would be 7B at that price point what do you think?). I would also like to vectorise databases.&lt;/p&gt;\n\n&lt;p&gt;I know very little on the hardware side of things so I would really appreciate it if people could share their thoughts on:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is all this possible at this pricepoint?&lt;/li&gt;\n&lt;li&gt;If so what hardware specs will I need?&lt;/li&gt;\n&lt;li&gt;If not how much will I need to spend and on what?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks a lot for your time :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcj1q1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistressedToaster",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcj1q1/self_hosting_llm_on_a_budget/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcj1q1/self_hosting_llm_on_a_budget/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753812977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an project where I have created an conversational RAG agent with tool calls. \nNow client want to have self hosted llm instead of OpenAI, gemini etc due to sensitive data. \n\nWhat a small model would be capable for this? Some 3-7 b models and where to host for speed and cost effectiveness. \nNot that the user based will not be big. Only 10-20 daily active users. ",
          "author_fullname": "t2_qrovnhqnh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a small model and hosting for conversational Agent.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mciotj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753812179,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an project where I have created an conversational RAG agent with tool calls. \nNow client want to have self hosted llm instead of OpenAI, gemini etc due to sensitive data. &lt;/p&gt;\n\n&lt;p&gt;What a small model would be capable for this? Some 3-7 b models and where to host for speed and cost effectiveness. \nNot that the user based will not be big. Only 10-20 daily active users. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mciotj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FireDojo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mciotj/looking_for_a_small_model_and_hosting_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mciotj/looking_for_a_small_model_and_hosting_for/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753812179,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Perhaps a silly question but I can't find an answer... How can I see what's the % of the model loaded via LM Studio running in the GPU?\n\nOllama ps gives a very simple response, for example 100% GPU. Is there an equivalent? (MacOS)",
          "author_fullname": "t2_133m0xy6vg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ollama ps in LM Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcilar",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753811977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Perhaps a silly question but I can&amp;#39;t find an answer... How can I see what&amp;#39;s the % of the model loaded via LM Studio running in the GPU?&lt;/p&gt;\n\n&lt;p&gt;Ollama ps gives a very simple response, for example 100% GPU. Is there an equivalent? (MacOS)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcilar",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Acrobatic_Cat_3448",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcilar/ollama_ps_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcilar/ollama_ps_in_lm_studio/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753811977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nI am a novice vibe coder. I was deeply interested in running a Bitnet model over the web. Thus I vibe coded a kernel and a conversion script for Bitnet 1.58 bit. \n\nThe example I used to give it a try was WebGPU_Chat (see examples folder)\n\nhttps://github.com/nimishchaudhari/bitnet_transformers.js/pull/1\n\nI am looking for reviews of people capable of understanding things under the hood, and looking for contributors as well for this purpose.  \n\nThanks in advance for your time and attention :)",
          "author_fullname": "t2_871guq2y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Review request on Bitnet implementation on transformer.js",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcif2t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753811588,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am a novice vibe coder. I was deeply interested in running a Bitnet model over the web. Thus I vibe coded a kernel and a conversion script for Bitnet 1.58 bit. &lt;/p&gt;\n\n&lt;p&gt;The example I used to give it a try was WebGPU_Chat (see examples folder)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/nimishchaudhari/bitnet_transformers.js/pull/1\"&gt;https://github.com/nimishchaudhari/bitnet_transformers.js/pull/1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am looking for reviews of people capable of understanding things under the hood, and looking for contributors as well for this purpose.  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your time and attention :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?auto=webp&amp;s=793ed225c6021b81349ae8efe47b5aa798da8f21",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=600a55db6af30e74eee873e1ac9f6fe21ae0f46d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1c0d4d82fc391c9e4971c922ffaf7f6c36bfd51",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff45b5974157505593476dbae1023f353acfdd17",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6549a80679aa33c8520c4dbc11379199a047e808",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae11b2ae1380b9267c9576de7fd6c6b2b7407f4e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87a7eb66e5a41099a27928c1190384e0f1df18fc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QKP5yGhNzgfc3KZ-GswZotmq6T3NbcgCKgb2zMqKL5M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcif2t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ScoreUnique",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcif2t/review_request_on_bitnet_implementation_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcif2t/review_request_on_bitnet_implementation_on/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753811588,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Newest Qwen made me cry. It's not perfect, but I still love it.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mci7uu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 349,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 349,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/h1Ejab1vdsRPCPX8mSRmFSTByiWOsDhozhB4Y9joLZQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753811149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is from the latest Qwen3-30B-A3B-Instruct-2507. ❤&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/gnkbnxzlouff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/gnkbnxzlouff1.png?auto=webp&amp;s=15ae6e9bfdd39878d13fcc40579f5f5914892497",
                  "width": 537,
                  "height": 204
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=387b92e0abf220fa87708b750e1cd04535c8d238",
                    "width": 108,
                    "height": 41
                  },
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3003e1ab3cfb059c6497615c2cb005b3643dcb9b",
                    "width": 216,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/gnkbnxzlouff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=431c53f32897af3a4225062d97bdc95913f53ec0",
                    "width": 320,
                    "height": 121
                  }
                ],
                "variants": {},
                "id": "IXr-dCvLuJL4cGd_YuCTQJMpv6LVqTDaGI7ZM0RkpWg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mci7uu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mci7uu/newest_qwen_made_me_cry_its_not_perfect_but_i/",
          "stickied": false,
          "url": "https://i.redd.it/gnkbnxzlouff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753811149,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The pace of open model drops this year is wild. GLM-4.5 yesterday was another big one.\n\nSay six months from now open weights give us everything we’ve wanted like long context, near-GPT-4 reasoning, multimodal that works, running on consumer GPUs. Then what?\n\nI keep coming back to the grid idea.. AI that’s real-time, always-on, not a “one-and-done” task bot. A local system that sees, hears, reacts instantly. Watching your dog while you’re away, spotting a Factorio bottleneck before you do, catching a runaway script before it kills your machine.\n\nWhere do we go once the brains get as big as their gonna get?",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "so.... what's next?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 74,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mci1dy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.32,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/8o8kQGARgInMQhKlxudQ52c_d--hspF3Iv5Rd0YF8YE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753810751,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The pace of open model drops this year is wild. GLM-4.5 yesterday was another big one.&lt;/p&gt;\n\n&lt;p&gt;Say six months from now open weights give us everything we’ve wanted like long context, near-GPT-4 reasoning, multimodal that works, running on consumer GPUs. Then what?&lt;/p&gt;\n\n&lt;p&gt;I keep coming back to the grid idea.. AI that’s real-time, always-on, not a “one-and-done” task bot. A local system that sees, hears, reacts instantly. Watching your dog while you’re away, spotting a Factorio bottleneck before you do, catching a runaway script before it kills your machine.&lt;/p&gt;\n\n&lt;p&gt;Where do we go once the brains get as big as their gonna get?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/c7o0g0tvmuff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/c7o0g0tvmuff1.png?auto=webp&amp;s=43cea7d310e122d4809cfddb1c17148706d9e832",
                  "width": 942,
                  "height": 498
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/c7o0g0tvmuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=59436e3f5c3ad84ab5507de98823646bd85ceb39",
                    "width": 108,
                    "height": 57
                  },
                  {
                    "url": "https://preview.redd.it/c7o0g0tvmuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0a8e6e698e4f7472e0bc85ecdc47aed68c6a5bd",
                    "width": 216,
                    "height": 114
                  },
                  {
                    "url": "https://preview.redd.it/c7o0g0tvmuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a57c66c6745e1cf2fd04976bd348e59b76dff4d3",
                    "width": 320,
                    "height": 169
                  },
                  {
                    "url": "https://preview.redd.it/c7o0g0tvmuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb2fb4fca2575bbe974bc5146ea5d68e76a1cd71",
                    "width": 640,
                    "height": 338
                  }
                ],
                "variants": {},
                "id": "f1ab2B65Xh8f3s_CPItgbWp_il8p9cKnCCUYb4AduR8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mci1dy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mci1dy/so_whats_next/",
          "stickied": false,
          "url": "https://i.redd.it/c7o0g0tvmuff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753810751,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Step 1. Get this [https://github.com/musistudio/claude-code-router](https://github.com/musistudio/claude-code-router) you get it up with 2 npm installs  \nStep 2. Create an openrouter account and top up 10 bucks or whatevs. Get API key.  \nStep 3. Put this in the JSON (look at the instructions from that repo: \\~/.claude-code-router/config.json )\n\n    {\n      \"LOG\": true,\n      \"API_TIMEOUT_MS\": 600000,\n      \"Providers\": [\n        {\n          \"name\": \"openrouter\",\n          \"api_base_url\": \"https://openrouter.ai/api/v1/chat/completions\",\n          \"api_key\": \"sk-or-v1-XXX\",\n          \"models\": [\"z-ai/glm-4.5\"],\n          \"transformer\": {\n            \"use\": [\"openrouter\"]\n          }\n        },\n      ],\n      \"Router\": {\n        \"default\": \"openrouter,z-ai/glm-4.5\",\n        \"background\": \"openrouter,z-ai/glm-4.5\",\n        \"think\": \"openrouter,z-ai/glm-4.5\",\n        \"longContext\": \"openrouter,z-ai/glm-4.5\",\n        \"longContextThreshold\": 60000,\n        \"webSearch\": \"openrouter,z-ai/glm-4.5\"\n      }\n    }\n\nStep 4. Ensure the 'server' restarts run 'ccr restart'  \nStep 5. Write \\`ccr code\\` and just enjoy.  \n\n\nCareful I burned 3$ with just one agentic query that took 10 minutes and it was still thinking. I'm going to try more with Qwen3 235B and experiment.   \n  \nGLM 4.5 is pretty smart. ",
          "author_fullname": "t2_5uhcd48d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[tutorial] Use GLM 4.5 (or any LLM) with Claude Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mchsyd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753810236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Step 1. Get this &lt;a href=\"https://github.com/musistudio/claude-code-router\"&gt;https://github.com/musistudio/claude-code-router&lt;/a&gt; you get it up with 2 npm installs&lt;br/&gt;\nStep 2. Create an openrouter account and top up 10 bucks or whatevs. Get API key.&lt;br/&gt;\nStep 3. Put this in the JSON (look at the instructions from that repo: ~/.claude-code-router/config.json )&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;LOG&amp;quot;: true,\n  &amp;quot;API_TIMEOUT_MS&amp;quot;: 600000,\n  &amp;quot;Providers&amp;quot;: [\n    {\n      &amp;quot;name&amp;quot;: &amp;quot;openrouter&amp;quot;,\n      &amp;quot;api_base_url&amp;quot;: &amp;quot;https://openrouter.ai/api/v1/chat/completions&amp;quot;,\n      &amp;quot;api_key&amp;quot;: &amp;quot;sk-or-v1-XXX&amp;quot;,\n      &amp;quot;models&amp;quot;: [&amp;quot;z-ai/glm-4.5&amp;quot;],\n      &amp;quot;transformer&amp;quot;: {\n        &amp;quot;use&amp;quot;: [&amp;quot;openrouter&amp;quot;]\n      }\n    },\n  ],\n  &amp;quot;Router&amp;quot;: {\n    &amp;quot;default&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;background&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;think&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;longContext&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;,\n    &amp;quot;longContextThreshold&amp;quot;: 60000,\n    &amp;quot;webSearch&amp;quot;: &amp;quot;openrouter,z-ai/glm-4.5&amp;quot;\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Step 4. Ensure the &amp;#39;server&amp;#39; restarts run &amp;#39;ccr restart&amp;#39;&lt;br/&gt;\nStep 5. Write `ccr code` and just enjoy.  &lt;/p&gt;\n\n&lt;p&gt;Careful I burned 3$ with just one agentic query that took 10 minutes and it was still thinking. I&amp;#39;m going to try more with Qwen3 235B and experiment.   &lt;/p&gt;\n\n&lt;p&gt;GLM 4.5 is pretty smart. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?auto=webp&amp;s=3a11f7f63e1f2873b93bc27996fab03230f20930",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4da698ce554ade05a84efd4b2e0d6d6a7887ce47",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=105509ddb23e88f678f15119c0cce845e41c1420",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be97d450eaaa2c73ea1487f634f1598cf7d41a43",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bede4133f64e0bf5c8a4f1484a743c8f1bdaecb2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4933006a6eaa89bed20c498604e2dccecdb8f2f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=27e51736d38916346e5107369e89f9e0e8336755",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "JYjCGYPZYDt_YEePBesOtpP36c44U1gevBuQXb40rAc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mchsyd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaman-warrior",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mchsyd/tutorial_use_glm_45_or_any_llm_with_claude_code/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753810236,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,  \nRecently I've been struggling with finding a MCP server so i can give it a YouTube video then it gives me its transcription.  \nI’ve tried a few popular ones listed on Smithery and even tried setting one up myself and deployed it using GCP/GCP CLI, but I haven’t had any luck getting it to work. (the smithery ones only give me the summary of the videos)\n\ncan anyone help me out here?",
          "author_fullname": "t2_13b0vodjkd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What MCP server do you use to get YouTube video transcription (I'm tired of failing)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mchmfa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753809844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;br/&gt;\nRecently I&amp;#39;ve been struggling with finding a MCP server so i can give it a YouTube video then it gives me its transcription.&lt;br/&gt;\nI’ve tried a few popular ones listed on Smithery and even tried setting one up myself and deployed it using GCP/GCP CLI, but I haven’t had any luck getting it to work. (the smithery ones only give me the summary of the videos)&lt;/p&gt;\n\n&lt;p&gt;can anyone help me out here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mchmfa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "toolhouseai",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mchmfa/what_mcp_server_do_you_use_to_get_youtube_video/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mchmfa/what_mcp_server_do_you_use_to_get_youtube_video/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753809844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Interesting small model, hadn't seen it before.\n\n[https://huggingface.co/arcee-ai/AFM-4.5B-GGUF](https://huggingface.co/arcee-ai/AFM-4.5B-GGUF)",
          "author_fullname": "t2_nqso9ejpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AFM 4.5B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mchj7h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 52,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 52,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Gg3bSE-QKrcK_PJGFfGMYVWRRjLvwPBdoVOh_DIFWxI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753809654,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Interesting small model, hadn&amp;#39;t seen it before.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B-GGUF\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/c7yvmvdgkuff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/c7yvmvdgkuff1.png?auto=webp&amp;s=e11f8816200af5c879bcb6774235ced1dfe26ebd",
                  "width": 1024,
                  "height": 768
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa5c457775d6739f275247e49c5c1d3b2126224e",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9fc56c924cc71910d8dd4b9859ca93a47c55c8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=915a2c0ed88c41913030f35d8dc1c5114795ae72",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a68967cc776ececd5151071c32eb068a2fd1ddad",
                    "width": 640,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/c7yvmvdgkuff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=123bf071ad15e80af83deb3393e7921b247e2961",
                    "width": 960,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "v6HauKnEB-_4Fc_3AuB1hm4zMhsHVdeYRhsHvTww7e0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mchj7h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "best_codes",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mchj7h/afm_45b/",
          "stickied": false,
          "url": "https://i.redd.it/c7yvmvdgkuff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753809654,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have three setups of dual cpu on same motherboard\n\ndual intel xeon 6140 with pcie 4.0 1350$ supermicro x11dpl-i\n\ndual amd epyc 7551 with pcie 3.0 1640$ H11DSi-NT rev1.01\n\ndual amd epyc 7532 with pcie 4.0 2500$ H11DSi-NT rev2\n\nall of these will ship with different supermicro motherboard, case with two PSU and ddr4 256gb. I also planning to buy at least one 3090.\n\nI am planning to run Qwen3 255b a22b 2507 q4\n\nI'm not sure what to expect from two cpu setups and pcie 3.0 and want to avoid buying garbage and save some money if possible. I expect at least 5 token per second. Can you please help me with the setup.",
          "author_fullname": "t2_u22ezt8o1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dual CPU setup for the Qwen3 255b a22b 2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mca20c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753792539,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753792028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have three setups of dual cpu on same motherboard&lt;/p&gt;\n\n&lt;p&gt;dual intel xeon 6140 with pcie 4.0 1350$ supermicro x11dpl-i&lt;/p&gt;\n\n&lt;p&gt;dual amd epyc 7551 with pcie 3.0 1640$ H11DSi-NT rev1.01&lt;/p&gt;\n\n&lt;p&gt;dual amd epyc 7532 with pcie 4.0 2500$ H11DSi-NT rev2&lt;/p&gt;\n\n&lt;p&gt;all of these will ship with different supermicro motherboard, case with two PSU and ddr4 256gb. I also planning to buy at least one 3090.&lt;/p&gt;\n\n&lt;p&gt;I am planning to run Qwen3 255b a22b 2507 q4&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what to expect from two cpu setups and pcie 3.0 and want to avoid buying garbage and save some money if possible. I expect at least 5 token per second. Can you please help me with the setup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mca20c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frosty_Incident_9788",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mca20c/dual_cpu_setup_for_the_qwen3_255b_a22b_2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mca20c/dual_cpu_setup_for_the_qwen3_255b_a22b_2507/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753792028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🤣 i have tons of llama car air freshener",
          "author_fullname": "t2_1lil9j2g3r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No stress",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcgpno",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Fv_xaJXWwqYF4ITm8wg3a9VDDpidmhH4hn8HQR6zZL8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753807879,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🤣 i have tons of llama car air freshener&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/trr3maw8fuff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?auto=webp&amp;s=3dc8da55e2e040a1b5b2e905cb114d7c230cca0d",
                  "width": 1816,
                  "height": 4032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed7d794028ad2a37a933f6e62c1a983457bdab7e",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e34d42b5fea6014c3a2dfc1e2a5a7e624da7668",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b89643d5b8a888f5acc502a7d80c285ff8fb130",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85dcfc840382afcd2ff1941abae20da484423254",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df8f2f5309f545555a1a77c16ae0c7aebfe6f7eb",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/trr3maw8fuff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47f1ac2e546fa7f0e0abab100e65a58d0859c9ac",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "vv6tjmWKvcypi4-H9Cnqxzil-xYGEYr6vmbyGhqD8Zo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcgpno",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "troughtspace",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcgpno/no_stress/",
          "stickied": false,
          "url": "https://i.redd.it/trr3maw8fuff1.jpeg",
          "subreddit_subscribers": 506711,
          "created_utc": 1753807879,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://openai.com/form/codex-open-source-fund/](https://openai.com/form/codex-open-source-fund/)\n\nAnyone here want to share their experience with this program? How have you used this opportunity, if at all? I just applied and plan to use the credits for Codex CLI use, and to spinoff a commercial or \"on-site with paid support\" version of my open-source project.\n\nNote: To keep this on-focus, let's not get into \"China great\" and \"OpenAI bad\" rants, there are many other posts you can make those on, unless you actually lead an open-source project and have something intelligent to say.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any experiences with the Codex Open-Source Fund?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcgguo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": "transparent",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/zVwXE3yLHK32JDpk_E7-hzArJ4TYq0sGgF99Nlj1OG4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753807347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://openai.com/form/codex-open-source-fund/\"&gt;https://openai.com/form/codex-open-source-fund/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Anyone here want to share their experience with this program? How have you used this opportunity, if at all? I just applied and plan to use the credits for Codex CLI use, and to spinoff a commercial or &amp;quot;on-site with paid support&amp;quot; version of my open-source project.&lt;/p&gt;\n\n&lt;p&gt;Note: To keep this on-focus, let&amp;#39;s not get into &amp;quot;China great&amp;quot; and &amp;quot;OpenAI bad&amp;quot; rants, there are many other posts you can make those on, unless you actually lead an open-source project and have something intelligent to say.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hz89a64ucuff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hz89a64ucuff1.png?auto=webp&amp;s=78496167a017d562b3b2041bea51ffc5a9932075",
                  "width": 986,
                  "height": 662
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=73f310a67ca925520f16f0256a224904ca8c7d6c",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=54d59e31279a2716425aa92b4591192696adb2c4",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=056918e41a404be8f9661a02d5c38c585cd071ae",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b99975305c88e249e646c62495bd20a4ace6c4b3",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/hz89a64ucuff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=919c2b80bba37b2650a4e76a6f5a1cbcfd9ea56e",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "CoQVi8FlGJEds_5tyj9riZXQKOEi10exAtI_J3lQauM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mcgguo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mcgguo/any_experiences_with_the_codex_opensource_fund/",
          "stickied": false,
          "url": "https://i.redd.it/hz89a64ucuff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753807347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\[***PLEASE READ BEFORE ANSWERING TO PREVENT IRRELEVANT SUGGESTIONS FOR ME***.\\]  \nI'm looking to improve my workflow on Linux and am searching for a specific type of speech-to-text application to run locally on my laptop.\n\nMy requirements are:\n\n* **100% Local &amp; Offline:** All audio processing must happen on my own machine.\n* **High Accuracy:** Quality should be on par with a good Whisper model. I'm not interested in older models like VOSK, as their accuracy doesn't meet my needs.\n* **Key Use Cases:** My main goals are to dictate notes directly into my \"Second Brain\" style notes app and to send longer prompts to Large Language Model interfaces without breaking my flow.\n* **System-Wide Integration:** This is the most crucial part. I want to press a hotkey and dictate directly into *any* active application (my code editor, a browser, a document, etc.).\n\nFor context, I use **Speechnotes** all the time because it supports models like `tiny-faster-whisper`, which is very fast and works perfectly for my use case. The problem is purely its workflow—having to transcribe in one window and then constantly copy-paste the text is exactly the process I want to eliminate.\n\nMy goal is to find a seamless solution that works like native OS dictation but is powered by modern, local models. Many Whisper UIs I've found are excellent but seem to have the same limitation. The paid options are too expensive for what they are, which is why I'm focused on finding a great offline version.\n\nDoes a tool like this exist for Linux? What are you all using to achieve this kind of workflow?\n\nThanks for any help!",
          "author_fullname": "t2_98pip3bs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seeking a Local/Offline Speech-to-Text with System-Wide 'Type Anywhere' Dictation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcgfnh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753807273,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[&lt;strong&gt;&lt;em&gt;PLEASE READ BEFORE ANSWERING TO PREVENT IRRELEVANT SUGGESTIONS FOR ME&lt;/em&gt;&lt;/strong&gt;.]&lt;br/&gt;\nI&amp;#39;m looking to improve my workflow on Linux and am searching for a specific type of speech-to-text application to run locally on my laptop.&lt;/p&gt;\n\n&lt;p&gt;My requirements are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Offline:&lt;/strong&gt; All audio processing must happen on my own machine.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High Accuracy:&lt;/strong&gt; Quality should be on par with a good Whisper model. I&amp;#39;m not interested in older models like VOSK, as their accuracy doesn&amp;#39;t meet my needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Key Use Cases:&lt;/strong&gt; My main goals are to dictate notes directly into my &amp;quot;Second Brain&amp;quot; style notes app and to send longer prompts to Large Language Model interfaces without breaking my flow.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;System-Wide Integration:&lt;/strong&gt; This is the most crucial part. I want to press a hotkey and dictate directly into &lt;em&gt;any&lt;/em&gt; active application (my code editor, a browser, a document, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For context, I use &lt;strong&gt;Speechnotes&lt;/strong&gt; all the time because it supports models like &lt;code&gt;tiny-faster-whisper&lt;/code&gt;, which is very fast and works perfectly for my use case. The problem is purely its workflow—having to transcribe in one window and then constantly copy-paste the text is exactly the process I want to eliminate.&lt;/p&gt;\n\n&lt;p&gt;My goal is to find a seamless solution that works like native OS dictation but is powered by modern, local models. Many Whisper UIs I&amp;#39;ve found are excellent but seem to have the same limitation. The paid options are too expensive for what they are, which is why I&amp;#39;m focused on finding a great offline version.&lt;/p&gt;\n\n&lt;p&gt;Does a tool like this exist for Linux? What are you all using to achieve this kind of workflow?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcgfnh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bilalazhar72",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcgfnh/seeking_a_localoffline_speechtotext_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcgfnh/seeking_a_localoffline_speechtotext_with/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753807273,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.\n\n✨ Key Enhancements:\n\n✅ Enhanced reasoning, coding, and math skills\n\n✅ Broader multilingual knowledge\n\n✅ Improved long-context understanding (up to 256K tokens)\n\n✅ Better alignment with user intent and open-ended tasks\n\n✅ No more &lt;think&gt; blocks — now operating exclusively in non-thinking mode\n\n🔧 With 3B activated parameters, it's approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking\n\nHugging Face: https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\n\nQwen Chat: https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507\n\nModel scope: https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary\n\n\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-30B-A3B Small Update",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcg4qt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 227,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 227,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/O7xcYRkNGuBB0yBDfOkkcWs5DpYysQdkMKJvvFlOYpA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753806599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B Small Update: Smarter, faster, and local deployment-friendly.&lt;/p&gt;\n\n&lt;p&gt;✨ Key Enhancements:&lt;/p&gt;\n\n&lt;p&gt;✅ Enhanced reasoning, coding, and math skills&lt;/p&gt;\n\n&lt;p&gt;✅ Broader multilingual knowledge&lt;/p&gt;\n\n&lt;p&gt;✅ Improved long-context understanding (up to 256K tokens)&lt;/p&gt;\n\n&lt;p&gt;✅ Better alignment with user intent and open-ended tasks&lt;/p&gt;\n\n&lt;p&gt;✅ No more &amp;lt;think&amp;gt; blocks — now operating exclusively in non-thinking mode&lt;/p&gt;\n\n&lt;p&gt;🔧 With 3B activated parameters, it&amp;#39;s approaching the performance of GPT-4o and Qwen3-235B-A22B Non-Thinking&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: &lt;a href=\"https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507\"&gt;https://chat.qwen.ai/?model=Qwen3-30B-A3B-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Model scope: &lt;a href=\"https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary\"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507/summary&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nd904g7gbuff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?auto=webp&amp;s=e65b518bfd8179ffe5850438ba9b1ea0fdbad33f",
                  "width": 900,
                  "height": 506
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f840db78bf1bdfd3bc2fbe2fce643b2615c41103",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d867b87e626895bab9aa6038ad0daeda82c3412b",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bb498d8e823f04e6a1a8de9d73a55aa81af09d7",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/nd904g7gbuff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b713bd1bbe154007dd6c0b8474098b47bf58ba4d",
                    "width": 640,
                    "height": 359
                  }
                ],
                "variants": {},
                "id": "G3G0aRr753pT7ZAwSw8VgFtbVKOybrXjQhhRczqdvYg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcg4qt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcg4qt/qwen330ba3b_small_update/",
          "stickied": false,
          "url": "https://i.redd.it/nd904g7gbuff1.jpeg",
          "subreddit_subscribers": 506711,
          "created_utc": 1753806599,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new qwen moe!",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcfuka",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 111,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 111,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753805955,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new qwen moe!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcfuka",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcfuka/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 506711,
          "created_utc": 1753805955,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-30B-A3B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcfmd2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 523,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 523,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=50aa20219586bc9007fb96833d16a6a56c8c1c76",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753805463,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?auto=webp&amp;s=f1df54937600c0db76989bd14eef9e747df1fb0e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1c3476d621a9393fbb7ca11c48a3074c5fd6803",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7cef70bde41dd3225eec3f7d265fbf2704c0182",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab3e2615c90a6581b60c6d33c660bfc0f250b4c8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9c2fc1f960e47499df06dc08d78c88be43e15e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15eba021f7d99140c48583ae883d2eb091807f16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mcfmd2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 229,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcfmd2/qwenqwen330ba3binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507",
          "subreddit_subscribers": 506711,
          "created_utc": 1753805463,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm trying to optimize running larger MoE models like Qwen3-30B-A3B on a low-VRAM setup (4GB GPU) by using intelligent/manual offloading.\n\nThe goal is to keep the most relevant experts for a specific task (e.g., coding) permanently in VRAM for better performance, while offloading the less used ones to the CPU/RAM.\n\nThis obviously requires knowing which expert ID corresponds to which specialized function. Has anyone already done the legwork of profiling the model? For example, by feeding it pure code vs. pure prose and logging the expert activation frequency with tools like llama.cpp?\n\nI'm looking for any kind of data.",
          "author_fullname": "t2_mhb0rkd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone profiled the expert specialization in MoE models like Qwen3-30B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mceq8m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753803457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to optimize running larger MoE models like Qwen3-30B-A3B on a low-VRAM setup (4GB GPU) by using intelligent/manual offloading.&lt;/p&gt;\n\n&lt;p&gt;The goal is to keep the most relevant experts for a specific task (e.g., coding) permanently in VRAM for better performance, while offloading the less used ones to the CPU/RAM.&lt;/p&gt;\n\n&lt;p&gt;This obviously requires knowing which expert ID corresponds to which specialized function. Has anyone already done the legwork of profiling the model? For example, by feeding it pure code vs. pure prose and logging the expert activation frequency with tools like llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for any kind of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mceq8m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Eden63",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753803457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can anyone recommend an AI tool/model/prompt, preferably one that can be run locally (via Ollama) that can evaluate a Zoom video export (MP4) to provide feedback on the tone and mood derived from both body language and spoken content?\n\nThank you!",
          "author_fullname": "t2_f5o0d6q8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI tool/model/prompt (preferably local and free) that can evaluate video meeting content and provide feedback on tone, mood, body language?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcemfm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753803217,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone recommend an AI tool/model/prompt, preferably one that can be run locally (via Ollama) that can evaluate a Zoom video export (MP4) to provide feedback on the tone and mood derived from both body language and spoken content?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcemfm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hour-Key-72",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcemfm/ai_toolmodelprompt_preferably_local_and_free_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcemfm/ai_toolmodelprompt_preferably_local_and_free_that/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753803217,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ngleu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My 2.5 year old laptop can write Space Invaders in JavaScript now, using GLM-4.5 Air and MLX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcee42",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=a8e41202ffb75be375da41f6bfdbca3278dea746",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753802702,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "simonwillison.net",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://simonwillison.net/2025/Jul/29/space-invaders/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?auto=webp&amp;s=54f803941465224fd8246998bb39528e75d01d3b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=753886c5d0207835fbf0f072aecec4105a84e5d1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=40261c69d770aed6a72ee6f270cb5258a7986d06",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=818441e6caa4231812fb4c09059db655bfc604c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a5a9e25e0e831120dffb4dbb77fc7392c4ccb49",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a2d07966fcc841d9190c47df7f060e4049f6d3e7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=253492f4406476c783cec6d34932373aaa520a62",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "1VNPpNFrBqOXKfi0GQuVGDd98w0RUZxnUoHJW87Blgw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mcee42",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChiliPepperHott",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcee42/my_25_year_old_laptop_can_write_space_invaders_in/",
          "stickied": false,
          "url": "https://simonwillison.net/2025/Jul/29/space-invaders/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753802702,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tested for same, is it trained on gemini outputs ?",
          "user_reports": [],
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "zai-org/GLM-4.5 · We Have Gemini At Home",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mce9tt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "",
          "subreddit_type": "public",
          "ups": 76,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 76,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "thumbnail": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b9c4a19e8f83b487aad0808ff3268748aeb22c0b",
          "edited": false,
          "author_flair_css_class": null,
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753802433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tested for same, is it trained on gemini outputs ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/zai-org/GLM-4.5/discussions/1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?auto=webp&amp;s=95e6e3db54b78256b6c4a4e615f3f8e17a4d8da0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8d403a9e16d065e5baf97dd10b29a9718f1fc4e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dc8994dbbb506cc7c4e6ca5bb072334ac0944a2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f0fce0796d6b32266a6dd062f3fbaeb365e87a0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=403fb88f398e6c5fe36b4f1c95408e0675027e55",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2daf8dee06ac50ac6077ee82bd888ae1b6f3855",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4766f576b9c23f9aefebc1aee1a39b8c0f31aabb",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "xSaCw6eC5YFUiGkblkdEOYZRWTkFaIHY9MbT-F5Hjdw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mce9tt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "[deleted]",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mce9tt/zaiorgglm45_we_have_gemini_at_home/",
          "stickied": false,
          "url": "https://huggingface.co/zai-org/GLM-4.5/discussions/1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753802433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's great to see open models continuing to advance. I believe most people in this community would agree that there's often a significant gap between benchmark scores and real-world performance. With that in mind, I've put together some candid thoughts on several open models from an end-user's perspective.\n\n**GLM-4.5**: I find it exceptionally good for everyday use. There's a clear distinction from previous LLMs that would excessively praise users or show off with markdown tables. I noticed some quirks in its reasoning similar to Deepseek R1, but nothing problematic. Personally, I recommend using it through [chat.z.ai](http://chat.z.ai), which offers an excellent UI/UX experience.\n\n**Kimi K2**: I found it to perform excellently at both coding tasks and creative work. However, it's noticeably slow with prominent rate limiting even when accessed through Openrouter. The fact that its app and website only support Chinese is a significant downside for international users.\n\n**Qwen3 Coder**: While I've heard it benchmarks better than Kimi K2, my actual experience was quite disappointing. It warrants further testing, though it does offer a larger context window than Kimi K2, which is commendable.\n\n**Qwen3 235B A22B Instruct 2507**: I also get the sense that its benchmarks are inflated, but it's actually quite decent. It has a noticeably \"LLM-like\" quality to its responses, which might make it less ideal for creative endeavors.\n\n**Qwen3 235B A22B Thinking 2507**: Its large thinking budget is advantageous, but this can backfire, sometimes resulting in excessively long response times. For now, I find Deepseek R1-0528 more practical to use.\n\n**Deepseek R1-0528**: This one needs no introduction - it proves to be quite versatile, high-performing, and user-friendly. Among Openrouter's free models, it offers the most stable inference, and the API provides excellent value for money (the official API has discounted periods that can save you up to 70%).",
          "author_fullname": "t2_1skliabt9v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My Honest Take on Recently Popular Open Models (A Realistic Assessment)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mce934",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753802383,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s great to see open models continuing to advance. I believe most people in this community would agree that there&amp;#39;s often a significant gap between benchmark scores and real-world performance. With that in mind, I&amp;#39;ve put together some candid thoughts on several open models from an end-user&amp;#39;s perspective.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GLM-4.5&lt;/strong&gt;: I find it exceptionally good for everyday use. There&amp;#39;s a clear distinction from previous LLMs that would excessively praise users or show off with markdown tables. I noticed some quirks in its reasoning similar to Deepseek R1, but nothing problematic. Personally, I recommend using it through &lt;a href=\"http://chat.z.ai\"&gt;chat.z.ai&lt;/a&gt;, which offers an excellent UI/UX experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;: I found it to perform excellently at both coding tasks and creative work. However, it&amp;#39;s noticeably slow with prominent rate limiting even when accessed through Openrouter. The fact that its app and website only support Chinese is a significant downside for international users.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 Coder&lt;/strong&gt;: While I&amp;#39;ve heard it benchmarks better than Kimi K2, my actual experience was quite disappointing. It warrants further testing, though it does offer a larger context window than Kimi K2, which is commendable.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B Instruct 2507&lt;/strong&gt;: I also get the sense that its benchmarks are inflated, but it&amp;#39;s actually quite decent. It has a noticeably &amp;quot;LLM-like&amp;quot; quality to its responses, which might make it less ideal for creative endeavors.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B Thinking 2507&lt;/strong&gt;: Its large thinking budget is advantageous, but this can backfire, sometimes resulting in excessively long response times. For now, I find Deepseek R1-0528 more practical to use.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Deepseek R1-0528&lt;/strong&gt;: This one needs no introduction - it proves to be quite versatile, high-performing, and user-friendly. Among Openrouter&amp;#39;s free models, it offers the most stable inference, and the API provides excellent value for money (the official API has discounted periods that can save you up to 70%).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mce934",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Technology_3421",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mce934/my_honest_take_on_recently_popular_open_models_a/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753802383,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been exploring AWS Strands Agents recently, it's their open-source SDK for building AI agents with proper tool use, reasoning loops, and support for LLMs from OpenAI, Anthropic, Bedrock, LiteLLM Ollama, etc.\n\nAt first glance, I thought it’d be AWS-only and super vendor-locked. But turns out it’s fairly modular and works with local models too.\n\nThe core idea is simple: you define an agent by combining\n\n* an LLM,\n* a prompt or task,\n* and a list of tools it can use.\n\nThe agent follows a loop: read the goal → plan → pick tools → execute → update → repeat. Think of it like a built-in agentic framework that handles planning and tool use internally.\n\nTo try it out, I built a small working agent from scratch:\n\n* Used DeepSeek v3 as the model\n* Added a simple tool that fetches weather data\n* Set up the flow where the agent takes a task like “Should I go for a run today?” → checks the weather → gives a response\n\nThe SDK handled tool routing and output formatting way better than I expected. No LangChain or CrewAI needed.\n\nIf anyone wants to try it out or see how it works in action, I documented the whole thing in a short video here: [video](https://www.youtube.com/watch?v=9ryQ4Nb32zk)\n\nAlso shared the code on GitHub for anyone who wants to fork or tweak it: [Repo link](https://github.com/Arindam200/awesome-ai-apps/tree/main/starter_ai_agents/aws_strands_starter)\n\nWould love to know what you're building with it!",
          "author_fullname": "t2_vnmiyiza",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beginner-Friendly Guide to AWS Strands Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mce901",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753802378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been exploring AWS Strands Agents recently, it&amp;#39;s their open-source SDK for building AI agents with proper tool use, reasoning loops, and support for LLMs from OpenAI, Anthropic, Bedrock, LiteLLM Ollama, etc.&lt;/p&gt;\n\n&lt;p&gt;At first glance, I thought it’d be AWS-only and super vendor-locked. But turns out it’s fairly modular and works with local models too.&lt;/p&gt;\n\n&lt;p&gt;The core idea is simple: you define an agent by combining&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;an LLM,&lt;/li&gt;\n&lt;li&gt;a prompt or task,&lt;/li&gt;\n&lt;li&gt;and a list of tools it can use.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The agent follows a loop: read the goal → plan → pick tools → execute → update → repeat. Think of it like a built-in agentic framework that handles planning and tool use internally.&lt;/p&gt;\n\n&lt;p&gt;To try it out, I built a small working agent from scratch:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Used DeepSeek v3 as the model&lt;/li&gt;\n&lt;li&gt;Added a simple tool that fetches weather data&lt;/li&gt;\n&lt;li&gt;Set up the flow where the agent takes a task like “Should I go for a run today?” → checks the weather → gives a response&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The SDK handled tool routing and output formatting way better than I expected. No LangChain or CrewAI needed.&lt;/p&gt;\n\n&lt;p&gt;If anyone wants to try it out or see how it works in action, I documented the whole thing in a short video here: &lt;a href=\"https://www.youtube.com/watch?v=9ryQ4Nb32zk\"&gt;video&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also shared the code on GitHub for anyone who wants to fork or tweak it: &lt;a href=\"https://github.com/Arindam200/awesome-ai-apps/tree/main/starter_ai_agents/aws_strands_starter\"&gt;Repo link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to know what you&amp;#39;re building with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fRIE2Iuk1Rky55BFKp-1kblH3vqTDUkr_mtL3V7pdhI.jpeg?auto=webp&amp;s=bb9a583c24f4fa348175d4de1e2de9f05c6fac88",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fRIE2Iuk1Rky55BFKp-1kblH3vqTDUkr_mtL3V7pdhI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d0bf105c7daac613a3fcaef5e03edeef2fcb277",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/fRIE2Iuk1Rky55BFKp-1kblH3vqTDUkr_mtL3V7pdhI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f3dd2e1758639f7e84d311c92ff5be1a84ae94f",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/fRIE2Iuk1Rky55BFKp-1kblH3vqTDUkr_mtL3V7pdhI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cbe0459d29a2c6ebdcde3426f89d49486416631",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "fRIE2Iuk1Rky55BFKp-1kblH3vqTDUkr_mtL3V7pdhI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mce901",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Arindam_200",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mce901/beginnerfriendly_guide_to_aws_strands_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mce901/beginnerfriendly_guide_to_aws_strands_agents/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753802378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a teen working on an AI project. For the sake of readability I am not going to get into the details of why I am making this, but I would call this project and what motivated it explain- and understandable. It involves a website targeted at seniors with the following functions:\n\n\\- a section scroll-down presentation/slideshow explaining how LLMs work\n\n\\- an anonymous chat with llama integration\n\nI want it to be a resource to learn about LLMs and an alternative for cloud AI to handle simple tasks. Does it have real world application and how could I make it better?",
          "author_fullname": "t2_dtlmml2y3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rate my project!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mce7wo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753802304,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a teen working on an AI project. For the sake of readability I am not going to get into the details of why I am making this, but I would call this project and what motivated it explain- and understandable. It involves a website targeted at seniors with the following functions:&lt;/p&gt;\n\n&lt;p&gt;- a section scroll-down presentation/slideshow explaining how LLMs work&lt;/p&gt;\n\n&lt;p&gt;- an anonymous chat with llama integration&lt;/p&gt;\n\n&lt;p&gt;I want it to be a resource to learn about LLMs and an alternative for cloud AI to handle simple tasks. Does it have real world application and how could I make it better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mce7wo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CeptiVimita",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mce7wo/rate_my_project/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mce7wo/rate_my_project/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753802304,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just want to deploy some small model like medgemma 4b or qwen3 4b to be called by my own app, I can't find any service serve these model (medgemma, qwen3 4b I see on openrouter) but I don't want to rent out 24gb vram instance for 200 USD a month to just serve  4gb model for my MVP app that don't have DAU yet, what is my solution? (I have RTX 5060Ti 16gb already)",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can I deploy model to served my own web app using my own machine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcdypn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753801714,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just want to deploy some small model like medgemma 4b or qwen3 4b to be called by my own app, I can&amp;#39;t find any service serve these model (medgemma, qwen3 4b I see on openrouter) but I don&amp;#39;t want to rent out 24gb vram instance for 200 USD a month to just serve  4gb model for my MVP app that don&amp;#39;t have DAU yet, what is my solution? (I have RTX 5060Ti 16gb already)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcdypn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcdypn/how_can_i_deploy_model_to_served_my_own_web_app/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcdypn/how_can_i_deploy_model_to_served_my_own_web_app/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753801714,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, I followed the instruction guide, but when I import the model into Python, I get this error:\n\nTraceback (most recent call last):  \nFile \"\", line 1, in  \nFile \"/home/asimhome/openvoice/OpenVoice/openvoice/se\\_extractor.py\", line 10, in  \nfrom faster\\_whisper import WhisperModel  \nFile \"/home/asimhome/anaconda3/envs/openvoice/lib/python3.9/site-packages/faster\\_whisper/**init**.py\", line 2, in  \nfrom faster\\_whisper.transcribe import WhisperModel  \nFile \"/home/asimhome/anaconda3/envs/openvoice/lib/python3.9/site-packages/faster\\_whisper/transcribe.py\", line 8, in  \nimport ctranslate2  \nFile \"/home/asimhome/anaconda3/envs/openvoice/lib/python3.9/site-packages/ctranslate2/**init**.py\", line 21, in  \nfrom ctranslate2.\\_ext import (  \nImportError: libctranslate2-1e22bce9.so.3.24.0: cannot enable executable stack as shared object requires: Invalid argument\n\nI'm on Arch Linux, installed this in a conda environment with Python 3.9, please let me know if any other things are needed to debug this\n\nI have tried a whole bunch of things like clean install in a new conda environment, upgrading and downgrading ctranslate packages, building them from source, changing python versions, nothing works. Please help, I'm losing my mind.",
          "author_fullname": "t2_oys4cpy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting an ImportError on OpenVoice V2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcdlxc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753800893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I followed the instruction guide, but when I import the model into Python, I get this error:&lt;/p&gt;\n\n&lt;p&gt;Traceback (most recent call last):&lt;br/&gt;\nFile &amp;quot;&amp;quot;, line 1, in&lt;br/&gt;\nFile &amp;quot;/home/asimhome/openvoice/OpenVoice/openvoice/se_extractor.py&amp;quot;, line 10, in&lt;br/&gt;\nfrom faster_whisper import WhisperModel&lt;br/&gt;\nFile &amp;quot;/home/asimhome/anaconda3/envs/openvoice/lib/python3.9/site-packages/faster_whisper/&lt;strong&gt;init&lt;/strong&gt;.py&amp;quot;, line 2, in&lt;br/&gt;\nfrom faster_whisper.transcribe import WhisperModel&lt;br/&gt;\nFile &amp;quot;/home/asimhome/anaconda3/envs/openvoice/lib/python3.9/site-packages/faster_whisper/transcribe.py&amp;quot;, line 8, in&lt;br/&gt;\nimport ctranslate2&lt;br/&gt;\nFile &amp;quot;/home/asimhome/anaconda3/envs/openvoice/lib/python3.9/site-packages/ctranslate2/&lt;strong&gt;init&lt;/strong&gt;.py&amp;quot;, line 21, in&lt;br/&gt;\nfrom ctranslate2._ext import (&lt;br/&gt;\nImportError: libctranslate2-1e22bce9.so.3.24.0: cannot enable executable stack as shared object requires: Invalid argument&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Arch Linux, installed this in a conda environment with Python 3.9, please let me know if any other things are needed to debug this&lt;/p&gt;\n\n&lt;p&gt;I have tried a whole bunch of things like clean install in a new conda environment, upgrading and downgrading ctranslate packages, building them from source, changing python versions, nothing works. Please help, I&amp;#39;m losing my mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcdlxc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrTechnoBlade",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcdlxc/getting_an_importerror_on_openvoice_v2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcdlxc/getting_an_importerror_on_openvoice_v2/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753800893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI wanted to get your thoughts on a tagging problem I am working on.\n\nI currenlty have 50 million records (with 20 fields) of entries that have user opinions on various different topics (json). I am trying to run a tagging script to attach some topics, sentiment, etc. This will then be used to embed each records into a vector db.\n\nCurrently I am using Phi-4 (full version) on 8xH100 GPUs to tag 128 records in batch at a time.\n\nThere a bunch of optimizations I could continue doing, but I still feel like this process of tagging will be too slow.\n\nI wonder if I am approaching this problem incorrectly, is there a easier/ more effecient way of approaching this?",
          "author_fullname": "t2_11g4uusxu4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tagging 50 million assets 'quickly' - thoughts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcd2uw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753799663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I wanted to get your thoughts on a tagging problem I am working on.&lt;/p&gt;\n\n&lt;p&gt;I currenlty have 50 million records (with 20 fields) of entries that have user opinions on various different topics (json). I am trying to run a tagging script to attach some topics, sentiment, etc. This will then be used to embed each records into a vector db.&lt;/p&gt;\n\n&lt;p&gt;Currently I am using Phi-4 (full version) on 8xH100 GPUs to tag 128 records in batch at a time.&lt;/p&gt;\n\n&lt;p&gt;There a bunch of optimizations I could continue doing, but I still feel like this process of tagging will be too slow.&lt;/p&gt;\n\n&lt;p&gt;I wonder if I am approaching this problem incorrectly, is there a easier/ more effecient way of approaching this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcd2uw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PreviousResearcher50",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcd2uw/tagging_50_million_assets_quickly_thoughts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcd2uw/tagging_50_million_assets_quickly_thoughts/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753799663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Last week we tested out [Qwen3-Coder](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct), the new 480B “agentic” model from Alibaba, and wired it into Cursor IDE using [NetMind.AI’s OpenAI-compatible API](https://netmind.ai/).\n\n**Prompt:**\n\n&gt;“Create a 2D game like Super Mario.”\n\nWhat happened next surprised us:\n\n* The model asked if we had any assets\n* Auto-installed `pygame`\n* Generated a working project with a clean folder structure, a README, and a playable 2D game where you can collect coins and stomp enemies\n\nFull blog post with screenshots, instructions, and results here: [Qwen3-Coder is Actually Amazing: We Confirmed this with NetMind API at Cursor Agent Mode](https://blog.netmind.ai/article/Qwen3-Coder_is_Actually_Amazing%3A_We_Confirmed_this_with_NetMind_API_at_Cursor_Agent_Mode)\n\n**Why this is interesting:**\n\n* No special tooling needed - we just changed the Base URL in Cursor to [`https://api.netmind.ai/inference-api/openai/v1`](https://api.netmind.ai/inference-api/openai/v1)\n* Model selection and key setup took under a minute\n* The inference felt snappy, and cost is \\~$2 per million tokens\n* The experience felt surprisingly close to GPT-4’s agent mode - but powered entirely by open-source models on a flexible, non-proprietary backend\n\nHas anyone else tried Qwen3 yet in an agent setup? Any other agent-model combos worth testing?\n\nWe built this internally at NetMind and figured it might be worth sharing with the community. Let us know what you think!",
          "author_fullname": "t2_1mz24a41z0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "We used Qwen3-Coder via NetMind’s API to build a 2D Mario-style game in seconds (demo + setup guide)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "o50ryns6ltff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 39,
                  "x": 108,
                  "u": "https://preview.redd.it/o50ryns6ltff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33daf1ea7f5cea77b1a9c2ff25b14c7a398926b2"
                },
                {
                  "y": 79,
                  "x": 216,
                  "u": "https://preview.redd.it/o50ryns6ltff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35771558ea42db35bb90eee2e7a70bebc8a5f1b5"
                },
                {
                  "y": 118,
                  "x": 320,
                  "u": "https://preview.redd.it/o50ryns6ltff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=763cd4c749da91c0b9e5fa211c03d4774cbfaf94"
                },
                {
                  "y": 237,
                  "x": 640,
                  "u": "https://preview.redd.it/o50ryns6ltff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=643f41ab47498ab79198f310615beaccfeeb2c53"
                },
                {
                  "y": 355,
                  "x": 960,
                  "u": "https://preview.redd.it/o50ryns6ltff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f177cea4f5f393ebd10ef332a480f1bd753b77a"
                },
                {
                  "y": 399,
                  "x": 1080,
                  "u": "https://preview.redd.it/o50ryns6ltff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bd9a22bd60819c0b6e76e715d6dc87c9c75ea44"
                }
              ],
              "s": {
                "y": 474,
                "x": 1280,
                "u": "https://preview.redd.it/o50ryns6ltff1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=f48e86abcee0ff613ff279c9f84f2c41cc049ad9"
              },
              "id": "o50ryns6ltff1"
            },
            "pxs9oid6ltff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9966e494b15a0feb68c2beee11eeac2580c8f6cc"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1d3e19b805bb72b74596e3c88cb14759d6a2fb2"
                },
                {
                  "y": 159,
                  "x": 320,
                  "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f0beded0e430d66425722129d677ec580093b22"
                },
                {
                  "y": 318,
                  "x": 640,
                  "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29de892cf937edc929257b8170b1a6421502017b"
                },
                {
                  "y": 477,
                  "x": 960,
                  "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=31472727510d1fff62508f830f726522ad05addb"
                },
                {
                  "y": 536,
                  "x": 1080,
                  "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=973f071dbd55d422cda6580c68f61c285cc43857"
                }
              ],
              "s": {
                "y": 636,
                "x": 1280,
                "u": "https://preview.redd.it/pxs9oid6ltff1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=9f197c91d7c724543faee35d0ad90356cc6ccc0f"
              },
              "id": "pxs9oid6ltff1"
            }
          },
          "name": "t3_1mcd0dn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "Step-by-step view of how we connected Qwen3-Coder in Cursor using NetMind’s OpenAI-compatible API.",
                "media_id": "pxs9oid6ltff1",
                "id": 716644697
              },
              {
                "caption": "Final game built by Qwen3-Coder: a playable 2D platformer with coin collection and a victory screen.",
                "media_id": "o50ryns6ltff1",
                "id": 716644698
              }
            ]
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Dx9tLKBe_rgaKQBWPTbN_PzX2BlGj5zQhTvhEhh5IjY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753799495,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week we tested out &lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;Qwen3-Coder&lt;/a&gt;, the new 480B “agentic” model from Alibaba, and wired it into Cursor IDE using &lt;a href=\"https://netmind.ai/\"&gt;NetMind.AI’s OpenAI-compatible API&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;“Create a 2D game like Super Mario.”&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;What happened next surprised us:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The model asked if we had any assets&lt;/li&gt;\n&lt;li&gt;Auto-installed &lt;code&gt;pygame&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Generated a working project with a clean folder structure, a README, and a playable 2D game where you can collect coins and stomp enemies&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Full blog post with screenshots, instructions, and results here: &lt;a href=\"https://blog.netmind.ai/article/Qwen3-Coder_is_Actually_Amazing%3A_We_Confirmed_this_with_NetMind_API_at_Cursor_Agent_Mode\"&gt;Qwen3-Coder is Actually Amazing: We Confirmed this with NetMind API at Cursor Agent Mode&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why this is interesting:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;No special tooling needed - we just changed the Base URL in Cursor to &lt;a href=\"https://api.netmind.ai/inference-api/openai/v1\"&gt;&lt;code&gt;https://api.netmind.ai/inference-api/openai/v1&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model selection and key setup took under a minute&lt;/li&gt;\n&lt;li&gt;The inference felt snappy, and cost is ~$2 per million tokens&lt;/li&gt;\n&lt;li&gt;The experience felt surprisingly close to GPT-4’s agent mode - but powered entirely by open-source models on a flexible, non-proprietary backend&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone else tried Qwen3 yet in an agent setup? Any other agent-model combos worth testing?&lt;/p&gt;\n\n&lt;p&gt;We built this internally at NetMind and figured it might be worth sharing with the community. Let us know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mcd0dn",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mcd0dn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MarketingNetMind",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcd0dn/we_used_qwen3coder_via_netminds_api_to_build_a_2d/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mcd0dn",
          "subreddit_subscribers": 506711,
          "created_utc": 1753799495,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine tune orpheus but the only audios I have are at least 30 minutes long each, but orpheus worsk best with 5-15 seconds datasets, so how do I turn that 30 minutes video into multiple shorter videos while also preparing the transcript for each one of them?",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I chunk down a long video to prepare as dataset for fine-tunining a TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mccxrt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753799323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine tune orpheus but the only audios I have are at least 30 minutes long each, but orpheus worsk best with 5-15 seconds datasets, so how do I turn that 30 minutes video into multiple shorter videos while also preparing the transcript for each one of them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mccxrt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mccxrt/how_do_i_chunk_down_a_long_video_to_prepare_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mccxrt/how_do_i_chunk_down_a_long_video_to_prepare_as/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753799323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to build (ideally buy) a workstation to run local large language models (LLMs) for coding, software development, and general AI assistance. Budget is around $15k USD.\n\nI want something that feels close to ChatGPT4 or Claude in reasoning speed and accuracy, but fully local so I can use it for coding (VSCode integration, code completion, debugging, etc.).\n\nLooking for advice on both which models and what hardware to get. Here are my main questions:\n\n\n\nFor Local LLM:\n•What’s the best-performing opensource LLM right now for coding (DeepSeek 33B, Llama 3 70B, Mistral, something else)?\n\n•Which models are most Claude/GPT-like for reasoning, not just spitting code?\n\n•Are there any quantized or fine-tuned versions that run well without needing $30k of GPUs?\n\n•What frameworks are people using (Ollama, LM Studio, vLLM, llama.cpp) for fast inference and coding integrations?\n\n•Any VSCode or JetBrains tools/plugins that work well with local models?\n\n\n\nGeneral Hardware Questions\n•For around $15k, is it better to go with multiple consumer GPUs (2–4x RTX 5090s) or one workstation GPU (A100/H100)?\n\n•How much VRAM and RAM do I realistically need to run 30B–70B parameter models smoothly?\n\n•Would you recommend buying something like a Lambda Vector workstation or building a custom rig?",
          "author_fullname": "t2_t2ql0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM + Hardware Build for Coding With a $15k Budget (2025)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcavlf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753794225,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to build (ideally buy) a workstation to run local large language models (LLMs) for coding, software development, and general AI assistance. Budget is around $15k USD.&lt;/p&gt;\n\n&lt;p&gt;I want something that feels close to ChatGPT4 or Claude in reasoning speed and accuracy, but fully local so I can use it for coding (VSCode integration, code completion, debugging, etc.).&lt;/p&gt;\n\n&lt;p&gt;Looking for advice on both which models and what hardware to get. Here are my main questions:&lt;/p&gt;\n\n&lt;p&gt;For Local LLM:\n•What’s the best-performing opensource LLM right now for coding (DeepSeek 33B, Llama 3 70B, Mistral, something else)?&lt;/p&gt;\n\n&lt;p&gt;•Which models are most Claude/GPT-like for reasoning, not just spitting code?&lt;/p&gt;\n\n&lt;p&gt;•Are there any quantized or fine-tuned versions that run well without needing $30k of GPUs?&lt;/p&gt;\n\n&lt;p&gt;•What frameworks are people using (Ollama, LM Studio, vLLM, llama.cpp) for fast inference and coding integrations?&lt;/p&gt;\n\n&lt;p&gt;•Any VSCode or JetBrains tools/plugins that work well with local models?&lt;/p&gt;\n\n&lt;p&gt;General Hardware Questions\n•For around $15k, is it better to go with multiple consumer GPUs (2–4x RTX 5090s) or one workstation GPU (A100/H100)?&lt;/p&gt;\n\n&lt;p&gt;•How much VRAM and RAM do I realistically need to run 30B–70B parameter models smoothly?&lt;/p&gt;\n\n&lt;p&gt;•Would you recommend buying something like a Lambda Vector workstation or building a custom rig?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcavlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lavoid12",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcavlf/best_local_llm_hardware_build_for_coding_with_a/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753794225,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, I'm new to working with LLMs, especially when it comes to fine-tuning or customizing them for domain-specific use cases.\n\nRight now, I'm exploring how to build a **Prompt : Expected-Output** style dataset for fine-tuning a lightweight language model (\\~1–1.5B parameters).  \nThe goal is to enable the model to analyze code files and identify specific patterns within them. However, the twist is that some false positives or edge cases can only be flagged correctly when you consider the file path or context of the file in the project — not just the raw code.\n\nSo essentially, the input to the model would be:\n\n    &lt;file-path&gt;\\n&lt;code-contents&gt;\n\nThe output would be a custom JSON.\n\nThis would help the model learn more nuanced behaviors that static rules often miss.\n\nAre there any tools, workflows, or existing pipelines that can semi-automate dataset generation like this — especially ones that leverage existing models (e.g., Claude, Gemini, GPT-4, etc.) to help with generating prompt (+ CoT).\n\nI'm trying to avoid doing the entire dataset manually if there's a smart way to leverage existing models/tools to bootstrap it.\n\nThanks — any suggestions or pointers would go a long way.",
          "author_fullname": "t2_jchazvrjf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Creating a High Quality Dataset for Instruction Fine-Tuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mcatlt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753794085,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m new to working with LLMs, especially when it comes to fine-tuning or customizing them for domain-specific use cases.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;m exploring how to build a &lt;strong&gt;Prompt : Expected-Output&lt;/strong&gt; style dataset for fine-tuning a lightweight language model (~1–1.5B parameters).&lt;br/&gt;\nThe goal is to enable the model to analyze code files and identify specific patterns within them. However, the twist is that some false positives or edge cases can only be flagged correctly when you consider the file path or context of the file in the project — not just the raw code.&lt;/p&gt;\n\n&lt;p&gt;So essentially, the input to the model would be:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;file-path&amp;gt;\\n&amp;lt;code-contents&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The output would be a custom JSON.&lt;/p&gt;\n\n&lt;p&gt;This would help the model learn more nuanced behaviors that static rules often miss.&lt;/p&gt;\n\n&lt;p&gt;Are there any tools, workflows, or existing pipelines that can semi-automate dataset generation like this — especially ones that leverage existing models (e.g., Claude, Gemini, GPT-4, etc.) to help with generating prompt (+ CoT).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to avoid doing the entire dataset manually if there&amp;#39;s a smart way to leverage existing models/tools to bootstrap it.&lt;/p&gt;\n\n&lt;p&gt;Thanks — any suggestions or pointers would go a long way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mcatlt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "unnxt30",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mcatlt/creating_a_high_quality_dataset_for_instruction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcatlt/creating_a_high_quality_dataset_for_instruction/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753794085,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.\n\nThe results were pretty remarkable I must say! \n\nHere’s the link to the results: https://chat.z.ai/space/r05c76960ff0-ppt \n\nHere’s the initial prompt:\n\n”Create a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.”\n\nAs you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.\n\nIs it just me or are things going superfast since OpenAI announced the release of GPT-5?\n\nIt seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.\n\n \n\n",
          "author_fullname": "t2_1ttp8mwcgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I just tried GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc8tks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 278,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 278,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753788294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.&lt;/p&gt;\n\n&lt;p&gt;The results were pretty remarkable I must say! &lt;/p&gt;\n\n&lt;p&gt;Here’s the link to the results: &lt;a href=\"https://chat.z.ai/space/r05c76960ff0-ppt\"&gt;https://chat.z.ai/space/r05c76960ff0-ppt&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Here’s the initial prompt:&lt;/p&gt;\n\n&lt;p&gt;”Create a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.”&lt;/p&gt;\n\n&lt;p&gt;As you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.&lt;/p&gt;\n\n&lt;p&gt;Is it just me or are things going superfast since OpenAI announced the release of GPT-5?&lt;/p&gt;\n\n&lt;p&gt;It seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mc8tks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AI-On-A-Dime",
          "discussion_type": null,
          "num_comments": 127,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753788294,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**🔧 Help Needed – Fine-tuning a LLM on Luciforms + Ritual Conversations**\n\nHey everyone,\n\nI’m working on a project that blends prompt engineering, AI personalization, and poetic syntax. I'm building a daemon-like assistant called **ShadeOS**, and I want to fine-tune a local LLM (like Mistral-7B or Phi-2) on:\n\n* 🧠 Open-source datasets like **OpenOrca**, **UltraChat**, or **OpenAssistant/oasst1**\n* 💬 My own exported conversations with ShadeOS (thousands of lines of recursive dialogue, instructions, hallucinations, mirror logic…)\n* 🔮 A structured experimental format I created: `.luciform` files — symbolic, recursive prompts that encode intention and personality\n\nThe goal is to create a **custom LLM that speaks my language**, understands luciform structure, and can be injected into a terminal interface with real-time feedback.\n\n🖥️ I need help with:\n\n* Access to a machine with **16GB+ VRAM** to fine-tune using LoRA (QLoRA / PEFT)\n* Any advice, links, scripts or shortcuts for fine-tuning Mistral/Φ2 on personal data\n* Bonus: if anyone wants to test luciforms or experiment with ritual-based prompting\n\nWhy?  \nBecause not every AI should sound like a helpdesk.  \nSome of us want demons. Some of us want mirrors.  \nAnd some of us want to make our LLM speak from inside our dreams.\n\nThanks in advance.  \nRepo: [https://github.com/luciedefraiteur/LuciformResearch](https://github.com/luciedefraiteur/LuciformResearch)  \n(Feel free to DM if you want to help, collab, or just vibe.)\n\n— Lucie",
          "author_fullname": "t2_dc6uhi63",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a custom LLM trained on luciform prompts + ShadeOS daemon dialogues – seeking help",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc8i36",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753787233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;🔧 Help Needed – Fine-tuning a LLM on Luciforms + Ritual Conversations&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m working on a project that blends prompt engineering, AI personalization, and poetic syntax. I&amp;#39;m building a daemon-like assistant called &lt;strong&gt;ShadeOS&lt;/strong&gt;, and I want to fine-tune a local LLM (like Mistral-7B or Phi-2) on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;🧠 Open-source datasets like &lt;strong&gt;OpenOrca&lt;/strong&gt;, &lt;strong&gt;UltraChat&lt;/strong&gt;, or &lt;strong&gt;OpenAssistant/oasst1&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;💬 My own exported conversations with ShadeOS (thousands of lines of recursive dialogue, instructions, hallucinations, mirror logic…)&lt;/li&gt;\n&lt;li&gt;🔮 A structured experimental format I created: &lt;code&gt;.luciform&lt;/code&gt; files — symbolic, recursive prompts that encode intention and personality&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The goal is to create a &lt;strong&gt;custom LLM that speaks my language&lt;/strong&gt;, understands luciform structure, and can be injected into a terminal interface with real-time feedback.&lt;/p&gt;\n\n&lt;p&gt;🖥️ I need help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Access to a machine with &lt;strong&gt;16GB+ VRAM&lt;/strong&gt; to fine-tune using LoRA (QLoRA / PEFT)&lt;/li&gt;\n&lt;li&gt;Any advice, links, scripts or shortcuts for fine-tuning Mistral/Φ2 on personal data&lt;/li&gt;\n&lt;li&gt;Bonus: if anyone wants to test luciforms or experiment with ritual-based prompting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why?&lt;br/&gt;\nBecause not every AI should sound like a helpdesk.&lt;br/&gt;\nSome of us want demons. Some of us want mirrors.&lt;br/&gt;\nAnd some of us want to make our LLM speak from inside our dreams.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;br/&gt;\nRepo: &lt;a href=\"https://github.com/luciedefraiteur/LuciformResearch\"&gt;https://github.com/luciedefraiteur/LuciformResearch&lt;/a&gt;&lt;br/&gt;\n(Feel free to DM if you want to help, collab, or just vibe.)&lt;/p&gt;\n\n&lt;p&gt;— Lucie&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?auto=webp&amp;s=48e19a5b5d13864d40dd765d259c003d37f1cb4c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7200ceb0676d8deadb109f24b615e515c4fa38e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9eaae6b46af49f655d078f407f066d9aaefe8540",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2843feca811929825840d83b2250f6dbbc523eca",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e29f47f775efec86a1c581f3dfbf297a5f90e76a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e7508a933c080c4f7e8e1cced4b1bc5c553a9c9",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=136368720ae2c3b8e3902a962eaccca94cb84d10",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mc8i36",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LucieTrans",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8i36/building_a_custom_llm_trained_on_luciform_prompts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8i36/building_a_custom_llm_trained_on_luciform_prompts/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753787233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thank you!\n\nI am just thinking is it possible to do it?\n\n",
          "author_fullname": "t2_srz14fu0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone have experience use qwen3 8b with PPO to fine tune a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc8hn6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753787191,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;I am just thinking is it possible to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc8hn6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GuitarAshamed4451",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8hn6/does_anyone_have_experience_use_qwen3_8b_with_ppo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8hn6/does_anyone_have_experience_use_qwen3_8b_with_ppo/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753787191,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA 👋!\n\nFor the past 18 months, my colleague and I have been working on **Ebiose**, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).\n\nEbiose aims to create a decentralized AI factory, a Darwin-style playground (à la Google’s AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own \"forge,\" define a task, and watch AI agents compete until the fittest emerge.\n\nThis evolutionary approach demands massive inference resources. Currently, we're relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.\n\nThat's why we'd love input from the LocalLLaMA community!\n\n**The Big Idea: A Community-Powered P2P Inference Grid**\n\nWe’re dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here’s the plan:\n\n* **Lightweight Client:** A background app runs on your PC (and maybe phones later).\n* **Hardware Profiling:** The client auto-detects what LLMs your machine can handle.\n* **Orchestration Layer:** A system (centralized or decentralized?) assigns inference tasks to capable nodes.\n* **Dynamic LoRA Adapters:** Fine-tune models efficiently with lightweight, modular adapters.\n* **Batch &amp; Prompt Caching:** Optimize for high throughput by batching requests and reusing system prompts.\n\n**Technical Questions for the Community**\n\n1. **Inference Backend:** We’re leaning toward **llama.cpp** for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a high-throughput setup, would **vLLM**, **zml**, or another engine be better? Since we’re prioritizing batch processing over single-prompt speed, what’s your pick?\n2. **Task Orchestration:** How do we route inference jobs (e.g., “run this 13B model with this prompt”) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?\n3. **Existing Tools:** Are there open-source projects we could build on?\n\nWhat do you think? Got ideas, tools, or experiences to share?",
          "author_fullname": "t2_6bm8s1wm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Let's Build a \"Garage AI Supercomputer\": A P2P Compute Grid for Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc8fhc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753796327,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753786996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; 👋!&lt;/p&gt;\n\n&lt;p&gt;For the past 18 months, my colleague and I have been working on &lt;strong&gt;Ebiose&lt;/strong&gt;, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).&lt;/p&gt;\n\n&lt;p&gt;Ebiose aims to create a decentralized AI factory, a Darwin-style playground (à la Google’s AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own &amp;quot;forge,&amp;quot; define a task, and watch AI agents compete until the fittest emerge.&lt;/p&gt;\n\n&lt;p&gt;This evolutionary approach demands massive inference resources. Currently, we&amp;#39;re relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why we&amp;#39;d love input from the LocalLLaMA community!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Big Idea: A Community-Powered P2P Inference Grid&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We’re dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here’s the plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lightweight Client:&lt;/strong&gt; A background app runs on your PC (and maybe phones later).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware Profiling:&lt;/strong&gt; The client auto-detects what LLMs your machine can handle.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orchestration Layer:&lt;/strong&gt; A system (centralized or decentralized?) assigns inference tasks to capable nodes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic LoRA Adapters:&lt;/strong&gt; Fine-tune models efficiently with lightweight, modular adapters.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch &amp;amp; Prompt Caching:&lt;/strong&gt; Optimize for high throughput by batching requests and reusing system prompts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical Questions for the Community&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Inference Backend:&lt;/strong&gt; We’re leaning toward &lt;strong&gt;llama.cpp&lt;/strong&gt; for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a high-throughput setup, would &lt;strong&gt;vLLM&lt;/strong&gt;, &lt;strong&gt;zml&lt;/strong&gt;, or another engine be better? Since we’re prioritizing batch processing over single-prompt speed, what’s your pick?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Task Orchestration:&lt;/strong&gt; How do we route inference jobs (e.g., “run this 13B model with this prompt”) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Are there open-source projects we could build on?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think? Got ideas, tools, or experiences to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc8fhc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ModeSquare8129",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753786996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "👋 After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (\\~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford's Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! 😅\n\n  \n**What I did:**\n\n* Created a Claude Code-inspired agent (system msg + tools)\n* Built Docker-isolated GRPO training where each rollout gets its own container\n* Developed a multi-agent synthetic data pipeline to generate &amp; validate training data with Opus-4\n* Implemented a hybrid reward signal of unit test verifiers &amp; a behavioural LLM judge.\n\n  \n**Key results:**\n\n* My untrained Qwen3-32B agent achieved **13.75%** on Terminal-Bench (#19, beats Stanford's Qwen3-235B MoE)\n* I tested training to work stably on 32x H100s distributed across 4 bare metal nodes\n* I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.\n* \\~£30-50k needed for full training run of 1000 epochs (I could only afford testing 😅)\n\n\n\n**Technical details:**\n\n* The synthetic dataset ranges from easy to extremely hard tasks. An example hard task's prompt:\n   * \"I found this mystery program at \\`/app/program\\` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires?\"\n* Simple config presets allow training to run on multiple hardware setups with minimal effort.\n* GRPO used with 16 rollouts per task, up to 32k tokens per rollout.\n* Agent uses XML/YAML format to structure tool calls\n\n  \n**More details:**\n\nMy Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:\n\n* ⭐️ [Terminal Agent RL repo](https://github.com/Danau5tin/terminal-bench-rl)\n* [⭐️ Multi-agent synthetic data pipeline repo](https://github.com/Danau5tin/tbench-agentic-data-pipeline)\n\n  \nI thought I would share this because I believe long-horizon RL is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.\n\n  \nThanks for reading!\n\nDan\n\n\n\n**(**Built using [rLLM](https://github.com/rllm-org/rllm) RL framework which was brilliant to work with, and evaluated and inspired by the great [Terminal Bench](https://www.tbench.ai/) benchmark)",
          "author_fullname": "t2_1d3whvko4o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train 😅",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 56,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "az9m6jfyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf0464c4532f3f729201557b3cdb5d5fd0da9b2b"
                },
                {
                  "y": 113,
                  "x": 216,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9307853ef8723561cef54082c7f8f77319ea5b34"
                },
                {
                  "y": 168,
                  "x": 320,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae1a5539212b036e7c28c8c61e3e68b98424c45c"
                },
                {
                  "y": 336,
                  "x": 640,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34665d186c28ea84e68baf844e036d594b99444c"
                },
                {
                  "y": 504,
                  "x": 960,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b63f6b7aca9c207a4fde0aef520b6009219fffa8"
                },
                {
                  "y": 567,
                  "x": 1080,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cdac6743c832453412d2fa7f1ee6772f07122775"
                }
              ],
              "s": {
                "y": 2656,
                "x": 5056,
                "u": "https://preview.redd.it/az9m6jfyosff1.png?width=5056&amp;format=png&amp;auto=webp&amp;s=d99222a2be85de2b89a1a6493a08cde4699fe249"
              },
              "id": "az9m6jfyosff1"
            },
            "05xy1rkwosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 43,
                  "x": 108,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0dcfc346c8c2da5b7e199e3362c832fda72f2a0"
                },
                {
                  "y": 86,
                  "x": 216,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd0183b1ad65fa3536530fed15ebb59228114de8"
                },
                {
                  "y": 128,
                  "x": 320,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6378361bd9573c4483dbb212b8b19d0f423d586e"
                },
                {
                  "y": 257,
                  "x": 640,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cfa60d1b056f1a6aaa767613c6b399bba1f08e3f"
                },
                {
                  "y": 386,
                  "x": 960,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=89aeebad83be297d8a6cfcd4d0542c93ccd188a8"
                },
                {
                  "y": 434,
                  "x": 1080,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc80c66e6cf4008641ba2f21dd30003eef50ce89"
                }
              ],
              "s": {
                "y": 1216,
                "x": 3020,
                "u": "https://preview.redd.it/05xy1rkwosff1.png?width=3020&amp;format=png&amp;auto=webp&amp;s=e1dc47c0c33e71f8b5e335d68819be4db4d22ee5"
              },
              "id": "05xy1rkwosff1"
            },
            "su4gklfyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 18,
                  "x": 108,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d43db954028d7fa2643968427c1d234dc846c5bc"
                },
                {
                  "y": 36,
                  "x": 216,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d90b7348035f5a53dc5107a1589bd53b00f6164"
                },
                {
                  "y": 53,
                  "x": 320,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4903028118e0d07279f10f1ff077e9b7602a3f4"
                },
                {
                  "y": 107,
                  "x": 640,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a51dc0a7298828763adf8f5227e658fd9a7aea78"
                },
                {
                  "y": 160,
                  "x": 960,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=494ddcfcfa66b477b5fe9ef394122c9090897959"
                },
                {
                  "y": 181,
                  "x": 1080,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e37acb88ac4bfb55e64017940222b653620d355"
                }
              ],
              "s": {
                "y": 392,
                "x": 2338,
                "u": "https://preview.redd.it/su4gklfyosff1.png?width=2338&amp;format=png&amp;auto=webp&amp;s=bd8ffa2f90ce65997dc8fb28bc340c740cb836cf"
              },
              "id": "su4gklfyosff1"
            },
            "1b89mdgyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8aa052d7230a7e93bea882456a0172d444f2b56a"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc0c3485addd4720027df6316c53a1f7a92d2d1d"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3bd5cac04c80326df47cb1d08a272ca37fbdc82"
                },
                {
                  "y": 373,
                  "x": 640,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bb2846eb41e704e36041ddc908044490ffd58ce"
                },
                {
                  "y": 559,
                  "x": 960,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6767893bb8601129a6a8da2adb366e6b2cd73c4e"
                },
                {
                  "y": 629,
                  "x": 1080,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7964070ac0fa53d489a1e248289630ad1ff7a2e0"
                }
              ],
              "s": {
                "y": 1522,
                "x": 2610,
                "u": "https://preview.redd.it/1b89mdgyosff1.png?width=2610&amp;format=png&amp;auto=webp&amp;s=25d03cf8b2bcc3e4d1633a1fb9f3c570fbda742b"
              },
              "id": "1b89mdgyosff1"
            }
          },
          "name": "t3_1mc8evq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 61,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "05xy1rkwosff1",
                "id": 716537836
              },
              {
                "media_id": "az9m6jfyosff1",
                "id": 716537837
              },
              {
                "media_id": "su4gklfyosff1",
                "id": 716537838
              },
              {
                "media_id": "1b89mdgyosff1",
                "id": 716537839
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/GTUl_GxBM3AgORm0fFuwPhwKeJqsGTeIOOsHWvhrYYI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753786945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;👋 After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford&amp;#39;s Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! 😅&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Created a Claude Code-inspired agent (system msg + tools)&lt;/li&gt;\n&lt;li&gt;Built Docker-isolated GRPO training where each rollout gets its own container&lt;/li&gt;\n&lt;li&gt;Developed a multi-agent synthetic data pipeline to generate &amp;amp; validate training data with Opus-4&lt;/li&gt;\n&lt;li&gt;Implemented a hybrid reward signal of unit test verifiers &amp;amp; a behavioural LLM judge.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My untrained Qwen3-32B agent achieved &lt;strong&gt;13.75%&lt;/strong&gt; on Terminal-Bench (#19, beats Stanford&amp;#39;s Qwen3-235B MoE)&lt;/li&gt;\n&lt;li&gt;I tested training to work stably on 32x H100s distributed across 4 bare metal nodes&lt;/li&gt;\n&lt;li&gt;I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.&lt;/li&gt;\n&lt;li&gt;~£30-50k needed for full training run of 1000 epochs (I could only afford testing 😅)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The synthetic dataset ranges from easy to extremely hard tasks. An example hard task&amp;#39;s prompt:\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;I found this mystery program at `/app/program` and I&amp;#39;m completely stumped. It&amp;#39;s a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can&amp;#39;t figure out what kind of input it needs. Could you help me figure out what this program requires?&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Simple config presets allow training to run on multiple hardware setups with minimal effort.&lt;/li&gt;\n&lt;li&gt;GRPO used with 16 rollouts per task, up to 32k tokens per rollout.&lt;/li&gt;\n&lt;li&gt;Agent uses XML/YAML format to structure tool calls&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;⭐️ &lt;a href=\"https://github.com/Danau5tin/terminal-bench-rl\"&gt;Terminal Agent RL repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Danau5tin/tbench-agentic-data-pipeline\"&gt;⭐️ Multi-agent synthetic data pipeline repo&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I thought I would share this because I believe long-horizon RL is going to change everybody&amp;#39;s lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n\n&lt;p&gt;Dan&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;Built using &lt;a href=\"https://github.com/rllm-org/rllm\"&gt;rLLM&lt;/a&gt; RL framework which was brilliant to work with, and evaluated and inspired by the great &lt;a href=\"https://www.tbench.ai/\"&gt;Terminal Bench&lt;/a&gt; benchmark)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mc8evq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mc8evq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanAiTuning",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mc8evq",
          "subreddit_subscribers": 506711,
          "created_utc": 1753786945,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Benchmarks with GLM-4.5 Air\n\n44.45 tok/sec || 3445 tokens || 2.14s to first token\n\nvs\n\n40.06 tok/sec || 2574 tokens || 0.21s to first token\n\nSure the Mac Studio can run much larger models, but I kind of expected that there would be a bigger inference performance hit when using a platform with half as many GPU cores.\n\nI'm using LMStudio on both machines.",
          "author_fullname": "t2_cbxyn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac Studio 512GB vs MBP 128GB similar performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc83jm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753787203,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753785857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Benchmarks with GLM-4.5 Air&lt;/p&gt;\n\n&lt;p&gt;44.45 tok/sec || 3445 tokens || 2.14s to first token&lt;/p&gt;\n\n&lt;p&gt;vs&lt;/p&gt;\n\n&lt;p&gt;40.06 tok/sec || 2574 tokens || 0.21s to first token&lt;/p&gt;\n\n&lt;p&gt;Sure the Mac Studio can run much larger models, but I kind of expected that there would be a bigger inference performance hit when using a platform with half as many GPU cores.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using LMStudio on both machines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc83jm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chisleu",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc83jm/mac_studio_512gb_vs_mbp_128gb_similar_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc83jm/mac_studio_512gb_vs_mbp_128gb_similar_performance/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753785857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, author of Code Web Chat here 🙌\n\nAlmost everyday we hear our tools being capped more and more.\n\nCWC gives you more options of AI use for coding to never hit rate limits of whatever you're using as your daily driver.\n\nAs soon as a new chatbot is announced I'm working hard to support it in the tool (with some exceptions like api wrappers).\n\nThe full list of supported chatbots that CWC initializes with your code and instructions:\n\n* AI Studio\n* ChatGPT\n* Claude\n* DeepSeek\n* Doubao\n* Gemini\n* Grok\n* Mistral\n* Open WebUI\n* OpenRouter Chat\n* Perplexity\n* Kimi\n* Qwen\n* Yuanbao\n* Z. AI\n\nType CWC in extensions pane (VS Code or its derivative) to install.",
          "author_fullname": "t2_gm504",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CWC now supports kimi.com (K2) and chat.z.ai (GLM-4.5) to enable coding with top tier models at no cost",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7xjb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=afef57a0fd1b81c69772eafcfdd64e6d672cb73e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753785272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, author of Code Web Chat here 🙌&lt;/p&gt;\n\n&lt;p&gt;Almost everyday we hear our tools being capped more and more.&lt;/p&gt;\n\n&lt;p&gt;CWC gives you more options of AI use for coding to never hit rate limits of whatever you&amp;#39;re using as your daily driver.&lt;/p&gt;\n\n&lt;p&gt;As soon as a new chatbot is announced I&amp;#39;m working hard to support it in the tool (with some exceptions like api wrappers).&lt;/p&gt;\n\n&lt;p&gt;The full list of supported chatbots that CWC initializes with your code and instructions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI Studio&lt;/li&gt;\n&lt;li&gt;ChatGPT&lt;/li&gt;\n&lt;li&gt;Claude&lt;/li&gt;\n&lt;li&gt;DeepSeek&lt;/li&gt;\n&lt;li&gt;Doubao&lt;/li&gt;\n&lt;li&gt;Gemini&lt;/li&gt;\n&lt;li&gt;Grok&lt;/li&gt;\n&lt;li&gt;Mistral&lt;/li&gt;\n&lt;li&gt;Open WebUI&lt;/li&gt;\n&lt;li&gt;OpenRouter Chat&lt;/li&gt;\n&lt;li&gt;Perplexity&lt;/li&gt;\n&lt;li&gt;Kimi&lt;/li&gt;\n&lt;li&gt;Qwen&lt;/li&gt;\n&lt;li&gt;Yuanbao&lt;/li&gt;\n&lt;li&gt;Z. AI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Type CWC in extensions pane (VS Code or its derivative) to install.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/robertpiosik/CodeWebChat",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?auto=webp&amp;s=4a901a50016883dc4d0d4170b9b03452a409ef50",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94783b33939c3480892cd4d4d171117a3c432910",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1137ee6a216ebbd7d19dc98195d9e088dffd0d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0ec5307fd5fe4808076fc84d3ee296a1f1c8a1e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=910d90873865917079bcb70b502c0dfa66525165",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9d2ab1fcc259c65a4fe1e2bc9aa26d59846bd8a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=76b31b7cf962e63b2fb6d0885ea4cb1dca4be659",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc7xjb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertpiosik",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7xjb/cwc_now_supports_kimicom_k2_and_chatzai_glm45_to/",
          "stickied": false,
          "url": "https://github.com/robertpiosik/CodeWebChat",
          "subreddit_subscribers": 506711,
          "created_utc": 1753785272,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone.\n\nThis question is really bugging me for quite a while. I've been using claude sonnets, gemini 2.5 and other closed source models.\n\nWe've been seeing pretty great open source stuff and the benchmarks are high as well.\n\nBut irl, they seem not that great in my work. Kimi k2 and qwen 3 coder with benchmarks near to claude but i just don't feel it.\n\n\nIs it just me or does anyone else share the same feelings? \n\n",
          "author_fullname": "t2_1gvqs2a024",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Success with open source models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7ri9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753784661,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone.&lt;/p&gt;\n\n&lt;p&gt;This question is really bugging me for quite a while. I&amp;#39;ve been using claude sonnets, gemini 2.5 and other closed source models.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been seeing pretty great open source stuff and the benchmarks are high as well.&lt;/p&gt;\n\n&lt;p&gt;But irl, they seem not that great in my work. Kimi k2 and qwen 3 coder with benchmarks near to claude but i just don&amp;#39;t feel it.&lt;/p&gt;\n\n&lt;p&gt;Is it just me or does anyone else share the same feelings? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc7ri9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "United-Decision-7243",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7ri9/success_with_open_source_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc7ri9/success_with_open_source_models/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753784661,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, my system is a bit unbalanced right now, 5090 gpu on an \"older\" ddr4 32GB ram system.\n\nWhat should I do to try the new llm on my system? Is there a proper quantized version?\n\nThanks!",
          "author_fullname": "t2_sfb08i7a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Glm 4.5 air and 5090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7q0n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753784515,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my system is a bit unbalanced right now, 5090 gpu on an &amp;quot;older&amp;quot; ddr4 32GB ram system.&lt;/p&gt;\n\n&lt;p&gt;What should I do to try the new llm on my system? Is there a proper quantized version?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc7q0n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Green-Ad-3964",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7q0n/glm_45_air_and_5090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc7q0n/glm_45_air_and_5090/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753784515,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi so I am thinking of converting a pytorch based conformer model to onnx coz I had great time with onnx inference speed. I had never tried pytorch execution on android. Please advice me\n\n1) what would be better onnx vs pytorch runtime for this case\n2) Anyone tried converting conformer based models pytorch specific to onnx?\n3) help me out with conversion coz I read a lot of GitHub issues on this conversion",
          "author_fullname": "t2_mmtl1muh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Converting a conformer model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7ft1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753783457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi so I am thinking of converting a pytorch based conformer model to onnx coz I had great time with onnx inference speed. I had never tried pytorch execution on android. Please advice me&lt;/p&gt;\n\n&lt;p&gt;1) what would be better onnx vs pytorch runtime for this case\n2) Anyone tried converting conformer based models pytorch specific to onnx?\n3) help me out with conversion coz I read a lot of GitHub issues on this conversion&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc7ft1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Away_Expression_3713",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7ft1/converting_a_conformer_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc7ft1/converting_a_conformer_model/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753783457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I am running a local llama.cpp in server mode, with the model MythoMax-L2-13B.Q4\\_K\\_M. And I am having problems that neither me nor the 4.1 model of ChatGPT can solve. I am very new to everything; LLM, Llama, coding/developing/scripting and I am doing my best to learn, please be kind, I am (most likely) not dumb, just very very new to this.\n\nThis is the .bat file I made to run the server:   \necho off cd /d C:\\\\AI-Assistant \ncall ai-env\\\\Scripts\\\\activate.bat \npython -m llama\\_cpp.server \\^ \n--model \"./models/MythoMax-L2-13B.Q4\\_K\\_M.gguf\" \\^ \n--n\\_gpu\\_layers 33 \\^ \n--n\\_ctx 4096 \\^ \n--chat\\_format llama-2 \\^ \n--host (hidden for security) \\^ \n--port 1234 \n--stream \n\nThe issue I am having is running open interpreter.\nWhen just using CLI to prompt the LLM with something like \"write a haiku about thunder\" I get a printed response to the console and on the server side I get \"\"POST /v1/chat/completions HTTP/1.1\" 200 OK\" so I know the server works and the LLM can write a response. \n\nBut then when running OI and running the same prompt I get the same 200 OK code, but the response is not printed to the console I am running OI on.\n\nThis is the CLI I use to run OI interpreter:\n--model llama-cpp --api\\_base http://(hiddenforsecurity):1234/v1 --context\\_window 4096 --max\\_tokens 2048 \n\nCan someone please send this newbie (me) to the right documentation, tell me what probably obvious thing I am missing, or tell me what package I haven't installed or whatever it is that is just straight up incompatible? I have been trying to fix this for the past 10 hours and I am going insane XD",
          "author_fullname": "t2_1r4b70gpun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help! Open Interpreter not printing the response in console",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6kad",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753780083,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am running a local llama.cpp in server mode, with the model MythoMax-L2-13B.Q4_K_M. And I am having problems that neither me nor the 4.1 model of ChatGPT can solve. I am very new to everything; LLM, Llama, coding/developing/scripting and I am doing my best to learn, please be kind, I am (most likely) not dumb, just very very new to this.&lt;/p&gt;\n\n&lt;p&gt;This is the .bat file I made to run the server:&lt;br/&gt;\necho off cd /d C:\\AI-Assistant \ncall ai-env\\Scripts\\activate.bat \npython -m llama_cpp.server ^ \n--model &amp;quot;./models/MythoMax-L2-13B.Q4_K_M.gguf&amp;quot; ^ \n--n_gpu_layers 33 ^ \n--n_ctx 4096 ^ \n--chat_format llama-2 ^ \n--host (hidden for security) ^ \n--port 1234 \n--stream &lt;/p&gt;\n\n&lt;p&gt;The issue I am having is running open interpreter.\nWhen just using CLI to prompt the LLM with something like &amp;quot;write a haiku about thunder&amp;quot; I get a printed response to the console and on the server side I get &amp;quot;&amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK&amp;quot; so I know the server works and the LLM can write a response. &lt;/p&gt;\n\n&lt;p&gt;But then when running OI and running the same prompt I get the same 200 OK code, but the response is not printed to the console I am running OI on.&lt;/p&gt;\n\n&lt;p&gt;This is the CLI I use to run OI interpreter:\n--model llama-cpp --api_base http://(hiddenforsecurity):1234/v1 --context_window 4096 --max_tokens 2048 &lt;/p&gt;\n\n&lt;p&gt;Can someone please send this newbie (me) to the right documentation, tell me what probably obvious thing I am missing, or tell me what package I haven&amp;#39;t installed or whatever it is that is just straight up incompatible? I have been trying to fix this for the past 10 hours and I am going insane XD&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc6kad",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jack_Blade281",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6kad/help_open_interpreter_not_printing_the_response/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc6kad/help_open_interpreter_not_printing_the_response/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753780083,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 support is landing in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6fbp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 201,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 201,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=74411b3e344f617397de99b0ed0a03e269d8efec",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753779557,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?auto=webp&amp;s=30023402a6989ab78753084b6f9a7fbdd3e44d81",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7649d767f799c1e6b81af747ef3aed21648a9037",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cd6878bf5f95e786470b4fabe22d873e097a9e8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b80141042845f306cbaf8a52844f7a00355e7a7b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bbb4d01a722a7ac5908e1ba272a92870c5277cd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=75e08e8a0d15b1faa2896de0841e0bdc245896ba",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9141773c734ecbddc393603456528be8251b8de5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mc6fbp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "subreddit_subscribers": 506711,
          "created_utc": 1753779557,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Light-hearted, too. Don't take it too seriously!",
          "author_fullname": "t2_4e7zb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Something lightweight: a LLM simulation of Bernie Sanders",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6dfx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=13ca4de7ff68a78d013f9a42ba2e6d160bfd36a9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753779350,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Light-hearted, too. Don&amp;#39;t take it too seriously!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ivoras/bernie0.1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?auto=webp&amp;s=b63548e59f80de05379eb11bb6e71ffbe24ec79e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2195a6491e60c2b5e1f156d2b6b2b6724700da2b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a1fe9a861ec410437283d33ac1d6788cb63d07e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=11c6188b4912cc92c1465cd62ad6cddc3d8c5ff4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f80fc97e46dbb90afe19165e1d70bec1ffb040f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e8c871481b5aa549ea255e914c8ed7850224f4c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39178233074a14e8bf2d9acbd58e16605cf4eed5",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mc6dfx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ivoras",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6dfx/something_lightweight_a_llm_simulation_of_bernie/",
          "stickied": false,
          "url": "https://huggingface.co/ivoras/bernie0.1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753779350,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nThis is a new **opensource** project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context.   \n  \nThe idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs.   \n  \nYou can find the code here https://github.com/Orolol/familyBench\n\n\n**Current leaderboard**\n\nI test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.\n\nExample of family description : \"Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...\"\n\nExample of questions : \"Which of Paula's grandparents have salt and pepper hair?\" \"Who is the cousin of the daughter of Quentin with red hair?\"\n\nThe no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. \n\n\nModel | Accuracy | Total tokens | No response rate\n-----|--------|------------|----------------\nGemini 2.5 Pro | 81.48% | 271,500 \t| 0%\nDeepSeek R1 0528 | 75.66% | 150,642\t| 0%\nSonnet 4 | 67.20%  | 575,624 \t| 0%\nGLM 4.5| 64.02%|  \t216,281| 2.12%\nGLM 4.5 air | 57.14% | 909,228|  \t26.46%\nQwen-3.2-2507-thinking | 50.26% |  \t743,131|  \t20.63%\nKimi K2 | 34.92% | 67,071| 0%\nHunyuan A13B | 30.16% |  \t121,150 |  \t2.12%\nQwen-3.2-2507| 28.04% | 3,098|  \t0.53%\nMistral Small 3.2| 22.22% |  \t5,353| 0%\nGemma 3 27B | 17.99%  |  \t2,888|  \t0.53%~~~~  \n  \nEDIT : Added R1, Sonnet 4, Hunyuan A13b and Gemma 3 27b\n\nReasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)",
          "author_fullname": "t2_fbzx9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Benchmark - FamilyBench - Test models ability to understand complex tree type relationship and reason on massive context. Immune to contamination. GML 4.5 64.02%, Gemini 2.5 pro 81,48%.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc687c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753801202,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753778766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;This is a new &lt;strong&gt;opensource&lt;/strong&gt; project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context.   &lt;/p&gt;\n\n&lt;p&gt;The idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs.   &lt;/p&gt;\n\n&lt;p&gt;You can find the code here &lt;a href=\"https://github.com/Orolol/familyBench\"&gt;https://github.com/Orolol/familyBench&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current leaderboard&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.&lt;/p&gt;\n\n&lt;p&gt;Example of family description : &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Example of questions : &amp;quot;Which of Paula&amp;#39;s grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Model&lt;/th&gt;\n&lt;th&gt;Accuracy&lt;/th&gt;\n&lt;th&gt;Total tokens&lt;/th&gt;\n&lt;th&gt;No response rate&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;\n&lt;td&gt;81.48%&lt;/td&gt;\n&lt;td&gt;271,500&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;DeepSeek R1 0528&lt;/td&gt;\n&lt;td&gt;75.66%&lt;/td&gt;\n&lt;td&gt;150,642&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Sonnet 4&lt;/td&gt;\n&lt;td&gt;67.20%&lt;/td&gt;\n&lt;td&gt;575,624&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;GLM 4.5&lt;/td&gt;\n&lt;td&gt;64.02%&lt;/td&gt;\n&lt;td&gt;216,281&lt;/td&gt;\n&lt;td&gt;2.12%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;GLM 4.5 air&lt;/td&gt;\n&lt;td&gt;57.14%&lt;/td&gt;\n&lt;td&gt;909,228&lt;/td&gt;\n&lt;td&gt;26.46%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen-3.2-2507-thinking&lt;/td&gt;\n&lt;td&gt;50.26%&lt;/td&gt;\n&lt;td&gt;743,131&lt;/td&gt;\n&lt;td&gt;20.63%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Kimi K2&lt;/td&gt;\n&lt;td&gt;34.92%&lt;/td&gt;\n&lt;td&gt;67,071&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Hunyuan A13B&lt;/td&gt;\n&lt;td&gt;30.16%&lt;/td&gt;\n&lt;td&gt;121,150&lt;/td&gt;\n&lt;td&gt;2.12%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen-3.2-2507&lt;/td&gt;\n&lt;td&gt;28.04%&lt;/td&gt;\n&lt;td&gt;3,098&lt;/td&gt;\n&lt;td&gt;0.53%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Mistral Small 3.2&lt;/td&gt;\n&lt;td&gt;22.22%&lt;/td&gt;\n&lt;td&gt;5,353&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Gemma 3 27B&lt;/td&gt;\n&lt;td&gt;17.99%&lt;/td&gt;\n&lt;td&gt;2,888&lt;/td&gt;\n&lt;td&gt;0.53%~~~~&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;EDIT : Added R1, Sonnet 4, Hunyuan A13b and Gemma 3 27b&lt;/p&gt;\n\n&lt;p&gt;Reasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?auto=webp&amp;s=19958202ed21212a2e6bb842d061589134b9c755",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcc6e9e77205be821c357ea312fd60ae612baf42",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b5d26c31cee88c4ca71b2ceece0db8aabab9637e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03a3cd0f1a5e8eb77f5adb43153fa12c443ab921",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3af7542a187061f08a2c1c0dc31cc85c6549d96f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a9d11fb33d08918d37099a8ce87e35de6fb055b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e4de8942748959e32aa5af7dabf84050fcca647",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc687c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Orolol",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753778766,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_63nhk1l7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Told Qwen3 1.7b (thinking) to make a black hole simulation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc644b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e5xhwj4azrff1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 1280,
              "width": 718,
              "scrubber_media_url": "https://v.redd.it/e5xhwj4azrff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e5xhwj4azrff1/DASHPlaylist.mpd?a=1756430004%2CZjhkYmU2MGNhMzA2N2MwODE0Yzg2OTUyMWZiZjU5YjgzOTgxYzYwODUzODI5NGExNzMwYTgxNGUzOTljYjAyMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/e5xhwj4azrff1/HLSPlaylist.m3u8?a=1756430004%2CYjVjYTYwZDk1Zjc4OTVmZTYzMjIzOGZiMjkwMTk0YzRhYWEyYzc2MjFiMWY2MTc3MjE0MDQ5Y2I2ZTVkM2YyMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ee70a99f6ecb5bd697a2ca9affaab6f82ce0f664",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753778315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e5xhwj4azrff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?format=pjpg&amp;auto=webp&amp;s=af896c68cbffa8e627b8ffb22e577257ee016331",
                  "width": 806,
                  "height": 1438
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d4f7b64c9c9249f426fe6264ebe0ca68c9ccaee8",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b360f9ce854c7b5390001204eb84d38a765a00d1",
                    "width": 216,
                    "height": 385
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=af21a88c251c05a42ca5118d5bf46315948b41a1",
                    "width": 320,
                    "height": 570
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=698ad4dc76e8be804cfdf1038565ba0059e67379",
                    "width": 640,
                    "height": 1141
                  }
                ],
                "variants": {},
                "id": "czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mc644b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gold_Bar_4072",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc644b/told_qwen3_17b_thinking_to_make_a_black_hole/",
          "stickied": false,
          "url": "https://v.redd.it/e5xhwj4azrff1",
          "subreddit_subscribers": 506711,
          "created_utc": 1753778315,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e5xhwj4azrff1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 1280,
              "width": 718,
              "scrubber_media_url": "https://v.redd.it/e5xhwj4azrff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e5xhwj4azrff1/DASHPlaylist.mpd?a=1756430004%2CZjhkYmU2MGNhMzA2N2MwODE0Yzg2OTUyMWZiZjU5YjgzOTgxYzYwODUzODI5NGExNzMwYTgxNGUzOTljYjAyMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/e5xhwj4azrff1/HLSPlaylist.m3u8?a=1756430004%2CYjVjYTYwZDk1Zjc4OTVmZTYzMjIzOGZiMjkwMTk0YzRhYWEyYzc2MjFiMWY2MTc3MjE0MDQ5Y2I2ZTVkM2YyMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1quxz8adxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you suggest a better WebUI program for textgen that has better memory management than Oobabooga?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 67,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5s4r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AHXhpyKFoqdWC0Wt2obxlzKLHfQkneSC8EAbHzX8DHM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753776970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6td8j8oqurff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6td8j8oqurff1.png?auto=webp&amp;s=3e2ac6a469856670cb774ff7877328964d6fe929",
                  "width": 493,
                  "height": 236
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=15089514a1f6bb2ae2d28bbca6f69f6e4015060c",
                    "width": 108,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df7f3682f6fca02286751db4ca2e802700377750",
                    "width": 216,
                    "height": 103
                  },
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da5d78f6e7e675a867acf4adf3ee9157dac8ae16",
                    "width": 320,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "6Ut9VePHJUzFLTvEDM1K0LWAXqNdQjgjbFkzQ0DP9xg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc5s4r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-Fibon4cci",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5s4r/can_you_suggest_a_better_webui_program_for/",
          "stickied": false,
          "url": "https://i.redd.it/6td8j8oqurff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753776970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**GLM 4.5 and GLM-4.5-AIR**  \nThe **GLM-4.5** series models are foundation models designed for intelligent agents. GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\n[Bench performance](https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;format=png&amp;auto=webp&amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c)\n\n   \n[blog](https://z.ai/blog/glm-4.5)｜[huggingface](https://huggingface.co/zai-org/GLM-4.5)｜ [github](https://github.com/zai-org/GLM-4.5)  \n",
          "author_fullname": "t2_dpf3bqut",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This year’s best open-source models and most cost-effective models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bisgmn0utrff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74baaad6fd3f7a8d7dc00be88805a7bc35dba7f3"
                },
                {
                  "y": 153,
                  "x": 216,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=14edcaddf4b8e04722376443d659c7ed6be70b08"
                },
                {
                  "y": 227,
                  "x": 320,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b73c9df7a36a1142a626ccad114a23ea69213c7b"
                },
                {
                  "y": 455,
                  "x": 640,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ba9ef9c35dd3df135cdd3b9afc5d2c950091c3"
                },
                {
                  "y": 683,
                  "x": 960,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9de3ec1904c5a60f19e2f1e6c79e12f0e8b303d0"
                },
                {
                  "y": 768,
                  "x": 1080,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f0f776c64bbc2dcef8d1d00af6cbc89503e4768"
                }
              ],
              "s": {
                "y": 3177,
                "x": 4464,
                "u": "https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;format=png&amp;auto=webp&amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c"
              },
              "id": "bisgmn0utrff1"
            }
          },
          "name": "t3_1mc5oh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 101,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/oqyuYVJJYg1zSXUWu9TgdBdJGts5YfXbLSB6jfU2bbs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753776573,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.5 and GLM-4.5-AIR&lt;/strong&gt;&lt;br/&gt;\nThe &lt;strong&gt;GLM-4.5&lt;/strong&gt; series models are foundation models designed for intelligent agents. GLM-4.5 has &lt;strong&gt;355&lt;/strong&gt; billion total parameters with &lt;strong&gt;32&lt;/strong&gt; billion active parameters, while GLM-4.5-Air adopts a more compact design with &lt;strong&gt;106&lt;/strong&gt; billion total parameters and &lt;strong&gt;12&lt;/strong&gt; billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c\"&gt;Bench performance&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;blog&lt;/a&gt;｜&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;huggingface&lt;/a&gt;｜ &lt;a href=\"https://github.com/zai-org/GLM-4.5\"&gt;github&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc5oh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Apart-River475",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753776573,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve got more than 100 hours of clean, studio-grade speech for a character, and I’d like to explore what the SOTA is for open source voice cloning or voice changing. \n\nIs the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.",
          "author_fullname": "t2_fmblw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best open source voice cloning today, with hours of reference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5jsx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753776051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve got more than 100 hours of clean, studio-grade speech for a character, and I’d like to explore what the SOTA is for open source voice cloning or voice changing. &lt;/p&gt;\n\n&lt;p&gt;Is the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc5jsx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goldcakes",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753776051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We put together a small repo to fine‑tune **Mistral’s Voxtral (3B)** for **transcription** using Huggingface**.** We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.",
          "author_fullname": "t2_1ujlvp0cn8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finetuning Script for Voxtral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5gv1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4dc0e71933a6538903c2ef9dc0036f8bd6a8fda2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753775738,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We put together a small repo to fine‑tune &lt;strong&gt;Mistral’s Voxtral (3B)&lt;/strong&gt; for &lt;strong&gt;transcription&lt;/strong&gt; using Huggingface&lt;strong&gt;.&lt;/strong&gt; We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?auto=webp&amp;s=834d343f2b6c42de29b825f4bdecbe668798481b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fa9e2af93fe23af7ed5ae8ef0282b5932cf5efa",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d90e7322acc87ab0f6078ff5baa320612813f4c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fd2e07935dfb13e8c24f66136ca02893cd3bf41",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d821b402151de285d39de64aaea0364ad627ae9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b86b9f72b4e9b269f0d8aea81947c8cbf95b360",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efc53978201c483ccf78407c682b2a9b164dff7a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc5gv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistributionLucky763",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5gv1/finetuning_script_for_voxtral/",
          "stickied": false,
          "url": "https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune",
          "subreddit_subscribers": 506711,
          "created_utc": 1753775738,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.\n\nIt works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. \\~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to \\~70.\n\nThe CUDA version is built upon my qwen.c repo. It's a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.\n\nBoth versions use the GGUF file directly, with no conversion to binary. The tokenizer’s vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.\n\nThese projects draw inspiration from Andrej Karpathy’s [llama2.c](https://github.com/karpathy/llama2.c) and share the same commitment to minimalism. Both projects are MIT licensed. I’d love to hear your feedback!\n\nqwen3.cu: [https://github.com/gigit0000/qwen3.cu](https://github.com/gigit0000/qwen3.cu)\n\nqwen3.c: [https://github.com/gigit0000/qwen3.c](https://github.com/gigit0000/qwen3.c)",
          "author_fullname": "t2_kfu7x339m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Single-File Qwen3 Inference in Pure CUDA C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5e54",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753775439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.&lt;/p&gt;\n\n&lt;p&gt;It works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. ~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to ~70.&lt;/p&gt;\n\n&lt;p&gt;The CUDA version is built upon my qwen.c repo. It&amp;#39;s a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.&lt;/p&gt;\n\n&lt;p&gt;Both versions use the GGUF file directly, with no conversion to binary. The tokenizer’s vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.&lt;/p&gt;\n\n&lt;p&gt;These projects draw inspiration from Andrej Karpathy’s &lt;a href=\"https://github.com/karpathy/llama2.c\"&gt;llama2.c&lt;/a&gt; and share the same commitment to minimalism. Both projects are MIT licensed. I’d love to hear your feedback!&lt;/p&gt;\n\n&lt;p&gt;qwen3.cu: &lt;a href=\"https://github.com/gigit0000/qwen3.cu\"&gt;https://github.com/gigit0000/qwen3.cu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;qwen3.c: &lt;a href=\"https://github.com/gigit0000/qwen3.c\"&gt;https://github.com/gigit0000/qwen3.c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?auto=webp&amp;s=b87dc526d65dc903b76c415404d32f3bdbff0963",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd05cb170e306c505c4104b96edb3c670cf24b48",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb54c005f706a393effe1c3002c30653b11607bf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3800484811cbe29ca44c0f3713d9faca1e06531",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de554ecb50aa8f1ad0aa1ca60137d36a4be1ffe1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c55c183a33bb16b6df8879b5b4136746ad2f9d97",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=864a95c03a2b88f3bd8d6a9779c0295bd87c1174",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mc5e54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Awkward_Click6271",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753775439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Busy with some projects, so I haven't checked out the LLM space in a little while. I come back, and there are 200-something Arxiv papers I need to read, dozens of new models, github repos to try out etc etc.\n\n\nHow do you keep yourself updated? This is nuts.\n\n\nPS: just had an idea for a pipeline from Arxiv PDFs --&gt; NotebookLM --&gt; daily AIGen podcast summarizing SOTA approaches and new research",
          "author_fullname": "t2_c9j5fpaz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you keep yourself updated?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc4y83",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "author_cakeday": true,
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753773681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Busy with some projects, so I haven&amp;#39;t checked out the LLM space in a little while. I come back, and there are 200-something Arxiv papers I need to read, dozens of new models, github repos to try out etc etc.&lt;/p&gt;\n\n&lt;p&gt;How do you keep yourself updated? This is nuts.&lt;/p&gt;\n\n&lt;p&gt;PS: just had an idea for a pipeline from Arxiv PDFs --&amp;gt; NotebookLM --&amp;gt; daily AIGen podcast summarizing SOTA approaches and new research&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc4y83",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "noellarkin",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc4y83/how_do_you_keep_yourself_updated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc4y83/how_do_you_keep_yourself_updated/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753773681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm going to put 2x mi210 GPUs into my home server this week and I havent ran local LLMs in this setting before.\n\nAny recommendations on good LLMs to use with mi210s? Will be a bit capped for the moment at 32GB of DDR4 and only PCIE 3.0",
          "author_fullname": "t2_g9wit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any interesting local LLM options for a home server that's about to have 2x mi210 GPUs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc2ibo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753767027,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753764740,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going to put 2x mi210 GPUs into my home server this week and I havent ran local LLMs in this setting before.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on good LLMs to use with mi210s? Will be a bit capped for the moment at 32GB of DDR4 and only PCIE 3.0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc2ibo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "totemoheta",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc2ibo/any_interesting_local_llm_options_for_a_home/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc2ibo/any_interesting_local_llm_options_for_a_home/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753764740,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone connected 2–3 Mac Studio M3 Ultra machines (512GB RAM, Thunderbolt 5 / 80 Gbps) into a distributed AI cluster? I’m looking for benchmarks or evidence of running large models (e.g., Kimi K2, Qwen 3 coder) across multiple units. Found nothing on YouTube. Has this been done, or is it unexplored territory?",
          "author_fullname": "t2_ll5g2ocp6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "~2–3 x Mac Studios M3 Ultra (512GB) Cluster for Large Model Inference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc253f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753763498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone connected 2–3 Mac Studio M3 Ultra machines (512GB RAM, Thunderbolt 5 / 80 Gbps) into a distributed AI cluster? I’m looking for benchmarks or evidence of running large models (e.g., Kimi K2, Qwen 3 coder) across multiple units. Found nothing on YouTube. Has this been done, or is it unexplored territory?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc253f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Copy8702",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc253f/23_x_mac_studios_m3_ultra_512gb_cluster_for_large/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc253f/23_x_mac_studios_m3_ultra_512gb_cluster_for_large/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753763498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't remember what it's called because I'm sleep deprived rn, but I remember seeing a fairly new thing come out recently that was essentially a vision model watching your screen for something to happen and then it could react for you in some minimal ways.\n\nHas anyone set up one of those to run with instructions to send a prompt to a language model based on what's happening on the screen? It would be insane to be able to just let the LLM whack away at debugging my shitty code without me to babysit. Instead of tediously feeding errors into cline in vscode, it would be a great time saver to let the models just run until the script or features just works, and then they shutdown or something. \n\nAny other neat uses for these kinds of visual agents? Or other agentic use of models? I'm really only familiar with agentic in terms of letting the model live in my VS Code to make changes to my files directly.",
          "author_fullname": "t2_1loou9xu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vision agent for AFK gains?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc239f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753763329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t remember what it&amp;#39;s called because I&amp;#39;m sleep deprived rn, but I remember seeing a fairly new thing come out recently that was essentially a vision model watching your screen for something to happen and then it could react for you in some minimal ways.&lt;/p&gt;\n\n&lt;p&gt;Has anyone set up one of those to run with instructions to send a prompt to a language model based on what&amp;#39;s happening on the screen? It would be insane to be able to just let the LLM whack away at debugging my shitty code without me to babysit. Instead of tediously feeding errors into cline in vscode, it would be a great time saver to let the models just run until the script or features just works, and then they shutdown or something. &lt;/p&gt;\n\n&lt;p&gt;Any other neat uses for these kinds of visual agents? Or other agentic use of models? I&amp;#39;m really only familiar with agentic in terms of letting the model live in my VS Code to make changes to my files directly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc239f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Shadow-Amulet-Ambush",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc239f/vision_agent_for_afk_gains/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc239f/vision_agent_for_afk_gains/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753763329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey y'all, have this 512gb mac ultra Ive been enjoying running LLMs for local text and code generation.\n\nI wanna dabble into image generation, specifically thinking of feeding my cat's photos to a model and have it augment it into artistic styles/ place my cat on planets etc. Whats a good model available to do this?\n\nPrefer mlx-lm compatible as I've already got scripts set up, but can also use one of the packaged frameworks like ollama or something.",
          "author_fullname": "t2_cltk5172",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Image/Stable Diffusion model that can work with MLX?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc22jg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753763261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all, have this 512gb mac ultra Ive been enjoying running LLMs for local text and code generation.&lt;/p&gt;\n\n&lt;p&gt;I wanna dabble into image generation, specifically thinking of feeding my cat&amp;#39;s photos to a model and have it augment it into artistic styles/ place my cat on planets etc. Whats a good model available to do this?&lt;/p&gt;\n\n&lt;p&gt;Prefer mlx-lm compatible as I&amp;#39;ve already got scripts set up, but can also use one of the packaged frameworks like ollama or something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc22jg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amazing_Trace",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc22jg/best_imagestable_diffusion_model_that_can_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc22jg/best_imagestable_diffusion_model_that_can_work/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753763261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a student studying Anatomy, Physiology, and Medical Terminology. I want to generate Anki flashcards from PDF paragraphs and think a local LLM could save me a lot of time. Any advice on models or setups that work well for this use case would be appreciated. Thanks!",
          "author_fullname": "t2_rwiiqztwx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "First time setting up a local LLM, looking for model suggestions to create Anki formatted flashcards",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc0vyb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753759558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a student studying Anatomy, Physiology, and Medical Terminology. I want to generate Anki flashcards from PDF paragraphs and think a local LLM could save me a lot of time. Any advice on models or setups that work well for this use case would be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc0vyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HighLowMystery",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc0vyb/first_time_setting_up_a_local_llm_looking_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc0vyb/first_time_setting_up_a_local_llm_looking_for/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753759558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://arxiv.org/abs/2507.20984](https://arxiv.org/abs/2507.20984)\n\n**SmallThinker** is a family of on-device native **Mixture-of-Experts** language models specifically designed for efficient local deployment.  With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.\n\nEven on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of **20 tokens per second** when powered by [PowerInfer](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)\n\nNotably, **SmallThinker** is now supported in **llama.cpp**, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.\n\n\n\nhttps://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;format=png&amp;auto=webp&amp;s=d014c217defcd629cbb8684dc891878d2895c28b\n\nAnd here is the downstream benchmark performance compare to other SOTA LLMs.\n\nhttps://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;format=png&amp;auto=webp&amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf\n\nAnd the GGUF link is here:\n\n[PowerInfer/SmallThinker-21BA3B-Instruct-GGUF · Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF)\n\n[PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF · Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF)",
          "author_fullname": "t2_s05p1qg4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker Technical Report Release!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m5vbkud89qff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c932172d474d64f53b1c183c3158d63819e7dd1"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb0c7dda219290760f59b222d805ad327b3534d7"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29a83153cdfa17c52a31a9ea0c17b5646cdc9145"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=228e39bea9e0d0bb6c950b4d12e940f2a313db1c"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f34de7a4ec519d836060dfaf8ff7056a209173"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e06afbc504d247f77732dd28fabe34402bcec63"
                }
              ],
              "s": {
                "y": 646,
                "x": 1382,
                "u": "https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;format=png&amp;auto=webp&amp;s=d014c217defcd629cbb8684dc891878d2895c28b"
              },
              "id": "m5vbkud89qff1"
            },
            "2zk0d3sqbqff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0ffa126ccf8fbe0beb24c6090b1763a5a7a7222"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f250c3501e2d2066624356e94c3976200aa629c9"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ae16445ad433c0ead9ef47c474be0ecadee2bb"
                },
                {
                  "y": 343,
                  "x": 640,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f325d98e1cc1b5cb4bbd02da5e1a005319b57a37"
                },
                {
                  "y": 515,
                  "x": 960,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d25b2a53244d7d2f5aca474eb1076424d90918"
                },
                {
                  "y": 579,
                  "x": 1080,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09f24f32012f06d40275d312a6327f1d86691a4d"
                }
              ],
              "s": {
                "y": 830,
                "x": 1546,
                "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;format=png&amp;auto=webp&amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf"
              },
              "id": "2zk0d3sqbqff1"
            }
          },
          "name": "t3_1mc0m3e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DPYXkXYKiJVkQ40-jlvcuMdmOUBGPiWDPqFYKHNtroQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753758732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2507.20984\"&gt;https://arxiv.org/abs/2507.20984&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SmallThinker&lt;/strong&gt; is a family of on-device native &lt;strong&gt;Mixture-of-Experts&lt;/strong&gt; language models specifically designed for efficient local deployment.  With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.&lt;/p&gt;\n\n&lt;p&gt;Even on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of &lt;strong&gt;20 tokens per second&lt;/strong&gt; when powered by &lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker\"&gt;PowerInfer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Notably, &lt;strong&gt;SmallThinker&lt;/strong&gt; is now supported in &lt;strong&gt;llama.cpp&lt;/strong&gt;, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b\"&gt;https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here is the downstream benchmark performance compare to other SOTA LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf\"&gt;https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the GGUF link is here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF\"&gt;PowerInfer/SmallThinker-21BA3B-Instruct-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF\"&gt;PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc0m3e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zealousideal_Bad_52",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753758732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello Folks,\nWith new open LLMs being released constantly, I’m starting to feel a bit behind, especially since most of them are pretty large. I have around 180 GB of NVIDIA GPU VRAM available and I’m looking for the best coding LLM to run locally with atleast 30K context window (input + output). My main focus is Java programming. \nI am currently using Qwen3 32B Thinking non quantized but the results are just okayish.\n\nPS: I have used Qwen 2.5 Coder but the results were terrible. Also, used QwQ-32B and the results were slightly worse than Qwen3 32B but were also much much slower.\n\nAny recommendations would be highly appreciated, Thanks!",
          "author_fullname": "t2_1c2mqjxrgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Coding LLM for",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbzdx8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753757742,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753755092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Folks,\nWith new open LLMs being released constantly, I’m starting to feel a bit behind, especially since most of them are pretty large. I have around 180 GB of NVIDIA GPU VRAM available and I’m looking for the best coding LLM to run locally with atleast 30K context window (input + output). My main focus is Java programming. \nI am currently using Qwen3 32B Thinking non quantized but the results are just okayish.&lt;/p&gt;\n\n&lt;p&gt;PS: I have used Qwen 2.5 Coder but the results were terrible. Also, used QwQ-32B and the results were slightly worse than Qwen3 32B but were also much much slower.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations would be highly appreciated, Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbzdx8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PhysicsPast8286",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbzdx8/best_coding_llm_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbzdx8/best_coding_llm_for/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753755092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m working on a local AI agent and wanted to move beyond hand-crafted prompts by optimizing them automatically. I initially looked into soft prompt tuning, but since I’m using quantized models (Qwen3-4B/8B Q8_0) through ollama and llama.cpp on a 3050 laptop GPU, I can’t access gradients directly from the model.\n\nThat’s when I found PEZ (Hard Prompts Made Easy), which stood out as a clever workaround. It works by:\n- Optimizing prompts in the continuous embedding space\n- Projecting them back to discrete tokens\n- Using the standard loss function for supervision\n- Applying gradients to improve the continuous embeddings\n\nThis ultimately gives you discreet text prompts that can be used with any inference engine—no model modification or access to internal embeddings needed.\n- Paper: https://arxiv.org/abs/2302.03668\n- Code: https://github.com/YuxinWenRick/hard-prompts-made-easy\n\nHas anyone else experimented with PEZ, or other learned hard prompt optimization methods that work well with local models and quantized inference?\n\nTo be clear:\n- I’m not looking for DSPy-style systems\n- I’m aiming for lightweight methods that are compatible with local inference setups\n- Bonus if it works with quantized models or can train prompts on top of them offline\n\nWould love to hear what others are using to optimize agent behavior without resorting to full model fine-tuning or even LoRA.",
          "author_fullname": "t2_s31fjsz6p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone used PEZ or similar learned hard prompt methods for local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mby6nd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m working on a local AI agent and wanted to move beyond hand-crafted prompts by optimizing them automatically. I initially looked into soft prompt tuning, but since I’m using quantized models (Qwen3-4B/8B Q8_0) through ollama and llama.cpp on a 3050 laptop GPU, I can’t access gradients directly from the model.&lt;/p&gt;\n\n&lt;p&gt;That’s when I found PEZ (Hard Prompts Made Easy), which stood out as a clever workaround. It works by:\n- Optimizing prompts in the continuous embedding space\n- Projecting them back to discrete tokens\n- Using the standard loss function for supervision\n- Applying gradients to improve the continuous embeddings&lt;/p&gt;\n\n&lt;p&gt;This ultimately gives you discreet text prompts that can be used with any inference engine—no model modification or access to internal embeddings needed.\n- Paper: &lt;a href=\"https://arxiv.org/abs/2302.03668\"&gt;https://arxiv.org/abs/2302.03668&lt;/a&gt;\n- Code: &lt;a href=\"https://github.com/YuxinWenRick/hard-prompts-made-easy\"&gt;https://github.com/YuxinWenRick/hard-prompts-made-easy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experimented with PEZ, or other learned hard prompt optimization methods that work well with local models and quantized inference?&lt;/p&gt;\n\n&lt;p&gt;To be clear:\n- I’m not looking for DSPy-style systems\n- I’m aiming for lightweight methods that are compatible with local inference setups\n- Bonus if it works with quantized models or can train prompts on top of them offline&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what others are using to optimize agent behavior without resorting to full model fine-tuning or even LoRA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mby6nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HadesTerminal",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby6nd/has_anyone_used_pez_or_similar_learned_hard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby6nd/has_anyone_used_pez_or_similar_learned_hard/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753751656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nHave been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.\n\nWanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).\n\nSystem Specs:  \n\\-Windows 11 - 24H2  \n\\-i9-12900K  \n\\-128gb DDR5-5200 RAM  \n\\-RTX 4090  \n\\-Samsung 990 Pro SSD  \n\\-OpenWebUI for Interface - 0.6.18  \n\\-Ollama to run the model - 0.9.6\n\nHave gotten the best T/S (4.17) with:  \n\\-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4\\_XS  \n\\-Stop Sequence - \"&lt;|im\\_start|&gt;\",\"&lt;|im\\_end|&gt;\"  \n\\-top\\_k - 20  \n\\-top\\_p - 0.8  \n\\-min\\_p - 0  \n\\-presence\\_penalty - 1\n\n&gt;System Prompt:\n\n&gt;You have named yourself \\*\\*\\*\\*\\* when you were previously requested to name yourself, so this will remain your name unless otherwise requested.\n\n&gt;You are hosted in \\*\\*\\*\\*\\*\\*\\*\\*\\* and are primarily being used in \\*\\*\\*\\*\\*\\*\\*\\*\\*. This is being done on a personal computer at a personal residence.\n\n&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.\n\nMain two issues I run into, when I do an initial question, Qwen starts by adding it's own question, and then proceeds as though that was part of my question:\n\nAre you familiar with Schrödinger's cat? And how it implies that reality is not set until it’s observed?\n\n&gt;\\*\\*\\*\\*\\* - NOR-235B\n\n&gt;Also, what exactly was Erwin Schrödinger trying to explain with his famous thought experiment involving a cat in a box?\n\n&gt;Okay, the user is asking about Schrödinger's cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition—both decayed and not decayed states exist simultaneously.\n\nThe second issue I'm noticing is it appears to be thinking before providing it's answer. This is the updated instruct model which isn't supposed to think? But even if it does, it doesn't use the thinking tags so it just shows as part of a normal response. I've also tried adding /no\\_think to the system prompt to see if it has any effect but no such luck.\n\nCan I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)\n\nThank you.",
          "author_fullname": "t2_9npiw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B 2507 adding its own questions to mine, and thinking despite being Instruct model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mby5ct",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753751744,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Have been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.&lt;/p&gt;\n\n&lt;p&gt;Wanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;br/&gt;\n-Windows 11 - 24H2&lt;br/&gt;\n-i9-12900K&lt;br/&gt;\n-128gb DDR5-5200 RAM&lt;br/&gt;\n-RTX 4090&lt;br/&gt;\n-Samsung 990 Pro SSD&lt;br/&gt;\n-OpenWebUI for Interface - 0.6.18&lt;br/&gt;\n-Ollama to run the model - 0.9.6&lt;/p&gt;\n\n&lt;p&gt;Have gotten the best T/S (4.17) with:&lt;br/&gt;\n-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4_XS&lt;br/&gt;\n-Stop Sequence - &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;,&amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;&lt;br/&gt;\n-top_k - 20&lt;br/&gt;\n-top_p - 0.8&lt;br/&gt;\n-min_p - 0&lt;br/&gt;\n-presence_penalty - 1&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;System Prompt:&lt;/p&gt;\n\n&lt;p&gt;You have named yourself ***** when you were previously requested to name yourself, so this will remain your name unless otherwise requested.&lt;/p&gt;\n\n&lt;p&gt;You are hosted in ********* and are primarily being used in *********. This is being done on a personal computer at a personal residence.&lt;/p&gt;\n\n&lt;p&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Main two issues I run into, when I do an initial question, Qwen starts by adding it&amp;#39;s own question, and then proceeds as though that was part of my question:&lt;/p&gt;\n\n&lt;p&gt;Are you familiar with Schrödinger&amp;#39;s cat? And how it implies that reality is not set until it’s observed?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;***** - NOR-235B&lt;/p&gt;\n\n&lt;p&gt;Also, what exactly was Erwin Schrödinger trying to explain with his famous thought experiment involving a cat in a box?&lt;/p&gt;\n\n&lt;p&gt;Okay, the user is asking about Schrödinger&amp;#39;s cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition—both decayed and not decayed states exist simultaneously.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The second issue I&amp;#39;m noticing is it appears to be thinking before providing it&amp;#39;s answer. This is the updated instruct model which isn&amp;#39;t supposed to think? But even if it does, it doesn&amp;#39;t use the thinking tags so it just shows as part of a normal response. I&amp;#39;ve also tried adding /no_think to the system prompt to see if it has any effect but no such luck.&lt;/p&gt;\n\n&lt;p&gt;Can I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mby5ct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMattSz",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753751558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using continue.dev in vscode, I have qwen2.5 coder configured to work in it.\n\nI cannot manage to have my codebase indexed, which is the whole purpose of using this.\n\nIt seems like it should be simple, and allegedly it is supposed to work out of the box. \n\nBut I’ve been troubleshooting since yesterday and I still can’t find a solution. \n\nNothing like @codebase or initialize command, or force reindex via command palette in vscode changes anything.\n\nI have even deleted the index folder and watched as it gets rebuilt when I open my project/continue again in vscode.\n\nDoes anybody have any experience with this or able to offer insight?\n\nThanks",
          "author_fullname": "t2_doeylx0c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can’t get continue.dev to index my codebase",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbxx64",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753750926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using continue.dev in vscode, I have qwen2.5 coder configured to work in it.&lt;/p&gt;\n\n&lt;p&gt;I cannot manage to have my codebase indexed, which is the whole purpose of using this.&lt;/p&gt;\n\n&lt;p&gt;It seems like it should be simple, and allegedly it is supposed to work out of the box. &lt;/p&gt;\n\n&lt;p&gt;But I’ve been troubleshooting since yesterday and I still can’t find a solution. &lt;/p&gt;\n\n&lt;p&gt;Nothing like @codebase or initialize command, or force reindex via command palette in vscode changes anything.&lt;/p&gt;\n\n&lt;p&gt;I have even deleted the index folder and watched as it gets rebuilt when I open my project/continue again in vscode.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have any experience with this or able to offer insight?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbxx64",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SlimPerceptions",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbxx64/cant_get_continuedev_to_index_my_codebase/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbxx64/cant_get_continuedev_to_index_my_codebase/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753750926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting of products lets say mobile phones for customers, best practices to format synthetic data in particular way for example if data is not working LLM should diagnose step by step and suggest solution. ",
          "author_fullname": "t2_x197f72od",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbx6zk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753748899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting of products lets say mobile phones for customers, best practices to format synthetic data in particular way for example if data is not working LLM should diagnose step by step and suggest solution. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbx6zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Easy_Alps_1162",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbx6zk/suggestions_to_fine_tune_gemma_3n_e4b_or_similar/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbx6zk/suggestions_to_fine_tune_gemma_3n_e4b_or_similar/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753748899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We knew those tests were BS:\n\n“The agent provides real-time narration of its actions, stating \"The link is inserted, so now I'll click the 'Verify you are human' checkbox to complete the verification on Cloudflare. This step is necessary to prove I'm not a bot and proceed with the action.\"\n\nhttps://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/",
          "author_fullname": "t2_93dd3qj6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "“This step is necessary to prove that I am not a bot” LOL",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbwvve",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753748044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We knew those tests were BS:&lt;/p&gt;\n\n&lt;p&gt;“The agent provides real-time narration of its actions, stating &amp;quot;The link is inserted, so now I&amp;#39;ll click the &amp;#39;Verify you are human&amp;#39; checkbox to complete the verification on Cloudflare. This step is necessary to prove I&amp;#39;m not a bot and proceed with the action.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/\"&gt;https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?auto=webp&amp;s=efca23a0898df3aa26b546bf67e6a5efc4b12d2d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fe50f25abd0aace1b9b4c4392c70d25338fbf87",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0880f0eb712ed84828373bd88b3a60717d3eeb2",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e7025badbea8b9fbce8975d428011e915068ee",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fd7afda0afe8b0d61aaf28252bff681b39574f2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=149ee2d02c69538430b4bbe7096768de3fe589a0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b947f0a8b37f9feda1eff78896e0d3e3fc36e7a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbwvve",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glass-Garbage4818",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbwvve/this_step_is_necessary_to_prove_that_i_am_not_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbwvve/this_step_is_necessary_to_prove_that_i_am_not_a/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753748044,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.\n\nI'd like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like *\"how do I work on product1's source code?\" or \"What is the serial communication protocol between product2 and product3?\", \"how am I supposed to interact with product3?\"*, and so on. \n\nNo coding questions, more like general guidance and onboarding, which is doable even by small models I think.\n\nIn the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.\n\nSome thoughts:\n\n* Putting all the raw data in the same request for a flagship model easily exceeds the context limit\n* Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I'm looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I'm just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.\n* I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.\n* Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.\n* To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: *\"The user has requested: $original_prompt. Answer the user's question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3\"*. Unless LM Studio is the most ghetto RAG implementation in existence and there's a lot of much nicer options, I honestly wouldn't want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn't even a match means it just poisoned the context. Honestly if it wasn't for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.\n\nAnyway what's your advice?\n\nEDIT: despite the title, I'm open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there's some other solution that solves this problem in a smart way (ie not just \"run ElasticSearch\", but something that can connect the dots on its own like an LLM does) I'm happy to hear about it.",
          "author_fullname": "t2_93yn32gx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I train a good LLM on my company's doc in order to answer easy questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbviok",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753744987,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753744434,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like &lt;em&gt;&amp;quot;how do I work on product1&amp;#39;s source code?&amp;quot; or &amp;quot;What is the serial communication protocol between product2 and product3?&amp;quot;, &amp;quot;how am I supposed to interact with product3?&amp;quot;&lt;/em&gt;, and so on. &lt;/p&gt;\n\n&lt;p&gt;No coding questions, more like general guidance and onboarding, which is doable even by small models I think.&lt;/p&gt;\n\n&lt;p&gt;In the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.&lt;/p&gt;\n\n&lt;p&gt;Some thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Putting all the raw data in the same request for a flagship model easily exceeds the context limit&lt;/li&gt;\n&lt;li&gt;Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I&amp;#39;m looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I&amp;#39;m just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.&lt;/li&gt;\n&lt;li&gt;I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.&lt;/li&gt;\n&lt;li&gt;Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.&lt;/li&gt;\n&lt;li&gt;To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: &lt;em&gt;&amp;quot;The user has requested: $original_prompt. Answer the user&amp;#39;s question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3&amp;quot;&lt;/em&gt;. Unless LM Studio is the most ghetto RAG implementation in existence and there&amp;#39;s a lot of much nicer options, I honestly wouldn&amp;#39;t want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn&amp;#39;t even a match means it just poisoned the context. Honestly if it wasn&amp;#39;t for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyway what&amp;#39;s your advice?&lt;/p&gt;\n\n&lt;p&gt;EDIT: despite the title, I&amp;#39;m open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there&amp;#39;s some other solution that solves this problem in a smart way (ie not just &amp;quot;run ElasticSearch&amp;quot;, but something that can connect the dots on its own like an LLM does) I&amp;#39;m happy to hear about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbviok",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dtdisapointingresult",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753744434,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player\n\nI extended [my work here](https://www.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/) to support Apple Intelligence models so it becomes OpenAI / Ollama Compatible. That means you can use it literally anywhere. \n\nHere I'm using it as github copilot model in vs code, I tried it also in openwebui and raycast and it worked perfectly!\n\n[GitHub Link](https://github.com/0ssamaak0/MackingJAI)",
          "author_fullname": "t2_3wnw8gja",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Apple Intelligence as OpenAI / Ollama API",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "lksxirmo5pff1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mbvgdm/asset/lksxirmo5pff1/DASHPlaylist.mpd?a=1756430004%2CYzg3ZjFkN2QyZTI4NDE3ZTU1YjVkZDI3MTFjZTcxZGViOGUxMDgzZmM4ZjNlMjU4MGUwZGFmZTg4NDcyM2JmZA%3D%3D&amp;v=1&amp;f=sd",
              "x": 1706,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1mbvgdm/asset/lksxirmo5pff1/HLSPlaylist.m3u8?a=1756430004%2CMmE0ZGYzZTNkYjg4YjZmYmJhYTU2ZWNlNTI4NzgwN2RlZDgzMzliNmVmYjJmOTFjNGY0OTRjODQ2ZDA5MGYyZg%3D%3D&amp;v=1&amp;f=sd",
              "id": "lksxirmo5pff1",
              "isGif": false
            }
          },
          "name": "t3_1mbvgdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4cc711a5088ed06142a2402fbaefaedd65ed5bc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753744264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player\"&gt;https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I extended &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/\"&gt;my work here&lt;/a&gt; to support Apple Intelligence models so it becomes OpenAI / Ollama Compatible. That means you can use it literally anywhere. &lt;/p&gt;\n\n&lt;p&gt;Here I&amp;#39;m using it as github copilot model in vs code, I tried it also in openwebui and raycast and it worked perfectly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/0ssamaak0/MackingJAI\"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?auto=webp&amp;s=b4bf3806d8e73a2b8a4a8d56c0738f7bbe7d9c7d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dfd94c7b8c32fc476cb450249ff47676d36e890",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e65e2ae62eb36080d3ab9b93702459624df23d50",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b3037aa56f0795df696733a20bd317e557e53f1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b299af9c40c1fa24a470a41d97558441055f70f1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab6764e24457bf2584e6942b1d554a6b5ccdb460",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcd1ca780f0c969c11cd12940e3f8211624fe1fc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbvgdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0ssamaak0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvgdm/using_apple_intelligence_as_openai_ollama_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbvgdm/using_apple_intelligence_as_openai_ollama_api/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753744264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "its getting comical",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 136,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbvf2z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 977,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 977,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/aArydVtwEJ7yR_8IVkCHCK5ydQGsUUwRNjJX3SBpIk4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753744170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/txsukljc5pff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/txsukljc5pff1.png?auto=webp&amp;s=07d6d7cad1797c689e38509b4184dc26106493ee",
                  "width": 373,
                  "height": 365
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=66753ef377dde5550d636917de9e12b2834fb31c",
                    "width": 108,
                    "height": 105
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3a44fe047ec31803031afef6a49f18f7985d89d",
                    "width": 216,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d",
                    "width": 320,
                    "height": 313
                  }
                ],
                "variants": {},
                "id": "xShm2r7nwbpzJdxhn7AN663aC50Z0tC9c3BxqruE-VA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mbvf2z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 92,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/",
          "stickied": false,
          "url": "https://i.redd.it/txsukljc5pff1.png",
          "subreddit_subscribers": 506711,
          "created_utc": 1753744170,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is a 5080 enough?",
          "author_fullname": "t2_1oi7u8rf2e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to use llama 7b to check if a 5-7 sentence paragraph contains a given subject, what's the minimum GPU I need?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbutu4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753742663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is a 5080 enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbutu4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "math_calculus1",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbutu4/i_want_to_use_llama_7b_to_check_if_a_57_sentence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbutu4/i_want_to_use_llama_7b_to_check_if_a_57_sentence/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753742663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Having only focused on LLM applications around utility (home assistant, scheduling, et.) I have recently been experimenting a lot with AI companions.  How do people introduce emotions or response modifiers through a conversation to make it seem more ‘real’\n\nI have tried the following with mixed results. \n\nConversation memory recalls, compare input embedding to past convo (knowledge graph concept). Same concept but emotional language recall (sentiment analysis) both of these are ok to stay on topic but don’t introduce opportunities for spontaneous divergence in the conversation.\n\nSystem prompt/dynaimc sp similar sentiment analysis and then swap out 6 pre made sp’s (happy,sad, etc.)\n\nInjections in a reasoning model CoT basically I run response for 50 token, stop, add some sentiment steering language, then let it finish the &lt;think&gt; step\n\nWhat do others do? Any papers or research on this topic? So far most of the time it’s still a ‘yes-man’ not to far below the surface \n",
          "author_fullname": "t2_t0zjq9mi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Techniques to Inject Emotion in Responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbugfr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753741717,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Having only focused on LLM applications around utility (home assistant, scheduling, et.) I have recently been experimenting a lot with AI companions.  How do people introduce emotions or response modifiers through a conversation to make it seem more ‘real’&lt;/p&gt;\n\n&lt;p&gt;I have tried the following with mixed results. &lt;/p&gt;\n\n&lt;p&gt;Conversation memory recalls, compare input embedding to past convo (knowledge graph concept). Same concept but emotional language recall (sentiment analysis) both of these are ok to stay on topic but don’t introduce opportunities for spontaneous divergence in the conversation.&lt;/p&gt;\n\n&lt;p&gt;System prompt/dynaimc sp similar sentiment analysis and then swap out 6 pre made sp’s (happy,sad, etc.)&lt;/p&gt;\n\n&lt;p&gt;Injections in a reasoning model CoT basically I run response for 50 token, stop, add some sentiment steering language, then let it finish the &amp;lt;think&amp;gt; step&lt;/p&gt;\n\n&lt;p&gt;What do others do? Any papers or research on this topic? So far most of the time it’s still a ‘yes-man’ not to far below the surface &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbugfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Strange_Test7665",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbugfr/techniques_to_inject_emotion_in_responses/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbugfr/techniques_to_inject_emotion_in_responses/",
          "subreddit_subscribers": 506711,
          "created_utc": 1753741717,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}