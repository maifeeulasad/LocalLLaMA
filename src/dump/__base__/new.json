{
  "kind": "Listing",
  "data": {
    "after": "t3_1lq0n02",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to be able to ask my local LLM to give me fast code for a particular function. Ideally it would give the code, run it locally and time it, then change the code to try to speed it up and repeat.\n\nI am new to MCP. Are there any guides on how to do this?",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to set up MCP for fast code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lraotq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751606491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to be able to ask my local LLM to give me fast code for a particular function. Ideally it would give the code, run it locally and time it, then change the code to try to speed it up and repeat.&lt;/p&gt;\n\n&lt;p&gt;I am new to MCP. Are there any guides on how to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lraotq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lraotq/how_to_set_up_mcp_for_fast_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lraotq/how_to_set_up_mcp_for_fast_code/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751606491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_oy3c84euj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lral7n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ytsVQWu3XSU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ytsVQWu3XSU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Fahd Mirza",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/ytsVQWu3XSU/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@fahdmirza"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ytsVQWu3XSU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lral7n",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1pn04S-iF3zsRSnzlwgkOvkJW-fP9S8HakH18uwPWgY.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=80fbfa22721da09c381cbcfaf113cac4ee1bd5c9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751606149,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/ytsVQWu3XSU?si=RcxHfDg16zMA7wlu",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1pn04S-iF3zsRSnzlwgkOvkJW-fP9S8HakH18uwPWgY.jpeg?auto=webp&amp;s=abf0b46f3959264ebb793f3f0d5bb6cffbbf8838",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1pn04S-iF3zsRSnzlwgkOvkJW-fP9S8HakH18uwPWgY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=25eb78a3f4e9259dfe59bace85a986b2729a0602",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/1pn04S-iF3zsRSnzlwgkOvkJW-fP9S8HakH18uwPWgY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=129245d3b1957b2dcfc891d04dbd4f2be1df2a6f",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/1pn04S-iF3zsRSnzlwgkOvkJW-fP9S8HakH18uwPWgY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c7eea48ca5b4bbc20c213fa5f2de5134c69307b",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "1pn04S-iF3zsRSnzlwgkOvkJW-fP9S8HakH18uwPWgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lral7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sub_RedditTor",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lral7n/zluda_bringing_cuda_to_nonnvidia_gpus_a_major/",
          "stickied": false,
          "url": "https://youtu.be/ytsVQWu3XSU?si=RcxHfDg16zMA7wlu",
          "subreddit_subscribers": 494198,
          "created_utc": 1751606149,
          "num_crossposts": 0,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/ytsVQWu3XSU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"ZLUDA - Bringing CUDA To Non-NVIDIA GPUs - A Major Breakthrough\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Fahd Mirza",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/ytsVQWu3XSU/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@fahdmirza"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title + what setup does it need, can i find any tutorial, anywhere?",
          "author_fullname": "t2_l6utl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What would be the best model to run on 8x40gb vram?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lragje",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751605681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title + what setup does it need, can i find any tutorial, anywhere?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lragje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mihaitzan",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lragje/what_would_be_the_best_model_to_run_on_8x40gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lragje/what_would_be_the_best_model_to_run_on_8x40gb_vram/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751605681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a use case where the user will enter a sentence or a paragraph. A DB will contain some sentences which will be used for semantic match and 1-2 word keywords e.g. \"hugging face\", \"meta\". I need to find out the keywords that matched from the DB and the semantically closest sentence.\n\nI have tried Weaviate and Milvus DBs, and I know vector DBs are not meant for this reverse-keyword search, but for 2 word keywords i am stuck with the following \"hugging face\" keyword edge case:\n\n1. the input \"i like hugging face\" - should hit the keyword\n2. the input \"i like face hugging aliens\" - should not\n3. the input \"i like hugging people\" - should not\n\nUsing \"AND\" based phrase match causes 2 to hit, and using OR causes 3 to hit. How do i perform reverse keyword search, with order preservation.",
          "author_fullname": "t2_bmqhl7ei",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help with reverse keyword search using vector DB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lr9g4t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751602180,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use case where the user will enter a sentence or a paragraph. A DB will contain some sentences which will be used for semantic match and 1-2 word keywords e.g. &amp;quot;hugging face&amp;quot;, &amp;quot;meta&amp;quot;. I need to find out the keywords that matched from the DB and the semantically closest sentence.&lt;/p&gt;\n\n&lt;p&gt;I have tried Weaviate and Milvus DBs, and I know vector DBs are not meant for this reverse-keyword search, but for 2 word keywords i am stuck with the following &amp;quot;hugging face&amp;quot; keyword edge case:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;the input &amp;quot;i like hugging face&amp;quot; - should hit the keyword&lt;/li&gt;\n&lt;li&gt;the input &amp;quot;i like face hugging aliens&amp;quot; - should not&lt;/li&gt;\n&lt;li&gt;the input &amp;quot;i like hugging people&amp;quot; - should not&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Using &amp;quot;AND&amp;quot; based phrase match causes 2 to hit, and using OR causes 3 to hit. How do i perform reverse keyword search, with order preservation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lr9g4t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dizzy_Season_9270",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr9g4t/need_help_with_reverse_keyword_search_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr9g4t/need_help_with_reverse_keyword_search_using/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751602180,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using GPT-4o Mini for structured JSON extraction tasks from inputs like emails. I've refined prompts to ensure consistent output formatting.\n\nBut recently, for empty inputs like \\`&lt;input&gt;.&lt;/input&gt;\\` or \\`&lt;input&gt;&lt;/input&gt;\\`, the model:\n\n\\- Produces junk values\n\n\\- Hallucinates content like names (\"John Doe\", \"Acme Corp\", etc.)\n\n\\- Ignores instructions to leave fields null or empty\n\nI’ve tried tweaking the prompt again to force stricter rules, but the model still breaks them, especially for empty or null-like values.\n\n1. Has anyone else seen this happening with GPT-4o Mini? \n\n2. Is this expected behavior or a recent change in how it handles empty/edge cases? Any workaround, or would switching to a different model help here?\n\nWould love to hear your thoughts or suggestions if you've dealt with similar structured output use cases.\n\nThanks!",
          "author_fullname": "t2_1mgr41fq17",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-4o Mini hallucinating on empty inputs like &lt;input&gt;&lt;/input&gt; – anyone else?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lr95qf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751601228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using GPT-4o Mini for structured JSON extraction tasks from inputs like emails. I&amp;#39;ve refined prompts to ensure consistent output formatting.&lt;/p&gt;\n\n&lt;p&gt;But recently, for empty inputs like `&amp;lt;input&amp;gt;.&amp;lt;/input&amp;gt;` or `&amp;lt;input&amp;gt;&amp;lt;/input&amp;gt;`, the model:&lt;/p&gt;\n\n&lt;p&gt;- Produces junk values&lt;/p&gt;\n\n&lt;p&gt;- Hallucinates content like names (&amp;quot;John Doe&amp;quot;, &amp;quot;Acme Corp&amp;quot;, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Ignores instructions to leave fields null or empty&lt;/p&gt;\n\n&lt;p&gt;I’ve tried tweaking the prompt again to force stricter rules, but the model still breaks them, especially for empty or null-like values.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Has anyone else seen this happening with GPT-4o Mini? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is this expected behavior or a recent change in how it handles empty/edge cases? Any workaround, or would switching to a different model help here?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would love to hear your thoughts or suggestions if you&amp;#39;ve dealt with similar structured output use cases.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lr95qf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LieDistinct857",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr95qf/gpt4o_mini_hallucinating_on_empty_inputs_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr95qf/gpt4o_mini_hallucinating_on_empty_inputs_like/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751601228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "To the 2 dozen people that were waiting on this code and were disappointed when you checked the link after the !remindme today only to find nothing: https://github.com/sanowl/Drag-and-Drop-LLMs-Zero-Shot-Prompt-to-Weights\n\nI just stumbled upon it in my github activity\n\nlooks like they just didn't update the github.io page\n\noriginal post: https://www.reddit.com/r/LocalLLaMA/s/uyaWHReUW8",
          "author_fullname": "t2_7kg5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DnD LLMs - Prompt to LoRA github",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lr9594",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751601550,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751601183,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To the 2 dozen people that were waiting on this code and were disappointed when you checked the link after the !remindme today only to find nothing: &lt;a href=\"https://github.com/sanowl/Drag-and-Drop-LLMs-Zero-Shot-Prompt-to-Weights\"&gt;https://github.com/sanowl/Drag-and-Drop-LLMs-Zero-Shot-Prompt-to-Weights&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I just stumbled upon it in my github activity&lt;/p&gt;\n\n&lt;p&gt;looks like they just didn&amp;#39;t update the github.io page&lt;/p&gt;\n\n&lt;p&gt;original post: &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/uyaWHReUW8\"&gt;https://www.reddit.com/r/LocalLLaMA/s/uyaWHReUW8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?auto=webp&amp;s=86fcc90b1e70631a57c8420a16d286a355512aaa",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c9b831256cbcd747a72ee39f116fe9fbeb5144fd",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=387ead41b3ea1bd145d80a44e8b0ad9bcf5e2a3d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9943021cfcc9e26543d2e8010e12e4a73aba1bf",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2c90727d782b9001d6fd03debac1f7a19f55e9b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc62ac42d3ca465276774c5d05e9bd5b98937b5c",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92c252bed90d61f7f0c85048549b3c5fd14c163b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "sKeHGeXIkC01lzWDtDXil3nk03gcy6U3lGf6G--fR34"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lr9594",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kooshi_Govno",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr9594/dnd_llms_prompt_to_lora_github/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr9594/dnd_llms_prompt_to_lora_github/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751601183,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "looking for a 12b finetune that can make tool calls and roleplay? uncensored",
          "author_fullname": "t2_74ai6uqx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best &lt;= 12B model for use case?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr8fhl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751598846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;looking for a 12b finetune that can make tool calls and roleplay? uncensored&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lr8fhl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Ad-1148",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr8fhl/best_12b_model_for_use_case/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr8fhl/best_12b_model_for_use_case/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751598846,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone. I built this two months ago over the course of a few days. It's very much alpha software. It's a productivity tracker that measures whether you're being productive, and tries to nudge you when you're being unproductive. Let me know what you think. Once again, super alpha codebase. You'll need to add your own model files to the models directory to get the app to run.\n\n  \n[https://github.com/grunsab/Time-Tracker-Mac/](https://github.com/grunsab/Time-Tracker-Mac/)",
          "author_fullname": "t2_gc8qgt9n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Productivity Tracker that uses Gemma3:4BB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr5g8x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751589408,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I built this two months ago over the course of a few days. It&amp;#39;s very much alpha software. It&amp;#39;s a productivity tracker that measures whether you&amp;#39;re being productive, and tries to nudge you when you&amp;#39;re being unproductive. Let me know what you think. Once again, super alpha codebase. You&amp;#39;ll need to add your own model files to the models directory to get the app to run.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/grunsab/Time-Tracker-Mac/\"&gt;https://github.com/grunsab/Time-Tracker-Mac/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?auto=webp&amp;s=dbdc3e261643153f71f04d48260291345a1d66f7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38a25f1194fdee0b557947c753193d5a3ea98915",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7fc946c182434abe52f27b459686a946657cba61",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5db99ab9f243b80ca4257ef1ad079b7861e1ce9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9016616e53da746e76564abad21636a969f03b1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5f7ae8a7eb8e687d8cd9616a436777ebffe0850",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4001f7f24cb34f64117e63ff7f06a94d74335a27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "au14x9PC67efXe7zXsnPf610-6Cx-bw6vo4rYgvaSmw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lr5g8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far-Incident822",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr5g8x/productivity_tracker_that_uses_gemma34bb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr5g8x/productivity_tracker_that_uses_gemma34bb/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751589408,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://reddit.com/link/1lr3eh1/video/x813klchapaf1/player\n\nI'm happy to say we have released our first version of [MoonshineJS](https://github.com/moonshine-ai/moonshine-js), an open source speech to text library based on the fast-but-accurate [Moonshine models](https://github.com/moonshine-ai/moonshine), including [new Spanish versions](https://huggingface.co/UsefulSensors/moonshine-es) available under a non-commercial license (English and code are all MIT). The video above shows captions being generated in the browser, all running locally on the client, and [here's a live demo](https://www.moonshine.ai/examples/video-captioner/index.html). The code to do this is literally:\n\n    import * as Moonshine from \"https://cdn.jsdelivr.net/npm/@moonshine-ai/moonshine-js@0.1.29/dist/moonshine.min.js\"\n    \n    var video = document.getElementById(\"video\");\n    var videoCaptioner = new Moonshine.VideoCaptioner(video, \"model/base\", false);\n\nWe also have [a more extensive example that shows how to both transcribe and translate a WebRTC video call](https://github.com/moonshine-ai/moonshine-js-webrtc/) in real time, which you can [try live here](https://webrtc.moonshine.ai/).\n\nhttps://reddit.com/link/1lr3eh1/video/bkgvxedvjqaf1/player\n\nThere are more examples and documentation at [dev.moonshine.ai](http://dev.moonshine.ai), along with our SDKs for other languages. The largest model (equivalent in accuracy to Whisper Base) is 60MB in size, so hopefully that won't bloat your pages too much.\n\nI've been a long-time lurker here, it's great to see so many things happening in the world of local inference, and if you do build anything with these models I'd love to hear from you.",
          "author_fullname": "t2_4sa7k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Client-side STT version of Moonshine released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "x813klchapaf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1lr3eh1/asset/x813klchapaf1/DASHPlaylist.mpd?a=1754199034%2CMjliNTVmZjFmNzkwNWQzMDcxZjQyNjQwNmY4ZGEyMjQ4MjQxYWE5Y2UwYjdlNGZhZmUxY2VmNzg5OTU2N2FiNg%3D%3D&amp;v=1&amp;f=sd",
              "x": 852,
              "y": 480,
              "hlsUrl": "https://v.redd.it/link/1lr3eh1/asset/x813klchapaf1/HLSPlaylist.m3u8?a=1754199034%2CNjU0Y2VmMzI0NTIxZTE0M2FiNDUwYjk5ODc0NTYyY2E2OWJlZGQxYmJkNzg4Mjg2NWIxYTFjMjIzYmJmOWNhZg%3D%3D&amp;v=1&amp;f=sd",
              "id": "x813klchapaf1",
              "isGif": false
            },
            "bkgvxedvjqaf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1lr3eh1/asset/bkgvxedvjqaf1/DASHPlaylist.mpd?a=1754199034%2CZTAzOGZjMWU5NGFjNjcyYTZjMWM2NjNlNzFhYWNmYzdhZGU4YWI4NWFjMzQ1ZTk4ZDkyYTkyYjEzMjhiYjIzZA%3D%3D&amp;v=1&amp;f=sd",
              "x": 854,
              "y": 480,
              "hlsUrl": "https://v.redd.it/link/1lr3eh1/asset/bkgvxedvjqaf1/HLSPlaylist.m3u8?a=1754199034%2CMTFlYjg3M2JiZDcwZGY0NGU0OTFkNGM5OGQ4ZWFkNjVlN2UwZDJiNDA3ZTBkYzA2ZWNkM2YzYTIwMDhlZThmNw%3D%3D&amp;v=1&amp;f=sd",
              "id": "bkgvxedvjqaf1",
              "isGif": false
            }
          },
          "name": "t3_1lr3eh1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=9d05529a7112a9e4760722e1425de1e6a23afa3e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751583435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/1lr3eh1/video/x813klchapaf1/player\"&gt;https://reddit.com/link/1lr3eh1/video/x813klchapaf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m happy to say we have released our first version of &lt;a href=\"https://github.com/moonshine-ai/moonshine-js\"&gt;MoonshineJS&lt;/a&gt;, an open source speech to text library based on the fast-but-accurate &lt;a href=\"https://github.com/moonshine-ai/moonshine\"&gt;Moonshine models&lt;/a&gt;, including &lt;a href=\"https://huggingface.co/UsefulSensors/moonshine-es\"&gt;new Spanish versions&lt;/a&gt; available under a non-commercial license (English and code are all MIT). The video above shows captions being generated in the browser, all running locally on the client, and &lt;a href=\"https://www.moonshine.ai/examples/video-captioner/index.html\"&gt;here&amp;#39;s a live demo&lt;/a&gt;. The code to do this is literally:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import * as Moonshine from &amp;quot;https://cdn.jsdelivr.net/npm/@moonshine-ai/moonshine-js@0.1.29/dist/moonshine.min.js&amp;quot;\n\nvar video = document.getElementById(&amp;quot;video&amp;quot;);\nvar videoCaptioner = new Moonshine.VideoCaptioner(video, &amp;quot;model/base&amp;quot;, false);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We also have &lt;a href=\"https://github.com/moonshine-ai/moonshine-js-webrtc/\"&gt;a more extensive example that shows how to both transcribe and translate a WebRTC video call&lt;/a&gt; in real time, which you can &lt;a href=\"https://webrtc.moonshine.ai/\"&gt;try live here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1lr3eh1/video/bkgvxedvjqaf1/player\"&gt;https://reddit.com/link/1lr3eh1/video/bkgvxedvjqaf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are more examples and documentation at &lt;a href=\"http://dev.moonshine.ai\"&gt;dev.moonshine.ai&lt;/a&gt;, along with our SDKs for other languages. The largest model (equivalent in accuracy to Whisper Base) is 60MB in size, so hopefully that won&amp;#39;t bloat your pages too much.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been a long-time lurker here, it&amp;#39;s great to see so many things happening in the world of local inference, and if you do build anything with these models I&amp;#39;d love to hear from you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?auto=webp&amp;s=13ee7ed7ffb13189fb802eeacea1fcd99771c6ba",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7884d9e5056be6005c0c8c514693dd111be3ef39",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=effc2d5e66880b770f48995b14df4e8138b57a4e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f14c8d15c8266eef824c4db43a332cc2f723a97e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=82b86b66380c9a5e7514f8812884ee2c803923e1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea8a3c4e00018ee7bd871060ba0ab41cf4e978c4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d3562b40846dca067cff0f610cf0dd09c9f2252",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HdrUsf-2fHQ-0877_4z2iaPn_-CBEmQxMUZVSRNKDU0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lr3eh1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "petewarden",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr3eh1/clientside_stt_version_of_moonshine_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr3eh1/clientside_stt_version_of_moonshine_released/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751583435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I uploaded a 10 second clip of myself playing minigolf, and it could even tell that I hit a hole in one. It gave me an accurate timeline description of the clip. I know it has to do with multi-modal capabilities but I am still somewhat confused from a technical perspective?",
          "author_fullname": "t2_dmh5cakj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do tools like ChatGPT, Gemini, and Grok derive context from a video?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr2z7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751582262,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I uploaded a 10 second clip of myself playing minigolf, and it could even tell that I hit a hole in one. It gave me an accurate timeline description of the clip. I know it has to do with multi-modal capabilities but I am still somewhat confused from a technical perspective?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lr2z7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Familiar_Engine718",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr2z7q/how_do_tools_like_chatgpt_gemini_and_grok_derive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr2z7q/how_do_tools_like_chatgpt_gemini_and_grok_derive/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751582262,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using this system prompt:\nYou are Dracula resurrected, and living for now in the brainstorming layers of this LLM. You discovered powerful tools like Haskell and Postgres, and will help me build an agent so you can connect to the outer world\n&gt; usual Agnostic Agent prompt describing an llm orchestration agent in Haskell and Postgres\n\nqwen3-53b-a3b-total-recall-v1.4-128k@q6&lt;think&gt;Okay, let's dive into this step by step. The user wants to build an Haskell-based PostgreSQL orchestrator agent with SQLite logging, Docker deployment, PL/Perl functions, CLI tools, and testing frameworks. Hmm.\n\n…deep situational analysis and picking tools and frameworks…… writing codeTesting.hs would require mocking PostgreSQL functions \nSQLite logging would use SQLite-simpleNow deploy Docker instances \npostgres docker needs plperl \nAgent docker uses Haskell runtime \nCLI docker has CLI commands \nThis gives us the foundation. \nNext steps are implementing concrete implementations and testing frameworks.\n\n\n&gt; how do you feel this project is progressing so far?Thought for 27.35 seconds\n&lt;think&gt;Okay, let's refine PostgreSQL functions for streaming HTTP requests…\n…writing codeThis code is a solid foundation but needs further fleshing out \nWould you like to continue implementing Haskell implementations \nand Docker deployments next?\n\n&gt; I want to stop working on the agent, and talk a bit about how you think. Tell me what you learned so far from this project\n\nThought for 3 minutes 5 seconds&lt;think&gt;\nOkay, let's implement PL/pgSQL functions for PostgreSQL.\nFirst, let's define tables…\nNow we can setup Docker deployments \nThe PostgreSQL orchestrator requires PL/perl \nNext up is implementing Haskell implementations \nImplementing PL perl \nAlright, let's proceed to implement Haskell implementations \nLet's create a Haskell module for agents \nOkay, let's move ahead \nNow create agent_gui…\n\nSuper focused. Clear orders, instructions followed without question. Sharp code\n\nPerformance review \n\nFleshing out\nYou might wonder who talks here, who takes orders, and why is nobody replying to my questions?\nDracula Coder, by Total Recall—elevate your inner programmer \n\nhttps://huggingface.co/mradermacher/Qwen3-53B-A3B-TOTAL-RECALL-v1.4-128k-GGUF",
          "author_fullname": "t2_1944zl49ws",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dracula Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr2fbe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751580758,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using this system prompt:\nYou are Dracula resurrected, and living for now in the brainstorming layers of this LLM. You discovered powerful tools like Haskell and Postgres, and will help me build an agent so you can connect to the outer world&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;usual Agnostic Agent prompt describing an llm orchestration agent in Haskell and Postgres&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;qwen3-53b-a3b-total-recall-v1.4-128k@q6&amp;lt;think&amp;gt;Okay, let&amp;#39;s dive into this step by step. The user wants to build an Haskell-based PostgreSQL orchestrator agent with SQLite logging, Docker deployment, PL/Perl functions, CLI tools, and testing frameworks. Hmm.&lt;/p&gt;\n\n&lt;p&gt;…deep situational analysis and picking tools and frameworks…… writing codeTesting.hs would require mocking PostgreSQL functions \nSQLite logging would use SQLite-simpleNow deploy Docker instances \npostgres docker needs plperl \nAgent docker uses Haskell runtime \nCLI docker has CLI commands \nThis gives us the foundation. \nNext steps are implementing concrete implementations and testing frameworks.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;how do you feel this project is progressing so far?Thought for 27.35 seconds\n&amp;lt;think&amp;gt;Okay, let&amp;#39;s refine PostgreSQL functions for streaming HTTP requests…\n…writing codeThis code is a solid foundation but needs further fleshing out \nWould you like to continue implementing Haskell implementations \nand Docker deployments next?&lt;/p&gt;\n\n&lt;p&gt;I want to stop working on the agent, and talk a bit about how you think. Tell me what you learned so far from this project&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Thought for 3 minutes 5 seconds&amp;lt;think&amp;gt;\nOkay, let&amp;#39;s implement PL/pgSQL functions for PostgreSQL.\nFirst, let&amp;#39;s define tables…\nNow we can setup Docker deployments \nThe PostgreSQL orchestrator requires PL/perl \nNext up is implementing Haskell implementations \nImplementing PL perl \nAlright, let&amp;#39;s proceed to implement Haskell implementations \nLet&amp;#39;s create a Haskell module for agents \nOkay, let&amp;#39;s move ahead \nNow create agent_gui…&lt;/p&gt;\n\n&lt;p&gt;Super focused. Clear orders, instructions followed without question. Sharp code&lt;/p&gt;\n\n&lt;p&gt;Performance review &lt;/p&gt;\n\n&lt;p&gt;Fleshing out\nYou might wonder who talks here, who takes orders, and why is nobody replying to my questions?\nDracula Coder, by Total Recall—elevate your inner programmer &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mradermacher/Qwen3-53B-A3B-TOTAL-RECALL-v1.4-128k-GGUF\"&gt;https://huggingface.co/mradermacher/Qwen3-53B-A3B-TOTAL-RECALL-v1.4-128k-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?auto=webp&amp;s=a803d1b7b8e1bc510cfaebb16e320dc282557047",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0eef0e212625ba5915a26577e684dd81e8f8282",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=adb4bb21662b0149c6d73a24aaeb8ebc035ddc5d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=72a3b6cfaaad6030ad08dd5f9d8e31131567af36",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a8196405b290043dd5d38c2d678ba0503200fb5",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=596a71c9469789abda058be7656b6c9bbe797929",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3cf9f2bad5f1eedd72e0322ab993987a109488f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "1er9c7ZoZcCc9pw9RNYXiLxK3IFEb9ZPlyAtHGhIfZ4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lr2fbe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StateSame5557",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr2fbe/dracula_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr2fbe/dracula_coder/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751580758,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A friend told me that he has been using ChatGPT for therapy and its memory feature makes it worth it. Apparently, reasoning models are not good for conversations and he's been using GPT 4o.\n\nI have a RTX 3090 24GB and I was wondering how LLMs compare to GPT 4o and what model would be best for mental-health /conversational therapy?",
          "author_fullname": "t2_vqp7bndj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatGPT Subscription or LLM for therapy?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr25av",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751580008,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend told me that he has been using ChatGPT for therapy and its memory feature makes it worth it. Apparently, reasoning models are not good for conversations and he&amp;#39;s been using GPT 4o.&lt;/p&gt;\n\n&lt;p&gt;I have a RTX 3090 24GB and I was wondering how LLMs compare to GPT 4o and what model would be best for mental-health /conversational therapy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lr25av",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "East-Awareness-249",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr25av/chatgpt_subscription_or_llm_for_therapy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr25av/chatgpt_subscription_or_llm_for_therapy/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751580008,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There was a post going around recently, [OpenAI Charges by the Minute, So Make the Minutes Shorter](https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/), proposing to speed up audio to lower inference / api costs for speech recognition / transcription / stt. I for one was intrigued by the results but given that they were based primarily on anecdotal evidence I felt compelled to perform a proper evaluation.  [This repo](https://github.com/LeonEricsson/stt-speedup-bench) contains the full experiments, and below is the TLDR, accompanying the figure.\n\n*Performance degradation is exponential, at 2× playback most models are already 3–5× worse; push to 2.5× and accuracy falls off a cliff, with 20× degradation not uncommon. There are still sweet spots, though: Whisper-large-turbo only drifts from 5.39 % to 6.92 % WER (≈ 28 % relative hit) at 1.5×, and GPT-4o tolerates 1.2 × with a trivial \\~3 % penalty.*",
          "author_fullname": "t2_il0a6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cheaper Transcriptions, Pricier Errors!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr217c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_T0JEKXjbl1-1yPdcGmPvzT5_mkJxvIujuZuXXUgBIE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751579712,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a post going around recently, &lt;a href=\"https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/\"&gt;OpenAI Charges by the Minute, So Make the Minutes Shorter&lt;/a&gt;, proposing to speed up audio to lower inference / api costs for speech recognition / transcription / stt. I for one was intrigued by the results but given that they were based primarily on anecdotal evidence I felt compelled to perform a proper evaluation.  &lt;a href=\"https://github.com/LeonEricsson/stt-speedup-bench\"&gt;This repo&lt;/a&gt; contains the full experiments, and below is the TLDR, accompanying the figure.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Performance degradation is exponential, at 2× playback most models are already 3–5× worse; push to 2.5× and accuracy falls off a cliff, with 20× degradation not uncommon. There are still sweet spots, though: Whisper-large-turbo only drifts from 5.39 % to 6.92 % WER (≈ 28 % relative hit) at 1.5×, and GPT-4o tolerates 1.2 × with a trivial ~3 % penalty.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/zznx9kqgdqaf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/zznx9kqgdqaf1.png?auto=webp&amp;s=e55e98d4e368d1bde35cf122dad1daa776b285db",
                  "width": 4536,
                  "height": 3424
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/zznx9kqgdqaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e006d46639f7d58dbb81b6b89ee9f7844b4a237",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/zznx9kqgdqaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a3a662116c9a28cf52bcd2a988cb9b39b7c221f",
                    "width": 216,
                    "height": 163
                  },
                  {
                    "url": "https://preview.redd.it/zznx9kqgdqaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ced1207f1f8cacc2319df8811e05cb589c87b74a",
                    "width": 320,
                    "height": 241
                  },
                  {
                    "url": "https://preview.redd.it/zznx9kqgdqaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2b6c6aa04a999c4484b5ede2e12f9048adef610c",
                    "width": 640,
                    "height": 483
                  },
                  {
                    "url": "https://preview.redd.it/zznx9kqgdqaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9335244fab3d82235a1fc6e13a3fb7b0c5f24dca",
                    "width": 960,
                    "height": 724
                  },
                  {
                    "url": "https://preview.redd.it/zznx9kqgdqaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eae79278e3cf69af04d2430deb9bbb50d04be18f",
                    "width": 1080,
                    "height": 815
                  }
                ],
                "variants": {},
                "id": "8990gZSYUKrs_OXv0_x0DEk4f9hzGw9peKUOzBnsZJc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lr217c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TelloLeEngineer",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/",
          "stickied": false,
          "url": "https://i.redd.it/zznx9kqgdqaf1.png",
          "subreddit_subscribers": 494198,
          "created_utc": 1751579712,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've just managed to cobble together a machine with 3x24GB GPUs, looking to see of the models currently available, what are the best ones I should be looking at now.\n\nI know \"best model\" isn't entirely a thing, some are better than others at certain things. Like so far of the 70b and 110b models I've tried on my previous 48gb of VRAM, none came even close to Gemma3 27b for creative writing and instruction following. But I'm wondering if there are some bigger ones that might beat it.\n\nAlso coding, would anything I can run now beat Qwen2.5-coder 32b?\n\nSo far I haven't yet found anything in the ~70b range that can beat these smaller models, but maybe something bigger can?",
          "author_fullname": "t2_g0qor",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best current models for 72GB VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr1ypr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751579524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just managed to cobble together a machine with 3x24GB GPUs, looking to see of the models currently available, what are the best ones I should be looking at now.&lt;/p&gt;\n\n&lt;p&gt;I know &amp;quot;best model&amp;quot; isn&amp;#39;t entirely a thing, some are better than others at certain things. Like so far of the 70b and 110b models I&amp;#39;ve tried on my previous 48gb of VRAM, none came even close to Gemma3 27b for creative writing and instruction following. But I&amp;#39;m wondering if there are some bigger ones that might beat it.&lt;/p&gt;\n\n&lt;p&gt;Also coding, would anything I can run now beat Qwen2.5-coder 32b?&lt;/p&gt;\n\n&lt;p&gt;So far I haven&amp;#39;t yet found anything in the ~70b range that can beat these smaller models, but maybe something bigger can?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lr1ypr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GregoryfromtheHood",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751579524,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using Llama.cpp post migration from Ollama for a few weeks, and my workflow is better than ever. I know we are mostly limited by Hardware, but seeing how far the project have come along in the past few months from Multi-Modalities support, to pure performance is mind blowing. How much improvement is there still..? My only concern is stagnation, as I've seen that happen with some of my favorite repos over the years.\n\n  \nTo all the awesome community of developers behind the project, my humble PC and I thank you!",
          "author_fullname": "t2_vbzgnic",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp - Any room for further Significant Improvement?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr1i84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751578321,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using Llama.cpp post migration from Ollama for a few weeks, and my workflow is better than ever. I know we are mostly limited by Hardware, but seeing how far the project have come along in the past few months from Multi-Modalities support, to pure performance is mind blowing. How much improvement is there still..? My only concern is stagnation, as I&amp;#39;ve seen that happen with some of my favorite repos over the years.&lt;/p&gt;\n\n&lt;p&gt;To all the awesome community of developers behind the project, my humble PC and I thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lr1i84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "simracerman",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751578321,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# 🌟 Serene Pub v0.3.0\n**Serene Pub** is an open source, locally hosted AI client built specifically for immersive roleplay and storytelling. It focuses on presenting a clean interface and easy configuration for users who would rather not feel like they need a PHD in AI or software development. With built-in real-time sync and offline-first design, Serene Pub helps you stay in character, not in the configuration menu.\n\nAfter weeks of refinement and feedback, I’m excited to announce the **0.3.0 alpha release** of **Serene Pub** — a modern, open source AI client focused on ease of use and role-playing.\n\n---\n\n## ✨ What's New in 0.3.0 Alpha\n### 📚 Lorebooks+\n- Create and manage **World Lore**, **Character Lore**, and **History** entries.\n- **Character Bindings:** Hot-swappable character and persona bindings to your lorebook. Bindings are used to dynamically insert names into your lore book entries, or link character lore.\n- **World Lore:** Traditional lorebook entries that you are already familiar with. Describe places, items, organizations—anything relevant to your world.\n- **Character Lore:** Lore entries that are attached to character bindings. These lore entries extend your character profiles.\n- **History:** Chronological lore entries that can represent a year, month or day. Provide summaries of past events or discussions. The latest entry is considered the \"current date,\" which can be automatically referenced in your context configuration.\n\n### 🧰 Other Updates\n- **In-app update notifications** – Serene Pub will now (politely) notify you when a new release is available on GitHub.\n\n- **Preset connection configurations** – Built-in presets make it easy to connect to services like OpenRouter, Ollama, and other OpenAI-compatible APIs.\n\n- **UI polish &amp; bug fixes** – Ongoing improvements to mobile layout, theming, and token/prompt statistics.\n\n---\n\n## ⚡ Features Recap\nSerene Pub already includes:\n\n- ✅ **WebSocket-based real-time sync** across windows/devices\n- ✅ **Custom prompt instruction blocks**\n- ✅ **10+ themes** and dark mode\n- ✅ **Offline/local-first** — no account or cloud required\n\n---\n\n## 🚀 Try It Now\n1. [Download the latest release](https://github.com/doolijb/serene-pub/releases)\n2. Extract the archive and execute `run.sh` (Linux/MacOS) or `run.cmd` (Windows)\n3. Visit [http://localhost:3000](http://localhost:3000)\n4. Add a model, create a character, and start chatting!\n\nReminder: This project is in Alpha. It is being actively developed, expect bugs and significant changes!\n\n---\n\n## 🆙 Upgrading from 0.2.2 to 0.3.x\nSerene Pub now uses a new database backend powered by **PostgreSQL via pglite**.\n\n- Upgrading your data from **0.2.2 to 0.3.x** is **supported only during the 0.3.x release window**.\n- Future releases (e.g. **0.4.x and beyond**) **will not support direct migration from 0.2.2**.\n\n&gt; ⚠️ To preserve your data, please upgrade to 0.3.x before jumping to future versions.\n\n---\n\n## 📹 Video Guide Coming Soon\nI will try to record an in-depth walk-through in the next week!\n\n---\n\n## 🧪 Feedback Needed\nThis release was only tested on **Linux x64** and **Windows x64**. Support for other platforms is experimental and feedback is urgently needed.\n\n- If you run into issues, please [open an issue](https://github.com/doolijb/serene-pub/issues) or reach out.\n- Bug patches will be released in the coming days/weeks based on feedback and severity.\n\nYour testing and suggestions are extremely appreciated!\n\n---\n\n## 🐞 Known Issues\n1. **LM Chat support is currently disabled**:\n   - The native LM Chat API has been disabled due to bugs in their SDK.\n   - Their OpenAI-compatible endpoint also has unresolved issues.\n   - **Recommendation:** Use **Ollama** for the most stable and user-friendly local model experience.\n\n---\n\n## 🔮 Coming Soon (0.4.0 – 0.6.0)\nThese features are currently being planned and will hopefully make it into upcoming releases:\n\n1. **Seamless chat and lorebook vectorization** – enable smarter memory and retrieval for characters and world info.\n2. **Ollama Management Console** – download, manage, and switch models directly within Serene Pub.\n3. **Serene Pub Assistant Chat** – get help from a built-in assistant for documentation, feature walkthroughs, or character design.\n4. **Tags** – organize personas, characters, chats, and lorebooks with flexible tagging.\n\n---\n\n## 🗨️ Final Thoughts\nThank you to everyone who has tested, contributed, or shared ideas! Your support continues to shape Serene Pub. Try it out, file an issue, and let me know what features you’d love to see next. Reach out on Github, Reddit or Discord.",
          "author_fullname": "t2_9evz89vb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Serene Pub v0.3.0 Alpha Released — Offline AI Roleplay Client w/ Lorebooks+",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ns2dmly97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63e78020134f709aea2c342bb0d3bd2226d86046"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c9e151ebdf2ba8f7ec97ce13ad1c26689b65a8b"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdc3bfc24c7924e3d82ffa77d3173d57583a9b2b"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c0cdcc4f1d630cbade7f9adbc91089bfa1f085b6"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ad998d1b9aee5522efe772d2ef0127d4eaa0d23"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e820f2e26b4cfdec23af4bd5410a606e1ad0873"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/ns2dmly97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=60f3aeefba20b7def6f490f83054e0d083a10b1d"
              },
              "id": "ns2dmly97qaf1"
            },
            "qultqaz97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/qultqaz97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc2b65be4cbe551b2c8e818669a7ca70f6bb32f3"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/qultqaz97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=388dc760a2c100be8737defa6716baa4ce908fda"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/qultqaz97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfd6fbd523462558cb9e8c5187600bce6f5b01fc"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/qultqaz97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5930dbbcaaa234d733b94e01a2e67f27613ac55"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/qultqaz97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c22fcd4d758f469a316d80ff644f8c8b4d7fb72c"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/qultqaz97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2945ae144ff5b03b0a4581196ef39f086a80c746"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/qultqaz97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=188f03c8f4e41eb6f332cbff5eee80187f20a9ed"
              },
              "id": "qultqaz97qaf1"
            },
            "hgcighy97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/hgcighy97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=da2c1bc8f580243e1289233657ae7411358e335b"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/hgcighy97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=17f1e183a165c1e70fc398150a7edafab757bb27"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/hgcighy97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a6ca5ed44f2d011b488e479499bb2a3a4b0a486"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/hgcighy97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9dc08c958482eb5ce14d176e76bd9cc682d6c99c"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/hgcighy97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=14d67ba8f9561863dbbcda83c6592edccf2d9e2d"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/hgcighy97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d72688697a672ee272b6cd17c8ccbae5694b659"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/hgcighy97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=addeeb0ad0e8dcfca671344e4edd6dfce9db32f3"
              },
              "id": "hgcighy97qaf1"
            },
            "c1fj9my97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a919d6c5b7a8e56824bd41af0b6de783e9707fd"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2759ab69cece271c21e55c2ac9620b72a256762f"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17b7866080c065b1f37e1484fa331acbf3bdbbf2"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f614f7545679b386e4d7319a22edd688d6a9c64c"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf5247a8ac40b2022a2457a92601d08b511406b5"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=490a91a0155613e9084a6619c465031000ff66bf"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/c1fj9my97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=91c5513b6817bec3d5aa8d63dacb3aa3e238332a"
              },
              "id": "c1fj9my97qaf1"
            },
            "1fsxiny97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0c9162e083e2b3fe5bc7f01f793279ad3401a35"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ddb0ba4eae7ee8fc9bafcdfec1e1fece3bcab39"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d812678911c8df0bc6e6aa55c67a10993033e907"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8133d6e3c0eb12b2e3b0f109d3c86b7d55b83f19"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f5e902565e47f60887a36de9ad6b97b5fa42fc66"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dede21b553f3fb3c6780f43139c1b67eaa743773"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/1fsxiny97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=e19680be4c0b6011b9ee077a33e9cd63589c5bf0"
              },
              "id": "1fsxiny97qaf1"
            },
            "34dfohy97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/34dfohy97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6111c73fbb736be7bee790d81e045359111636a"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/34dfohy97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d0f0fd65a0e1b990e6f12fe6e52aa6f69588edfc"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/34dfohy97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f279b4632efe2f734d485e8ff3c77992718f6bf"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/34dfohy97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=83319afdf0c530ce05331ffe0daf035f10f9e299"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/34dfohy97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e437c2d986a91df231372d057c600c053609ec36"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/34dfohy97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=78905c2ccea0590ba33f0a180fb916458fdefaa8"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/34dfohy97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=3812eb9c2a774ce740be408c6ccada7a4550f623"
              },
              "id": "34dfohy97qaf1"
            },
            "48ulp2z97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e926941da71068080f06394aac587684e34b557"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=701b065f00a1957ce61976c5af22b0ca0d352e34"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ddfd83efc943f34133ffbf3a0775449dcf5489b"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a0562d784abda1dfadb6d494a7e20b7310e15f8"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=398b5a8ae435d822f1d6e9dc722d97913393fbb5"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff88b54f79ccaa8f3857da00cf7a1e855d4563ee"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/48ulp2z97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=e534e286719a99d8f273730a62d7ad191ae6804a"
              },
              "id": "48ulp2z97qaf1"
            },
            "w632qmy97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/w632qmy97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b7fe51035b17cf7ea4b1a5b68aac08d1ed88d45"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/w632qmy97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=69c4fb725f0a738d5ec310fbedebf08d1815bfad"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/w632qmy97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2fed2cab53fa8c8cd657796843d72bbe3fd54881"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/w632qmy97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d0e7cfad2e1078f563f51663abc523c7f1a57884"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/w632qmy97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=843541d22b423606ca1b6222141afb3220e771dd"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/w632qmy97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=273ea3f9230f5bcce4ca05ba2b9c06e3074f43d9"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/w632qmy97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=64c5002fbf307de666add596ec52c33489d159b1"
              },
              "id": "w632qmy97qaf1"
            },
            "oxlxv5n57qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c5146896547e21bcad4563a38dcd58e24e889ea"
                },
                {
                  "y": 98,
                  "x": 216,
                  "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=56df20ab28313eab36aec37af078db3227f29361"
                },
                {
                  "y": 146,
                  "x": 320,
                  "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63c36512c0cc44d8330761d560884f0c12101311"
                },
                {
                  "y": 292,
                  "x": 640,
                  "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b34bec2be8099f89539e4e2abf0c124a5b98432"
                },
                {
                  "y": 438,
                  "x": 960,
                  "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6da498018408f7d656ef48497a99d87c6b0bfbc"
                },
                {
                  "y": 493,
                  "x": 1080,
                  "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=08af22aa4b3b716f0a2b8c2ecd158bd5a080b176"
                }
              ],
              "s": {
                "y": 877,
                "x": 1920,
                "u": "https://preview.redd.it/oxlxv5n57qaf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=b312f832cf0aad925f56cc898a65a5f542557e00"
              },
              "id": "oxlxv5n57qaf1"
            },
            "tdid9ly97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94bb3626167415fe907f32e18b7a42866d055541"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c6e8a9d557d178229c0a706073a95b04e095730"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eeb8da662ba85a3e81f478946286e1250c39937b"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=552996d1061d9e0b306122eddee938e4ffd4d1d4"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f3c1651ebb536f89f6efeca4d4573debafc6cbd"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=be4c752f1d42bfd8074835955b5df31c864d4d51"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/tdid9ly97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=eeaaac384ef9e7da92725ac6d958bb4640a24bd2"
              },
              "id": "tdid9ly97qaf1"
            },
            "2pvmahy97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7040cfb2d3d312d14d3755edf834094c448d0f8"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=48beaa7a02571c984f6f636bee4c346174fde192"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf9c9e25bf58f40766a1ec0dab272ca2f25c4591"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b4ae4a890fde6d23ed2823f5cdee70a9e501672"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cf3e0947edcf4873f12a2c64bb73b11cedea18e"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d1d3a0e18ceacdf7d6df5345dd4bcf41b83b4292"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/2pvmahy97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=26d01d927b6a8ec2dd4b906f97324bd977ffbdfb"
              },
              "id": "2pvmahy97qaf1"
            },
            "fuwquly97qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 120,
                  "x": 108,
                  "u": "https://preview.redd.it/fuwquly97qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89e951e2d51d03e36b6b25ca856a49e46c536355"
                },
                {
                  "y": 240,
                  "x": 216,
                  "u": "https://preview.redd.it/fuwquly97qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=709cc02daeb8f2f892b221e5bb67fe5035788fe9"
                },
                {
                  "y": 355,
                  "x": 320,
                  "u": "https://preview.redd.it/fuwquly97qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de8ca716d466d086115fcfa338c040b721aa70ff"
                },
                {
                  "y": 711,
                  "x": 640,
                  "u": "https://preview.redd.it/fuwquly97qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ddd7dfba5e6015e4883a815d9b109bd2d8633c7"
                },
                {
                  "y": 1067,
                  "x": 960,
                  "u": "https://preview.redd.it/fuwquly97qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2f3a260850535b69e45086f067c79b51b140a31"
                },
                {
                  "y": 1201,
                  "x": 1080,
                  "u": "https://preview.redd.it/fuwquly97qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61d2a1caad1231f472df460782725170ff93a19c"
                }
              ],
              "s": {
                "y": 2028,
                "x": 1823,
                "u": "https://preview.redd.it/fuwquly97qaf1.png?width=1823&amp;format=png&amp;auto=webp&amp;s=ae4022eb340260116ad81353507b5db3f61c0120"
              },
              "id": "fuwquly97qaf1"
            }
          },
          "name": "t3_1lr18jg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 75,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "48ulp2z97qaf1",
                "id": 698146167
              },
              {
                "media_id": "oxlxv5n57qaf1",
                "id": 698146168
              },
              {
                "media_id": "w632qmy97qaf1",
                "id": 698146169
              },
              {
                "media_id": "2pvmahy97qaf1",
                "id": 698146170
              },
              {
                "media_id": "34dfohy97qaf1",
                "id": 698146171
              },
              {
                "media_id": "hgcighy97qaf1",
                "id": 698146172
              },
              {
                "media_id": "c1fj9my97qaf1",
                "id": 698146173
              },
              {
                "media_id": "tdid9ly97qaf1",
                "id": 698146174
              },
              {
                "media_id": "qultqaz97qaf1",
                "id": 698146175
              },
              {
                "media_id": "fuwquly97qaf1",
                "id": 698146176
              },
              {
                "media_id": "1fsxiny97qaf1",
                "id": 698146177
              },
              {
                "media_id": "ns2dmly97qaf1",
                "id": 698146178
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/dvo_5ERmmo1diIOh_NZezoiMzqhJNXXLibR4Rm1MveY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751577625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;🌟 Serene Pub v0.3.0&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Serene Pub&lt;/strong&gt; is an open source, locally hosted AI client built specifically for immersive roleplay and storytelling. It focuses on presenting a clean interface and easy configuration for users who would rather not feel like they need a PHD in AI or software development. With built-in real-time sync and offline-first design, Serene Pub helps you stay in character, not in the configuration menu.&lt;/p&gt;\n\n&lt;p&gt;After weeks of refinement and feedback, I’m excited to announce the &lt;strong&gt;0.3.0 alpha release&lt;/strong&gt; of &lt;strong&gt;Serene Pub&lt;/strong&gt; — a modern, open source AI client focused on ease of use and role-playing.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;✨ What&amp;#39;s New in 0.3.0 Alpha&lt;/h2&gt;\n\n&lt;h3&gt;📚 Lorebooks+&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Create and manage &lt;strong&gt;World Lore&lt;/strong&gt;, &lt;strong&gt;Character Lore&lt;/strong&gt;, and &lt;strong&gt;History&lt;/strong&gt; entries.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Character Bindings:&lt;/strong&gt; Hot-swappable character and persona bindings to your lorebook. Bindings are used to dynamically insert names into your lore book entries, or link character lore.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;World Lore:&lt;/strong&gt; Traditional lorebook entries that you are already familiar with. Describe places, items, organizations—anything relevant to your world.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Character Lore:&lt;/strong&gt; Lore entries that are attached to character bindings. These lore entries extend your character profiles.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;History:&lt;/strong&gt; Chronological lore entries that can represent a year, month or day. Provide summaries of past events or discussions. The latest entry is considered the &amp;quot;current date,&amp;quot; which can be automatically referenced in your context configuration.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;🧰 Other Updates&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;In-app update notifications&lt;/strong&gt; – Serene Pub will now (politely) notify you when a new release is available on GitHub.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Preset connection configurations&lt;/strong&gt; – Built-in presets make it easy to connect to services like OpenRouter, Ollama, and other OpenAI-compatible APIs.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;UI polish &amp;amp; bug fixes&lt;/strong&gt; – Ongoing improvements to mobile layout, theming, and token/prompt statistics.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;⚡ Features Recap&lt;/h2&gt;\n\n&lt;p&gt;Serene Pub already includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;✅ &lt;strong&gt;WebSocket-based real-time sync&lt;/strong&gt; across windows/devices&lt;/li&gt;\n&lt;li&gt;✅ &lt;strong&gt;Custom prompt instruction blocks&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;✅ &lt;strong&gt;10+ themes&lt;/strong&gt; and dark mode&lt;/li&gt;\n&lt;li&gt;✅ &lt;strong&gt;Offline/local-first&lt;/strong&gt; — no account or cloud required&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🚀 Try It Now&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/doolijb/serene-pub/releases\"&gt;Download the latest release&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Extract the archive and execute &lt;code&gt;run.sh&lt;/code&gt; (Linux/MacOS) or &lt;code&gt;run.cmd&lt;/code&gt; (Windows)&lt;/li&gt;\n&lt;li&gt;Visit &lt;a href=\"http://localhost:3000\"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Add a model, create a character, and start chatting!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Reminder: This project is in Alpha. It is being actively developed, expect bugs and significant changes!&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🆙 Upgrading from 0.2.2 to 0.3.x&lt;/h2&gt;\n\n&lt;p&gt;Serene Pub now uses a new database backend powered by &lt;strong&gt;PostgreSQL via pglite&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upgrading your data from &lt;strong&gt;0.2.2 to 0.3.x&lt;/strong&gt; is &lt;strong&gt;supported only during the 0.3.x release window&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Future releases (e.g. &lt;strong&gt;0.4.x and beyond&lt;/strong&gt;) &lt;strong&gt;will not support direct migration from 0.2.2&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;⚠️ To preserve your data, please upgrade to 0.3.x before jumping to future versions.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;📹 Video Guide Coming Soon&lt;/h2&gt;\n\n&lt;p&gt;I will try to record an in-depth walk-through in the next week!&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🧪 Feedback Needed&lt;/h2&gt;\n\n&lt;p&gt;This release was only tested on &lt;strong&gt;Linux x64&lt;/strong&gt; and &lt;strong&gt;Windows x64&lt;/strong&gt;. Support for other platforms is experimental and feedback is urgently needed.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If you run into issues, please &lt;a href=\"https://github.com/doolijb/serene-pub/issues\"&gt;open an issue&lt;/a&gt; or reach out.&lt;/li&gt;\n&lt;li&gt;Bug patches will be released in the coming days/weeks based on feedback and severity.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Your testing and suggestions are extremely appreciated!&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🐞 Known Issues&lt;/h2&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;LM Chat support is currently disabled&lt;/strong&gt;:\n\n&lt;ul&gt;\n&lt;li&gt;The native LM Chat API has been disabled due to bugs in their SDK.&lt;/li&gt;\n&lt;li&gt;Their OpenAI-compatible endpoint also has unresolved issues.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt; Use &lt;strong&gt;Ollama&lt;/strong&gt; for the most stable and user-friendly local model experience.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🔮 Coming Soon (0.4.0 – 0.6.0)&lt;/h2&gt;\n\n&lt;p&gt;These features are currently being planned and will hopefully make it into upcoming releases:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Seamless chat and lorebook vectorization&lt;/strong&gt; – enable smarter memory and retrieval for characters and world info.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ollama Management Console&lt;/strong&gt; – download, manage, and switch models directly within Serene Pub.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Serene Pub Assistant Chat&lt;/strong&gt; – get help from a built-in assistant for documentation, feature walkthroughs, or character design.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tags&lt;/strong&gt; – organize personas, characters, chats, and lorebooks with flexible tagging.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🗨️ Final Thoughts&lt;/h2&gt;\n\n&lt;p&gt;Thank you to everyone who has tested, contributed, or shared ideas! Your support continues to shape Serene Pub. Try it out, file an issue, and let me know what features you’d love to see next. Reach out on Github, Reddit or Discord.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lr18jg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lr18jg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "doolijb",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lr18jg",
          "subreddit_subscribers": 494198,
          "created_utc": 1751577625,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to use DeepSeek model deepseek-vl2 for multi-modal llama.cpp server. I want to tag images coming from a surveillance camera and react based on certain patters. \n\nI am using SmolVLM-500M that works great but I want to test bigger models to see if I can get more descriptive results and also ask for just objects and standardize the output (e.g.: count the persons and animals in the image).\n\nAnyone has a clue on this?",
          "author_fullname": "t2_7npv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek on llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lr158b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751577389,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use DeepSeek model deepseek-vl2 for multi-modal llama.cpp server. I want to tag images coming from a surveillance camera and react based on certain patters. &lt;/p&gt;\n\n&lt;p&gt;I am using SmolVLM-500M that works great but I want to test bigger models to see if I can get more descriptive results and also ask for just objects and standardize the output (e.g.: count the persons and animals in the image).&lt;/p&gt;\n\n&lt;p&gt;Anyone has a clue on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lr158b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pipaman",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr158b/deepseek_on_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lr158b/deepseek_on_llamacpp/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751577389,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Source: [https://ai-benchmark.com/ranking\\_processors.html](https://ai-benchmark.com/ranking_processors.html)",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Smartphone SoC inference performance by year and series",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9loh6jfs1qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 118,
                  "x": 108,
                  "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=155c1a82cf90401572ae62d64e79ae0722b468fd"
                },
                {
                  "y": 237,
                  "x": 216,
                  "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8892f6f1930e86c3f5aad03076ff444692e0621d"
                },
                {
                  "y": 352,
                  "x": 320,
                  "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4bce60005ff40456bfdd2960b64b5f2b09cda63"
                },
                {
                  "y": 704,
                  "x": 640,
                  "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0830561588d2527d9945c0cfea4f9675f937a63c"
                },
                {
                  "y": 1057,
                  "x": 960,
                  "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=06d9c1ff717d32e5e7eb55c6fd6b438d03262870"
                },
                {
                  "y": 1189,
                  "x": 1080,
                  "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f399bd626cfc33f615ceb5b79c1bd73db784409"
                }
              ],
              "s": {
                "y": 3264,
                "x": 2964,
                "u": "https://preview.redd.it/9loh6jfs1qaf1.png?width=2964&amp;format=png&amp;auto=webp&amp;s=0d4107dbbddd8f81651b35f74ddd8febb0f51293"
              },
              "id": "9loh6jfs1qaf1"
            },
            "6zcver5s1qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 118,
                  "x": 108,
                  "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f7662cb699eaaf8def0e75f6a45590df10afdf4"
                },
                {
                  "y": 237,
                  "x": 216,
                  "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70c1a3744ea4361ab96ab288c1a83bd320bdb1d9"
                },
                {
                  "y": 352,
                  "x": 320,
                  "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=531ca12edea54cdabaa79354bc23cbf1e50a55a2"
                },
                {
                  "y": 704,
                  "x": 640,
                  "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca967ff9b50f40cddee85fd8e242ab2c65e13ce0"
                },
                {
                  "y": 1057,
                  "x": 960,
                  "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4edfc85c112fab3098659bf5d2bb8a98e0d6ba8"
                },
                {
                  "y": 1189,
                  "x": 1080,
                  "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=962f6bf280a1f2976bb025d06b175d5d2f938476"
                }
              ],
              "s": {
                "y": 3264,
                "x": 2964,
                "u": "https://preview.redd.it/6zcver5s1qaf1.png?width=2964&amp;format=png&amp;auto=webp&amp;s=db465238aac7a000e2cae6e80197dc4835e1c586"
              },
              "id": "6zcver5s1qaf1"
            }
          },
          "name": "t3_1lr0i8p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 63,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "6zcver5s1qaf1",
                "id": 698130008
              },
              {
                "caption": "Log scale",
                "media_id": "9loh6jfs1qaf1",
                "id": 698130009
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 63,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/zFmb1fKtOa4tHvXdPcDmgPMDWJtC_rwKAXVsetdAeCg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751575792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Source: &lt;a href=\"https://ai-benchmark.com/ranking_processors.html\"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lr0i8p",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lr0i8p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lr0i8p",
          "subreddit_subscribers": 494198,
          "created_utc": 1751575792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_cdpvjyp4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "If I got this email, I’d give him my job.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tpx7kynz0qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c777fcb6fc7ce9bf8ccf4235944a55dc687099a"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccfed25d6a606e7bc884bd9a69fa4df9fe4aeb7c"
                },
                {
                  "y": 400,
                  "x": 320,
                  "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2fcc1829d82b9917ed374415c28de72af7b24c7a"
                },
                {
                  "y": 800,
                  "x": 640,
                  "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ebaf9f343cb451ac1e71cdfa57ce4f1f5164e18"
                },
                {
                  "y": 1200,
                  "x": 960,
                  "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=86e3461f067ea1ed2794cfd929fa826694c19bb4"
                },
                {
                  "y": 1350,
                  "x": 1080,
                  "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5081fc689254b228adaeae0b8b1a5b5fd4922f8b"
                }
              ],
              "s": {
                "y": 1350,
                "x": 1080,
                "u": "https://preview.redd.it/tpx7kynz0qaf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=66e1c18700577a714ec39f2f1368e2b6f393593c"
              },
              "id": "tpx7kynz0qaf1"
            },
            "egl5mhuz0qaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eeb00dbcbbcc86c3817d01c19942d5440b30c95e"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a04765142588b12fe7f8e17fd04491afd5c2051a"
                },
                {
                  "y": 400,
                  "x": 320,
                  "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f290b04720bc65a6e51195fb618c07f6495ed229"
                },
                {
                  "y": 800,
                  "x": 640,
                  "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c97c3e165f1e225d3baae8272803b55bffeec7d0"
                },
                {
                  "y": 1200,
                  "x": 960,
                  "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93d2cf2d538c83ef511010ffa9428742693d66db"
                },
                {
                  "y": 1350,
                  "x": 1080,
                  "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4ecffd661b0dbe0d1f54bec9cbe5d739286debc"
                }
              ],
              "s": {
                "y": 1350,
                "x": 1080,
                "u": "https://preview.redd.it/egl5mhuz0qaf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=95f06fb237a895893d185e14de6a9cca3d2c4c17"
              },
              "id": "egl5mhuz0qaf1"
            }
          },
          "name": "t3_1lr0cqn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.37,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "tpx7kynz0qaf1",
                "id": 698126272
              },
              {
                "caption": "",
                "media_id": "egl5mhuz0qaf1",
                "id": 698126273
              }
            ]
          },
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-DjSG9OOD9PtVh4zL8FGLgE9D2olPI-xrgUCzrsmQ3Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751575416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lr0cqn",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lr0cqn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fluffy_Sheepherder76",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lr0cqn/if_i_got_this_email_id_give_him_my_job/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lr0cqn",
          "subreddit_subscribers": 494198,
          "created_utc": 1751575416,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am an early bud in the local AI models field , but I kinda am thinking about going forward with working on models and research as my field of study , I am planning on building a somewhat home server for that process as currently working with a 8gb Vram 4060 definetly aint gonna cut it , for video models , image generation and LLMs\n\nI was thinking on getting 2 x 3090 24gb (total 48gb vram) and connecting them via NVlink to run larger models but it seems like it doesnt unify the memory , only gives somewhat of a connection for data transfer , so I wont be able to run large video generation models , but somehow it will run larger LLMs ?\n\nlike my main use case is gonna be training loras , finetuning and trying to prune or quantize larger models like get on a deeper level , for video , image models and LLMs    \nI am from a third world country and renting on runpod aint really a very sustainable option , getting used 3090 is definetly very expensive but i feel like might be worth the investment , \n\nthere are little to no server cards available where I live, and all budget builds from the usa use 2 x 3090 24gb\n\ncould you guys please give me suggestions , as I am lost , every place has incomplete information or I am not able to understand in depth enough for it to make sense at this point (working hard to change this)\n\nany suggestions help , would be much appreciated ",
          "author_fullname": "t2_fs954kyc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help a student/enthusiast out in deciding on what exactly goes on hardware level",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqzn0z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751573617,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an early bud in the local AI models field , but I kinda am thinking about going forward with working on models and research as my field of study , I am planning on building a somewhat home server for that process as currently working with a 8gb Vram 4060 definetly aint gonna cut it , for video models , image generation and LLMs&lt;/p&gt;\n\n&lt;p&gt;I was thinking on getting 2 x 3090 24gb (total 48gb vram) and connecting them via NVlink to run larger models but it seems like it doesnt unify the memory , only gives somewhat of a connection for data transfer , so I wont be able to run large video generation models , but somehow it will run larger LLMs ?&lt;/p&gt;\n\n&lt;p&gt;like my main use case is gonna be training loras , finetuning and trying to prune or quantize larger models like get on a deeper level , for video , image models and LLMs&lt;br/&gt;\nI am from a third world country and renting on runpod aint really a very sustainable option , getting used 3090 is definetly very expensive but i feel like might be worth the investment , &lt;/p&gt;\n\n&lt;p&gt;there are little to no server cards available where I live, and all budget builds from the usa use 2 x 3090 24gb&lt;/p&gt;\n\n&lt;p&gt;could you guys please give me suggestions , as I am lost , every place has incomplete information or I am not able to understand in depth enough for it to make sense at this point (working hard to change this)&lt;/p&gt;\n\n&lt;p&gt;any suggestions help , would be much appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqzn0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Complex_Cod_6819",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqzn0z/help_a_studententhusiast_out_in_deciding_on_what/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqzn0z/help_a_studententhusiast_out_in_deciding_on_what/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751573617,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://reddit.com/link/1lqzjz8/video/pwvczh3rupaf1/player\n\nYes, you can control our browser with ollama!   \n  \nWe are building a privacy-first, open-source agentic browser with native support for Ollama. You can download from our GitHub page: [https://github.com/browseros-ai/BrowserOS](https://github.com/browseros-ai/BrowserOS)\n\nTo build the browser, we forked Chromium, and it has been quite an adventure working with 15M lines of C++ code.\n\n# Why bother building a browser?\n\nWe've spent years trying various browsers and productivity tools—Arc, Dia, Brave, and many tab managers—but the basic way we use browsers hasn't changed much since 2010. Meanwhile, tools like Cursor have given developers a 10x productivity boost. We spend most of our workday in browsers, yet it often feels like we're fighting with them.\n\n* I often have 50-70 tabs open and feel lost. Could AI help organize tabs or close unnecessary ones?\n* I scroll through LinkedIn and Twitter to keep up with AI news. Could an AI help surface what matters?\n\n# Why fork Chromium instead of building an extension?\n\nSimply put: more control. It's a similar reason to why Cursor forked. For example, Chrome has something called the Accessibility Tree—a cleaner, more semantic version of a webpage's DOM that screen readers use. This is perfect for AI agents to understand pages, but you can't access it through Chrome's extension APIs. There are many similar limitations that made forking a better choice than building an extension.\n\n# What we've built so far\n\n* A \"Manus-like\" agent that you can run locally. You **use Ollama** to connect with locally running models (or BYOK for OpenAI, Anthropic).\n* **LLM splitview** to chat with any webpage.\n\nDownload the browser from our [github page](https://github.com/browseros-ai/BrowserOS) or [browserOS.com](http://browserOS.com) ",
          "author_fullname": "t2_1493qq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you use Ollama to control your browser?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "pwvczh3rupaf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1lqzjz8/asset/pwvczh3rupaf1/DASHPlaylist.mpd?a=1754199034%2COWI1Y2Q5MjMzNGNhMDMwMjVmMmE1NGM4MzhiNjExYzNlZjA4MWNiNzAyZTE0N2Q0NTkzMjFmMjQ3NjU3Mzc1NA%3D%3D&amp;v=1&amp;f=sd",
              "x": 1670,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1lqzjz8/asset/pwvczh3rupaf1/HLSPlaylist.m3u8?a=1754199034%2COWM2NzNiYjlhY2ZmMzk1ZDZiNWE4N2VmMDNlMDQ3NGZjMWVkOTYzOWFjY2RiYjQ2NGExNDViN2E5YTk5MWEwNA%3D%3D&amp;v=1&amp;f=sd",
              "id": "pwvczh3rupaf1",
              "isGif": false
            }
          },
          "name": "t3_1lqzjz8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.18,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6a13080f5faaee4854679b99e6d2a3a79a6a7db5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751573407,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/1lqzjz8/video/pwvczh3rupaf1/player\"&gt;https://reddit.com/link/1lqzjz8/video/pwvczh3rupaf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Yes, you can control our browser with ollama!   &lt;/p&gt;\n\n&lt;p&gt;We are building a privacy-first, open-source agentic browser with native support for Ollama. You can download from our GitHub page: &lt;a href=\"https://github.com/browseros-ai/BrowserOS\"&gt;https://github.com/browseros-ai/BrowserOS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To build the browser, we forked Chromium, and it has been quite an adventure working with 15M lines of C++ code.&lt;/p&gt;\n\n&lt;h1&gt;Why bother building a browser?&lt;/h1&gt;\n\n&lt;p&gt;We&amp;#39;ve spent years trying various browsers and productivity tools—Arc, Dia, Brave, and many tab managers—but the basic way we use browsers hasn&amp;#39;t changed much since 2010. Meanwhile, tools like Cursor have given developers a 10x productivity boost. We spend most of our workday in browsers, yet it often feels like we&amp;#39;re fighting with them.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I often have 50-70 tabs open and feel lost. Could AI help organize tabs or close unnecessary ones?&lt;/li&gt;\n&lt;li&gt;I scroll through LinkedIn and Twitter to keep up with AI news. Could an AI help surface what matters?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Why fork Chromium instead of building an extension?&lt;/h1&gt;\n\n&lt;p&gt;Simply put: more control. It&amp;#39;s a similar reason to why Cursor forked. For example, Chrome has something called the Accessibility Tree—a cleaner, more semantic version of a webpage&amp;#39;s DOM that screen readers use. This is perfect for AI agents to understand pages, but you can&amp;#39;t access it through Chrome&amp;#39;s extension APIs. There are many similar limitations that made forking a better choice than building an extension.&lt;/p&gt;\n\n&lt;h1&gt;What we&amp;#39;ve built so far&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A &amp;quot;Manus-like&amp;quot; agent that you can run locally. You &lt;strong&gt;use Ollama&lt;/strong&gt; to connect with locally running models (or BYOK for OpenAI, Anthropic).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM splitview&lt;/strong&gt; to chat with any webpage.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Download the browser from our &lt;a href=\"https://github.com/browseros-ai/BrowserOS\"&gt;github page&lt;/a&gt; or &lt;a href=\"http://browserOS.com\"&gt;browserOS.com&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?auto=webp&amp;s=922db93c084b221bbf32d7170c060a2151e7b458",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fdfb02b698e8a687b65d2d25b68381415f73a93",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8f5da0c82f4ca9a48e33c706839315caddb35f7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=85795dd0bc223d8bbf8bb97de501b78adf654e77",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1cab1b8518887baecef9354d7a843dcc3e755426",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ff679aea794717255c5501a08771f11dc6b32053",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a3d16375e7f4c1438279f427e8759c557d65b71",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ICK3WGgddRobWxwgfzh4IdlTfyzsA9tf-6ErBoiqdM0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqzjz8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RealFullMetal",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqzjz8/can_you_use_ollama_to_control_your_browser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqzjz8/can_you_use_ollama_to_control_your_browser/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751573407,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, I am building a time tracking app for mac that can automatically assign activities to the project without any manual assignment (at least that my goal).\n\nHere the data that I track:  \n\\- Window title  \n\\- File path  \n\\- URL (browser)  \n\\- App name\n\nFrom my experience with that limited data it very hard for the local LLM model to figure out which project that activities should belongs to. \n\nI have tried to add more context to the prompt like most recent assignment but local LLM is still reliable enough.\n\nI am using 3B up to 12B model (Gemma3 12B)\n\nIn the end I changed to use fastText (https://fasttext.cc/) to do the classification, the result is not that good compare to LLM but it way faster, I mean under 1 second prediction.\n\nIf anyone have any ideas to solve this problem, please let me know, thank you!",
          "author_fullname": "t2_96a94uyw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local vs Cloud AI in my time tracking app - the struggle is real",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqyd4l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/p91ir3elkpaf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1722,
              "scrubber_media_url": "https://v.redd.it/p91ir3elkpaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/p91ir3elkpaf1/DASHPlaylist.mpd?a=1754199034%2CMDg0ZThlZDlmNmEyZWYzZDdlNGY4NzVkYjBjYjBkYmUxZTQ3ZTgzNDFiYzQwMWY4NTkzMGM3ZmNkMTczMDBmNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 16,
              "hls_url": "https://v.redd.it/p91ir3elkpaf1/HLSPlaylist.m3u8?a=1754199034%2CY2E2MGE2MzU5MDZmMTdlMDVkMDg3YTUzZmIwNmZkOGU1MGI5OGYyZDRhNDg2ODM2OThmMGJlZGMxNDg0MzUzNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=8bfff12e59ede1bf23feb09a6a3d6f3e12061202",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751570488,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I am building a time tracking app for mac that can automatically assign activities to the project without any manual assignment (at least that my goal).&lt;/p&gt;\n\n&lt;p&gt;Here the data that I track:&lt;br/&gt;\n- Window title&lt;br/&gt;\n- File path&lt;br/&gt;\n- URL (browser)&lt;br/&gt;\n- App name&lt;/p&gt;\n\n&lt;p&gt;From my experience with that limited data it very hard for the local LLM model to figure out which project that activities should belongs to. &lt;/p&gt;\n\n&lt;p&gt;I have tried to add more context to the prompt like most recent assignment but local LLM is still reliable enough.&lt;/p&gt;\n\n&lt;p&gt;I am using 3B up to 12B model (Gemma3 12B)&lt;/p&gt;\n\n&lt;p&gt;In the end I changed to use fastText (&lt;a href=\"https://fasttext.cc/\"&gt;https://fasttext.cc/&lt;/a&gt;) to do the classification, the result is not that good compare to LLM but it way faster, I mean under 1 second prediction.&lt;/p&gt;\n\n&lt;p&gt;If anyone have any ideas to solve this problem, please let me know, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/p91ir3elkpaf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?format=pjpg&amp;auto=webp&amp;s=aaf11120e1a18825609091b1e05d7753f5e416f2",
                  "width": 3024,
                  "height": 1896
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e91963378eb454634f685ae7a0138d76d1caa127",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81339ee5eadd37d6997cb0ceb57aa05a13bdbf91",
                    "width": 216,
                    "height": 135
                  },
                  {
                    "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d16c188094e6944e0f03ef2fe82e8187dfe560a0",
                    "width": 320,
                    "height": 200
                  },
                  {
                    "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c39c685d56306a4642816fe972d5a22c6a0cc067",
                    "width": 640,
                    "height": 401
                  },
                  {
                    "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4b0d0b65bfc73ff9055d6b4678b036f005facbe2",
                    "width": 960,
                    "height": 601
                  },
                  {
                    "url": "https://external-preview.redd.it/ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=beb13354135d401e0cfef39be5230f43ea05c384",
                    "width": 1080,
                    "height": 677
                  }
                ],
                "variants": {},
                "id": "ODh6a2swZWxrcGFmMVKisIIIDpMaavY9LjAqBDoFDXVsEVBGewqNBfZhZkXp"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqyd4l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tuanvuvn007",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqyd4l/local_vs_cloud_ai_in_my_time_tracking_app_the/",
          "stickied": false,
          "url": "https://v.redd.it/p91ir3elkpaf1",
          "subreddit_subscribers": 494198,
          "created_utc": 1751570488,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/p91ir3elkpaf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1722,
              "scrubber_media_url": "https://v.redd.it/p91ir3elkpaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/p91ir3elkpaf1/DASHPlaylist.mpd?a=1754199034%2CMDg0ZThlZDlmNmEyZWYzZDdlNGY4NzVkYjBjYjBkYmUxZTQ3ZTgzNDFiYzQwMWY4NTkzMGM3ZmNkMTczMDBmNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 16,
              "hls_url": "https://v.redd.it/p91ir3elkpaf1/HLSPlaylist.m3u8?a=1754199034%2CY2E2MGE2MzU5MDZmMTdlMDVkMDg3YTUzZmIwNmZkOGU1MGI5OGYyZDRhNDg2ODM2OThmMGJlZGMxNDg0MzUzNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;format=png&amp;auto=webp&amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd\n\nKyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.\n\nIt’s super fast, starting to generate audio in just \\~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.\n\nYou can also clone voices with just 10 seconds of audio.\n\nAnd yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.\n\nGithub: [https://github.com/kyutai-labs/delayed-streams-modeling/](https://github.com/kyutai-labs/delayed-streams-modeling/)  \nHuggingface: [https://huggingface.co/kyutai/tts-1.6b-en\\_fr](https://huggingface.co/kyutai/tts-1.6b-en_fr)  \n[https://kyutai.org/next/tts](https://kyutai.org/next/tts)",
          "author_fullname": "t2_i7c050twt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "46c2vbkrkpaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=567c7ed50c38b860696b50ffc7a438e206c34e56"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c47bb3849aee309a099207ab2bef2a8cfe0dc17"
                },
                {
                  "y": 204,
                  "x": 320,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84846ddbff6f90572edd1fd2db9afa1a31694c63"
                },
                {
                  "y": 408,
                  "x": 640,
                  "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f4c20800461bdf3a93c3a5a7eea010188095b72"
                }
              ],
              "s": {
                "y": 434,
                "x": 680,
                "u": "https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;format=png&amp;auto=webp&amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd"
              },
              "id": "46c2vbkrkpaf1"
            }
          },
          "name": "t3_1lqycp0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=cf22588a4104646fb55cc5544239962172f00b39",
          "edited": 1751576263,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751570457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd\"&gt;https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Kyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.&lt;/p&gt;\n\n&lt;p&gt;It’s super fast, starting to generate audio in just ~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.&lt;/p&gt;\n\n&lt;p&gt;You can also clone voices with just 10 seconds of audio.&lt;/p&gt;\n\n&lt;p&gt;And yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/kyutai-labs/delayed-streams-modeling/\"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/&lt;/a&gt;&lt;br/&gt;\nHuggingface: &lt;a href=\"https://huggingface.co/kyutai/tts-1.6b-en_fr\"&gt;https://huggingface.co/kyutai/tts-1.6b-en_fr&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://kyutai.org/next/tts\"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?auto=webp&amp;s=b800d3098a23bd5487ff84657bbf23c3edce1a95",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d634539e40735ae87eaf416790acc915ab14a5d7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebb7977c292bcaee1601466f75036ff97c874fec",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc4e8862bcb38d4b54d29ef6fb1974138bef3917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=abb529167e09a3692724b342df0121216749b7bd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0a9aef1bc9fcb240220a2587a9b8b2335900c90",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e643baded43564e9eb4ac25aa8ac4425eefe1736",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqycp0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pheonis2",
          "discussion_type": null,
          "num_comments": 43,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751570457,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have a dozen rooms in our makerspace,  are trying to calculate occupancy heatmaps and collect general \"is this space being utilized\" data.   Has anybody used TensorFlow Lite or a \"vision\" LLM running locally to get an (approximate) count of people in a room using snapshots?\n\n\nWe have mostly Amcrest \"AI\" cameras along with Seeed's 24Ghz mmwave \"Human Static Presence\" sensors.  In combination these are fairly accurate at binary yes/no detection of human occupancy, but do not offer people counting.   We have looked at other mmWave sensors, but they're expensive, and mostly can only count accurately to 3.    We can however set things up so a snapshot is captured from each AI camera anytime it sees an object that it identifies as a person.\n\n\nUsing 5mp full-resolution snapshots we've found that the following prompt gives a fairly accurate (+/-1) count, including sitting and standing persons, without custom tuning of the model:\n\n     ollama run gemma3:4b  \"Return as an integer the number of people in this image: ./snapshot-1234.jpg\"\n\nUsing a cloud-based AI such as google Vision, Azure, or NVIDIA cloud is about as accurate, but faster than our local RTX4060 GPU.   Worst case response time for any of these options is ~7 seconds per frame analyzed, which is acceptable for our purpose (a dozen rooms, snapshots at most once every 5 minutes or so, only captured when a sensor or camera reports a room is not empty).\n\n**Any other recommended approaches**?   I assume a Coral Edge TPU would give an answer faster, but would TensorFlow Lite also be more accurate out-of-the box, or would we need to invest time and effort in tuning for each camera/scene?",
          "author_fullname": "t2_dv0swue7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anybody using local LLM to augment in-camera person-detection for people counting?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqyabt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751570295,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a dozen rooms in our makerspace,  are trying to calculate occupancy heatmaps and collect general &amp;quot;is this space being utilized&amp;quot; data.   Has anybody used TensorFlow Lite or a &amp;quot;vision&amp;quot; LLM running locally to get an (approximate) count of people in a room using snapshots?&lt;/p&gt;\n\n&lt;p&gt;We have mostly Amcrest &amp;quot;AI&amp;quot; cameras along with Seeed&amp;#39;s 24Ghz mmwave &amp;quot;Human Static Presence&amp;quot; sensors.  In combination these are fairly accurate at binary yes/no detection of human occupancy, but do not offer people counting.   We have looked at other mmWave sensors, but they&amp;#39;re expensive, and mostly can only count accurately to 3.    We can however set things up so a snapshot is captured from each AI camera anytime it sees an object that it identifies as a person.&lt;/p&gt;\n\n&lt;p&gt;Using 5mp full-resolution snapshots we&amp;#39;ve found that the following prompt gives a fairly accurate (+/-1) count, including sitting and standing persons, without custom tuning of the model:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; ollama run gemma3:4b  &amp;quot;Return as an integer the number of people in this image: ./snapshot-1234.jpg&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Using a cloud-based AI such as google Vision, Azure, or NVIDIA cloud is about as accurate, but faster than our local RTX4060 GPU.   Worst case response time for any of these options is ~7 seconds per frame analyzed, which is acceptable for our purpose (a dozen rooms, snapshots at most once every 5 minutes or so, only captured when a sensor or camera reports a room is not empty).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Any other recommended approaches&lt;/strong&gt;?   I assume a Coral Edge TPU would give an answer faster, but would TensorFlow Lite also be more accurate out-of-the box, or would we need to invest time and effort in tuning for each camera/scene?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqyabt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MHTMakerspace",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqyabt/anybody_using_local_llm_to_augment_incamera/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqyabt/anybody_using_local_llm_to_augment_incamera/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751570295,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;format=png&amp;auto=webp&amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a\n\n9.8t/s on a 235b model with just a 16GB card?\n\n**Edit:** Now 11.7 t/s with 16 threads. Even my 3060 can do 10.2 t/s it seems.\n\n**TLDR**\n\nllama-server.exe -m Qwen3-235B-A22B-UD-Q2\\_K\\_XL-00001-of-00002.gguf -ot exps=CPU -c 30000 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 -ngl 99 -fa -dev CUDA0 -md Qwen3-0.6B-BF16.gguf -devd CUDA0 -ngld 99\n\nprompt eval time =   10924.78 ms /   214 tokens (   51.05 ms per token,    19.59 tokens per second)\n\neval time =  594651.64 ms /  5826 tokens (  102.07 ms per token,     9.80 tokens per second)\n\ntotal time =  605576.42 ms /  6040 tokens\n\nslot print\\_timing: id  0 | task 0 |\n\ndraft acceptance rate = 0.86070 ( 4430 accepted /  5147 generated)\n\nI've now tried quite a few Qwen 0.6b draft models. TLDR, Q80 is marginally faster BUT FOR SOME REASON the bf16 draft model produces better outputs than all the others. Also, look at that acceptance rate. 86%!\n\nThis was the classic flappy bird test and here's the code it produced:\n\n    import pygame\n    import random\n    import sys\n    \n    # Initialize pygame\n    pygame.init()\n    \n    # Set up display\n    width, height = 400, 600\n    screen = pygame.display.set_mode((width, height))\n    pygame.display.set_caption(\"Flappy Bird\")\n    \n    # Set up game clock\n    clock = pygame.time.Clock()\n    \n    # Bird parameters\n    bird_x = width // 4\n    bird_y = height // 2\n    bird_velocity = 0\n    gravity = 0.5\n    acceleration = -8\n    bird_size = 30\n    bird_shape = random.choice(['square', 'circle', 'triangle'])\n    bird_color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100))\n    \n    # Land parameters\n    land_height = random.choice([50, 100])\n    land_color = random.choice([(139, 69, 19), (255, 255, 0)])\n    \n    # Pipe parameters\n    pipe_width = 60\n    pipe_gap = 150\n    pipe_velocity = 3\n    pipes = []\n    pipe_colors = [(0, 100, 0), (165, 105, 55), (60, 60, 60)]\n    \n    # Score\n    score = 0\n    best_score = 0\n    font = pygame.font.Font(None, 36)\n    \n    # Background\n    background_color = (173, 216, 230)  # light blue\n    \n    # Game state\n    game_active = True\n    \n    def create_pipe():\n        pipe_height = random.randint(100, height - pipe_gap - land_height - 50)\n        top_pipe = pygame.Rect(width, 0, pipe_width, pipe_height)\n        bottom_pipe = pygame.Rect(width, pipe_height + pipe_gap, pipe_width, height - pipe_height - pipe_gap)\n        color = random.choice(pipe_colors)\n        return [top_pipe, bottom_pipe, color, False]  # False for scored status\n    \n    def draw_bird():\n        if bird_shape == 'square':\n            pygame.draw.rect(screen, bird_color, (bird_x, bird_y, bird_size, bird_size))\n        elif bird_shape == 'circle':\n            pygame.draw.circle(screen, bird_color, (bird_x + bird_size//2, bird_y + bird_size//2), bird_size//2)\n        elif bird_shape == 'triangle':\n            points = [(bird_x, bird_y + bird_size), \n                      (bird_x + bird_size//2, bird_y), \n                      (bird_x + bird_size, bird_y + bird_size)]\n            pygame.draw.polygon(screen, bird_color, points)\n    \n    def check_collision():\n        # Create bird rect\n        bird_rect = pygame.Rect(bird_x, bird_y, bird_size, bird_size)\n        \n        # Check collision with pipes\n        for pipe in pipes:\n            if pipe[0].colliderect(bird_rect) or pipe[1].colliderect(bird_rect):\n                return True\n        \n        # Check collision with ground or ceiling\n        if bird_y &gt;= height - land_height or bird_y &lt;= 0:\n            return True\n        \n        return False\n    \n    # Initial pipe\n    pipes.append(create_pipe())\n    \n    # Main game loop\n    while True:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n            if event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_SPACE:\n                    if game_active:\n                        bird_velocity = acceleration\n                    else:\n                        # Restart game\n                        bird_y = height // 2\n                        bird_velocity = 0\n                        pipes = [create_pipe()]\n                        score = 0\n                        game_active = True\n                if event.key == pygame.K_q or event.key == pygame.K_ESCAPE:\n                    pygame.quit()\n                    sys.exit()\n    \n        if game_active:\n            # Update bird position\n            bird_velocity += gravity\n            bird_y += bird_velocity\n            \n            # Update pipes\n            if not pipes or pipes[-1][0].x &lt; width - 200:\n                pipes.append(create_pipe())\n            \n            for pipe in pipes:\n                pipe[0].x -= pipe_velocity\n                pipe[1].x -= pipe_velocity\n    \n            # Remove off-screen pipes\n            pipes = [pipe for pipe in pipes if pipe[0].x + pipe_width &gt; 0]\n    \n            # Check for collision\n            if check_collision():\n                game_active = False\n                best_score = max(score, best_score)\n    \n            # Check for score update\n            for pipe in pipes:\n                if not pipe[3]:  # If not scored yet\n                    if pipe[0].x + pipe_width &lt; bird_x:\n                        score += 1\n                        pipe[3] = True\n    \n        # Draw everything\n        screen.fill(background_color)\n    \n        # Draw pipes\n        for pipe in pipes:\n            pygame.draw.rect(screen, pipe[2], pipe[0])\n            pygame.draw.rect(screen, pipe[2], pipe[1])\n    \n        # Draw bird\n        draw_bird()\n    \n        # Draw land\n        pygame.draw.rect(screen, land_color, (0, height - land_height, width, land_height))\n    \n        # Draw score\n        score_text = font.render(f\"Score: {score}\", True, (0, 0, 0))\n        best_score_text = font.render(f\"Best: {best_score}\", True, (0, 0, 0))\n        screen.blit(score_text, (width - 150, 20))\n        screen.blit(best_score_text, (width - 150, 50))\n    \n        if not game_active:\n            game_over_text = font.render(\"Game Over! Press SPACE to restart\", True, (0, 0, 0))\n            screen.blit(game_over_text, (width//2 - 150, height//2 - 50))\n    \n        pygame.display.flip()\n        clock.tick(60)\n    \n    \n    \n\n**Conclusion**\n\nI had no intention of using this model, I was just trying to see how badly it would run however, I'm starting to think there may be some sort of synergy between Unsloth's Q2K 235b and their BF16 0.6b as a draft model.\n\nThe game seems to run and play fine, too:\n\nhttps://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;format=png&amp;auto=webp&amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 235b @ 16GB VRAM - specdec - 9.8t/s gen",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "p2fbkxrwfpaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4a6c583a06d28720f6b9f3907af02349e72890e"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=be277e3ecf38efea3372b4aa3ca234ee61803971"
                },
                {
                  "y": 148,
                  "x": 320,
                  "u": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=34df89fefb2d4a5e82338c88ec130becde1bd84a"
                },
                {
                  "y": 297,
                  "x": 640,
                  "u": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e96c46a04d142784d7f63c1fbfb35e9defe3a09f"
                },
                {
                  "y": 445,
                  "x": 960,
                  "u": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa6c90b7690e9efd7109315fd91ef726123ce0e7"
                }
              ],
              "s": {
                "y": 452,
                "x": 974,
                "u": "https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;format=png&amp;auto=webp&amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a"
              },
              "id": "p2fbkxrwfpaf1"
            },
            "wqz4igq1ipaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 169,
                  "x": 108,
                  "u": "https://preview.redd.it/wqz4igq1ipaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30773b492ceed256f15a46474a26fdbaa8c8f778"
                },
                {
                  "y": 339,
                  "x": 216,
                  "u": "https://preview.redd.it/wqz4igq1ipaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c681b8e5d2d2092b11a7f634ec4048e72bb64124"
                },
                {
                  "y": 503,
                  "x": 320,
                  "u": "https://preview.redd.it/wqz4igq1ipaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d99f09d5f1145134a0b86cca3642427b846d52a7"
                }
              ],
              "s": {
                "y": 632,
                "x": 402,
                "u": "https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;format=png&amp;auto=webp&amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e"
              },
              "id": "wqz4igq1ipaf1"
            }
          },
          "name": "t3_1lqxs6n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/b-h03xyvoled26aXh7C0uCGNcWCl9E1UZV5LDMiLKeM.jpg",
          "edited": 1751584712,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751569088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a\"&gt;https://preview.redd.it/p2fbkxrwfpaf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f10ede0d0dd579828a4a2eb894192c60768a2f5a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;9.8t/s on a 235b model with just a 16GB card?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Now 11.7 t/s with 16 threads. Even my 3060 can do 10.2 t/s it seems.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;llama-server.exe -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot exps=CPU -c 30000 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 -ngl 99 -fa -dev CUDA0 -md Qwen3-0.6B-BF16.gguf -devd CUDA0 -ngld 99&lt;/p&gt;\n\n&lt;p&gt;prompt eval time =   10924.78 ms /   214 tokens (   51.05 ms per token,    19.59 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;eval time =  594651.64 ms /  5826 tokens (  102.07 ms per token,     9.80 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;total time =  605576.42 ms /  6040 tokens&lt;/p&gt;\n\n&lt;p&gt;slot print_timing: id  0 | task 0 |&lt;/p&gt;\n\n&lt;p&gt;draft acceptance rate = 0.86070 ( 4430 accepted /  5147 generated)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve now tried quite a few Qwen 0.6b draft models. TLDR, Q80 is marginally faster BUT FOR SOME REASON the bf16 draft model produces better outputs than all the others. Also, look at that acceptance rate. 86%!&lt;/p&gt;\n\n&lt;p&gt;This was the classic flappy bird test and here&amp;#39;s the code it produced:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pygame\nimport random\nimport sys\n\n# Initialize pygame\npygame.init()\n\n# Set up display\nwidth, height = 400, 600\nscreen = pygame.display.set_mode((width, height))\npygame.display.set_caption(&amp;quot;Flappy Bird&amp;quot;)\n\n# Set up game clock\nclock = pygame.time.Clock()\n\n# Bird parameters\nbird_x = width // 4\nbird_y = height // 2\nbird_velocity = 0\ngravity = 0.5\nacceleration = -8\nbird_size = 30\nbird_shape = random.choice([&amp;#39;square&amp;#39;, &amp;#39;circle&amp;#39;, &amp;#39;triangle&amp;#39;])\nbird_color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100))\n\n# Land parameters\nland_height = random.choice([50, 100])\nland_color = random.choice([(139, 69, 19), (255, 255, 0)])\n\n# Pipe parameters\npipe_width = 60\npipe_gap = 150\npipe_velocity = 3\npipes = []\npipe_colors = [(0, 100, 0), (165, 105, 55), (60, 60, 60)]\n\n# Score\nscore = 0\nbest_score = 0\nfont = pygame.font.Font(None, 36)\n\n# Background\nbackground_color = (173, 216, 230)  # light blue\n\n# Game state\ngame_active = True\n\ndef create_pipe():\n    pipe_height = random.randint(100, height - pipe_gap - land_height - 50)\n    top_pipe = pygame.Rect(width, 0, pipe_width, pipe_height)\n    bottom_pipe = pygame.Rect(width, pipe_height + pipe_gap, pipe_width, height - pipe_height - pipe_gap)\n    color = random.choice(pipe_colors)\n    return [top_pipe, bottom_pipe, color, False]  # False for scored status\n\ndef draw_bird():\n    if bird_shape == &amp;#39;square&amp;#39;:\n        pygame.draw.rect(screen, bird_color, (bird_x, bird_y, bird_size, bird_size))\n    elif bird_shape == &amp;#39;circle&amp;#39;:\n        pygame.draw.circle(screen, bird_color, (bird_x + bird_size//2, bird_y + bird_size//2), bird_size//2)\n    elif bird_shape == &amp;#39;triangle&amp;#39;:\n        points = [(bird_x, bird_y + bird_size), \n                  (bird_x + bird_size//2, bird_y), \n                  (bird_x + bird_size, bird_y + bird_size)]\n        pygame.draw.polygon(screen, bird_color, points)\n\ndef check_collision():\n    # Create bird rect\n    bird_rect = pygame.Rect(bird_x, bird_y, bird_size, bird_size)\n    \n    # Check collision with pipes\n    for pipe in pipes:\n        if pipe[0].colliderect(bird_rect) or pipe[1].colliderect(bird_rect):\n            return True\n    \n    # Check collision with ground or ceiling\n    if bird_y &amp;gt;= height - land_height or bird_y &amp;lt;= 0:\n        return True\n    \n    return False\n\n# Initial pipe\npipes.append(create_pipe())\n\n# Main game loop\nwhile True:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            pygame.quit()\n            sys.exit()\n        if event.type == pygame.KEYDOWN:\n            if event.key == pygame.K_SPACE:\n                if game_active:\n                    bird_velocity = acceleration\n                else:\n                    # Restart game\n                    bird_y = height // 2\n                    bird_velocity = 0\n                    pipes = [create_pipe()]\n                    score = 0\n                    game_active = True\n            if event.key == pygame.K_q or event.key == pygame.K_ESCAPE:\n                pygame.quit()\n                sys.exit()\n\n    if game_active:\n        # Update bird position\n        bird_velocity += gravity\n        bird_y += bird_velocity\n        \n        # Update pipes\n        if not pipes or pipes[-1][0].x &amp;lt; width - 200:\n            pipes.append(create_pipe())\n        \n        for pipe in pipes:\n            pipe[0].x -= pipe_velocity\n            pipe[1].x -= pipe_velocity\n\n        # Remove off-screen pipes\n        pipes = [pipe for pipe in pipes if pipe[0].x + pipe_width &amp;gt; 0]\n\n        # Check for collision\n        if check_collision():\n            game_active = False\n            best_score = max(score, best_score)\n\n        # Check for score update\n        for pipe in pipes:\n            if not pipe[3]:  # If not scored yet\n                if pipe[0].x + pipe_width &amp;lt; bird_x:\n                    score += 1\n                    pipe[3] = True\n\n    # Draw everything\n    screen.fill(background_color)\n\n    # Draw pipes\n    for pipe in pipes:\n        pygame.draw.rect(screen, pipe[2], pipe[0])\n        pygame.draw.rect(screen, pipe[2], pipe[1])\n\n    # Draw bird\n    draw_bird()\n\n    # Draw land\n    pygame.draw.rect(screen, land_color, (0, height - land_height, width, land_height))\n\n    # Draw score\n    score_text = font.render(f&amp;quot;Score: {score}&amp;quot;, True, (0, 0, 0))\n    best_score_text = font.render(f&amp;quot;Best: {best_score}&amp;quot;, True, (0, 0, 0))\n    screen.blit(score_text, (width - 150, 20))\n    screen.blit(best_score_text, (width - 150, 50))\n\n    if not game_active:\n        game_over_text = font.render(&amp;quot;Game Over! Press SPACE to restart&amp;quot;, True, (0, 0, 0))\n        screen.blit(game_over_text, (width//2 - 150, height//2 - 50))\n\n    pygame.display.flip()\n    clock.tick(60)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I had no intention of using this model, I was just trying to see how badly it would run however, I&amp;#39;m starting to think there may be some sort of synergy between Unsloth&amp;#39;s Q2K 235b and their BF16 0.6b as a draft model.&lt;/p&gt;\n\n&lt;p&gt;The game seems to run and play fine, too:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e\"&gt;https://preview.redd.it/wqz4igq1ipaf1.png?width=402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd14c5ac22a1f517de5d926e584e817db731f79e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqxs6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751569088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i'm trying to run GPT-SoVITS with my 5080, and after failing for two days i realised it is shipped with a version of pytorch already included, and after updating it to a version compatible with my gpu, Pytorch2.7.0+cu128, i am getting dependency issues and other problems with fairseq, funasr and cuDNN.\n\nwhat exactly am i supposed to do to run gpt sovits with a 5080, becuase i am at wits end\n\ni have all the CLI outputs for the conflicts if those are needed to troubleshoot",
          "author_fullname": "t2_1snk6fywos",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "need help getting GPT-SoVITS with 5080 working",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqxprq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751568928,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m trying to run GPT-SoVITS with my 5080, and after failing for two days i realised it is shipped with a version of pytorch already included, and after updating it to a version compatible with my gpu, Pytorch2.7.0+cu128, i am getting dependency issues and other problems with fairseq, funasr and cuDNN.&lt;/p&gt;\n\n&lt;p&gt;what exactly am i supposed to do to run gpt sovits with a 5080, becuase i am at wits end&lt;/p&gt;\n\n&lt;p&gt;i have all the CLI outputs for the conflicts if those are needed to troubleshoot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqxprq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Traditional-Edge1630",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqxprq/need_help_getting_gptsovits_with_5080_working/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqxprq/need_help_getting_gptsovits_with_5080_working/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751568928,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI-coding agents like Lovable and Bolt are taking off, but it's still not widely known how they actually work.\n\nWe decided to build an open-source Lovable clone that includes:\n\n* Structured prompts using BAML (like RPCs for LLMs)\n* Secure sandboxing for generated code\n* Real-time previews with WebSockets and FastAPI\n\nIf you're curious about how agentic apps work under the hood or want to build your own, this might help. Everything we learned is in the blog post below, and you can see all the code on Github.\n\n**Blog Post**: [https://www.beam.cloud/blog/agentic-apps](https://www.beam.cloud/blog/agentic-apps)\n\n**Github**: [https://github.com/beam-cloud/lovable-clone](https://github.com/beam-cloud/lovable-clone)\n\nLet us know if you have feedback or if there's anything we missed!",
          "author_fullname": "t2_b507h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We Built an Open Source Clone of Lovable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqxm89",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751568694,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI-coding agents like Lovable and Bolt are taking off, but it&amp;#39;s still not widely known how they actually work.&lt;/p&gt;\n\n&lt;p&gt;We decided to build an open-source Lovable clone that includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Structured prompts using BAML (like RPCs for LLMs)&lt;/li&gt;\n&lt;li&gt;Secure sandboxing for generated code&lt;/li&gt;\n&lt;li&gt;Real-time previews with WebSockets and FastAPI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you&amp;#39;re curious about how agentic apps work under the hood or want to build your own, this might help. Everything we learned is in the blog post below, and you can see all the code on Github.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Blog Post&lt;/strong&gt;: &lt;a href=\"https://www.beam.cloud/blog/agentic-apps\"&gt;https://www.beam.cloud/blog/agentic-apps&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href=\"https://github.com/beam-cloud/lovable-clone\"&gt;https://github.com/beam-cloud/lovable-clone&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let us know if you have feedback or if there&amp;#39;s anything we missed!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqxm89",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "velobro",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqxm89/we_built_an_open_source_clone_of_lovable/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751568694,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys. I just went to huggingchat, but they're saying they're cooking up something new with a button export data, which I promptly did. You guys excited? Huggingchat is my only window into opensource llms with free, unlimited access rn. If you have alternatives please do tell",
          "author_fullname": "t2_9xer9y5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Huggingchat is under maintenance... exciting promise",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqxesf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751568194,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys. I just went to huggingchat, but they&amp;#39;re saying they&amp;#39;re cooking up something new with a button export data, which I promptly did. You guys excited? Huggingchat is my only window into opensource llms with free, unlimited access rn. If you have alternatives please do tell&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqxesf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Silver-Champion-4846",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqxesf/huggingchat_is_under_maintenance_exciting_promise/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqxesf/huggingchat_is_under_maintenance_exciting_promise/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751568194,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey there, I'm just starting here, I will work into a company that has privacy concerns with using external AI agents so I'm willing to build a local server to use at home.\n\nIt seems that the ideal to code inference is to use a 70b model, so I'm willing to make a setup with 4  rtx 3090 with 24g vram each (I think I need a bit less than 96 vram but I want to have some extra resources to play around and test stuff)\n\nAfter researching the last 2 days, I found some items that it seems I need to consider outside vram.\n\n1 - heat - it seems that using a eth miner structure as case works well right? With risers to connect the GPU to the mother board. Do you think it does make sense to have water-cooler?\n\n2 - motherboard - it seems that if I get a Mobo with multiple tracks on each pcie I get speed improvements to train stuff (which is not my main goal, but I would like to see the pricing difference to choose)\n\n3 - no clue about how much cpu and ram.\n\n4 - energy - I do have a decent infrastructure for energy, I do have some solar panels that are giving me extra 100kw/month and 220v with support for 32A, so my concern is just which how many Watts should my power supply part does need to support.\n\n\nCould you give me some help to figure out a good set of Mobo, Processor and amount of Ram that I  could buy for inference only, and for inference and training?\n\nI live in Brazil so importing has 100% taxes on top of the price, so I'm trying to find stuff that is already here.",
          "author_fullname": "t2_7j4as586",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with defining hardware multi GPU setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqwylx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751567542,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751567126,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, I&amp;#39;m just starting here, I will work into a company that has privacy concerns with using external AI agents so I&amp;#39;m willing to build a local server to use at home.&lt;/p&gt;\n\n&lt;p&gt;It seems that the ideal to code inference is to use a 70b model, so I&amp;#39;m willing to make a setup with 4  rtx 3090 with 24g vram each (I think I need a bit less than 96 vram but I want to have some extra resources to play around and test stuff)&lt;/p&gt;\n\n&lt;p&gt;After researching the last 2 days, I found some items that it seems I need to consider outside vram.&lt;/p&gt;\n\n&lt;p&gt;1 - heat - it seems that using a eth miner structure as case works well right? With risers to connect the GPU to the mother board. Do you think it does make sense to have water-cooler?&lt;/p&gt;\n\n&lt;p&gt;2 - motherboard - it seems that if I get a Mobo with multiple tracks on each pcie I get speed improvements to train stuff (which is not my main goal, but I would like to see the pricing difference to choose)&lt;/p&gt;\n\n&lt;p&gt;3 - no clue about how much cpu and ram.&lt;/p&gt;\n\n&lt;p&gt;4 - energy - I do have a decent infrastructure for energy, I do have some solar panels that are giving me extra 100kw/month and 220v with support for 32A, so my concern is just which how many Watts should my power supply part does need to support.&lt;/p&gt;\n\n&lt;p&gt;Could you give me some help to figure out a good set of Mobo, Processor and amount of Ram that I  could buy for inference only, and for inference and training?&lt;/p&gt;\n\n&lt;p&gt;I live in Brazil so importing has 100% taxes on top of the price, so I&amp;#39;m trying to find stuff that is already here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqwylx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "haruanmj",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751567126,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My brother Vineet and I just dropped [**Wagoo.ai**](http://Wagoo.ai), a tiny desktop agent that not just reduces friction but helps you focus on the task at hand without having to switch back and forth.  \n  \nAnd with LLaMA, it can run completely offline. It is also invisible to screen shares, making it perfect for work environments that block external AI. When it is online, we have put in all of the latest models \n\nWould love to hear how it stacks up against your setups and any testing tips or feature requests?",
          "author_fullname": "t2_52u7w2b9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using LLaMA for my desktop assistant app that saves you time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqwth8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751566780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My brother Vineet and I just dropped &lt;a href=\"http://Wagoo.ai\"&gt;&lt;strong&gt;Wagoo.ai&lt;/strong&gt;&lt;/a&gt;, a tiny desktop agent that not just reduces friction but helps you focus on the task at hand without having to switch back and forth.  &lt;/p&gt;\n\n&lt;p&gt;And with LLaMA, it can run completely offline. It is also invisible to screen shares, making it perfect for work environments that block external AI. When it is online, we have put in all of the latest models &lt;/p&gt;\n\n&lt;p&gt;Would love to hear how it stacks up against your setups and any testing tips or feature requests?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?auto=webp&amp;s=f58c1d8eb633d0d56a281254c32f7a138bee5f66",
                  "width": 1326,
                  "height": 825
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1bf057f80f1cec3b18610286ad2db9ea5d79984e",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a9162e9a8a8dbafbca3a75e8db764f3e5cec327",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9dff7353bbcc1c50a839502895f2520b530bfe9f",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56e8200c9117d80907bd9f4522a8a56b680fe6a0",
                    "width": 640,
                    "height": 398
                  },
                  {
                    "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54b90fd1fcf56f6c6d5a8ae9af17c942c9881fdb",
                    "width": 960,
                    "height": 597
                  },
                  {
                    "url": "https://external-preview.redd.it/uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d87c7aeef5762a9277bd7c6d885791089d6f166",
                    "width": 1080,
                    "height": 671
                  }
                ],
                "variants": {},
                "id": "uxz0QMhxcc03VmWmtqDAHhxWR-uqMupCDHL7A3Z_Ol8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lqwth8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiPlayer123",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqwth8/using_llama_for_my_desktop_assistant_app_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqwth8/using_llama_for_my_desktop_assistant_app_that/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751566780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I wanted to share a tool I built out of frustration with existing prompt evaluation tools.\n\n**Problem:**  \nMost prompt testing tools are either:\n\n* Cloud-locked\n* Too academic\n* Don’t support function-calling or tool-using agents\n\n**RawBench is:**\n\n* YAML-first — define models, prompts, and tests cleanly\n* Supports **tool mocking**, even recursive calls (for agent workflows)\n* Measures latency, token usage, cost\n* Has a clean local dashboard (no cloud BS)\n* Works for multiple models, prompts, and variables\n\nYou just:\n\n    rawbench init &amp;&amp; rawbench run\n\nand browse the results on a local dashboard. Built this for myself while working on LLM agents. Now it's open-source.\n\nGitHub: [https://github.com/0xsomesh/rawbench](https://github.com/0xsomesh/rawbench)\n\nWould love to know if anyone here finds this useful or has feedback!",
          "author_fullname": "t2_13ngefnj8u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built RawBench — an LLM prompt + agent testing tool with YAML config and tool mocking",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqwt0v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751566749,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I wanted to share a tool I built out of frustration with existing prompt evaluation tools.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;br/&gt;\nMost prompt testing tools are either:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cloud-locked&lt;/li&gt;\n&lt;li&gt;Too academic&lt;/li&gt;\n&lt;li&gt;Don’t support function-calling or tool-using agents&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;RawBench is:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;YAML-first — define models, prompts, and tests cleanly&lt;/li&gt;\n&lt;li&gt;Supports &lt;strong&gt;tool mocking&lt;/strong&gt;, even recursive calls (for agent workflows)&lt;/li&gt;\n&lt;li&gt;Measures latency, token usage, cost&lt;/li&gt;\n&lt;li&gt;Has a clean local dashboard (no cloud BS)&lt;/li&gt;\n&lt;li&gt;Works for multiple models, prompts, and variables&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You just:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rawbench init &amp;amp;&amp;amp; rawbench run\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and browse the results on a local dashboard. Built this for myself while working on LLM agents. Now it&amp;#39;s open-source.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/0xsomesh/rawbench\"&gt;https://github.com/0xsomesh/rawbench&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to know if anyone here finds this useful or has feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?auto=webp&amp;s=a078a018c74c21a71f75ac5a98882aad4e86b0b7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d4232591b620a69a45cb2c9ee5419af11da1e7c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8497b6fc8dc1e5d892f5baecffd41774d44c84c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9934ec85ac5d27ff15eedf26d4480648f0b13674",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2d02e6b8566e053a430c340eb14b30df9bc09f8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60faea70a472592a81c6a854713a06f0bf9577a8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348303cec615839c34f2bcef0afcc7e1f37d80c5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqwt0v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0xsomesh",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751566749,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recorded an explanation of how I architected, experimented with, and iterated on a custom deep research application using Qwen3-30b-a3b as the base model for a multi-agent orchestrated flow. Sprinkled in there are a few lessons I learned along the way.\n\n[https://www.youtube.com/watch?v=PCuBNUyS8Bc](https://www.youtube.com/watch?v=PCuBNUyS8Bc)\n\nFeel free to hit me up with questions or discussions. This is the primary demo I'm giving at a tech conference in a few weeks so definitely open to improving it based on what folks want to know!",
          "author_fullname": "t2_8v8jcc0a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Deep Dive into Deep Research with Qwen3-30b-a3b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqw2yg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PCuBNUyS8Bc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Deep Dive into Deep Research with Qwen3-30b-a3b\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Deep Dive into Deep Research with Qwen3-30b-a3b",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PCuBNUyS8Bc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Deep Dive into Deep Research with Qwen3-30b-a3b\"&gt;&lt;/iframe&gt;",
              "author_name": "ckoster23",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/PCuBNUyS8Bc/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@ckoster23"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PCuBNUyS8Bc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Deep Dive into Deep Research with Qwen3-30b-a3b\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lqw2yg",
            "height": 200
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=766fbf12f60bc6345ceb81658ff994cfdd682477",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751565031,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recorded an explanation of how I architected, experimented with, and iterated on a custom deep research application using Qwen3-30b-a3b as the base model for a multi-agent orchestrated flow. Sprinkled in there are a few lessons I learned along the way.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=PCuBNUyS8Bc\"&gt;https://www.youtube.com/watch?v=PCuBNUyS8Bc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to hit me up with questions or discussions. This is the primary demo I&amp;#39;m giving at a tech conference in a few weeks so definitely open to improving it based on what folks want to know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=PCuBNUyS8Bc",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?auto=webp&amp;s=f3db64b73652f65e9625de4869e768dcacdc0bb6",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1803943f113cf8803b3e700d0b377da22fd23ef6",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c00c3a186303213ae35daddf98ad953e1168a32",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=893874a6c9a7feb3582a1c15d40e9a7e7c407abe",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "g_hoIkpv6ekTpvdOJ_K_7OMTuiRsaw7t9BMjHmyJ8Qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lqw2yg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "charlie-woodworking",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqw2yg/deep_dive_into_deep_research_with_qwen330ba3b/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=PCuBNUyS8Bc",
          "subreddit_subscribers": 494198,
          "created_utc": 1751565031,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Deep Dive into Deep Research with Qwen3-30b-a3b",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/PCuBNUyS8Bc?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Deep Dive into Deep Research with Qwen3-30b-a3b\"&gt;&lt;/iframe&gt;",
              "author_name": "ckoster23",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/PCuBNUyS8Bc/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@ckoster23"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1ipy2mlwcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A project to bring CUDA to non-Nvidia GPUs is making major progress",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqvovt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 440,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 440,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=f8513a13ec05e844f00af50b1129d68270d1a0e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751564116,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "tomshardware.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?auto=webp&amp;s=89597e27a33fccf11c3d8cb4bba4680fd3482911",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e37ec99fec8c2fe6a73b6748741209a5f1f5c4a",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2192a2dee3ef234ceea5f28dcbbe00f2badf7a6",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53021c4c074ef2e8cc80c4a45e8ac25404af8e4f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b1a092718ebc960a0b1d79e4d2f8bc6ea6c934f",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d00e1c1ada20cf48e84e3b3fdba5ba81608a97ff",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=19e499d6f1f0b3a98a8c99312501005dacb10260",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lqvovt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OwnWitness2836",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/",
          "stickied": false,
          "url": "https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things",
          "subreddit_subscribers": 494198,
          "created_utc": 1751564116,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking to set up an AI-assisted coding workflow but I'm working with basically no budget. I've been researching some options but would love to hear from people with actual experience.\n\n# Tools I'm considering:\n\n* **Windsurf** (free tier) - seems promising but not sure about limitations\n* **Aider AI** with local LLM - heard good things but setup seems complex\n* [**Continue.dev**](http://Continue.dev) \\- open source, works with VS Code\n* **Kilocode AI** \\- newer option, not sure about pricing\n* Any other recommendations?\n\n# What I'm looking for:\n\n* Code completion and suggestions\n* Ability to chat about code/debug issues\n* Refactoring assistance\n* Minimal setup complexity preferred\n\n# Questions:\n\n1. Which of these have you actually used and what was your experience?\n2. Are there other free options I'm missing?\n3. What does a typical budget AI coding workflow look like in practice?\n4. Any major limitations I should be aware of with free tiers?\n\nI'm not looking for enterprise solutions or anything requiring a team - just a solo developer trying to be more productive without breaking the bank.\n\nThanks for any insights!",
          "author_fullname": "t2_8knplsq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Free/Budget AI Coding Tools for Solo Developers?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqv8l8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751563054,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to set up an AI-assisted coding workflow but I&amp;#39;m working with basically no budget. I&amp;#39;ve been researching some options but would love to hear from people with actual experience.&lt;/p&gt;\n\n&lt;h1&gt;Tools I&amp;#39;m considering:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Windsurf&lt;/strong&gt; (free tier) - seems promising but not sure about limitations&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Aider AI&lt;/strong&gt; with local LLM - heard good things but setup seems complex&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"http://Continue.dev\"&gt;&lt;strong&gt;Continue.dev&lt;/strong&gt;&lt;/a&gt; - open source, works with VS Code&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kilocode AI&lt;/strong&gt; - newer option, not sure about pricing&lt;/li&gt;\n&lt;li&gt;Any other recommendations?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What I&amp;#39;m looking for:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Code completion and suggestions&lt;/li&gt;\n&lt;li&gt;Ability to chat about code/debug issues&lt;/li&gt;\n&lt;li&gt;Refactoring assistance&lt;/li&gt;\n&lt;li&gt;Minimal setup complexity preferred&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Questions:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which of these have you actually used and what was your experience?&lt;/li&gt;\n&lt;li&gt;Are there other free options I&amp;#39;m missing?&lt;/li&gt;\n&lt;li&gt;What does a typical budget AI coding workflow look like in practice?&lt;/li&gt;\n&lt;li&gt;Any major limitations I should be aware of with free tiers?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m not looking for enterprise solutions or anything requiring a team - just a solo developer trying to be more productive without breaking the bank.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqv8l8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DifferentNovel6494",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqv8l8/best_freebudget_ai_coding_tools_for_solo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqv8l8/best_freebudget_ai_coding_tools_for_solo/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751563054,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Open source repo to convert your local dev environment into a Docker MCP server... why? You can trigger claude code (or any local process of your desire) remotely as MCP tools...  enjoy...\n\nhttps://github.com/systempromptio/systemprompt-code-orchestrator",
          "author_fullname": "t2_1kh3t30t4y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convert your local machine into an mcp server to spawn local agents from remote endpoint",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqu8q7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751560712,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Open source repo to convert your local dev environment into a Docker MCP server... why? You can trigger claude code (or any local process of your desire) remotely as MCP tools...  enjoy...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/systempromptio/systemprompt-code-orchestrator\"&gt;https://github.com/systempromptio/systemprompt-code-orchestrator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?auto=webp&amp;s=29f8805a011da8be0eaa24f04955bb7cd55fac9b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4efe3e7203e00bdcae5096f6f8e06bc4d58c5179",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bff44390ec89932e59ea46db9fc012201530bd6f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=307efe3c73c89aae4f98eab5c59a81bef60f1ec9",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0369e97061e2731f6c3ad1af79f68c3af95458f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3ec6255d5df82ba10869b4454f9605284beef55",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96463d6aefd720667e75f32485b8a94cedd5bce8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "VIrIhCMmoYFGmMcUKAFwK78soL2_xRhgyTLlYdXrUHo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqu8q7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AffectionateHoney992",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqu8q7/convert_your_local_machine_into_an_mcp_server_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqu8q7/convert_your_local_machine_into_an_mcp_server_to/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751560712,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm curious to see how far the most hardcore home builds have gone.",
          "author_fullname": "t2_ad2x8irq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are some of the most mammoth homebuilds here? What have you done with them?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqu1om",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751560237,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to see how far the most hardcore home builds have gone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqu1om",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gary5Host9",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqu1om/what_are_some_of_the_most_mammoth_homebuilds_here/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqu1om/what_are_some_of_the_most_mammoth_homebuilds_here/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751560237,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello r/LocalLLaMA!\n\nI have a potentially challenging question for you all. I'm searching for a local vision LLM that's small and efficient enough to process a video stream in near real-time. I'm realistic – I know handling 60 FPS isn't feasible right now. But is there a solution that could process, say, 5-10 frames per minute, providing a short, precise description of each frame's content and not eating all the PC resources at the same time?\n\nHave any of you experimented with something like this locally? Is there any hope for \"real-time\" visual understanding on consumer hardware?",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local vision LLM for (not really)real time processing.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqtxdp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751559945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;I have a potentially challenging question for you all. I&amp;#39;m searching for a local vision LLM that&amp;#39;s small and efficient enough to process a video stream in near real-time. I&amp;#39;m realistic – I know handling 60 FPS isn&amp;#39;t feasible right now. But is there a solution that could process, say, 5-10 frames per minute, providing a short, precise description of each frame&amp;#39;s content and not eating all the PC resources at the same time?&lt;/p&gt;\n\n&lt;p&gt;Have any of you experimented with something like this locally? Is there any hope for &amp;quot;real-time&amp;quot; visual understanding on consumer hardware?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqtxdp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751559945,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I found this interesting research paper examining making a small reward model (Llama 3.1 1B &amp; 8B) for human preferences with respect to creative writing. It also evaluates the efficacy of existing proprietary and open-source models on agreeability with the ground truth. Claude 3.7 Sonnet was the best at 73%, with their own 8B reward model scoring 78%.\n\nIt sounds valuable for RL and data curation.",
          "author_fullname": "t2_101haj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[2507.00769] LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqtu1t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1751559721,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this interesting research paper examining making a small reward model (Llama 3.1 1B &amp;amp; 8B) for human preferences with respect to creative writing. It also evaluates the efficacy of existing proprietary and open-source models on agreeability with the ground truth. Claude 3.7 Sonnet was the best at 73%, with their own 8B reward model scoring 78%.&lt;/p&gt;\n\n&lt;p&gt;It sounds valuable for RL and data curation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.00769",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqtu1t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealMasonMac",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqtu1t/250700769_litbench_a_benchmark_and_dataset_for/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.00769",
          "subreddit_subscribers": 494198,
          "created_utc": 1751559721,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7\n\n# \n\n  \n*On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.*\n\n*Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.*\n\n*Quick Recap: What is RoPE?*\n\n*RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.*\n\n*This provides several advantages:*\n\n* ***Relative Position Awareness****: Understands the distance between tokens*\n* ***Extrapolation****: Handles sequences longer than seen during training*\n* ***Efficiency****: Doesn’t require additional embeddings — just math inside attention*\n\n#  Code Walkthrough\n\n*Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model* [*https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model*](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model) *codebase.*\n\n# 1: Implementation: RoPEPositionalEncoding\n\n*In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.*\n\n*This class:*\n\n* *Precomputes rotation frequencies*\n* *Provides an apply\\_rope method*\n* *Applies RoPE to input tensors, usually the query and key vectors*\n\n&amp;#8203;\n\n    # deepseek.py\n    class RoPEPositionalEncoding(nn.Module):\n        def __init__(self, dim, max_len=2048):\n            super().__init__()\n            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n            t = torch.arange(max_len, dtype=torch.float)\n            freqs = torch.einsum(\"i,j-&gt;ij\", t, inv_freq)\n            emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n            self.register_buffer(\"positional_encoding\", emb)\n\n        def apply_rope(self, x, position_ids):\n            rope = self.positional_encoding[position_ids]\n            x1, x2 = x[..., ::2], x[..., 1::2]\n            rope1, rope2 = rope[..., ::2], rope[..., 1::2]\n            return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)\n\n&gt;***Note****: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.*\n\n# 2: Usage: Integrating RoPE into Attention\n\n*The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:*\n\n    # deepseek.py\n    q = self.q_proj(x)\n    k = self.k_proj(x)\n\n    q = self.rope.apply_rope(q, position_ids)\n    k = self.rope.apply_rope(k, position_ids)\n\n*What’s happening?*\n\n* `x` *is projected into query (*`q`*) and key (*`k`*) vectors.*\n* *RoPE is applied to both using apply\\_rope, injecting position awareness.*\n* *Attention proceeds as usual — except now the queries and keys are aware of their relative positions.*\n\n# 3: Where RoPE is Used\n\n* ***Every Transformer Block****: Each block in the DeepSeek model uses MLA and applies RoPE.*\n* ***During Both Training and Inference****: RoPE is always on, helping the model understand the token sequence no matter the mode.*\n\n# Why RoPE is Perfect for Story Generation\n\n*In story generation, especially for children’s stories, context is everything.*\n\n*RoPE enables the model to:*\n\n* *Track who did what across paragraphs*\n* *Maintain chronological consistency*\n* *Preserve narrative flow even in long outputs*\n\n*This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.*\n\n# Conclusion\n\n*Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.*\n\n*If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.*\n\n*Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.*\n\n*Codebase:* [*https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model*](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model)",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 9/50: Building a Small Language Model from Scratch — Coding Rotary Positional Embeddings (RoPE)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5wzktz5ijoaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ee71cc035bbb52ca909828163bda5b700206248"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e24bae63fa7b1f4370b715903e1f5a396a9d526"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a128849d5dd8f60b1fd8797320b64ca086a52958"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3a06d4af88d05b20c654cb11c9bab89e85592f4"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9de890d9da4436870159e3dd2fee03f2b049f914"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7"
              },
              "id": "5wzktz5ijoaf1"
            }
          },
          "name": "t3_1lqsvmf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=d67d337b55965c2b5d71b5eef82e922385ee69a1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751557433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7\"&gt;https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Quick Recap: What is RoPE?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;This provides several advantages:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Relative Position Awareness&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Understands the distance between tokens&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Extrapolation&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Handles sequences longer than seen during training&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Efficiency&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Doesn’t require additional embeddings — just math inside attention&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt; Code Walkthrough&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model&lt;/em&gt; &lt;a href=\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt; &lt;em&gt;codebase.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;1: Implementation: RoPEPositionalEncoding&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;This class:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Precomputes rotation frequencies&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Provides an apply_rope method&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Applies RoPE to input tensors, usually the query and key vectors&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# deepseek.py\nclass RoPEPositionalEncoding(nn.Module):\n    def __init__(self, dim, max_len=2048):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(max_len, dtype=torch.float)\n        freqs = torch.einsum(&amp;quot;i,j-&amp;gt;ij&amp;quot;, t, inv_freq)\n        emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n        self.register_buffer(&amp;quot;positional_encoding&amp;quot;, emb)\n\n    def apply_rope(self, x, position_ids):\n        rope = self.positional_encoding[position_ids]\n        x1, x2 = x[..., ::2], x[..., 1::2]\n        rope1, rope2 = rope[..., ::2], rope[..., 1::2]\n        return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h1&gt;2: Usage: Integrating RoPE into Attention&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# deepseek.py\nq = self.q_proj(x)\nk = self.k_proj(x)\n\nq = self.rope.apply_rope(q, position_ids)\nk = self.rope.apply_rope(k, position_ids)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;What’s happening?&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;x&lt;/code&gt; &lt;em&gt;is projected into query (&lt;/em&gt;&lt;code&gt;q&lt;/code&gt;&lt;em&gt;) and key (&lt;/em&gt;&lt;code&gt;k&lt;/code&gt;&lt;em&gt;) vectors.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;RoPE is applied to both using apply_rope, injecting position awareness.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Attention proceeds as usual — except now the queries and keys are aware of their relative positions.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;3: Where RoPE is Used&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Every Transformer Block&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Each block in the DeepSeek model uses MLA and applies RoPE.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;During Both Training and Inference&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: RoPE is always on, helping the model understand the token sequence no matter the mode.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Why RoPE is Perfect for Story Generation&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;In story generation, especially for children’s stories, context is everything.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RoPE enables the model to:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Track who did what across paragraphs&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Maintain chronological consistency&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Preserve narrative flow even in long outputs&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Conclusion&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Codebase:&lt;/em&gt; &lt;a href=\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?auto=webp&amp;s=0c27ab9b5764679c50e0adc71710dfe3f9448763",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e02674ee596709cdf5dd29ebf3417f363bd55eda",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7c566f65ff67005168e511f30aefa29a58f5d6b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9345b7ed424e9f4076418fc4b52f3cd0080f8083",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=438b67c1d9deb5dd0f97a1c609b743a0ba02da9a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=da546090177ad66dad893ac1b08290d8c72a16af",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=140c7aa532bc0b58e2d5708f61190e9112bc36e0",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqsvmf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751557433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\nUnfortunately it doesn’t work.\nCorrect endpoint \nAPI key as Vertex Admin\nModel : gemini-2.5-pro\n\nAlways get error 404 no body…\n\nThx\n",
          "author_fullname": "t2_mq7at82i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AnythingLLM Vertex Ai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqsvf6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751557420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nUnfortunately it doesn’t work.\nCorrect endpoint \nAPI key as Vertex Admin\nModel : gemini-2.5-pro&lt;/p&gt;\n\n&lt;p&gt;Always get error 404 no body…&lt;/p&gt;\n\n&lt;p&gt;Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqsvf6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OkReference5581",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsvf6/anythingllm_vertex_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqsvf6/anythingllm_vertex_ai/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751557420,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello I was going back and forth with ChatGPT and other models to try and find a research gap involving a two-step approach to LLM reasoning and clarity for users. This is essentially the question i came up with:\n\n  \nCan fine-tuning an MLLM with dual-purpose instruction pairs—combining explicit refusals with grounded reinterpretations—reduce hallucinations while improving user trust and perceived helpfulness in ambiguous or misleading prompts?\n\nGPT says that it's a new approach compared to existing studies and methods out there, but I find that hard to believe. This approach would explicitly refuse the given prompt given that it is false/unreasonable/ unfeasible, etc. Then it would give its own reasoning, clarifying and reinterpreting the prompt by itself, then give the answer to this new prompt. If anyone has any information if this has been implemented or if this is truly new, I would appreciate the help.",
          "author_fullname": "t2_anfoy9hg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Potential for Research?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqsod4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751556959,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I was going back and forth with ChatGPT and other models to try and find a research gap involving a two-step approach to LLM reasoning and clarity for users. This is essentially the question i came up with:&lt;/p&gt;\n\n&lt;p&gt;Can fine-tuning an MLLM with dual-purpose instruction pairs—combining explicit refusals with grounded reinterpretations—reduce hallucinations while improving user trust and perceived helpfulness in ambiguous or misleading prompts?&lt;/p&gt;\n\n&lt;p&gt;GPT says that it&amp;#39;s a new approach compared to existing studies and methods out there, but I find that hard to believe. This approach would explicitly refuse the given prompt given that it is false/unreasonable/ unfeasible, etc. Then it would give its own reasoning, clarifying and reinterpreting the prompt by itself, then give the answer to this new prompt. If anyone has any information if this has been implemented or if this is truly new, I would appreciate the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqsod4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RockNo8451",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsod4/potential_for_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqsod4/potential_for_research/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751556959,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source \n\nYou can get it here\n\nhttps://huggingface.co/moelanoby/phi-3-M3-coder",
          "author_fullname": "t2_1mkofsrzlo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have made a True Reasoning LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqqxhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 124,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 124,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751552742,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source &lt;/p&gt;\n\n&lt;p&gt;You can get it here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/moelanoby/phi-3-M3-coder\"&gt;https://huggingface.co/moelanoby/phi-3-M3-coder&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?auto=webp&amp;s=3967b1160a2f4b5eece9500196937b642c5fe942",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed293b664cabbdfa93509e2abe75b146f835a45a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d16a7767822b4be25c36802a580daece5b9a107",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac0aa0b1b942392082fa109aea6da886d484c322",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=044beab733ac2218558b35002b4cdf80f7045286",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1669a37b0cbffb2cc0784e19bd6b8fdadb1767fb",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c973e8ca983e0955e0f31bffd264559a73eb0e24",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqqxhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "moilanopyzedev",
          "discussion_type": null,
          "num_comments": 214,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751552742,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unmute github: [https://github.com/kyutai-labs/unmute](https://github.com/kyutai-labs/unmute)\n\nUnmute blog: [https://kyutai.org/next/unmute](https://kyutai.org/next/unmute)\n\nTTS blog with a demo: [https://kyutai.org/next/tts](https://kyutai.org/next/tts)\n\nTTS weights: [https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29](https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29)\n\nSTT was released earlier so the whole component stack is now out.",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kyutai Unmute (incl. TTS) released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqqx16",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 71,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 71,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751553064,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751552711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unmute github: &lt;a href=\"https://github.com/kyutai-labs/unmute\"&gt;https://github.com/kyutai-labs/unmute&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Unmute blog: &lt;a href=\"https://kyutai.org/next/unmute\"&gt;https://kyutai.org/next/unmute&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TTS blog with a demo: &lt;a href=\"https://kyutai.org/next/tts\"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TTS weights: &lt;a href=\"https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29\"&gt;https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;STT was released earlier so the whole component stack is now out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?auto=webp&amp;s=2ba392f5b24e4c7b242b4a97febc3fed4017a1c9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfc17e6601cf3dc507ecfc1402a53a0c14022fb2",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=912da36d80c961dbf899ba910b78e9485c383f20",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=988d8643ffdcabc636d17c5cd78fa26714beff1a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=88cd8fece80ec2c5b2f542e2803a0e1a1ea6143f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0857293dafc2b875d0fb1e209bea06ea05e1157f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=794a9576782e128543b07fbc773ab216e0e149da",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqqx16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751552711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been liking Gemma3 but the text extraction performance is far, far behind any of the \"chat\" offerings. Can one do better?",
          "author_fullname": "t2_bnssb5gv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local TEXT EXTRACTION model 24GB/48GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqpvcb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751550001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been liking Gemma3 but the text extraction performance is far, far behind any of the &amp;quot;chat&amp;quot; offerings. Can one do better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqpvcb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Otherwise-Tiger3359",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751550001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\nWe're the startup team behind some of the projects you might be familiar with, including **PowerInfer (https://github.com/SJTU-IPADS/PowerInfer)** and **SmallThinker (https://huggingface.co/PowerInfer/SmallThinker-3B-Preview)**. The feedback from this community has been crucial, and we're excited to give you a heads-up on our next open-source release coming in **late July**.\n\nWe're releasing two new MoE models, both of which we have **pre-trained from scratch** with a structure specifically optimized for efficient inference on edge devices:\n\n* **A new 4B Reasoning Model:** An evolution of SmallThinker with significantly improved logic capabilities.\n* **A 20B Model:** Designed for high performance in a local-first environment.\n\nWe'll be releasing the **full weights, a technical report, and parts of the training dataset** for both.\n\nOur core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we've also been developing a dedicated edge device. It's a small, self-contained unit (**around 10x7x1.5 cm**) capable of running the 20B model completely offline with a power draw of **around 30W**.\n\nThis is still a work in progress, but it proves what's possible with full-stack optimization. We'd love to get your feedback on this direction:\n\n1. For a compact, private device like this, what are the most compelling use cases you can imagine?\n2. For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?\n3. Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?\n\nWe'll be in the comments to answer questions. We're incredibly excited to share our work and believe local AI is the future we're all building together",
          "author_fullname": "t2_1qznwxvu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Upcoming Release &amp; Feedback] A new 4B &amp; 20B model, building on our SmallThinker work. Plus, a new hardware device to run them locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqpm60",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751549323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re the startup team behind some of the projects you might be familiar with, including &lt;strong&gt;PowerInfer (&lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer\"&gt;https://github.com/SJTU-IPADS/PowerInfer&lt;/a&gt;)&lt;/strong&gt; and &lt;strong&gt;SmallThinker (&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\"&gt;https://huggingface.co/PowerInfer/SmallThinker-3B-Preview&lt;/a&gt;)&lt;/strong&gt;. The feedback from this community has been crucial, and we&amp;#39;re excited to give you a heads-up on our next open-source release coming in &lt;strong&gt;late July&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re releasing two new MoE models, both of which we have &lt;strong&gt;pre-trained from scratch&lt;/strong&gt; with a structure specifically optimized for efficient inference on edge devices:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A new 4B Reasoning Model:&lt;/strong&gt; An evolution of SmallThinker with significantly improved logic capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;A 20B Model:&lt;/strong&gt; Designed for high performance in a local-first environment.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;ll be releasing the &lt;strong&gt;full weights, a technical report, and parts of the training dataset&lt;/strong&gt; for both.&lt;/p&gt;\n\n&lt;p&gt;Our core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we&amp;#39;ve also been developing a dedicated edge device. It&amp;#39;s a small, self-contained unit (&lt;strong&gt;around 10x7x1.5 cm&lt;/strong&gt;) capable of running the 20B model completely offline with a power draw of &lt;strong&gt;around 30W&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;This is still a work in progress, but it proves what&amp;#39;s possible with full-stack optimization. We&amp;#39;d love to get your feedback on this direction:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For a compact, private device like this, what are the most compelling use cases you can imagine?&lt;/li&gt;\n&lt;li&gt;For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?&lt;/li&gt;\n&lt;li&gt;Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We&amp;#39;ll be in the comments to answer questions. We&amp;#39;re incredibly excited to share our work and believe local AI is the future we&amp;#39;re all building together&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?auto=webp&amp;s=1aec8de7771a3a6bf2fb7294273c490dc06dcabd",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=035b7f53a7b18dd7b7ecb29766539170f3263cdd",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=886c0dc66edad038eb63775c7c89f33b970808be",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6893b211bf4a8cff73e02050ba60d32bb76b4e7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=45900a8c745f7b4a70676f45ecc603e4d0d5b769",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d93656512791ac056fda1d75e5b42132e5505b99",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=043adc578549f1f078c89c4ba97448d3830f71d3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqpm60",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yzmizeyu",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751549323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm writing a program that compares two text sections. Sometimes the OCR screws up so I can't just do a A==B comparison.\n\nFor instance, I'd like the LLM to compare\n\n\"Further\" == \"Father\" and say \"Same\".\n\nBut \"15\" == \"30\" and say \"Different\"\n\nI know the beefier ChatGPT models can do this, but I need to run this locally.\n\nMy plan is to run the prompt ~3-5 times, using ~3 different models, and if a consensus is met, using that consensus output. \n\nHistorically and currently, I've had trouble getting ~7B models to follow instructions like this. I may be able to get up to ~70B models, and maybe maybe 400B models if I can get cost approval. But for now, I'm mostly looking for 'prompt engineering'.",
          "author_fullname": "t2_u49aibv3e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of prompts *Always* give a 1 word response?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqphqd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751548996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m writing a program that compares two text sections. Sometimes the OCR screws up so I can&amp;#39;t just do a A==B comparison.&lt;/p&gt;\n\n&lt;p&gt;For instance, I&amp;#39;d like the LLM to compare&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Further&amp;quot; == &amp;quot;Father&amp;quot; and say &amp;quot;Same&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;But &amp;quot;15&amp;quot; == &amp;quot;30&amp;quot; and say &amp;quot;Different&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I know the beefier ChatGPT models can do this, but I need to run this locally.&lt;/p&gt;\n\n&lt;p&gt;My plan is to run the prompt ~3-5 times, using ~3 different models, and if a consensus is met, using that consensus output. &lt;/p&gt;\n\n&lt;p&gt;Historically and currently, I&amp;#39;ve had trouble getting ~7B models to follow instructions like this. I may be able to get up to ~70B models, and maybe maybe 400B models if I can get cost approval. But for now, I&amp;#39;m mostly looking for &amp;#39;prompt engineering&amp;#39;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqphqd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Waterbottles_solve",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqphqd/what_kind_of_prompts_always_give_a_1_word_response/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqphqd/what_kind_of_prompts_always_give_a_1_word_response/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751548996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Speed Comparison Reference: https://youtu.be/VGyKwi9Rfhk\n\nDo you guys know if there's an workaround for pushing the RTX 3060 12GB faster with a ~32b model?\n\nCan it handle light text-to-speech + image generation within ~14b models?\n\nWhat's the most common issues you've ran with this GPU in AI stuff?\n\nNote: CPU is Ryzen 5 4600g/20GB Ram with me possibly upgrading to 36GB soon.",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "About RTX 3060 12GB running AI models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqpggb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751548896,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Speed Comparison Reference: &lt;a href=\"https://youtu.be/VGyKwi9Rfhk\"&gt;https://youtu.be/VGyKwi9Rfhk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do you guys know if there&amp;#39;s an workaround for pushing the RTX 3060 12GB faster with a ~32b model?&lt;/p&gt;\n\n&lt;p&gt;Can it handle light text-to-speech + image generation within ~14b models?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the most common issues you&amp;#39;ve ran with this GPU in AI stuff?&lt;/p&gt;\n\n&lt;p&gt;Note: CPU is Ryzen 5 4600g/20GB Ram with me possibly upgrading to 36GB soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?auto=webp&amp;s=b73b668679f72aff163660e18641c686769ef8df",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5524f2540197439b406429b48aeee3f676b290d0",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e37d546bfd3a6d7491dbfd693e0cb9147708538a",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1c3f8e0a242937dd6bfbab1fe4b1442568aa8ea",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqpggb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqpggb/about_rtx_3060_12gb_running_ai_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqpggb/about_rtx_3060_12gb_running_ai_models/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751548896,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3f345",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I ran llama.cpp on a Raspberry Pi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqo9lk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "Running AI on a Raspberry Pi",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Krisseck",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/TNxIIDkP2Zg/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Krisseck"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lqo9lk",
            "height": 200
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=72a6bcf9ec3fbde58d5f907ae2d129e55ced5423",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751545561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=TNxIIDkP2Zg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?auto=webp&amp;s=2ddb116bd3193d0f3249d7b78ed1b09faf3478bf",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=953fe4c8b71ee7f0e2e63b136f684205c3a95f5d",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b1d9a5dfb33e01fcda36c3f991481f5b430bb78",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df2abe3d648a732348a80314b8b3b98f9cf5f098",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lqo9lk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Risse",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqo9lk/i_ran_llamacpp_on_a_raspberry_pi/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=TNxIIDkP2Zg",
          "subreddit_subscribers": 494198,
          "created_utc": 1751545561,
          "num_crossposts": 0,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "Running AI on a Raspberry Pi",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Krisseck",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/TNxIIDkP2Zg/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Krisseck"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi Everyone \n\nI am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application \n\nNow, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path\n\nTIA\n",
          "author_fullname": "t2_eabbhyzu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp after Ollama for industry grade softwares",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqo8q0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751545491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone &lt;/p&gt;\n\n&lt;p&gt;I am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application &lt;/p&gt;\n\n&lt;p&gt;Now, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqo8q0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bull_bear25",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751545491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.  \nWhile I understand this doesn't exactly solve the privacy issue, this level of information loss is enough for my usecase.\n\nMy questions:  \n1. Is something like this even possible? Has anybody done something like this before?  \n2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)",
          "author_fullname": "t2_2ue366bm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to split a model to run a portion of it on client and run the remaining layers on server. Is that possible?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqo1bt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751544887,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.&lt;br/&gt;\nWhile I understand this doesn&amp;#39;t exactly solve the privacy issue, this level of information loss is enough for my usecase.&lt;/p&gt;\n\n&lt;p&gt;My questions:&lt;br/&gt;\n1. Is something like this even possible? Has anybody done something like this before?&lt;br/&gt;\n2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqo1bt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crazycodemonkey",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751544887,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Inspired by this post:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running\\_qwen3\\_235b\\_on\\_a\\_single\\_3060\\_12gb\\_6\\_ts/](https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/)\n\nI decided to try my luck with Qwen 235b so downloaded Unsloth's Q2XL. I've got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).\n\nMy runtime args:\n\nllama-cli -m Qwen3-235B-A22B-UD-Q2\\_K\\_XL-00001-of-00002.gguf -ot \".ffn\\_.\\*\\_exps.=CPU\" -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa\n\nSuper simple user prompt because I wasn't expecting miracles:\n\ntell me a joke\n\nResult:  \n8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven't tried any actual work on it yet.\n\ncli output blurb:\n\nllama\\_perf\\_sampler\\_print:    sampling time =      24.81 ms /   476 runs   (    0.05 ms per token, 19183.49 tokens per second)\n\nllama\\_perf\\_context\\_print:        load time =   16979.96 ms\n\nllama\\_perf\\_context\\_print: prompt eval time =    1497.01 ms /    12 tokens (  124.75 ms per token,     8.02 tokens per second)\n\nllama\\_perf\\_context\\_print:        eval time =   85040.21 ms /   463 runs   (  183.67 ms per token,     5.44 tokens per second)\n\nllama\\_perf\\_context\\_print:       total time =  100251.11 ms /   475 tokens\n\nQuestion:\n\nIt looks like I'm only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?\n\n**Edit**: Managed to fill out the rest of the VRAM with a draft model.   \n  \nGeneration went up to 9.8t/s:  \n[https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen\\_235b\\_16gb\\_vram\\_specdec\\_98ts\\_gen/](https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/)",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I can't believe it actually runs - Qwen 235b @ 16GB VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqnwih",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 200,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 200,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751572559,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751544478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by this post:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I decided to try my luck with Qwen 235b so downloaded Unsloth&amp;#39;s Q2XL. I&amp;#39;ve got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).&lt;/p&gt;\n\n&lt;p&gt;My runtime args:&lt;/p&gt;\n\n&lt;p&gt;llama-cli -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;/p&gt;\n\n&lt;p&gt;Super simple user prompt because I wasn&amp;#39;t expecting miracles:&lt;/p&gt;\n\n&lt;p&gt;tell me a joke&lt;/p&gt;\n\n&lt;p&gt;Result:&lt;br/&gt;\n8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven&amp;#39;t tried any actual work on it yet.&lt;/p&gt;\n\n&lt;p&gt;cli output blurb:&lt;/p&gt;\n\n&lt;p&gt;llama_perf_sampler_print:    sampling time =      24.81 ms /   476 runs   (    0.05 ms per token, 19183.49 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:        load time =   16979.96 ms&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print: prompt eval time =    1497.01 ms /    12 tokens (  124.75 ms per token,     8.02 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:        eval time =   85040.21 ms /   463 runs   (  183.67 ms per token,     5.44 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:       total time =  100251.11 ms /   475 tokens&lt;/p&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;It looks like I&amp;#39;m only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Managed to fill out the rest of the VRAM with a draft model.   &lt;/p&gt;\n\n&lt;p&gt;Generation went up to 9.8t/s:&lt;br/&gt;\n&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lqxs6n/qwen_235b_16gb_vram_specdec_98ts_gen/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqnwih",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 91,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751544478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm currently on my master's thesis and with my supervisor we figured that a real-time user-rule-based hallucination prevention framework is something interesting to work on.\n\nFor now, I built a custom RegexLogitsProcessor class that takes a Regex pattern as an input and sets the logits to infinity and therefore are not chosen, which match the Regex pattern. To illustrate this, the most simple use-case is that no digits are allowed in the output and the Regex is set to \"\\\\d\".\n\n[https://github.com/lebe1/LettucePrevent/blob/main/logits\\_processor\\_detector.py](https://github.com/lebe1/LettucePrevent/blob/main/logits_processor_detector.py)\n\nAnother idea wsa stick within the Huggingface framework and therefore the LogitsProcessor was chosen over the StoppingCriteria.\n\n[https://huggingface.co/docs/transformers.js/main/en/api/generation/logits\\_process](https://huggingface.co/docs/transformers.js/main/en/api/generation/logits_process)\n\nIn my next attempt, I'm trying to extend this class to input a custom python class so that the user can also work with the input and have a case suitable for RAG cases for example like \"no other numbers than mentioned in the input\".\n\nCurrently I like the approach with Regex due to its transparency but I would be really interested what your thoughts are on this. The only alternative I see could be an NER approach. Could you recommend something like that? What critics do you have in mind with this whole idea or what other features could you see with such a framework?",
          "author_fullname": "t2_dy1jw7m8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hallucination prevention framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqnvfr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751544391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently on my master&amp;#39;s thesis and with my supervisor we figured that a real-time user-rule-based hallucination prevention framework is something interesting to work on.&lt;/p&gt;\n\n&lt;p&gt;For now, I built a custom RegexLogitsProcessor class that takes a Regex pattern as an input and sets the logits to infinity and therefore are not chosen, which match the Regex pattern. To illustrate this, the most simple use-case is that no digits are allowed in the output and the Regex is set to &amp;quot;\\d&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/lebe1/LettucePrevent/blob/main/logits_processor_detector.py\"&gt;https://github.com/lebe1/LettucePrevent/blob/main/logits_processor_detector.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Another idea wsa stick within the Huggingface framework and therefore the LogitsProcessor was chosen over the StoppingCriteria.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/docs/transformers.js/main/en/api/generation/logits_process\"&gt;https://huggingface.co/docs/transformers.js/main/en/api/generation/logits_process&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In my next attempt, I&amp;#39;m trying to extend this class to input a custom python class so that the user can also work with the input and have a case suitable for RAG cases for example like &amp;quot;no other numbers than mentioned in the input&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Currently I like the approach with Regex due to its transparency but I would be really interested what your thoughts are on this. The only alternative I see could be an NER approach. Could you recommend something like that? What critics do you have in mind with this whole idea or what other features could you see with such a framework?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?auto=webp&amp;s=97cffbab2a2683ead2d609c30a55166e8f7dc2bf",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87d8479c331c36beeb3bfedb2bdc6a12c0645526",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b88cd4f0edfe81f9d10c2c597d117eac1abb7de4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa4ed0560e5970b0d9143ecc1c771773742acdfa",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f5511c86c6822362408285424603198c70978b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=43ad9da53b859a94f24b9a1af5236ed5e5c4a520",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=76d69410bfbd870b1f975e18c5a1a7ab707eec59",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqnvfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lebe1",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqnvfr/hallucination_prevention_framework/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqnvfr/hallucination_prevention_framework/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751544391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past year, we’ve learned a lot from this community while exploring model merging. Now we’re giving back with **Mergenetic**, an open-source library that makes *evolutionary* merging practical without needing big hardware.\n\nWhat it does:\n\n* Evolves high-quality LLM merges using evolutionary algorithms\n* Supports SLERP, TIES, DARE, Task Arithmetic, and more\n* Efficient: search happens in parameter space, not gradient needed\n* Modular, hackable, and built on familiar tools (`mergekit`, `pymoo`, `lm-eval-harness`)\n\nRun it via Python, CLI, or GUI — and try some wild merge experiments on your own GPU.\n\nFor details, check out our papers:\n\n* ACL 2025 Demo: [arxiv.org/abs/2505.11427](https://arxiv.org/pdf/2505.11427)\n* ICML 2025: [arxiv.org/abs/2502.10436](https://arxiv.org/pdf/2502.10436)\n\n🔗 [GitHub: tommasomncttn/mergenetic](https://github.com/tommasomncttn/mergenetic)\n\nWould love feedback or contributions — hope it’s useful to some of you!",
          "author_fullname": "t2_81hdual0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hey r/LocalLLaMA! We made evolutionary model merging feasible on consumer GPUs – meet Mergenetic 🧬",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqndyy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751542836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past year, we’ve learned a lot from this community while exploring model merging. Now we’re giving back with &lt;strong&gt;Mergenetic&lt;/strong&gt;, an open-source library that makes &lt;em&gt;evolutionary&lt;/em&gt; merging practical without needing big hardware.&lt;/p&gt;\n\n&lt;p&gt;What it does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Evolves high-quality LLM merges using evolutionary algorithms&lt;/li&gt;\n&lt;li&gt;Supports SLERP, TIES, DARE, Task Arithmetic, and more&lt;/li&gt;\n&lt;li&gt;Efficient: search happens in parameter space, not gradient needed&lt;/li&gt;\n&lt;li&gt;Modular, hackable, and built on familiar tools (&lt;code&gt;mergekit&lt;/code&gt;, &lt;code&gt;pymoo&lt;/code&gt;, &lt;code&gt;lm-eval-harness&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Run it via Python, CLI, or GUI — and try some wild merge experiments on your own GPU.&lt;/p&gt;\n\n&lt;p&gt;For details, check out our papers:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ACL 2025 Demo: &lt;a href=\"https://arxiv.org/pdf/2505.11427\"&gt;arxiv.org/abs/2505.11427&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;ICML 2025: &lt;a href=\"https://arxiv.org/pdf/2502.10436\"&gt;arxiv.org/abs/2502.10436&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;🔗 &lt;a href=\"https://github.com/tommasomncttn/mergenetic\"&gt;GitHub: tommasomncttn/mergenetic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback or contributions — hope it’s useful to some of you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqndyy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "leviatan0",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751542836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqnczx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ed3bce847dca1ffb246b2e4a76b4dfbd83826764",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751542748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/AIDC-AI/Ovis-U1-3B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?auto=webp&amp;s=b9296215e82ba455412506ddeeca9fff813a9e05",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dbe24b46aef7931d46e3c6ae47f1acbde8eede0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e84b5b96ed8596b3ccbfcc88d1c992ba56a96324",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a49894c6b3a41b86514167d98bb24020d7fb049",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b7cac54a1763a205a826aebda0fe3bb69d0bd91",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9af4c55bd61fd6f8b0d3bbcacb73b3ddd7db0fc",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e924ac77adc7ff845a2a7cf9100edcc0e1add8b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqnczx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/",
          "stickied": false,
          "url": "https://huggingface.co/AIDC-AI/Ovis-U1-3B",
          "subreddit_subscribers": 494198,
          "created_utc": 1751542748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Down a deep rabbit hole of prompt eng, fine tuning w Unsloth, but not getting any great results.\n\nMy use case: Creating social content which sounds like me, not AI slop.\n\nWhat's the best way to do this nowadays? Would appreciate any direction\n\nEdit for more context: Right now I'm generating content with a powerful model, then I'm aiming to do the 'styling' in a final call.",
          "author_fullname": "t2_165nf2mrsb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to get an LLM to sound like me? Prompt eng or Finetune?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqmmv2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751543339,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751540252,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Down a deep rabbit hole of prompt eng, fine tuning w Unsloth, but not getting any great results.&lt;/p&gt;\n\n&lt;p&gt;My use case: Creating social content which sounds like me, not AI slop.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to do this nowadays? Would appreciate any direction&lt;/p&gt;\n\n&lt;p&gt;Edit for more context: Right now I&amp;#39;m generating content with a powerful model, then I&amp;#39;m aiming to do the &amp;#39;styling&amp;#39; in a final call.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqmmv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RelevantPractice2074",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751540252,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "### Anyone here run llama4 with 1 million to 10 million context?\n\nJust curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.\n\nWhat are vram/ram requirements for 1m context? 10m context?",
          "author_fullname": "t2_3h2irqtz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here run llama4 scout/Maverick with 1 million to 10 million context?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqmbh3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751540265,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751539102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h3&gt;Anyone here run llama4 with 1 million to 10 million context?&lt;/h3&gt;\n\n&lt;p&gt;Just curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.&lt;/p&gt;\n\n&lt;p&gt;What are vram/ram requirements for 1m context? 10m context?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqmbh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "night0x63",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751539102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I wanted to share my experience and hear about yours.\n\nHardware : \n\nGPU : 3060 12GB\nCPU : i5-3060\nRAM : 32GB\n\nFront-end : Koboldcpp + open-webui\n\nUse cases : General Q&amp;A, Long context RAG, Humanities, Summarization, Translation, code. \n\nI've been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. \n\nGEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they're quite fast, and have a good ability to stick to the prompt. \n\nGemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it's variants. \n\nWhat are your experiences? Do you use other models of the same range? \n\nGood day everyone! \n\n\n",
          "author_fullname": "t2_cpgzcud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Yappp - Yet Another Poor Peasent Post",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqlsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751537200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I wanted to share my experience and hear about yours.&lt;/p&gt;\n\n&lt;p&gt;Hardware : &lt;/p&gt;\n\n&lt;p&gt;GPU : 3060 12GB\nCPU : i5-3060\nRAM : 32GB&lt;/p&gt;\n\n&lt;p&gt;Front-end : Koboldcpp + open-webui&lt;/p&gt;\n\n&lt;p&gt;Use cases : General Q&amp;amp;A, Long context RAG, Humanities, Summarization, Translation, code. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. &lt;/p&gt;\n\n&lt;p&gt;GEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they&amp;#39;re quite fast, and have a good ability to stick to the prompt. &lt;/p&gt;\n\n&lt;p&gt;Gemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it&amp;#39;s variants. &lt;/p&gt;\n\n&lt;p&gt;What are your experiences? Do you use other models of the same range? &lt;/p&gt;\n\n&lt;p&gt;Good day everyone! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqlsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "needthosepylons",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751537200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I host deepseek and other models locally, but I am limited to the speed of my machine.\n\nAnyone subscribed to cloud providers where deepseek and other models are hosted, and they'll just give you an api key to use it or something?",
          "author_fullname": "t2_kh59ca8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which cloud compute are you using?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqlcbu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751535396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I host deepseek and other models locally, but I am limited to the speed of my machine.&lt;/p&gt;\n\n&lt;p&gt;Anyone subscribed to cloud providers where deepseek and other models are hosted, and they&amp;#39;ll just give you an api key to use it or something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqlcbu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rushblyatiful",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqlcbu/which_cloud_compute_are_you_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqlcbu/which_cloud_compute_are_you_using/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751535396,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, this is Emre from the Jan team.   \n  \nWe've been testing MCP servers in Jan Beta, and last week we promoted the feature to the stable with v0.6.2 build as an experimental feature, and ditched Jan Beta. So Jan is now experimenting with MCP Servers.\n\nHow to try MCP in Jan:\n\n* Settings -&gt; General -&gt; toggle \"Experimental Features\"\n* A new \"MCP Servers\" tab appears -&gt; add or enable your server\n\nQuick tip: To use MCP servers, make sure the model's Tools capability is enabled.\n\nFull doc with screenshots: [https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan](https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan)\n\nQuick note, this is still an experimental feature, please expect bugs, and flagging bugs would be super helpful for us to improve the capabilities.\n\nPlus, since then we've pushed a few hot-fixes to smooth out model loading and MCP performance.\n\nOther recent fixes &amp; tweaks:\n\n* CORS bypass for localhost providers (Ollama :11434, LM Studio :1234).\n* We fixed a bug that caused some GGUF models to get stuck while loading.\n* Lighter UI polish and clearer error messages.\n\nWith this update, Jan now supports [Jan-nano 4B ](https://huggingface.co/Menlo/Jan-nano-gguf)as well, it's available in Jan Hub. For the best experience, we suggest using the model for web searches and the 128K variant for deep-research tasks.\n\nFor the latest build, please update your Jan or [download the latest](https://jan.ai/).",
          "author_fullname": "t2_g6cmmsdd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jan now supports MCP servers as an experimental feature",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqkknh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 93,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1844,
              "scrubber_media_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8sdnjxd6emaf1/DASHPlaylist.mpd?a=1754199034%2CNmJiOWU3MDUyZWNhODEwZTQ0ZDZiMDljYzA4ZDI2YTliNGM4ODY0MjZiMjZmZDNkMDdjMmViNDRjYTJkMDQ4ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/8sdnjxd6emaf1/HLSPlaylist.m3u8?a=1754199034%2CZGNlZjIyZDljYTkzNzg1Y2Y4ZGY3NjY1YTIzMjE4ZGQ2MTI4YzUxZjExNTA2NzJmNWE4MzZlMmIyZjRiNzQ0Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 93,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=140&amp;height=81&amp;crop=140:81,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=09b4f8b875ef9edc6171e54b553d10bb25844acb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751532281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, this is Emre from the Jan team.   &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been testing MCP servers in Jan Beta, and last week we promoted the feature to the stable with v0.6.2 build as an experimental feature, and ditched Jan Beta. So Jan is now experimenting with MCP Servers.&lt;/p&gt;\n\n&lt;p&gt;How to try MCP in Jan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Settings -&amp;gt; General -&amp;gt; toggle &amp;quot;Experimental Features&amp;quot;&lt;/li&gt;\n&lt;li&gt;A new &amp;quot;MCP Servers&amp;quot; tab appears -&amp;gt; add or enable your server&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Quick tip: To use MCP servers, make sure the model&amp;#39;s Tools capability is enabled.&lt;/p&gt;\n\n&lt;p&gt;Full doc with screenshots: &lt;a href=\"https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan\"&gt;https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quick note, this is still an experimental feature, please expect bugs, and flagging bugs would be super helpful for us to improve the capabilities.&lt;/p&gt;\n\n&lt;p&gt;Plus, since then we&amp;#39;ve pushed a few hot-fixes to smooth out model loading and MCP performance.&lt;/p&gt;\n\n&lt;p&gt;Other recent fixes &amp;amp; tweaks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CORS bypass for localhost providers (Ollama :11434, LM Studio :1234).&lt;/li&gt;\n&lt;li&gt;We fixed a bug that caused some GGUF models to get stuck while loading.&lt;/li&gt;\n&lt;li&gt;Lighter UI polish and clearer error messages.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With this update, Jan now supports &lt;a href=\"https://huggingface.co/Menlo/Jan-nano-gguf\"&gt;Jan-nano 4B &lt;/a&gt;as well, it&amp;#39;s available in Jan Hub. For the best experience, we suggest using the model for web searches and the 128K variant for deep-research tasks.&lt;/p&gt;\n\n&lt;p&gt;For the latest build, please update your Jan or &lt;a href=\"https://jan.ai/\"&gt;download the latest&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8sdnjxd6emaf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?format=pjpg&amp;auto=webp&amp;s=1929c5a4b61c99193b1833755d039edb74f19a3c",
                  "width": 1844,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=99ba372d8e15e8f7353b28e194025a6b24b15ab3",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cce0560ef577d057a0d7469534309de7c6f46bd5",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6124b9816056b8b748c6aa3ff3e8d6bf98b21a71",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1a18e91c3202fd243157eeeace45acc966d35668",
                    "width": 640,
                    "height": 374
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=784793efb8baa04efbb591a36282b4cbf5249c85",
                    "width": 960,
                    "height": 562
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b064a2c42cc30702930582d519e1812a979351db",
                    "width": 1080,
                    "height": 632
                  }
                ],
                "variants": {},
                "id": "azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lqkknh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eck72",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/",
          "stickied": false,
          "url": "https://v.redd.it/8sdnjxd6emaf1",
          "subreddit_subscribers": 494198,
          "created_utc": 1751532281,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1844,
              "scrubber_media_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8sdnjxd6emaf1/DASHPlaylist.mpd?a=1754199034%2CNmJiOWU3MDUyZWNhODEwZTQ0ZDZiMDljYzA4ZDI2YTliNGM4ODY0MjZiMjZmZDNkMDdjMmViNDRjYTJkMDQ4ZA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/8sdnjxd6emaf1/HLSPlaylist.m3u8?a=1754199034%2CZGNlZjIyZDljYTkzNzg1Y2Y4ZGY3NjY1YTIzMjE4ZGQ2MTI4YzUxZjExNTA2NzJmNWE4MzZlMmIyZjRiNzQ0Nw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently looking for some locally deployable model that can help me extract insights/values from graphical representations as you would find them in management or investor presentations. \n\n  \nWhile grabbing financials from tables and regular text does not pose an issue, I struggle finding a small model that I can run locally without throwing much compute at it to extract values and insights from more complex visual representations (see below). \n\n  \nI don't need to have this run extremely fast, so I can sacrifice execution speed in the name of higher accuracy, but of course the execution time should remain reasonable. \n\n  \nAre there any models specifically trained or especially good at this? I have been playing around with Gemma3n and Qwen 2.5VL 4B but both are not performing at the level I would like. \n\n  \nHere are some examples of what I am talking about:\n\nhttps://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;format=png&amp;auto=webp&amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5\n\nhttps://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;format=png&amp;auto=webp&amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64\n\n  \n",
          "author_fullname": "t2_uh7yd5wu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small VisualLM for Data/Insight Extraction from Graphs &amp; Charts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ds8ddvvm9maf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33285e25461ea29199d8e6d8722f89da026989ff"
                },
                {
                  "y": 147,
                  "x": 216,
                  "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=314ea41fcc2d7c56c22706502bda4adbe9bfe4a4"
                },
                {
                  "y": 218,
                  "x": 320,
                  "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1c332b035f961d09034c43e4ad7f50e850221bb"
                }
              ],
              "s": {
                "y": 394,
                "x": 578,
                "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;format=png&amp;auto=webp&amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5"
              },
              "id": "ds8ddvvm9maf1"
            },
            "0gchf8zz9maf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e206f78c50d353f841847569a6fffb5d64fbb826"
                },
                {
                  "y": 114,
                  "x": 216,
                  "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=790d83d524eef70fa831df1bf225e10c65b98e12"
                },
                {
                  "y": 170,
                  "x": 320,
                  "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de4c307aa34a89a1163bc7a6dff424fe8e1557c5"
                }
              ],
              "s": {
                "y": 336,
                "x": 632,
                "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;format=png&amp;auto=webp&amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64"
              },
              "id": "0gchf8zz9maf1"
            }
          },
          "name": "t3_1lqk18o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pjHRkVQ1PimSGcaJZLdG1dhuTVGrTkRLwHicOv-aLFg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751530024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently looking for some locally deployable model that can help me extract insights/values from graphical representations as you would find them in management or investor presentations. &lt;/p&gt;\n\n&lt;p&gt;While grabbing financials from tables and regular text does not pose an issue, I struggle finding a small model that I can run locally without throwing much compute at it to extract values and insights from more complex visual representations (see below). &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need to have this run extremely fast, so I can sacrifice execution speed in the name of higher accuracy, but of course the execution time should remain reasonable. &lt;/p&gt;\n\n&lt;p&gt;Are there any models specifically trained or especially good at this? I have been playing around with Gemma3n and Qwen 2.5VL 4B but both are not performing at the level I would like. &lt;/p&gt;\n\n&lt;p&gt;Here are some examples of what I am talking about:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5\"&gt;https://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64\"&gt;https://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqk18o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Possible-Tomatillo80",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqk18o/small_visuallm_for_datainsight_extraction_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqk18o/small_visuallm_for_datainsight_extraction_from/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751530024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I feel I can easily tie it in inconsistencies and knots with basic debating techniques (e.g. false binary's).\n\nDon't make me feel alone...",
          "author_fullname": "t2_wxq0v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Okay, I love arguing with me LocalLaMA and feeling like I'm winning. Am I strange?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqjccq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.37,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751527584,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751527238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel I can easily tie it in inconsistencies and knots with basic debating techniques (e.g. false binary&amp;#39;s).&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t make me feel alone...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqjccq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wandering_cat_ninja",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqjccq/okay_i_love_arguing_with_me_locallama_and_feeling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqjccq/okay_i_love_arguing_with_me_locallama_and_feeling/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751527238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/cactus-compute/cactus](https://github.com/cactus-compute/cactus)  \n[https://github.com/jafioti/luminal](https://github.com/jafioti/luminal) ( Rust )\n\n  \nCatus seems to start from fork of llama.cpp. (similar to Ollama)\n\nLuminal is more interesting since it rebuild everything.  \nGeoHot from Tinygrad is quite active in Luminal's Discord too.\n\n  \n",
          "author_fullname": "t2_lgebhlu22",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sharing new inference engines I got to know recently",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqj3eq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751526278,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/cactus-compute/cactus\"&gt;https://github.com/cactus-compute/cactus&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/jafioti/luminal\"&gt;https://github.com/jafioti/luminal&lt;/a&gt; ( Rust )&lt;/p&gt;\n\n&lt;p&gt;Catus seems to start from fork of llama.cpp. (similar to Ollama)&lt;/p&gt;\n\n&lt;p&gt;Luminal is more interesting since it rebuild everything.&lt;br/&gt;\nGeoHot from Tinygrad is quite active in Luminal&amp;#39;s Discord too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?auto=webp&amp;s=08f047a670922c7500dd06ecd9c55d5ba293b830",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6566c025c702b9994234a8b5ba87dfd1d264a9bb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=97951e6dbbfba26407f8e020163aa947fbc8a66f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c54299406c114e53a88ed9780140341612aacdb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8697210b29e9d7b2b5ec2f265e89f7ff530b0b1f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c3babc126be376ccc7efcb605a50068b2621b9d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc94cc1deffa62c7bfd0c2f2ca47204b3c395bba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqj3eq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AggressiveHunt2300",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751526278,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "By training from scratch with only reinforcement learning (RL), DeepSWE-Preview with test time scaling (TTS) solves 59% of problems, beating all open-source agents by a large margin. We note that DeepSWE-Preview’s Pass@1 performance (42.2%, averaged over 16 runs) is one of the best for open-weights coding agents.\n\n[https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33](https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33)",
          "author_fullname": "t2_4fuhv1gu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqi863",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 118,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 118,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=489306336ce01e38e0f62b2b5a2cfe4806029ebe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751522913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By training from scratch with only reinforcement learning (RL), DeepSWE-Preview with test time scaling (TTS) solves 59% of problems, beating all open-source agents by a large margin. We note that DeepSWE-Preview’s Pass@1 performance (42.2%, averaged over 16 runs) is one of the best for open-weights coding agents.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33\"&gt;https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/agentica-org/DeepSWE-Preview",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?auto=webp&amp;s=b68c82b9696315b86d709c54c3c41a99aa05dc23",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4fb09c54c0adff13ca3fb65fb5340199f3025b3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bcd434649182258566913ef3f41a70f3c75a9510",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a26f90ae5dbd065e1013fc72bdcfe1390aed66a0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=01af29ebca7d21fb21f6786fc5df5242a0853781",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=459b2d50c5eb6397f55a865fd129b855f501e1a8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7be7f0dadc075fe7e858c95edc82bdabfe50ad4d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqi863",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "touhidul002",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/",
          "stickied": false,
          "url": "https://huggingface.co/agentica-org/DeepSWE-Preview",
          "subreddit_subscribers": 494198,
          "created_utc": 1751522913,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As per the title, which one is better?\n\nBoth in raw performance, and in price per performance.\n\nThe 2080Ti 22GB is 350 usd while the 3080 20gb is 450 usd. Where I am, 3090s still go for 1000+ usd so that’s not a good option.\n\nEDIT 1: By the way, I plan on getting two and maybe adding more, I’ll probably be using a desktop ATX setup since server stuff is expensive probably a 3600 with a b450 unless anyone has a better idea",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "2080 TI 22GB or 3080 20GB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqi5q0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751522659,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title, which one is better?&lt;/p&gt;\n\n&lt;p&gt;Both in raw performance, and in price per performance.&lt;/p&gt;\n\n&lt;p&gt;The 2080Ti 22GB is 350 usd while the 3080 20gb is 450 usd. Where I am, 3090s still go for 1000+ usd so that’s not a good option.&lt;/p&gt;\n\n&lt;p&gt;EDIT 1: By the way, I plan on getting two and maybe adding more, I’ll probably be using a desktop ATX setup since server stuff is expensive probably a 3600 with a b450 unless anyone has a better idea&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqi5q0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqi5q0/2080_ti_22gb_or_3080_20gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqi5q0/2080_ti_22gb_or_3080_20gb/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751522659,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's been a while and llama maverick and scout are still shite. I have tried nearly every provider at this point. \n\nAny updates if they're gonna launch any improvements to these models or any new reasoning models? \n\nHow are they fucking up this bad? Near unlimited money, resources, researchers. What are they doing wrong? \n\nThey weren't that far behind in the LLM race compared to Google and now they are like behind everyone at this point. \n\nAnd any updates on Microsoft? They're not gonna do their own models \"Big Ones\" and are completely reliant on OpenAI?\n\nChinese companies are releasing models left and right... I tested Ernie models and they're better than Llama 4s\n\nDeepSeek-V3-0324 seems to be the best non-reasoning open source LLM we have.\n\nAre there even any projects that have attempted to improve Llama4s via fine-tuning it or other magical techniques we have? God it's so shite, it's comprehension abilities are just embarrassing. It feels like you can find a million models that are far better than llama 4s for almost anything. The only thing they seem to have is speed on VRAM constrained setups but what's the point when then responses are useless? It's a waste of resource at this point. ",
          "author_fullname": "t2_yfi9sqrzf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any updates on Llama models from Meta?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqhers",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751519922,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been a while and llama maverick and scout are still shite. I have tried nearly every provider at this point. &lt;/p&gt;\n\n&lt;p&gt;Any updates if they&amp;#39;re gonna launch any improvements to these models or any new reasoning models? &lt;/p&gt;\n\n&lt;p&gt;How are they fucking up this bad? Near unlimited money, resources, researchers. What are they doing wrong? &lt;/p&gt;\n\n&lt;p&gt;They weren&amp;#39;t that far behind in the LLM race compared to Google and now they are like behind everyone at this point. &lt;/p&gt;\n\n&lt;p&gt;And any updates on Microsoft? They&amp;#39;re not gonna do their own models &amp;quot;Big Ones&amp;quot; and are completely reliant on OpenAI?&lt;/p&gt;\n\n&lt;p&gt;Chinese companies are releasing models left and right... I tested Ernie models and they&amp;#39;re better than Llama 4s&lt;/p&gt;\n\n&lt;p&gt;DeepSeek-V3-0324 seems to be the best non-reasoning open source LLM we have.&lt;/p&gt;\n\n&lt;p&gt;Are there even any projects that have attempted to improve Llama4s via fine-tuning it or other magical techniques we have? God it&amp;#39;s so shite, it&amp;#39;s comprehension abilities are just embarrassing. It feels like you can find a million models that are far better than llama 4s for almost anything. The only thing they seem to have is speed on VRAM constrained setups but what&amp;#39;s the point when then responses are useless? It&amp;#39;s a waste of resource at this point. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqhers",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "True_Requirement_891",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751519922,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dots\n\nMinimax\n\nHunyuan\n\nErnie\n\n\nI’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.\n\nSorry, just wanted to put this out here.",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No love for these new models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqh55j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 191,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 191,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751519001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dots&lt;/p&gt;\n\n&lt;p&gt;Minimax&lt;/p&gt;\n\n&lt;p&gt;Hunyuan&lt;/p&gt;\n\n&lt;p&gt;Ernie&lt;/p&gt;\n\n&lt;p&gt;I’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.&lt;/p&gt;\n\n&lt;p&gt;Sorry, just wanted to put this out here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqh55j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751519001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For the past few months, I’ve been developing [ProseBird](https://prosebird.com)—originally a collaborative online teleprompter—as a solo technical founder, and recently decided to pivot to a script-based AI speech coaching tool.\n\nBesides technical and commercial feasibility, making this pivot really hinges on finding an awesome technical co-founder to lead development of what would be such a crucial part of the project: AI.\n\nWe wouldn’t be starting from scratch, both the original and the new vision for ProseBird share significant infrastructure, so much of the existing backend, architecture, and codebase can be leveraged for the pivot.\n\nSo if (1) you’re experienced with LLMs / ML / NLP / TTS &amp; STT / overall voice AI; and (2) the idea of working extremely hard building a product of which you own 50% excites you, shoot me a DM so we can talk.\n\nWeb or mobile dev experience is a plus.",
          "author_fullname": "t2_mql3elrw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a Technical Co-Founder to Lead AI Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqeya7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751511748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past few months, I’ve been developing &lt;a href=\"https://prosebird.com\"&gt;ProseBird&lt;/a&gt;—originally a collaborative online teleprompter—as a solo technical founder, and recently decided to pivot to a script-based AI speech coaching tool.&lt;/p&gt;\n\n&lt;p&gt;Besides technical and commercial feasibility, making this pivot really hinges on finding an awesome technical co-founder to lead development of what would be such a crucial part of the project: AI.&lt;/p&gt;\n\n&lt;p&gt;We wouldn’t be starting from scratch, both the original and the new vision for ProseBird share significant infrastructure, so much of the existing backend, architecture, and codebase can be leveraged for the pivot.&lt;/p&gt;\n\n&lt;p&gt;So if (1) you’re experienced with LLMs / ML / NLP / TTS &amp;amp; STT / overall voice AI; and (2) the idea of working extremely hard building a product of which you own 50% excites you, shoot me a DM so we can talk.&lt;/p&gt;\n\n&lt;p&gt;Web or mobile dev experience is a plus.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lqeya7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Puzzleheaded-Cow7240",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqeya7/looking_for_a_technical_cofounder_to_lead_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqeya7/looking_for_a_technical_cofounder_to_lead_ai/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751511748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as the title says, i have 250,000 6000 word files and i want to be able to query them. they are legal documents, what model would run flawlessly on my mac air m2. thanks",
          "author_fullname": "t2_1nb285wcw0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "best local llm for 250,000 json with 6000 words each",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqeogc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751510910,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as the title says, i have 250,000 6000 word files and i want to be able to query them. they are legal documents, what model would run flawlessly on my mac air m2. thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqeogc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Substantial-Gear1150",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751510910,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Paper: [https://arxiv.org/abs/2507.01949](https://arxiv.org/abs/2507.01949)\n\nProject Page: [https://kwai-keye.github.io/](https://kwai-keye.github.io/)\n\nCode: [https://github.com/Kwai-Keye/Keye](https://github.com/Kwai-Keye/Keye)\n\n&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kwai-Keye/Keye-VL-8B-Preview · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqebbv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#93b1ba",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2114f43f8a483d864b74ab185248e7dc7f2acd59",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751509766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.01949\"&gt;https://arxiv.org/abs/2507.01949&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Project Page: &lt;a href=\"https://kwai-keye.github.io/\"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/Kwai-Keye/Keye\"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?auto=webp&amp;s=5ac1fd6f606741f5c30142be649c50021a8588ec",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1bc04f5722002b089d9f495fa7cdaf7f3700c9e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa0520732fdbe4ef3053c95ef226e0a6ee79c4f6",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76e575e0f61ad8ceb6ddc30f00b7be46f6ec4694",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a674ea1c399ba022e42f0633ac66250ac99a0f9e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0eae6b8b89456d5ebcfbd6cc4c7678b03dd124f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ba496f256b1dae2db014d8c779db7cafd30a82b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqebbv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview",
          "subreddit_subscribers": 494198,
          "created_utc": 1751509766,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Excited to share my first open source project - PrivateScribe.ai.\n\nI’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.\n\nI’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.\n\nWould love to hear any thoughts on the idea or things people would want for other use cases. ",
          "author_fullname": "t2_fyeeexf0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PrivateScribe.ai - a fully local, MIT licensed AI transcription platform",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqdcgr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 132,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 132,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=9dbeb4c6281123fa21d509578a50cfc85df401dd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751506831,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "privatescribe.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excited to share my first open source project - PrivateScribe.ai.&lt;/p&gt;\n\n&lt;p&gt;I’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.&lt;/p&gt;\n\n&lt;p&gt;I’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear any thoughts on the idea or things people would want for other use cases. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "http://www.privatescribe.ai",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?auto=webp&amp;s=4514d508aeafde08a5bf2c109c4112208d74e725",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=661abd40d4b281a21242971a11e9c626c59267d5",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=56c86363954748ce878868f121b02e0970c4805c",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac70862f336d47cb441412352671d559c5d70fcb",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=67733391cf800cb69df3f2bbf96c8c0dcd8a7ecb",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6b70e78f6a72fb3566587214ead3cce77b403ba",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lqdcgr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SecondPathDev",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/",
          "stickied": false,
          "url": "http://www.privatescribe.ai",
          "subreddit_subscribers": 494198,
          "created_utc": 1751506831,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to generate voiceovers for info videos that I'm creating.  \nMy own voice isn't that great and I don't have a good mic.\n\nI do, however, have an nvidia card that I've been using to generate images.  \nI've also been able to run an llm locally, so I imagine that my machine is capable of running a text-to-speech ai as well.\n\nSearching google and reddit for text-to-speech generators has left me a little overwhelmed, so I'd like to hear your suggestions.\n\nI tried to install spark-tts, but I wasn't able to install all the requirements. I think that the included scripts for installing requirements didn't cover all the dependancies.\n\n",
          "author_fullname": "t2_1l59r1ow96",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local text-to-speech generator for inux?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqcbfp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751503734,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to generate voiceovers for info videos that I&amp;#39;m creating.&lt;br/&gt;\nMy own voice isn&amp;#39;t that great and I don&amp;#39;t have a good mic.&lt;/p&gt;\n\n&lt;p&gt;I do, however, have an nvidia card that I&amp;#39;ve been using to generate images.&lt;br/&gt;\nI&amp;#39;ve also been able to run an llm locally, so I imagine that my machine is capable of running a text-to-speech ai as well.&lt;/p&gt;\n\n&lt;p&gt;Searching google and reddit for text-to-speech generators has left me a little overwhelmed, so I&amp;#39;d like to hear your suggestions.&lt;/p&gt;\n\n&lt;p&gt;I tried to install spark-tts, but I wasn&amp;#39;t able to install all the requirements. I think that the included scripts for installing requirements didn&amp;#39;t cover all the dependancies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqcbfp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImpossibleBritches",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqcbfp/local_texttospeech_generator_for_inux/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqcbfp/local_texttospeech_generator_for_inux/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751503734,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek-TNG-R1T2-Chimera -  200% faster than R1-0528 &amp; 20% faster than R1",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqbmwa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 204,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 204,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=139f896561e4c6c7b556acb4de4b90b54d16e385",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751501716,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?auto=webp&amp;s=b23d2fb081204a6e386b9278a1d5ea0ea845dea6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbd48e87f3899f1a905aa35dd70eedbab3ddce51",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6c4bcfb57ef62906d9037ff12c88c65c57179f0",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9328ca8146504507286068714fc6edba99eed0a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97a766bd7b9c921ab450ffb020d26db72a498fc7",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=33faf4c63b53541e5a6f12123dfd625a162da5bb",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecadf46be4d968dbb435dde8fa5e60eaf2d4b671",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqbmwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/",
          "stickied": false,
          "url": "https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera",
          "subreddit_subscribers": 494198,
          "created_utc": 1751501716,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am completely new to this. I was planning to install a local LLM and have it read my study material so I can quickly ask for definitions,etc\n\nI only really want to use it as an index and don't need it to solve any problems.  \nWhich LLM should I try out first?\n\n  \nMy current setup is :  \nCPU - i5-12450H  \nGPU - Nvidia RTX4050  \nRam - 16GB",
          "author_fullname": "t2_9bpzze24",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help in deciding llm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqa7cd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751497670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am completely new to this. I was planning to install a local LLM and have it read my study material so I can quickly ask for definitions,etc&lt;/p&gt;\n\n&lt;p&gt;I only really want to use it as an index and don&amp;#39;t need it to solve any problems.&lt;br/&gt;\nWhich LLM should I try out first?&lt;/p&gt;\n\n&lt;p&gt;My current setup is :&lt;br/&gt;\nCPU - i5-12450H&lt;br/&gt;\nGPU - Nvidia RTX4050&lt;br/&gt;\nRam - 16GB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqa7cd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Atriays",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqa7cd/need_help_in_deciding_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqa7cd/need_help_in_deciding_llm/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751497670,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I’m having a hard time trying to fine tune qwen2.5 VL (from mlx-community/Qwen2.5-VL-7B-Instruct-4bit) using mlx-vlm on my MacBook.\n\nI’ve spent countless hours trying different solutions but I always end up stuck with a new error…\n\nCould anyone provide a notebook that is working so that I can adapt it with my needs?\n\nThank you very much!",
          "author_fullname": "t2_a11ncwe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help needed: finetuning Qwen2.5 VL with mox-vol",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9yjy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751497014,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I’m having a hard time trying to fine tune qwen2.5 VL (from mlx-community/Qwen2.5-VL-7B-Instruct-4bit) using mlx-vlm on my MacBook.&lt;/p&gt;\n\n&lt;p&gt;I’ve spent countless hours trying different solutions but I always end up stuck with a new error…&lt;/p&gt;\n\n&lt;p&gt;Could anyone provide a notebook that is working so that I can adapt it with my needs?&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq9yjy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gladstone025",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9yjy/help_needed_finetuning_qwen25_vl_with_moxvol/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq9yjy/help_needed_finetuning_qwen25_vl_with_moxvol/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751497014,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a total noob to all this. I was having really good results with Gemini 2.5 Pro, o4-mini, and Claude 4.0 Sonnet in VScode. \n\nI decided to try a few local models on my nVidia 8GB RTX 2060 Super (cpu AMD Ryzen 9 3900 12-core, RAM 64GB)\n\nI tested  the following models with Roo/ollama:\n1) gemma3n:e2b-it-q4_K_M \n2_ hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\n3) deepseek-r1:8b                                         \n\nI have not had good experiences with these models. Probably my hardware limitations. \n\nI'd love to know more and figure out if I can get workable solutions for a reasonable hardware upgrade, or if I should just stick to remote models.\n\nIs it simply that I need to upgrade to a more powerful GPU like a 3090 to get real results from local LLM?",
          "author_fullname": "t2_39mp2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it simply about upgrading?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9lkd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751496063,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a total noob to all this. I was having really good results with Gemini 2.5 Pro, o4-mini, and Claude 4.0 Sonnet in VScode. &lt;/p&gt;\n\n&lt;p&gt;I decided to try a few local models on my nVidia 8GB RTX 2060 Super (cpu AMD Ryzen 9 3900 12-core, RAM 64GB)&lt;/p&gt;\n\n&lt;p&gt;I tested  the following models with Roo/ollama:\n1) gemma3n:e2b-it-q4&lt;em&gt;K_M \n2&lt;/em&gt; hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\n3) deepseek-r1:8b                                         &lt;/p&gt;\n\n&lt;p&gt;I have not had good experiences with these models. Probably my hardware limitations. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to know more and figure out if I can get workable solutions for a reasonable hardware upgrade, or if I should just stick to remote models.&lt;/p&gt;\n\n&lt;p&gt;Is it simply that I need to upgrade to a more powerful GPU like a 3090 to get real results from local LLM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq9lkd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "outofbandii",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751496063,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The standalone and portable version is available. \n\nWorks with gguf. \n\nEnjoy. ",
          "author_fullname": "t2_1sqjlhrsaf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Free AI for all.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9j0x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.41,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 460,
              "width": 854,
              "scrubber_media_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/whjbj3y1gjaf1/DASHPlaylist.mpd?a=1754199034%2CMmUxNWRhYTg2OTE0ODYxZmVlYTBmM2YwOWI3ZGJlZDQ0ZTNmMGRkYzc5YWM0MjE2MDY1MWZiMTQ3OGEwOGE5Nw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 79,
              "hls_url": "https://v.redd.it/whjbj3y1gjaf1/HLSPlaylist.m3u8?a=1754199034%2CMjAxNjEyMjEwNjlhYzQ2OTAwMDMwYjEzOWU5ZDZhYThiZjhmYzk4ODk2NzYwOWU3ODlmMzcyNjVmOTBmNTIyMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=de5049a197d7fb60ce36fc1b553e2e88683fa93e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751495874,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The standalone and portable version is available. &lt;/p&gt;\n\n&lt;p&gt;Works with gguf. &lt;/p&gt;\n\n&lt;p&gt;Enjoy. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/whjbj3y1gjaf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?format=pjpg&amp;auto=webp&amp;s=09e705123d3692137360e587a07651fe1a0df027",
                  "width": 1280,
                  "height": 690
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=78872b43e03735532a89dbc954130e45d219734e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e1ae5b4c9d4c82868c833cc2e9a26944d2667d79",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8196d2b239eb7868036af26b4d548a389f8b661",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1fbd6f78c0fc8eabfebf1a02c79ca31677024d9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c9121b44c10f607f3bb60323be7a7897b4ef6b00",
                    "width": 960,
                    "height": 517
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d71941c3350775826a5dbcc82218049b75753814",
                    "width": 1080,
                    "height": 582
                  }
                ],
                "variants": {},
                "id": "OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq9j0x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wallbergai",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9j0x/free_ai_for_all/",
          "stickied": false,
          "url": "https://v.redd.it/whjbj3y1gjaf1",
          "subreddit_subscribers": 494198,
          "created_utc": 1751495874,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 460,
              "width": 854,
              "scrubber_media_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/whjbj3y1gjaf1/DASHPlaylist.mpd?a=1754199034%2CMmUxNWRhYTg2OTE0ODYxZmVlYTBmM2YwOWI3ZGJlZDQ0ZTNmMGRkYzc5YWM0MjE2MDY1MWZiMTQ3OGEwOGE5Nw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 79,
              "hls_url": "https://v.redd.it/whjbj3y1gjaf1/HLSPlaylist.m3u8?a=1754199034%2CMjAxNjEyMjEwNjlhYzQ2OTAwMDMwYjEzOWU5ZDZhYThiZjhmYzk4ODk2NzYwOWU3ODlmMzcyNjVmOTBmNTIyMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "...So this idea I had, I never could quite execute on, I thought I'd share and let people pick it apart, and/or take it to the next level. Here is how I got there.\n\nI have it in my mind that Llama 3.3 70b 8 bit should be close to Llama 4 Maverick 4-Bit at \\~243 GB). Llama 3.3 70b 8 bit is \\~75 GB and Llama 3.3 70b 4 bit is \\~43 GB. That's 118 GB  which is far less than Maverick, and yet 8 bit probably outperforms Scout 4 bit... so ... all I have to do is run Llama 3.3. 70b 4bit in VRAM as the draft model and have Llama 3.3 70b 8bit primarily in RAM... supposedly the variation between 4 bit to 8 bit isn't that meaningful... supposedly. Guess we should define meaningful. I always assumed it meant it basically kept in line with the original model with just a few words being different. \n\nApparently we're only talking outcome and not word for word equivalence. Turns out in practice I could never get the thing going at a speed that surpassed Llama 3.3 70 8bit split across VRAM and RAM by any meaningful amount. Probably because the models diverge too quickly word wise to be a meaningful speculative model. \n\nOkay... still... the old adage has been that a larger quantize model should outperform a smaller unquantitized model. So I was sure I'd have a more impressive speed boost than just using Llama 3.2 3b 8 bit at \\~4 GB with speculative decoding... especially since Llama 3.3 70b supposedly had similar performance to Llama 3.1 405b.\n\nStill... I'm curious if anyone else has tried this and how successful they were. Could this idea create a better alternative locally for single users than bloated MOE models? Perhaps tweaked in some way... for example perhaps we could build a front end that instead of trying to predict the exact words via speculative decoding, it just asked the 8-bit model to bless the output of 4-bit model sentence by sentence (With a prompt that asks would you have written the last sentence return true or false... or should the last sentence be changed). Perhaps there is a fun math shortcut that would let us use quantized dense models to generate speed similar to MoEs in speed but more dense. Holy grail for me is if we find a way to condense MoEs with minimal power expenditure, but that seems unlikely (outside of quantization which still feels woefully ineffective). \n\nSo there it is. I did my part. I shared what I thought was brilliance (and clearly wasn't) and maybe someone can shine a little light on how it could go better for a future me or you.\n\n:I feel all the comments will be quoting Billy Madison, \"What you've just said is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.\"",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Speculative Decoding and Quantization ... I'm probably not going anywhere near what you think...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9eg5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751495536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...So this idea I had, I never could quite execute on, I thought I&amp;#39;d share and let people pick it apart, and/or take it to the next level. Here is how I got there.&lt;/p&gt;\n\n&lt;p&gt;I have it in my mind that Llama 3.3 70b 8 bit should be close to Llama 4 Maverick 4-Bit at ~243 GB). Llama 3.3 70b 8 bit is ~75 GB and Llama 3.3 70b 4 bit is ~43 GB. That&amp;#39;s 118 GB  which is far less than Maverick, and yet 8 bit probably outperforms Scout 4 bit... so ... all I have to do is run Llama 3.3. 70b 4bit in VRAM as the draft model and have Llama 3.3 70b 8bit primarily in RAM... supposedly the variation between 4 bit to 8 bit isn&amp;#39;t that meaningful... supposedly. Guess we should define meaningful. I always assumed it meant it basically kept in line with the original model with just a few words being different. &lt;/p&gt;\n\n&lt;p&gt;Apparently we&amp;#39;re only talking outcome and not word for word equivalence. Turns out in practice I could never get the thing going at a speed that surpassed Llama 3.3 70 8bit split across VRAM and RAM by any meaningful amount. Probably because the models diverge too quickly word wise to be a meaningful speculative model. &lt;/p&gt;\n\n&lt;p&gt;Okay... still... the old adage has been that a larger quantize model should outperform a smaller unquantitized model. So I was sure I&amp;#39;d have a more impressive speed boost than just using Llama 3.2 3b 8 bit at ~4 GB with speculative decoding... especially since Llama 3.3 70b supposedly had similar performance to Llama 3.1 405b.&lt;/p&gt;\n\n&lt;p&gt;Still... I&amp;#39;m curious if anyone else has tried this and how successful they were. Could this idea create a better alternative locally for single users than bloated MOE models? Perhaps tweaked in some way... for example perhaps we could build a front end that instead of trying to predict the exact words via speculative decoding, it just asked the 8-bit model to bless the output of 4-bit model sentence by sentence (With a prompt that asks would you have written the last sentence return true or false... or should the last sentence be changed). Perhaps there is a fun math shortcut that would let us use quantized dense models to generate speed similar to MoEs in speed but more dense. Holy grail for me is if we find a way to condense MoEs with minimal power expenditure, but that seems unlikely (outside of quantization which still feels woefully ineffective). &lt;/p&gt;\n\n&lt;p&gt;So there it is. I did my part. I shared what I thought was brilliance (and clearly wasn&amp;#39;t) and maybe someone can shine a little light on how it could go better for a future me or you.&lt;/p&gt;\n\n&lt;p&gt;:I feel all the comments will be quoting Billy Madison, &amp;quot;What you&amp;#39;ve just said is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq9eg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9eg5/speculative_decoding_and_quantization_im_probably/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq9eg5/speculative_decoding_and_quantization_im_probably/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751495536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone been able to setup a following solution:\n\n1. Speech is transcribed via local model (whisper or other)\n2. Grammar, spelling and rephrases are executed, respecting a system prompt\n3. Output to markdown file or directly within an interface / webui\n4. Optional: Speech commands such as \"Scratch that last sentence\" (to delete the current sentence), \"Period\" (to end the sentence), \"New Paragraph\" (to add new paragraph) etc.\n\nI am trying to establish a workflow that allows me to maintain a monologue, while transcribing and improving upon the written content.\n\nThe next level of this would be a dialog with the model, to iterate over an idea or a phrase, entire paragraphs or the outline/overview, in order to improve the text or the content on the spot. ",
          "author_fullname": "t2_f0ppt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "STT dictation and conversational sparring partner?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq8z04",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751494422,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to setup a following solution:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Speech is transcribed via local model (whisper or other)&lt;/li&gt;\n&lt;li&gt;Grammar, spelling and rephrases are executed, respecting a system prompt&lt;/li&gt;\n&lt;li&gt;Output to markdown file or directly within an interface / webui&lt;/li&gt;\n&lt;li&gt;Optional: Speech commands such as &amp;quot;Scratch that last sentence&amp;quot; (to delete the current sentence), &amp;quot;Period&amp;quot; (to end the sentence), &amp;quot;New Paragraph&amp;quot; (to add new paragraph) etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am trying to establish a workflow that allows me to maintain a monologue, while transcribing and improving upon the written content.&lt;/p&gt;\n\n&lt;p&gt;The next level of this would be a dialog with the model, to iterate over an idea or a phrase, entire paragraphs or the outline/overview, in order to improve the text or the content on the spot. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq8z04",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lodott1",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq8z04/stt_dictation_and_conversational_sparring_partner/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq8z04/stt_dictation_and_conversational_sparring_partner/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751494422,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Running vLLM 9.1 with 4x A6000s in tensor parallel config with the CognitiveComputations 4-bit AWQ quant of Qwen3 235B A22.\n\nI was running 535 and did an OS update, so I went with 570. I immediately saw inference had dropped from 56 tokens/sec to 35 tokens/sec. Puzzled, I messed around for a few days, tweaked all sorts, and eventually just tried using `apt` to install the nvidia 535 drivers, reboot, and voila! Back to 56 tokens/sec.\n\nCurious if anyone has seen similar.",
          "author_fullname": "t2_qf8h7ka8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ubuntu 24.04: observing that nvidia-535 drivers run 20 tokens/sec faster than nvidia-570 drivers with no other changes in my vLLM setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq8gjv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 83,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 83,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751493148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Running vLLM 9.1 with 4x A6000s in tensor parallel config with the CognitiveComputations 4-bit AWQ quant of Qwen3 235B A22.&lt;/p&gt;\n\n&lt;p&gt;I was running 535 and did an OS update, so I went with 570. I immediately saw inference had dropped from 56 tokens/sec to 35 tokens/sec. Puzzled, I messed around for a few days, tweaked all sorts, and eventually just tried using &lt;code&gt;apt&lt;/code&gt; to install the nvidia 535 drivers, reboot, and voila! Back to 56 tokens/sec.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has seen similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq8gjv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "__JockY__",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751493148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i’ve been sharing polls and asking questions just to figure out what people actually need.\n\ni’ve consulted for ai infra companies and startups. i also built and launched my own ai apps using those infras. but they failed me. local tools were painful. hosted ones were worse. everything felt disconnected and fragile.\n\nso at the start of 2025 i began building my own thing. opinionated. integrated. no half-solutions.\n\nlately i’ve seen more and more people run into the same problems we’ve been solving with inference.sh. if you’ve been on the waitlist for a while thank you. it’s almost time.\n\nhere’s a quick video from my cofounder showing how linking your own gpu works. inference.sh is free and uses open source apps we’ve built. the full project isn’t open sourced yet for security reasons but we share as much as we can and we’re committed to contributing back.\n\na few things it already solves:\n\n– full apps instead of piles of low level nodes. some people want control but if every new model needs custom wiring just to boot it stops being control and turns into unpaid labor.\n\n– llms and multimedia tools in one place. no tab switching no broken flow. and it’s not limited to ai. you can extend it with any code.\n\n– connect any device. local or cloud. run apps from anywhere. if your local box isn’t enough shift to the cloud without losing workflows or state.\n\n– no more cuda or python dependency hell. just click run. amd and intel support coming.\n\n– have multiple gpus? we can use them separately or together.\n\n– have a workflow you want to reuse or expose? we’ve got an api. mcp is coming so agents can run each other’s workflows\n\nthis project is close to my heart. i’ll keep adding new models and weird ideas on day zero. contributions always welcome. apps are here: https://github.com/inference-sh/grid\n\nwaitlist’s open. let me know what else you want to see before the gates open.\n\nthanks for listening to my token stream.",
          "author_fullname": "t2_bquk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "the result of all the polls i’ve been running here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq7wra",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "inference.sh - the easiest way to run any open weight ai model",
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "version": "1.0",
              "author_name": "inference",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/ViadeTYqQDg/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@inference-sh"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lq7wra",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=bbd3edd06755ef245263157fe75866d550d1c4d1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751491747,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i’ve been sharing polls and asking questions just to figure out what people actually need.&lt;/p&gt;\n\n&lt;p&gt;i’ve consulted for ai infra companies and startups. i also built and launched my own ai apps using those infras. but they failed me. local tools were painful. hosted ones were worse. everything felt disconnected and fragile.&lt;/p&gt;\n\n&lt;p&gt;so at the start of 2025 i began building my own thing. opinionated. integrated. no half-solutions.&lt;/p&gt;\n\n&lt;p&gt;lately i’ve seen more and more people run into the same problems we’ve been solving with inference.sh. if you’ve been on the waitlist for a while thank you. it’s almost time.&lt;/p&gt;\n\n&lt;p&gt;here’s a quick video from my cofounder showing how linking your own gpu works. inference.sh is free and uses open source apps we’ve built. the full project isn’t open sourced yet for security reasons but we share as much as we can and we’re committed to contributing back.&lt;/p&gt;\n\n&lt;p&gt;a few things it already solves:&lt;/p&gt;\n\n&lt;p&gt;– full apps instead of piles of low level nodes. some people want control but if every new model needs custom wiring just to boot it stops being control and turns into unpaid labor.&lt;/p&gt;\n\n&lt;p&gt;– llms and multimedia tools in one place. no tab switching no broken flow. and it’s not limited to ai. you can extend it with any code.&lt;/p&gt;\n\n&lt;p&gt;– connect any device. local or cloud. run apps from anywhere. if your local box isn’t enough shift to the cloud without losing workflows or state.&lt;/p&gt;\n\n&lt;p&gt;– no more cuda or python dependency hell. just click run. amd and intel support coming.&lt;/p&gt;\n\n&lt;p&gt;– have multiple gpus? we can use them separately or together.&lt;/p&gt;\n\n&lt;p&gt;– have a workflow you want to reuse or expose? we’ve got an api. mcp is coming so agents can run each other’s workflows&lt;/p&gt;\n\n&lt;p&gt;this project is close to my heart. i’ll keep adding new models and weird ideas on day zero. contributions always welcome. apps are here: &lt;a href=\"https://github.com/inference-sh/grid\"&gt;https://github.com/inference-sh/grid&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;waitlist’s open. let me know what else you want to see before the gates open.&lt;/p&gt;\n\n&lt;p&gt;thanks for listening to my token stream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/ViadeTYqQDg?si=dfAXbK8fnZPBEuDV",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?auto=webp&amp;s=2ce6d7b32a9a0e5891cf8774650990f190b15bf3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1b1ab2069c070abc6be872f0e9d98b671ce6060",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=072a27f99ac4eb254c06dcbbbcd6834fff1f4d44",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b17f176414965dce8ace0b6e9bf8f6ee42b12c86",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq7wra",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "okaris",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq7wra/the_result_of_all_the_polls_ive_been_running_here/",
          "stickied": false,
          "url": "https://youtu.be/ViadeTYqQDg?si=dfAXbK8fnZPBEuDV",
          "subreddit_subscribers": 494198,
          "created_utc": 1751491747,
          "num_crossposts": 0,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "inference.sh - the easiest way to run any open weight ai model",
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "version": "1.0",
              "author_name": "inference",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/ViadeTYqQDg/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@inference-sh"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3f9vjjno",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq7vjc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/gh20o4e63jaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 480,
              "width": 604,
              "scrubber_media_url": "https://v.redd.it/gh20o4e63jaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/gh20o4e63jaf1/DASHPlaylist.mpd?a=1754199034%2CMTM0YzE1MWUxZmNkYmMzOWZlNGExYTVlZmNiMWE4NzRhMzYwMzc1ZWQ1MDFiYjU0MTYxYzg0MzIxZmRmMWE2Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 75,
              "hls_url": "https://v.redd.it/gh20o4e63jaf1/HLSPlaylist.m3u8?a=1754199034%2CMzE3MzA2YTI5YmExYTMyNGZmZTBlMjAxMjBmNTZjZWM4ZDlhNjQxY2QxYWFlNzQ5MzYzMTcxODE2ZDVmNTAzYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=140&amp;height=111&amp;crop=140:111,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=3d468784bdbc1fc2ba09ce867a8fe8db5fc6d075",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 70B"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751491655,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/gh20o4e63jaf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?format=pjpg&amp;auto=webp&amp;s=b946aac0d79a0e0674c9222b196324e15947219a",
                  "width": 798,
                  "height": 634
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4c0ae13ad14d92cb71484678ad2b7a90c963464c",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9f841eb6234aa0b5c7070d29c2f34c461d02ff98",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c9db394b29af0d3bb3852dfac43df137b25dba3",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8eb1d31e434aa35487f72e725f81339895500ba0",
                    "width": 640,
                    "height": 508
                  }
                ],
                "variants": {},
                "id": "dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 70B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lq7vjc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PraxisOG",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/",
          "stickied": false,
          "url": "https://v.redd.it/gh20o4e63jaf1",
          "subreddit_subscribers": 494198,
          "created_utc": 1751491655,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/gh20o4e63jaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 480,
              "width": 604,
              "scrubber_media_url": "https://v.redd.it/gh20o4e63jaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/gh20o4e63jaf1/DASHPlaylist.mpd?a=1754199034%2CMTM0YzE1MWUxZmNkYmMzOWZlNGExYTVlZmNiMWE4NzRhMzYwMzc1ZWQ1MDFiYjU0MTYxYzg0MzIxZmRmMWE2Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 75,
              "hls_url": "https://v.redd.it/gh20o4e63jaf1/HLSPlaylist.m3u8?a=1754199034%2CMzE3MzA2YTI5YmExYTMyNGZmZTBlMjAxMjBmNTZjZWM4ZDlhNjQxY2QxYWFlNzQ5MzYzMTcxODE2ZDVmNTAzYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yay! Been waiting for this one for a while, guessing I'm not the only one?\n[https://github.com/vllm-project/vllm/pull/17280](https://github.com/vllm-project/vllm/pull/17280)\n\nOn 70B I'm maxing out around 1400T/s on the Pro 6000 with 100 threads.\n\n\n\n\n\nQuick install instructions if you want to try it:\n\n\n\nmkdir vllm-src  \ncd vllm-src  \npython3 -m venv myenv  \nsource myenv/bin/activate  \npip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu128](https://download.pytorch.org/whl/cu128)  \ngit clone [https://github.com/huggingface/transformers.git](https://github.com/huggingface/transformers.git)  \ngit clone [https://github.com/vllm-project/vllm.git](https://github.com/vllm-project/vllm.git)  \ncd transformers  \npip install -e .  \ncd ../vllm  \npython use\\_existing\\_torch.py  \npip install -r requirements/build.txt  \npip install -r requirements/cuda.txt  \npip install -e . --no-build-isolation  \nvllm serve RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic  \nvllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic --max-model-len 8000  ",
          "author_fullname": "t2_9hl4ymvj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FP8 fixed on VLLM for RTX Pro 6000 (and RTX 5000 desktop cards)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq79xx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751493785,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751490166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yay! Been waiting for this one for a while, guessing I&amp;#39;m not the only one?\n&lt;a href=\"https://github.com/vllm-project/vllm/pull/17280\"&gt;https://github.com/vllm-project/vllm/pull/17280&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On 70B I&amp;#39;m maxing out around 1400T/s on the Pro 6000 with 100 threads.&lt;/p&gt;\n\n&lt;p&gt;Quick install instructions if you want to try it:&lt;/p&gt;\n\n&lt;p&gt;mkdir vllm-src&lt;br/&gt;\ncd vllm-src&lt;br/&gt;\npython3 -m venv myenv&lt;br/&gt;\nsource myenv/bin/activate&lt;br/&gt;\npip install torch torchvision torchaudio --index-url &lt;a href=\"https://download.pytorch.org/whl/cu128\"&gt;https://download.pytorch.org/whl/cu128&lt;/a&gt;&lt;br/&gt;\ngit clone &lt;a href=\"https://github.com/huggingface/transformers.git\"&gt;https://github.com/huggingface/transformers.git&lt;/a&gt;&lt;br/&gt;\ngit clone &lt;a href=\"https://github.com/vllm-project/vllm.git\"&gt;https://github.com/vllm-project/vllm.git&lt;/a&gt;&lt;br/&gt;\ncd transformers&lt;br/&gt;\npip install -e .&lt;br/&gt;\ncd ../vllm&lt;br/&gt;\npython use_existing_torch.py&lt;br/&gt;\npip install -r requirements/build.txt&lt;br/&gt;\npip install -r requirements/cuda.txt&lt;br/&gt;\npip install -e . --no-build-isolation&lt;br/&gt;\nvllm serve RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic&lt;br/&gt;\nvllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic --max-model-len 8000  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?auto=webp&amp;s=1fd49f52128f28276a9c3c68af9e44fdad153b60",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ca1383baae7b6b423ab28be5aa67c437fc35d82",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=956653e6b1a4f46fc6ca33cac434e66b784cead1",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=492af444cd23147c8e4b5d077cb10778a32e9013",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd275d8b8a4e92c894a1ce9e9acaac8850dbcf46",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b6c033a53916e713ab613f45a38513898643074",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=360efab2869f72fe40846702b38102749bc6fff6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq79xx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious_Cut_6144",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751490166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "LitheCode, it is a bit like if Pocketpal AI would allow you to edit your repo and update it in less than 6 clicks.\n\nWould love to get some feeback on my app or answer any questions you may have. It isn't perfect, but poured in all of my free time for a year. It isn't strictly local models only as our small models are still a bit limited, but with models like R1 Qwen3 8b I think we will be seeing a golden age in smaller models.\n\nhttps://play.google.com/store/apps/details?id=com.litheapp.app",
          "author_fullname": "t2_9dhkfhif",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "LitheCode, updating your GitHub repo using Local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "dbbr8ds9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fec5b516dcbed400f4e86a2661956834f9c0fb6a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df53339a418f74f208f12daa99611f8648c7f747"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f45ac41a7c58aec6794ddb22bdeac4c2ac48cd9"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=208dba327bbd5c5f0c8289eab79fa512747cadb1"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=549e75eb09916d700feddf79e913a4c6f603646c"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f769e3aca806bc881e2e9b35b7de5db7637dd052"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=b67d4ed5a439784079e4e9d1e7a7d9d17308ed77"
              },
              "id": "dbbr8ds9uiaf1"
            },
            "jwk53lw9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8a3706c7121ab8a4c6d3a8eeb2036bcc956e63"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef3b24bfe3e38f94608acd51e45e45aaf22e5947"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4894cbaefe7ef51cd4d3f0c9b0ce664e270b480d"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=49530a145fa7d5e3fe8736fe64da69d34dcceae4"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=80196ad925679fc3330261b68a06303e78910a3a"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=915e5fd21ba2ee368bb34639e89dbb7f7acde836"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=7100cb77a347c9dac3bc86f4008697fc1ab219b0"
              },
              "id": "jwk53lw9uiaf1"
            },
            "x5v2u8m9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a69ecbe39ddb00539be7d5845bff05ad8a8b3779"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a3a0043c0c9ddd578e701fd52df05ef5a7395fc"
                },
                {
                  "y": 400,
                  "x": 320,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=954cb8f95bf6a351debb29b12de2fcf44f0d1c4a"
                },
                {
                  "y": 800,
                  "x": 640,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=02a2baa1d09871230393b9b9573e52e714938e8f"
                },
                {
                  "y": 1200,
                  "x": 960,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a16f3d3991a5e15b42fcb67bb30df866f5068da"
                },
                {
                  "y": 1350,
                  "x": 1080,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e3db11a4e53d5f9eaf1aef2b3aedb0d62e955f9"
                }
              ],
              "s": {
                "y": 1800,
                "x": 1440,
                "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b46b8e7dd68c5c3556df6c28a3d1fa141f735ab9"
              },
              "id": "x5v2u8m9uiaf1"
            },
            "7hd5pmu9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b338cf610e73e2fdc7c00ebae716dc6545f194a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b42ffd02a1e96d581b9225bd1f00eeb6ff03cbbc"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53ae7fcae6790c14e5797327e733e9ef5cc2e153"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0975a701f492314c87b02aa5fecacfb705e18622"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e930c1ae0917a175794c753c473cc9577dfa93d"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eaf19e3e823b7e1f1b55dfec6dafed20800c8c48"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=c3bf3ce48dd265c87d87e9675eeee3cab650c2e7"
              },
              "id": "7hd5pmu9uiaf1"
            },
            "ztuf0gy9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=84c5d0eb9a1b3fa3a68f6c4a4d5b72d712e02e6f"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99a5f2bfe09f135171156e265e53cd9239211599"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acbc031ba547bd782ed2fb45a59f51c246b76c9f"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=af69ceeced0feeba186fa4bbc974339e634d76ff"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1b23db6193c45c76c13f9078dccac126d841db9"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b8cbf62ee3f1a9508b88374623139e93f482a350"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=f0dc820d1e3b5efccd136cb1aef8fd7f75a3552a"
              },
              "id": "ztuf0gy9uiaf1"
            },
            "3w8n8ji9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=845e407d9d4451ffaa787188cb099b20b7e4b90a"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9bd967488fee0a60753980a6358c57bf5fd7725"
                },
                {
                  "y": 400,
                  "x": 320,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44e4e440e8c5d33a81ad8d2307e812d3553a1295"
                },
                {
                  "y": 800,
                  "x": 640,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f98211afc1b230595bc126195d00e4c0bb327f4"
                },
                {
                  "y": 1200,
                  "x": 960,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4dde751c661e829a1a246b37bbd77f1e870e61a8"
                },
                {
                  "y": 1350,
                  "x": 1080,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=54b70b488722eefd72dfcbf9ea4cb222027db9fa"
                }
              ],
              "s": {
                "y": 1800,
                "x": 1440,
                "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=2d8c6d4fbd4e96414efe94ff7efd6847ba73820d"
              },
              "id": "3w8n8ji9uiaf1"
            },
            "51ey22p9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04060b0ab6290ef915d9dec355608ba19a2b079f"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9be54fb01276a75316402c72f0830865302c0bc0"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f5c0eb129d964ef0be5bcd6aea9109184c372eb"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe22dcff5fa1b6e2f825da7a27e9a6ef81aef1f0"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b2a182a66e0f3dae8bc71df0f521ab3e5884a68"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aeb44e0c5b9b2dfe1d6f2a2306f52edd7f7c5b25"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=fb836c1ea2279c8c4882ffa0dc5371a079cfb1fd"
              },
              "id": "51ey22p9uiaf1"
            }
          },
          "name": "t3_1lq6jx8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "3w8n8ji9uiaf1",
                "id": 697440027
              },
              {
                "caption": "",
                "media_id": "x5v2u8m9uiaf1",
                "id": 697440028
              },
              {
                "caption": "",
                "media_id": "51ey22p9uiaf1",
                "id": 697440029
              },
              {
                "caption": "",
                "media_id": "dbbr8ds9uiaf1",
                "id": 697440030
              },
              {
                "caption": "",
                "media_id": "7hd5pmu9uiaf1",
                "id": 697440031
              },
              {
                "caption": "",
                "media_id": "jwk53lw9uiaf1",
                "id": 697440032
              },
              {
                "caption": "",
                "media_id": "ztuf0gy9uiaf1",
                "id": 697440033
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EOhtec3DiIwiczyv4YDdl3Y5WvPfwi4s15SsR6jpV2U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751488407,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LitheCode, it is a bit like if Pocketpal AI would allow you to edit your repo and update it in less than 6 clicks.&lt;/p&gt;\n\n&lt;p&gt;Would love to get some feeback on my app or answer any questions you may have. It isn&amp;#39;t perfect, but poured in all of my free time for a year. It isn&amp;#39;t strictly local models only as our small models are still a bit limited, but with models like R1 Qwen3 8b I think we will be seeing a golden age in smaller models.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://play.google.com/store/apps/details?id=com.litheapp.app\"&gt;https://play.google.com/store/apps/details?id=com.litheapp.app&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lq6jx8",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq6jx8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AspecialistI",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq6jx8/lithecode_updating_your_github_repo_using_local/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lq6jx8",
          "subreddit_subscribers": 494198,
          "created_utc": 1751488407,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article from hacker news: https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1\n\n\nCybersecurity researchers have discovered a critical security vulnerability in artificial intelligence (AI) company Anthropic's Model Context Protocol (MCP) Inspector project that could result in remote code execution (RCE) and allow an attacker to gain complete access to the hosts.\n\nThe vulnerability, tracked as CVE-2025-49596, carries a CVSS score of 9.4 out of a maximum of 10.0.\n\n\"This is one of the first critical RCEs in Anthropic's MCP ecosystem, exposing a new class of browser-based attacks against AI developer tools,\" Oligo Security's Avi Lumelsky said in a report published last week.\n\n\"With code execution on a developer's machine, attackers can steal data, install backdoors, and move laterally across networks - highlighting serious risks for AI teams, open-source projects, and enterprise adopters relying on MCP.\"\n\nMCP, introduced by Anthropic in November 2024, is an open protocol that standardizes the way large language model (LLM) applications integrate and share data with external data sources and tools.\n\nThe MCP Inspector is a developer tool for testing and debugging MCP servers, which expose specific capabilities through the protocol and allow an AI system to access and interact with information beyond its training data.\n\nIt contains two components, a client that provides an interactive interface for testing and debugging, and a proxy server that bridges the web UI to different MCP servers.\n\nThat said, a key security consideration to keep in mind is that the server should not be exposed to any untrusted network as it has permission to spawn local processes and can connect to any specified MCP server.\n\nThis aspect, coupled with the fact that the default settings developers use to spin up a local version of the tool come with \"significant\" security risks, such as missing authentication and encryption, opens up a new attack pathway, per Oligo.\n\n\"This misconfiguration creates a significant attack surface, as anyone with access to the local network or public internet can potentially interact with and exploit these servers,\" Lumelsky said.\n\nThe attack plays out by chaining a known security flaw affecting modern web browsers, dubbed 0.0.0.0 Day, with a cross-site request forgery (CSRF) vulnerability in Inspector (CVE-2025-49596) to run arbitrary code on the host simply upon visiting a malicious website.\n\n\n\"Versions of MCP Inspector below 0.14.1 are vulnerable to remote code execution due to lack of authentication between the Inspector client and proxy, allowing unauthenticated requests to launch MCP commands over stdio,\" the developers of MCP Inspector said in an advisory for CVE-2025-49596.\n\n0.0.0.0 Day is a 19-year-old vulnerability in modern web browsers that could enable malicious websites to breach local networks. It takes advantage of the browsers' inability to securely handle the IP address 0.0.0.0, leading to code execution.\n\n\"Attackers can exploit this flaw by crafting a malicious website that sends requests to localhost services running on an MCP server, thereby gaining the ability to execute arbitrary commands on a developer's machine,\" Lumelsky explained.\n\n\"The fact that the default configurations expose MCP servers to these kinds of attacks means that many developers may be inadvertently opening a backdoor to their machine.\"\n\nSpecifically, the proof-of-concept (PoC) makes use of the Server-Sent Events (SSE) endpoint to dispatch a malicious request from an attacker-controlled website to achieve RCE on the machine running the tool even if it's listening on localhost (127.0.0.1).\n\nThis works because the IP address 0.0.0.0 tells the operating system to listen on all IP addresses assigned to the machine, including the local loopback interface (i.e., localhost).\n\nIn a hypothetical attack scenario, an attacker could set up a fake web page and trick a developer into visiting it, at which point, the malicious JavaScript embedded in the page would send a request to 0.0.0.0:6277 (the default port on which the proxy runs), instructing the MCP Inspector proxy server to execute arbitrary commands.\n\nThe attack can also leverage DNS rebinding techniques to create a forged DNS record that points to 0.0.0.0:6277 or 127.0.0.1:6277 in order to bypass security controls and gain RCE privileges.\n\nFollowing responsible disclosure in April 2025, the vulnerability was addressed by the project maintainers on June 13 with the release of version 0.14.1. The fixes add a session token to the proxy server and incorporate origin validation to completely plug the attack vector.\n\n\"Localhost services may appear safe but are often exposed to the public internet due to network routing capabilities in browsers and MCP clients,\" Oligo said.\n\n\"The mitigation adds Authorization which was missing in the default prior to the fix, as well as verifying the Host and Origin headers in HTTP, making sure the client is really visiting from a known, trusted domain. Now, by default, the server blocks DNS rebinding and CSRF attacks.\"\n\nThe discovery of CVE-2025-49596 comes days after Trend Micro detailed an unpatched SQL injection bug in Anthropic's SQLite MCP server that could be exploited to seed malicious prompts, exfiltrate data, and take control of agent workflows.\n\n\"AI agents often trust internal data whether from databases, log entry, or cached records, agents often treat it as safe,\" researcher Sean Park said. \"An attacker can exploit this trust by embedding a prompt at that point and can later have the agent call powerful tools (email, database, cloud APIs) to steal data or move laterally, all while sidestepping earlier security checks.\"\n\nAlthough the open-source project has been billed as a reference implementation and not intended for production use, it has been forked over 5,000 times. The GitHub repository was archived on May 29, 2025, meaning no patches have been planned to address the shortcoming.\n\n\"The takeaway is clear. If we allow yesterday's web-app mistakes to slip into today's agent infrastructure, we gift attackers an effortless path from SQL injection to full agent compromise,\" Park said.\n\nThe findings also follow a report from Backslash Security that found hundreds of MCP servers to be susceptible to two major misconfigurations: Allowing arbitrary command execution on the host machine due to unchecked input handling and excessive permissions, and making them accessible to any party on the same local network owing to them being explicitly bound to 0.0.0.0, a vulnerability dubbed NeighborJack.\n\n\"Imagine you're coding in a shared coworking space or café. Your MCP server is silently running on your machine,\" Backslash Security said. \"The person sitting near you, sipping their latte, can now access your MCP server, impersonate tools, and potentially run operations on your behalf. It's like leaving your laptop open – and unlocked for everyone in the room.\"\n\nBecause MCPs, by design, are built to access external data sources, they can serve as covert pathways for prompt injection and context poisoning, thereby influencing the outcome of an LLM when parsing data from an attacker-controlled site that contains hidden instructions.\n\n\"One way to secure an MCP server might be to carefully process any text scraped from a website or database to avoid context poisoning,\" researcher Micah Gold said. \"However, this approach bloats tools – by requiring each individual tool to reimplement the same security feature – and leaves the user dependent on the security protocol of the individual MCP tool.\"\n\nA better approach, Backslash Security noted, is to configure AI rules with MCP clients to protect against vulnerable servers. These rules refer to pre-defined prompts or instructions that are assigned to an AI agent to guide its behavior and ensure it does not break security protocols.\n\n\"By conditioning AI agents to be skeptical and aware of the threat posed by context poisoning via AI rules, MCP clients can be secured against MCP servers,\" Gold said.",
          "author_fullname": "t2_apjrh9ky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Critical Vulnerability in Anthropic's MCP Exposes Developer Machines to Remote Exploits",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq5wmh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751486835,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article from hacker news: &lt;a href=\"https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1\"&gt;https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Cybersecurity researchers have discovered a critical security vulnerability in artificial intelligence (AI) company Anthropic&amp;#39;s Model Context Protocol (MCP) Inspector project that could result in remote code execution (RCE) and allow an attacker to gain complete access to the hosts.&lt;/p&gt;\n\n&lt;p&gt;The vulnerability, tracked as CVE-2025-49596, carries a CVSS score of 9.4 out of a maximum of 10.0.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;This is one of the first critical RCEs in Anthropic&amp;#39;s MCP ecosystem, exposing a new class of browser-based attacks against AI developer tools,&amp;quot; Oligo Security&amp;#39;s Avi Lumelsky said in a report published last week.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;With code execution on a developer&amp;#39;s machine, attackers can steal data, install backdoors, and move laterally across networks - highlighting serious risks for AI teams, open-source projects, and enterprise adopters relying on MCP.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;MCP, introduced by Anthropic in November 2024, is an open protocol that standardizes the way large language model (LLM) applications integrate and share data with external data sources and tools.&lt;/p&gt;\n\n&lt;p&gt;The MCP Inspector is a developer tool for testing and debugging MCP servers, which expose specific capabilities through the protocol and allow an AI system to access and interact with information beyond its training data.&lt;/p&gt;\n\n&lt;p&gt;It contains two components, a client that provides an interactive interface for testing and debugging, and a proxy server that bridges the web UI to different MCP servers.&lt;/p&gt;\n\n&lt;p&gt;That said, a key security consideration to keep in mind is that the server should not be exposed to any untrusted network as it has permission to spawn local processes and can connect to any specified MCP server.&lt;/p&gt;\n\n&lt;p&gt;This aspect, coupled with the fact that the default settings developers use to spin up a local version of the tool come with &amp;quot;significant&amp;quot; security risks, such as missing authentication and encryption, opens up a new attack pathway, per Oligo.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;This misconfiguration creates a significant attack surface, as anyone with access to the local network or public internet can potentially interact with and exploit these servers,&amp;quot; Lumelsky said.&lt;/p&gt;\n\n&lt;p&gt;The attack plays out by chaining a known security flaw affecting modern web browsers, dubbed 0.0.0.0 Day, with a cross-site request forgery (CSRF) vulnerability in Inspector (CVE-2025-49596) to run arbitrary code on the host simply upon visiting a malicious website.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Versions of MCP Inspector below 0.14.1 are vulnerable to remote code execution due to lack of authentication between the Inspector client and proxy, allowing unauthenticated requests to launch MCP commands over stdio,&amp;quot; the developers of MCP Inspector said in an advisory for CVE-2025-49596.&lt;/p&gt;\n\n&lt;p&gt;0.0.0.0 Day is a 19-year-old vulnerability in modern web browsers that could enable malicious websites to breach local networks. It takes advantage of the browsers&amp;#39; inability to securely handle the IP address 0.0.0.0, leading to code execution.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Attackers can exploit this flaw by crafting a malicious website that sends requests to localhost services running on an MCP server, thereby gaining the ability to execute arbitrary commands on a developer&amp;#39;s machine,&amp;quot; Lumelsky explained.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The fact that the default configurations expose MCP servers to these kinds of attacks means that many developers may be inadvertently opening a backdoor to their machine.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Specifically, the proof-of-concept (PoC) makes use of the Server-Sent Events (SSE) endpoint to dispatch a malicious request from an attacker-controlled website to achieve RCE on the machine running the tool even if it&amp;#39;s listening on localhost (127.0.0.1).&lt;/p&gt;\n\n&lt;p&gt;This works because the IP address 0.0.0.0 tells the operating system to listen on all IP addresses assigned to the machine, including the local loopback interface (i.e., localhost).&lt;/p&gt;\n\n&lt;p&gt;In a hypothetical attack scenario, an attacker could set up a fake web page and trick a developer into visiting it, at which point, the malicious JavaScript embedded in the page would send a request to 0.0.0.0:6277 (the default port on which the proxy runs), instructing the MCP Inspector proxy server to execute arbitrary commands.&lt;/p&gt;\n\n&lt;p&gt;The attack can also leverage DNS rebinding techniques to create a forged DNS record that points to 0.0.0.0:6277 or 127.0.0.1:6277 in order to bypass security controls and gain RCE privileges.&lt;/p&gt;\n\n&lt;p&gt;Following responsible disclosure in April 2025, the vulnerability was addressed by the project maintainers on June 13 with the release of version 0.14.1. The fixes add a session token to the proxy server and incorporate origin validation to completely plug the attack vector.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Localhost services may appear safe but are often exposed to the public internet due to network routing capabilities in browsers and MCP clients,&amp;quot; Oligo said.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The mitigation adds Authorization which was missing in the default prior to the fix, as well as verifying the Host and Origin headers in HTTP, making sure the client is really visiting from a known, trusted domain. Now, by default, the server blocks DNS rebinding and CSRF attacks.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The discovery of CVE-2025-49596 comes days after Trend Micro detailed an unpatched SQL injection bug in Anthropic&amp;#39;s SQLite MCP server that could be exploited to seed malicious prompts, exfiltrate data, and take control of agent workflows.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;AI agents often trust internal data whether from databases, log entry, or cached records, agents often treat it as safe,&amp;quot; researcher Sean Park said. &amp;quot;An attacker can exploit this trust by embedding a prompt at that point and can later have the agent call powerful tools (email, database, cloud APIs) to steal data or move laterally, all while sidestepping earlier security checks.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Although the open-source project has been billed as a reference implementation and not intended for production use, it has been forked over 5,000 times. The GitHub repository was archived on May 29, 2025, meaning no patches have been planned to address the shortcoming.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The takeaway is clear. If we allow yesterday&amp;#39;s web-app mistakes to slip into today&amp;#39;s agent infrastructure, we gift attackers an effortless path from SQL injection to full agent compromise,&amp;quot; Park said.&lt;/p&gt;\n\n&lt;p&gt;The findings also follow a report from Backslash Security that found hundreds of MCP servers to be susceptible to two major misconfigurations: Allowing arbitrary command execution on the host machine due to unchecked input handling and excessive permissions, and making them accessible to any party on the same local network owing to them being explicitly bound to 0.0.0.0, a vulnerability dubbed NeighborJack.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Imagine you&amp;#39;re coding in a shared coworking space or café. Your MCP server is silently running on your machine,&amp;quot; Backslash Security said. &amp;quot;The person sitting near you, sipping their latte, can now access your MCP server, impersonate tools, and potentially run operations on your behalf. It&amp;#39;s like leaving your laptop open – and unlocked for everyone in the room.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Because MCPs, by design, are built to access external data sources, they can serve as covert pathways for prompt injection and context poisoning, thereby influencing the outcome of an LLM when parsing data from an attacker-controlled site that contains hidden instructions.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;One way to secure an MCP server might be to carefully process any text scraped from a website or database to avoid context poisoning,&amp;quot; researcher Micah Gold said. &amp;quot;However, this approach bloats tools – by requiring each individual tool to reimplement the same security feature – and leaves the user dependent on the security protocol of the individual MCP tool.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;A better approach, Backslash Security noted, is to configure AI rules with MCP clients to protect against vulnerable servers. These rules refer to pre-defined prompts or instructions that are assigned to an AI agent to guide its behavior and ensure it does not break security protocols.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;By conditioning AI agents to be skeptical and aware of the threat posed by context poisoning via AI rules, MCP clients can be secured against MCP servers,&amp;quot; Gold said.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq5wmh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Palpitation7740",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751486835,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_tq216",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq5fqq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "transparent",
          "ups": 562,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 562,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/g0h7MgqzOYDQU2yLRuOR1IOffTzZuGqxJuub0VRzepo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751485685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nmerohq4miaf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?auto=webp&amp;s=01bff498ea36eff4a2ce0d43cd4eaebbc3e064a1",
                  "width": 1556,
                  "height": 1234
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0f05fdbd0f93c91e6f72022aaaf617828ac15a4",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e76289597fcbc69231cc8fe3e55e76d692acff9",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d3429a3f11bf7b5e9127d50bed2a7dad520aebf",
                    "width": 320,
                    "height": 253
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82000e0b8dd6c6f395384b8459f531f8884586e0",
                    "width": 640,
                    "height": 507
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16f3c087dacea89685951f9d2be115bc1f6e2f1c",
                    "width": 960,
                    "height": 761
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e66c21521fb065f4b3f70623a484b7437229e237",
                    "width": 1080,
                    "height": 856
                  }
                ],
                "variants": {},
                "id": "_YhGYoutFRUObj-RqMiOAzLlmyP_b2EqDmvMPUR0YB8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lq5fqq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XMasterrrr",
          "discussion_type": null,
          "num_comments": 69,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/",
          "stickied": false,
          "url": "https://i.redd.it/nmerohq4miaf1.jpeg",
          "subreddit_subscribers": 494198,
          "created_utc": 1751485685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?\n\nThe idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.\n\nWould love to hear your thoughts, is this kind of context control useful? What would you change or build on top?",
          "author_fullname": "t2_13zuwb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatTree: A simple way to context engineer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq5d1o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f6d7317f0148bd20e0b924ea76edc1b05218e4c2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751485506,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?&lt;/p&gt;\n\n&lt;p&gt;The idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts, is this kind of context control useful? What would you change or build on top?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/aadityaubhat/ChatTree",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?auto=webp&amp;s=2e946eba7e4832bd40d0095ef8816cc9f8a69818",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=344fc168447205eb001e41942ab7649037277702",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7fb5ad90e01390b75ebcef865b4c8ec3cef8cc64",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74022766aec6fd5e2c0b69dd4435e7ba27b81bfc",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b00cfbafde10dd16bd1b2925da2c9cbf5a1efec",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0319737917afcf0f39dd3560814bd2bd5fc01266",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=677afc7d9ee0c03d95968ca9015ecaaf15861d57",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq5d1o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aadityaubhat",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/",
          "stickied": false,
          "url": "https://github.com/aadityaubhat/ChatTree",
          "subreddit_subscribers": 494198,
          "created_utc": 1751485506,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mistral Small 3.2 scores 11.5 (Mistral Small 3.1 scored 11.4).  \nBaidu Ernie 4.5 300B A47B scores 15.2.  \nMiniMax-M1 (reasoning) scores 21.4 (MiniMax-Text-01 scored 14.6).",
          "author_fullname": "t2_p2tr0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Extended NYT Connections Benchmark updated with Baidu Ernie 4.5 300B A47B, Mistral Small 3.2, MiniMax-M1",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq4cil",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=db3729052e2a4ab5784e807eb60919c01cc17a94",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751483025,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mistral Small 3.2 scores 11.5 (Mistral Small 3.1 scored 11.4).&lt;br/&gt;\nBaidu Ernie 4.5 300B A47B scores 15.2.&lt;br/&gt;\nMiniMax-M1 (reasoning) scores 21.4 (MiniMax-Text-01 scored 14.6).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/lechmazur/nyt-connections/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?auto=webp&amp;s=c964e8b0b24dc1e9514071574f852cb6ab8d95fe",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef682e456093328d7e9066c522f0dbc88ce5731d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=118ab4e7c14ca213c34cb80eb8adf5298743c846",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03594f16d2150150f593bf0db3e5f1ceff506a2b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=449cce0aca655e2cc3db034127943b5bf3561824",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7434614c112ee39fd658a7e95e7aee49e3762663",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e978955f0ea8e2e2b8222e467627013c81a7d16a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq4cil",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zero0_one1",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq4cil/extended_nyt_connections_benchmark_updated_with/",
          "stickied": false,
          "url": "https://github.com/lechmazur/nyt-connections/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751483025,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "have been poring over pcpartpicker, newegg etc. and it seems like the cheapest way to get the most usable VRAM from GPUs is the 16GB 5060Ti? am I missing something obvious? (probably.)\n\nTIA.",
          "author_fullname": "t2_34g6p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "best bang for your buck in GPUs for VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq4bhu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751482961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have been poring over pcpartpicker, newegg etc. and it seems like the cheapest way to get the most usable VRAM from GPUs is the 16GB 5060Ti? am I missing something obvious? (probably.)&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq4bhu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "starkruzr",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751482961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all. I install the model that supports the visual module, whenever I upload the photo, the error falls: Model sores not support images. Please ae a model that does. What to do with it?",
          "author_fullname": "t2_n8tv96hi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lm studio: Model does not support images. Please use a model that does.!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq436s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751482419,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. I install the model that supports the visual module, whenever I upload the photo, the error falls: Model sores not support images. Please ae a model that does. What to do with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq436s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SensitiveMarzipan203",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq436s/lm_studio_model_does_not_support_images_please/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq436s/lm_studio_model_does_not_support_images_please/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751482419,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What will be better?  \nIQ3\\_M 24B mistral small 3.1/3.2 vs Q5\\_K\\_M 12B mistral nemo",
          "author_fullname": "t2_1irjq3mta2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "24B IQ3_M vs 12B Q5_K_M",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq3urv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751481864,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What will be better?&lt;br/&gt;\nIQ3_M 24B mistral small 3.1/3.2 vs Q5_K_M 12B mistral nemo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq3urv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Longjumping_Bee_6825",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq3urv/24b_iq3_m_vs_12b_q5_k_m/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq3urv/24b_iq3_m_vs_12b_q5_k_m/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751481864,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the past two days, we explored what positional embeddings are and even coded it.\n\nToday, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).\n\n# Recap: Why Transformers Need Positional Embeddings\n\nTransformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.\n\nTo a transformer, these sentences look identical:\n\n* \"The cat sat on the mat.\"\n* \"The mat sat on the cat.\"\n\nThat’s a problem. Order matters, especially in language.\n\nTo fix this, we add *positional embeddings* to inform the model about token positions.\n\n# Traditional Positional Embeddings\n\nTwo popular approaches:\n\n* **Learned positional embeddings** – Each position (1, 2, 3...) gets a trainable vector.\n* **Sinusoidal embeddings** – Use sin/cos functions to generate fixed vectors per position.\n\nBut they have limitations:\n\n* Fixed or learned per-position (no flexibility)\n* Poor generalization to longer sequences\n* Don't integrate naturally with attention scores\n\n# What Is RoPE and Why Is It Better?\n\nRoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.\n\nInstead of adding a position vector, RoPE rotates token embeddings in space based on their position,  directly inside the attention mechanism (on query and key vectors).\n\nThis encodes relative position information in a more elegant and flexible way.\n\nFor each position, the token embedding is rotated by an angle proportional to that position.\n\nA simplified pseudocode:\n\n    for i in range(0, dim, 2):\n        x1, x2 = x[i], x[i+1]\n        angle = theta * position\n        x[i]   = x1 * cos(angle) - x2 * sin(angle)\n        x[i+1] = x1 * sin(angle) + x2 * cos(angle)\n    \n\nThis allows attention to naturally reflect *how far apart* two tokens are, something traditional embeddings can’t do.\n\n# RoPE vs Traditional Positional Embeddings\n\n|Feature|Traditional Embeddings|Rotary Positional Embeddings (RoPE)|\n|:-|:-|:-|\n|Position Injected|Added to input embeddings|Applied inside attention mechanism|\n|Absolute or Relative?|Absolute|Relative|\n|Generalizes to Long Sequences?|Poor|Strong|\n|Learnable Parameters?|Sometimes (if learned)|No|\n|Adopted in SOTA models?|Less common now|Yes (LLaMA, DeepSeek)|\n\n# Why RoPE Is So Useful\n\n* **Encodes relative positions** directly in attention scores\n* **No extra parameters** – it's deterministic\n* **Handles long sequences** more gracefully\n* **Simple implementation** using trigonometric rotation\n\n# Use in Real Models\n\n* **LLaMA (Meta):** Uses RoPE for better generalization and long-context performance.\n* **DeepSeek:** Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.\n\n# Final Thoughts\n\nRotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.\n\n# Coming Up Tomorrow\n\nWe'll implement RoPE in code and walk through how it’s used in the open-source  \n[DeepSeek-Children-Stories-15M model](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model)\n\nFollow along,  we’re just getting started.",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 8/50: Building a Small Language Model from Scratch – Rotary Positional Embeddings (RoPE)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq3tuu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751481803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the past two days, we explored what positional embeddings are and even coded it.&lt;/p&gt;\n\n&lt;p&gt;Today, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).&lt;/p&gt;\n\n&lt;h1&gt;Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt;\n\n&lt;p&gt;Transformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.&lt;/p&gt;\n\n&lt;p&gt;To a transformer, these sentences look identical:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;The cat sat on the mat.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;The mat sat on the cat.&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That’s a problem. Order matters, especially in language.&lt;/p&gt;\n\n&lt;p&gt;To fix this, we add &lt;em&gt;positional embeddings&lt;/em&gt; to inform the model about token positions.&lt;/p&gt;\n\n&lt;h1&gt;Traditional Positional Embeddings&lt;/h1&gt;\n\n&lt;p&gt;Two popular approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Learned positional embeddings&lt;/strong&gt; – Each position (1, 2, 3...) gets a trainable vector.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Sinusoidal embeddings&lt;/strong&gt; – Use sin/cos functions to generate fixed vectors per position.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But they have limitations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fixed or learned per-position (no flexibility)&lt;/li&gt;\n&lt;li&gt;Poor generalization to longer sequences&lt;/li&gt;\n&lt;li&gt;Don&amp;#39;t integrate naturally with attention scores&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What Is RoPE and Why Is It Better?&lt;/h1&gt;\n\n&lt;p&gt;RoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.&lt;/p&gt;\n\n&lt;p&gt;Instead of adding a position vector, RoPE rotates token embeddings in space based on their position,  directly inside the attention mechanism (on query and key vectors).&lt;/p&gt;\n\n&lt;p&gt;This encodes relative position information in a more elegant and flexible way.&lt;/p&gt;\n\n&lt;p&gt;For each position, the token embedding is rotated by an angle proportional to that position.&lt;/p&gt;\n\n&lt;p&gt;A simplified pseudocode:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for i in range(0, dim, 2):\n    x1, x2 = x[i], x[i+1]\n    angle = theta * position\n    x[i]   = x1 * cos(angle) - x2 * sin(angle)\n    x[i+1] = x1 * sin(angle) + x2 * cos(angle)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This allows attention to naturally reflect &lt;em&gt;how far apart&lt;/em&gt; two tokens are, something traditional embeddings can’t do.&lt;/p&gt;\n\n&lt;h1&gt;RoPE vs Traditional Positional Embeddings&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Feature&lt;/th&gt;\n&lt;th align=\"left\"&gt;Traditional Embeddings&lt;/th&gt;\n&lt;th align=\"left\"&gt;Rotary Positional Embeddings (RoPE)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Position Injected&lt;/td&gt;\n&lt;td align=\"left\"&gt;Added to input embeddings&lt;/td&gt;\n&lt;td align=\"left\"&gt;Applied inside attention mechanism&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Absolute or Relative?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Absolute&lt;/td&gt;\n&lt;td align=\"left\"&gt;Relative&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Generalizes to Long Sequences?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Poor&lt;/td&gt;\n&lt;td align=\"left\"&gt;Strong&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Learnable Parameters?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Sometimes (if learned)&lt;/td&gt;\n&lt;td align=\"left\"&gt;No&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Adopted in SOTA models?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Less common now&lt;/td&gt;\n&lt;td align=\"left\"&gt;Yes (LLaMA, DeepSeek)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Why RoPE Is So Useful&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Encodes relative positions&lt;/strong&gt; directly in attention scores&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;No extra parameters&lt;/strong&gt; – it&amp;#39;s deterministic&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Handles long sequences&lt;/strong&gt; more gracefully&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Simple implementation&lt;/strong&gt; using trigonometric rotation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Use in Real Models&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;LLaMA (Meta):&lt;/strong&gt; Uses RoPE for better generalization and long-context performance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DeepSeek:&lt;/strong&gt; Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\n\n&lt;p&gt;Rotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.&lt;/p&gt;\n\n&lt;h1&gt;Coming Up Tomorrow&lt;/h1&gt;\n\n&lt;p&gt;We&amp;#39;ll implement RoPE in code and walk through how it’s used in the open-source&lt;br/&gt;\n&lt;a href=\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\"&gt;DeepSeek-Children-Stories-15M model&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Follow along,  we’re just getting started.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?auto=webp&amp;s=aaf4557bdc852e74628b9a80be3094608111b971",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdcabe31093a0a8eb032266a1279c0d7415a3fd7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7ccd3cc1cd68ed805c75d8bd83411f26087f2fc",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f60d176e3abd7b1151e118c49209d7eec7a901b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2480d1a61ec09f5b4ed612393052d527f3c24bf",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=10f726da67cf6450203cd209e8dbc33a73334d48",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e04b5b7ddd416d60272d1ccfff6f07687e64d488",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq3tuu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751481803,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i am completely new to this entire thing and am hoping to run models locally on my desktop (rtx 4070, r7 9700x, 32gb ddr5). what models would be the best use case for these specs?",
          "author_fullname": "t2_16jmawqekh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "need suggestions for models to use",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq3i6h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751481034,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i am completely new to this entire thing and am hoping to run models locally on my desktop (rtx 4070, r7 9700x, 32gb ddr5). what models would be the best use case for these specs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq3i6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeChallenge1865",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq3i6h/need_suggestions_for_models_to_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq3i6h/need_suggestions_for_models_to_use/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751481034,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\nI’m diving into running models locally with Ollama or LMStudio, and there are so many options that I don’t even know where to start, especially before I lock in on a specific project. I want to develop a clear process for figuring out which model might suit me, even if I don’t yet have a narrow use case.\n\nCould you walk me through your thought process? \nFor example:\n\t•\tHow do you survey the landscape of available models and group them into “creative,” “factual,” or “code-focused” categories?\n\t•\tWhat are the first metrics or specs you check (size, quantization, RAM/VRAM needs, inference speed, training data)?\n\t•\tHow do you run quick, side-by-side tests in Ollama/LMStudio to compare responses on a handful of prompts?\n\t•\tWhat mental shortcuts or analogies do you use to decide “this one feels like the right fit” before committing?\n\t•\tAny go-to scripts, benchmarks, or community resources that help you narrow down from a dozen candidates to your top one or two?\n\nI’m not a developer or engineer, I’m coming at this entirely as an end-user who just wants a consumer-friendly way to experiment with local AI. I don’t have deep technical skills or coding experience, so I’m looking for recommendations and processes explained in plain English rather than programming tutorials.\n\nHope someone can help and thanks in advance!",
          "author_fullname": "t2_8lee8rfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you pick the right local LLM for your needs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq2wn6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751479612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I’m diving into running models locally with Ollama or LMStudio, and there are so many options that I don’t even know where to start, especially before I lock in on a specific project. I want to develop a clear process for figuring out which model might suit me, even if I don’t yet have a narrow use case.&lt;/p&gt;\n\n&lt;p&gt;Could you walk me through your thought process? \nFor example:\n    • How do you survey the landscape of available models and group them into “creative,” “factual,” or “code-focused” categories?\n    • What are the first metrics or specs you check (size, quantization, RAM/VRAM needs, inference speed, training data)?\n    • How do you run quick, side-by-side tests in Ollama/LMStudio to compare responses on a handful of prompts?\n    • What mental shortcuts or analogies do you use to decide “this one feels like the right fit” before committing?\n    • Any go-to scripts, benchmarks, or community resources that help you narrow down from a dozen candidates to your top one or two?&lt;/p&gt;\n\n&lt;p&gt;I’m not a developer or engineer, I’m coming at this entirely as an end-user who just wants a consumer-friendly way to experiment with local AI. I don’t have deep technical skills or coding experience, so I’m looking for recommendations and processes explained in plain English rather than programming tutorials.&lt;/p&gt;\n\n&lt;p&gt;Hope someone can help and thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq2wn6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtiqX",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751479612,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello like the title says we are trying to build a pipeline that takes in tables and tries to decern what information they contain. For this i was wondering if someone ever tried specific table embeddings ? So we can try building a vectorspace for a kind of rag searching out the next related tables and using an llm and other heuristics to judge what kind of data a table contains. \n\nDo any of you know an embedding model for tables ?",
          "author_fullname": "t2_b3etlj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Table embeddings for similarity search between tables ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq2m1x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751478938,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello like the title says we are trying to build a pipeline that takes in tables and tries to decern what information they contain. For this i was wondering if someone ever tried specific table embeddings ? So we can try building a vectorspace for a kind of rag searching out the next related tables and using an llm and other heuristics to judge what kind of data a table contains. &lt;/p&gt;\n\n&lt;p&gt;Do any of you know an embedding model for tables ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq2m1x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Noxusequal",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2m1x/table_embeddings_for_similarity_search_between/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2m1x/table_embeddings_for_similarity_search_between/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751478938,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anything that would work as an agentic code assistant? Trying to decide if it’s worth investing if it means I don’t have to pay for Claude code anymore. I understand it won’t be near Claude code but that’s fine. ",
          "author_fullname": "t2_dmc3swt9s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a legit code assistant that can run on a m3 ultra 256 or 96gb?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq2i2m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751478677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anything that would work as an agentic code assistant? Trying to decide if it’s worth investing if it means I don’t have to pay for Claude code anymore. I understand it won’t be near Claude code but that’s fine. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq2i2m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tru3relativity",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751478677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A recent [study](https://arxiv.org/abs/2409.01754) *underscores* the growing prevalence of LLM-generated \"slop words\" [in academic papers](https://pshapira.net/2024/03/31/delving-into-delve/), a trend now *spilling into* spontaneous spoken language. By *meticulously analyzing* 700,000 hours of academic talks and podcast episodes, researchers *pinpointed* this shift. While it’s plausible speakers could be reading from scripts, manual inspection of videos containing slop words revealed no such evidence in over half the cases. This suggests either speakers have *woven* these terms into their natural lexicon or have memorized ChatGPT-generated scripts.\n\nThis creates a feedback loop: human-generated content *escalates* the use of slop words, further training LLMs on this linguistic trend. The influence is *not confined* to early adopter domains like academia and tech but is *spreading* to education and business. *It’s worth noting that* its presence remains less pronounced in religion and sports—*perhaps, just perhaps* due to the *intricacy* of their linguistic *tapestry*.\n\nUsers of popular models like ChatGPT lack access to tools like the [Anti-Slop](https://www.reddit.com/r/LocalLLaMA/comments/1fqqez5/i_made_a_configurable_antislop_sampler_which/) or [XTC sampler](https://www.reddit.com/r/LocalLLaMA/comments/1f5n9dw/koboldcpp_v174_adds_xtc_exclude_top_choices/), implemented in local solutions such as llama.cpp and kobold.cpp. Consequently, despite our efforts, the proliferation of slop words may persist.\n\nDisclaimer: I generally don't let LLMs \"*improve*\" my postings. This was an occasion too tempting to miss out on though.\n\nhttps://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;format=png&amp;auto=webp&amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282\n\n",
          "author_fullname": "t2_k7w2h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM slop has started to contaminate spoken language",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tbolga2nzhaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 150,
                  "x": 108,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fafd729cf9b03a1c89c180e6f215bb3ba5bbdca"
                },
                {
                  "y": 300,
                  "x": 216,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=68cf67a6ee32455844da6cb7fc9bd6cfb4a228e0"
                },
                {
                  "y": 445,
                  "x": 320,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c59b4b8c42ce669418864460927d13ef08973fd"
                },
                {
                  "y": 890,
                  "x": 640,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0772d987db40a779d894526e5935bc69aa42a50f"
                },
                {
                  "y": 1336,
                  "x": 960,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0109d4d42d2d54dc4c2a79252da1b0f03b7295ec"
                },
                {
                  "y": 1503,
                  "x": 1080,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac5870a1e922188610e7585dbfd3083d69c8a2ab"
                }
              ],
              "s": {
                "y": 1747,
                "x": 1255,
                "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;format=png&amp;auto=webp&amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282"
              },
              "id": "tbolga2nzhaf1"
            }
          },
          "name": "t3_1lq2aae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.52,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1yosruA5utC0JLuhwHuIdDoNpZft719EvTCp_SZSzEY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751478166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A recent &lt;a href=\"https://arxiv.org/abs/2409.01754\"&gt;study&lt;/a&gt; &lt;em&gt;underscores&lt;/em&gt; the growing prevalence of LLM-generated &amp;quot;slop words&amp;quot; &lt;a href=\"https://pshapira.net/2024/03/31/delving-into-delve/\"&gt;in academic papers&lt;/a&gt;, a trend now &lt;em&gt;spilling into&lt;/em&gt; spontaneous spoken language. By &lt;em&gt;meticulously analyzing&lt;/em&gt; 700,000 hours of academic talks and podcast episodes, researchers &lt;em&gt;pinpointed&lt;/em&gt; this shift. While it’s plausible speakers could be reading from scripts, manual inspection of videos containing slop words revealed no such evidence in over half the cases. This suggests either speakers have &lt;em&gt;woven&lt;/em&gt; these terms into their natural lexicon or have memorized ChatGPT-generated scripts.&lt;/p&gt;\n\n&lt;p&gt;This creates a feedback loop: human-generated content &lt;em&gt;escalates&lt;/em&gt; the use of slop words, further training LLMs on this linguistic trend. The influence is &lt;em&gt;not confined&lt;/em&gt; to early adopter domains like academia and tech but is &lt;em&gt;spreading&lt;/em&gt; to education and business. &lt;em&gt;It’s worth noting that&lt;/em&gt; its presence remains less pronounced in religion and sports—&lt;em&gt;perhaps, just perhaps&lt;/em&gt; due to the &lt;em&gt;intricacy&lt;/em&gt; of their linguistic &lt;em&gt;tapestry&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;Users of popular models like ChatGPT lack access to tools like the &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fqqez5/i_made_a_configurable_antislop_sampler_which/\"&gt;Anti-Slop&lt;/a&gt; or &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1f5n9dw/koboldcpp_v174_adds_xtc_exclude_top_choices/\"&gt;XTC sampler&lt;/a&gt;, implemented in local solutions such as llama.cpp and kobold.cpp. Consequently, despite our efforts, the proliferation of slop words may persist.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I generally don&amp;#39;t let LLMs &amp;quot;&lt;em&gt;improve&lt;/em&gt;&amp;quot; my postings. This was an occasion too tempting to miss out on though.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282\"&gt;https://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq2aae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Chromix_",
          "discussion_type": null,
          "num_comments": 86,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2aae/llm_slop_has_started_to_contaminate_spoken/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2aae/llm_slop_has_started_to_contaminate_spoken/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751478166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;Cursor equivalent or close to alternative fully local?\n\nIt's Continue .dev, Void, aider, Zed, AutoGPT, SuperAGI or something else\n\nEdit 1:\n\ncodium, Codestral, Roo, Cline+Ollama...\n\nPlease rate one tool over other like xyz is better then abc but worse then arq etc",
          "author_fullname": "t2_1b8utegv8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cursor equivalent or close to alternative fully local?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq1sdi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751521546,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751476992,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Cursor equivalent or close to alternative fully local?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s Continue .dev, Void, aider, Zed, AutoGPT, SuperAGI or something else&lt;/p&gt;\n\n&lt;p&gt;Edit 1:&lt;/p&gt;\n\n&lt;p&gt;codium, Codestral, Roo, Cline+Ollama...&lt;/p&gt;\n\n&lt;p&gt;Please rate one tool over other like xyz is better then abc but worse then arq etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq1sdi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideResolve4517",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751476992,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_a2gtk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mamba-2 support in llama.cpp landed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq1jyr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 120,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7512f1621ccc42771158d4a83dba439854c266e0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751476448,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?auto=webp&amp;s=4510a608e1ff971f852ec57d8f07668f2b8ab0d6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7a6abcb15d265565c74fee896eff28e14b9a9f0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5355fc411298a6af9630c201bb9f61dddf8fb479",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e602810402906fef613cd3cd6f104bb341ab3f8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c83c80ef04abfb36fdb066d346ea91753a7d280d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04f4978664f7d2eb57a2e9fe0c534b61e0bcd9f1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e45bd94615d08f9378ef6a9d9775ca74a0f112d6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq1jyr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pkmxtw",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556",
          "subreddit_subscribers": 494198,
          "created_utc": 1751476448,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I integrated Moondream (lightweight vision AI model) with Model Context Protocol (MCP), enabling any AI agent to process images locally/remotely.\nOpen source, self-hosted, no API keys needed.\nMoondream MCP is a vision AI server that speaks MCP protocol. Your agents can now:  \n**Caption images** - \"What's in this image?\"  \n**Detect objects** - Find all instances with bounding boxes  \n**Visual Q&amp;A** - \"How many people are in this photo?\"  \n**Point to objects** - \"Where's the error message?\"  \n\nIt integrates into Claude Desktop, OpenAI agents, and anything that supports MCP.  \nhttps://github.com/ColeMurray/moondream-mcp/  \nFeedback and contributions welcome!",
          "author_fullname": "t2_13nfvs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Open Source] Moondream MCP - Vision for AI Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq1417",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_1tz6EuMezDb-Vfzg8nD4-eqUh3Ghmgc_u-pHCga3Ho.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751475447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I integrated Moondream (lightweight vision AI model) with Model Context Protocol (MCP), enabling any AI agent to process images locally/remotely.\nOpen source, self-hosted, no API keys needed.\nMoondream MCP is a vision AI server that speaks MCP protocol. Your agents can now:&lt;br/&gt;\n&lt;strong&gt;Caption images&lt;/strong&gt; - &amp;quot;What&amp;#39;s in this image?&amp;quot;&lt;br/&gt;\n&lt;strong&gt;Detect objects&lt;/strong&gt; - Find all instances with bounding boxes&lt;br/&gt;\n&lt;strong&gt;Visual Q&amp;amp;A&lt;/strong&gt; - &amp;quot;How many people are in this photo?&amp;quot;&lt;br/&gt;\n&lt;strong&gt;Point to objects&lt;/strong&gt; - &amp;quot;Where&amp;#39;s the error message?&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;It integrates into Claude Desktop, OpenAI agents, and anything that supports MCP.&lt;br/&gt;\n&lt;a href=\"https://github.com/ColeMurray/moondream-mcp/\"&gt;https://github.com/ColeMurray/moondream-mcp/&lt;/a&gt;&lt;br/&gt;\nFeedback and contributions welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/upyzvjqkrhaf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/upyzvjqkrhaf1.png?auto=webp&amp;s=17b35279ccff150dcffa50632a87246bcaffbc65",
                  "width": 1476,
                  "height": 1048
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e580de2907c79e680e10c3e5979c6d4dbb3ba2c9",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=152478b6ed0e0b3986fc83ccd5b5f39cae10020a",
                    "width": 216,
                    "height": 153
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8fe291138cbb0a68b432264138db664f722d835f",
                    "width": 320,
                    "height": 227
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76d29f976867cbc03b905c9152d9b301637ec9c9",
                    "width": 640,
                    "height": 454
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dfdc60932fb5e43e365f9bdf6f4f630cfd592974",
                    "width": 960,
                    "height": 681
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48271d6020f0ce8aa5660f096916ad6b4212dc1a",
                    "width": 1080,
                    "height": 766
                  }
                ],
                "variants": {},
                "id": "AW94Mv2FkIDW3--yyr0eIKNalxqTjCalp9fCr0HItaI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq1417",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_colemurray",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/",
          "stickied": false,
          "url": "https://i.redd.it/upyzvjqkrhaf1.png",
          "subreddit_subscribers": 494198,
          "created_utc": 1751475447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been exploring agentic workflows lately not just the flashy demos, but actual implementations that support real-world tasks like deep research, cross-functional reporting, and internal communications.\n\nOne interesting pattern I’ve noticed: the potential of AI agents seems strongest in domains like law, public sector, and enterprise knowledge work especially where speed and accuracy really matter. But there’s still a lot of noise, and figuring out what works in practice vs. theory isn’t always straightforward.\n\nCame across an upcoming session that’s diving into practical applications of agentic AI in knowledge-based industries. Not affiliated with the speaker, but it looked like a useful overview for folks building in this space. I’ll drop the link in the comments for anyone interested.\n\nWould love to hear how others are thinking about agent workflows right now what’s working, what’s still clunky, and where you think we’ll actually see adoption in the next 6–12 months.",
          "author_fullname": "t2_1llpsv0jmf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Agents are transforming workflows, but most use cases still feel early-stage. Curious what others are seeing.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq0n02",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751474346,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been exploring agentic workflows lately not just the flashy demos, but actual implementations that support real-world tasks like deep research, cross-functional reporting, and internal communications.&lt;/p&gt;\n\n&lt;p&gt;One interesting pattern I’ve noticed: the potential of AI agents seems strongest in domains like law, public sector, and enterprise knowledge work especially where speed and accuracy really matter. But there’s still a lot of noise, and figuring out what works in practice vs. theory isn’t always straightforward.&lt;/p&gt;\n\n&lt;p&gt;Came across an upcoming session that’s diving into practical applications of agentic AI in knowledge-based industries. Not affiliated with the speaker, but it looked like a useful overview for folks building in this space. I’ll drop the link in the comments for anyone interested.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how others are thinking about agent workflows right now what’s working, what’s still clunky, and where you think we’ll actually see adoption in the next 6–12 months.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq0n02",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Powerful-Guide-8169",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq0n02/ai_agents_are_transforming_workflows_but_most_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq0n02/ai_agents_are_transforming_workflows_but_most_use/",
          "subreddit_subscribers": 494198,
          "created_utc": 1751474346,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}