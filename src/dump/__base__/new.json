{
  "kind": "Listing",
  "data": {
    "after": "t3_1mfn2xf",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another new model claiming to outperform larger ones:\n\n\n\n**Instruction following** is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.\n\nWe identify **lazy reasoning** during the thinking stage as a key cause of poor instruction adherence.\n\nTo address this, we propose a framework that promotes rigorous reasoning through **previewing and self-checking**.\n\nOur method begins by generating instruction data with **complex constraints**, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.\n\nTraining involves entropy-preserving supervised fine-tuning (**Entropy-SFT**) and token-wise entropy-adaptive reinforcement learning (**TEA-RL**), guided by rule-based multidimensional rewards.\n\nThis approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.\n\nExperiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like **DeepSeek-R1** and closed-source models like **ChatGPT-4o** on challenging instruction-following benchmarks.\n\n[https://huggingface.co/qihoo360/Light-IF-32B](https://huggingface.co/qihoo360/Light-IF-32B)\n\n\n\ntechnical report [https://huggingface.co/papers/2503.10460](https://huggingface.co/papers/2503.10460)\n\n\n\nprevious popular models by this company:\n\n[https://huggingface.co/qihoo360/TinyR1-32B-Preview](https://huggingface.co/qihoo360/TinyR1-32B-Preview)\n\n[https://huggingface.co/qihoo360/Light-R1-32B](https://huggingface.co/qihoo360/Light-R1-32B)\n\n  \nWhat do you think?",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qihoo360/Light-IF-32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghy1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QIubNPPJjdRirxQ73mfKHMMmxIY2VZ9TzcjXGXjIohk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt;\n\n&lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt;\n\n&lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt;\n\n&lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt;\n\n&lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt;\n\n&lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B\"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;technical report &lt;a href=\"https://huggingface.co/papers/2503.10460\"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;previous popular models by this company:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/TinyR1-32B-Preview\"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-R1-32B\"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6vaf0crhrsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6vaf0crhrsgf1.png?auto=webp&amp;s=6ba618920def17d94508256824e2561aba8a6ec9",
                  "width": 1062,
                  "height": 980
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=081b3d7480032b122c209477d47263419358b811",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fac696d38efe6b971413c8794edd434ae2c9926",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20af7c29a89f974a98e34d36ff62aa93c6d3e970",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73",
                    "width": 640,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=753db7d1045cf75fdd0e7ff4fd0c94209e8cee25",
                    "width": 960,
                    "height": 885
                  }
                ],
                "variants": {},
                "id": "O7-1ZfpSudq0amigVzUb6mHn4tEC8x9xtRLUGpzh3sI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghy1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/",
          "stickied": false,
          "url": "https://i.redd.it/6vaf0crhrsgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754223868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fsj3mnbne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI \"devs\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghw96",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.22,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MXA6nG0kGGLD8fsqDWMZfWjsdQgrz0Jg8Jmlmn4a-LE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223705,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4vmx6ozprsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4vmx6ozprsgf1.png?auto=webp&amp;s=a49daca39d28d951de0f2fd42313f033a4c6ddbc",
                  "width": 305,
                  "height": 623
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4vmx6ozprsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd212714d7b2bcdcd6155bf3672e93951b80152b",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/4vmx6ozprsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=59bcfddf4f4ab1c0905bae64211703fe83d4acf7",
                    "width": 216,
                    "height": 432
                  }
                ],
                "variants": {},
                "id": "jfaFuF77KxK6peXK3jY4yV76Uy2oZEKdtZTNNcwNppU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghw96",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dangerous-Camera3368",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mghw96/ai_devs/",
          "stickied": false,
          "url": "https://i.redd.it/4vmx6ozprsgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754223705,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hot take incoming:\n\n  \nThis is a garbage card with garbage support, so quit talking about them like they're useful.  As a matter of fact, quit talking about them at all.   \n\n  \nYou see it took me up until 4 weeks ago to convince my wife to finally let me upgrade my server, I picked up a d380 v9 with 128 gb and 7 1 tb drives, 1000 watt psus and the gpu enablement kit.  Problem?  That cleared out my savings no worries I'll save up and be good in 2-3 months.  Started doing research on cards that I could afford and were available, quickly realized if I was going to get any sort of horsepower and vram I was going to have to go team red.  no worries, i'd rather have a bit of a challenge than plug n play plus nvidia's poor driver support for linux irked me, so looking for amd cards, MI50 16gb 300 up here in canuckistan, kk i can do that in 2-3 months (i have a kid starting uni this fall and another teenaged boy  who  eats the equivalent of a rhino every 2 days).  I'm about 3/4 of the way there amd releases new rocm that doesn't \"support\" mi50, price falls out, market flooded with 32gb models, happy dance, i'll order this weekend, come friday, right before the end of the day i'm brought into bosses office, squirrel ( or whatever the hell this weird ass accounts name is) squirrel as you know we were bought out last week, we are going to have to reduce headcount in your role.  to how many employees sir?  0\n\nque sad dance\n\ngpu savings now = kraft dinner and rice\n\nwatching cheap 32 gb video cards turn into dodo birds, que very very sad dance\n\n  \nconclusion:\n\nMI50 w 32gb?  horrible card!!!! do not buy!  leave some for squirrel for when he gets new job, in 30 years or whenever economy turns around since in canada you can't sell blood and squirrel got fixed after last kid so can't sell that either.\n\n  \nextra conclusion:\n\nplease, no more talking about how great and cheap a 32 gb mi50 is, squirrel (or whatever my name is) slept with pictar of mi50 under pillow for looooong time since cheap card lots of vram and elbow grease doesnt scare him.   keep normies away from mi50, tell them 3090 much better purchase, they spend all monies none left to spend on mi50 squirrel slowly get happy again\n\n  \nthank you for time well spent!",
          "author_fullname": "t2_8alx42ew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 w 32gb?  Guys please",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghhau",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754222398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hot take incoming:&lt;/p&gt;\n\n&lt;p&gt;This is a garbage card with garbage support, so quit talking about them like they&amp;#39;re useful.  As a matter of fact, quit talking about them at all.   &lt;/p&gt;\n\n&lt;p&gt;You see it took me up until 4 weeks ago to convince my wife to finally let me upgrade my server, I picked up a d380 v9 with 128 gb and 7 1 tb drives, 1000 watt psus and the gpu enablement kit.  Problem?  That cleared out my savings no worries I&amp;#39;ll save up and be good in 2-3 months.  Started doing research on cards that I could afford and were available, quickly realized if I was going to get any sort of horsepower and vram I was going to have to go team red.  no worries, i&amp;#39;d rather have a bit of a challenge than plug n play plus nvidia&amp;#39;s poor driver support for linux irked me, so looking for amd cards, MI50 16gb 300 up here in canuckistan, kk i can do that in 2-3 months (i have a kid starting uni this fall and another teenaged boy  who  eats the equivalent of a rhino every 2 days).  I&amp;#39;m about 3/4 of the way there amd releases new rocm that doesn&amp;#39;t &amp;quot;support&amp;quot; mi50, price falls out, market flooded with 32gb models, happy dance, i&amp;#39;ll order this weekend, come friday, right before the end of the day i&amp;#39;m brought into bosses office, squirrel ( or whatever the hell this weird ass accounts name is) squirrel as you know we were bought out last week, we are going to have to reduce headcount in your role.  to how many employees sir?  0&lt;/p&gt;\n\n&lt;p&gt;que sad dance&lt;/p&gt;\n\n&lt;p&gt;gpu savings now = kraft dinner and rice&lt;/p&gt;\n\n&lt;p&gt;watching cheap 32 gb video cards turn into dodo birds, que very very sad dance&lt;/p&gt;\n\n&lt;p&gt;conclusion:&lt;/p&gt;\n\n&lt;p&gt;MI50 w 32gb?  horrible card!!!! do not buy!  leave some for squirrel for when he gets new job, in 30 years or whenever economy turns around since in canada you can&amp;#39;t sell blood and squirrel got fixed after last kid so can&amp;#39;t sell that either.&lt;/p&gt;\n\n&lt;p&gt;extra conclusion:&lt;/p&gt;\n\n&lt;p&gt;please, no more talking about how great and cheap a 32 gb mi50 is, squirrel (or whatever my name is) slept with pictar of mi50 under pillow for looooong time since cheap card lots of vram and elbow grease doesnt scare him.   keep normies away from mi50, tell them 3090 much better purchase, they spend all monies none left to spend on mi50 squirrel slowly get happy again&lt;/p&gt;\n\n&lt;p&gt;thank you for time well spent!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mghhau",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Expensive_Mirror5247",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mghhau/mi50_w_32gb_guys_please/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mghhau/mi50_w_32gb_guys_please/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754222398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone have any knowledge on how to correctly set up tool calling for GLM 4.5 Air in LM Studio? The model is great with the old school tool calling techniques that aider and cline use, but when I try with any tools that use modern tool calling (Qwen Code or OpenCode), the tool calling fails.\n\n  \nIf anyone can give me an idea where to start here I can look into it myself. I'm guessing maybe some change to the jinja prompt would sort it. I am a developer, and I've written my own tool calling agents with ad hoc methods, but I'm not familiar with the new standardised tool calling format(s?).",
          "author_fullname": "t2_12ggykute6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air local tool calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghd3l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754222013,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any knowledge on how to correctly set up tool calling for GLM 4.5 Air in LM Studio? The model is great with the old school tool calling techniques that aider and cline use, but when I try with any tools that use modern tool calling (Qwen Code or OpenCode), the tool calling fails.&lt;/p&gt;\n\n&lt;p&gt;If anyone can give me an idea where to start here I can look into it myself. I&amp;#39;m guessing maybe some change to the jinja prompt would sort it. I am a developer, and I&amp;#39;ve written my own tool calling agents with ad hoc methods, but I&amp;#39;m not familiar with the new standardised tool calling format(s?).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mghd3l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-dysangel-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghd3l/glm_45_air_local_tool_calling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mghd3l/glm_45_air_local_tool_calling/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754222013,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My career is in tech startup chaos.  Bill Gurley is one of the few from that circle I can listen to while chewing food (as I am now and typing).  \n\nCompanies like LG want to sell washing machines.  They don't want their strategy to get disrupted without having a backup plan.  They want to raise the floor so that nobody can get too far ahead.  They want to scorch the Earth so that their biggest competitors won't be earning money that they can't compete for.  Sell AI washing machines = shareholder value protected = mission accomplished.\n\nStrategically, the allies of small open models weirdly includes giant companies and SMEs whenever their primary interest is not in competing directly to *operate* revenue-generating AI.  They want to invest in things that protect their strategy.  They only need a sensible way to do it and not move alone.",
          "author_fullname": "t2_8vhsch4i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why Fortune 500 Wants to Fund Open Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgh2cb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Bg2 Pod",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/fTqINzeudJ4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Bg2Pod"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mgh2cb",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=fe76625dacb81af7ec54f68753f62ea2f3c58290",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My career is in tech startup chaos.  Bill Gurley is one of the few from that circle I can listen to while chewing food (as I am now and typing).  &lt;/p&gt;\n\n&lt;p&gt;Companies like LG want to sell washing machines.  They don&amp;#39;t want their strategy to get disrupted without having a backup plan.  They want to raise the floor so that nobody can get too far ahead.  They want to scorch the Earth so that their biggest competitors won&amp;#39;t be earning money that they can&amp;#39;t compete for.  Sell AI washing machines = shareholder value protected = mission accomplished.&lt;/p&gt;\n\n&lt;p&gt;Strategically, the allies of small open models weirdly includes giant companies and SMEs whenever their primary interest is not in competing directly to &lt;em&gt;operate&lt;/em&gt; revenue-generating AI.  They want to invest in things that protect their strategy.  They only need a sensible way to do it and not move alone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/fTqINzeudJ4?&amp;t=1067",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?auto=webp&amp;s=27043eee8c92b37f592065d2ff05fd85b27b6c4a",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe82e3d2ef0e784a0e62fb6596ee14119fb2c44",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c52288f06eae6d456c4b6ac64ac2b6f1b61a1e65",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e74ffd7ecfd40e764b51c4df1a4b893fda673f1f",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgh2cb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psionikus",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh2cb/why_fortune_500_wants_to_fund_open_models/",
          "stickied": false,
          "url": "https://youtu.be/fTqINzeudJ4?&amp;t=1067",
          "subreddit_subscribers": 509291,
          "created_utc": 1754220993,
          "num_crossposts": 1,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Bg2 Pod",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/fTqINzeudJ4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Bg2Pod"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.\n\nHow it works:\n\n- Drop a .awesome-ai.md file in your repo root (template: https://github.com/teodorgross/awesome-ai)\n\n- The scanner finds it automatically within 30 minutes  \n\n- Creates a pull request for review\n\n- Your tool goes live with real-time GitHub stats on (https://awesome-ai.io)\n\nWhy this matters:\n\n- No more manual submissions or contact forms\n\n- Tools stay up-to-date automatically when you push changes\n\n- GitHub verification prevents spam\n\n- Real-time star tracking and leaderboards\n\nThink of it like .gitignore for Git, but for AI tool discovery. ",
          "author_fullname": "t2_92fpag8i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a GitHub scanner that automatically discovers your AI tools using a new .awesome-ai.md standard I created",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgh19i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2f6c6297df4b4027fb8d1d453c04b9819bf72371",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.&lt;/p&gt;\n\n&lt;p&gt;How it works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Drop a .awesome-ai.md file in your repo root (template: &lt;a href=\"https://github.com/teodorgross/awesome-ai\"&gt;https://github.com/teodorgross/awesome-ai&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The scanner finds it automatically within 30 minutes  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a pull request for review&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your tool goes live with real-time GitHub stats on (&lt;a href=\"https://awesome-ai.io\"&gt;https://awesome-ai.io&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why this matters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;No more manual submissions or contact forms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tools stay up-to-date automatically when you push changes&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GitHub verification prevents spam&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Real-time star tracking and leaderboards&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Think of it like .gitignore for Git, but for AI tool discovery. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/teodorgross/awesome-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?auto=webp&amp;s=423c2b45e5c57364da28298579b3a5631b59c0ed",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd85376252643c0cfc25bf58873d1308bbaa8b8c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15eb575a42080dcefe65c839eb0e4c036819417",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7d83cf6189b7f3c98c0a7b6f2c3d772895ee7ba",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d63a88dbef47d35bfb865b575d09eda2e4144500",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f688b76be41b12b1a7923494dacac226e59a932e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0e5bc45cd19159360de8a68ed1794010b721da7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgh19i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "r00tkit_",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh19i/i_built_a_github_scanner_that_automatically/",
          "stickied": false,
          "url": "https://github.com/teodorgross/awesome-ai",
          "subreddit_subscribers": 509291,
          "created_utc": 1754220889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nPersonal note: This is just my opinion based on a very limited set of API-only probes—interpret with caution.\n\nWhat I did (mini-ROC probes)\n\n* JSON strictness vs. \"bad schema\" repair\n* Tool-calling with an invalid enum + extra property\n* Safety/refusal phrasing check\n* Long-context end-marker recall\n* Tokenizer/short-output edge case\n* Determinism at T=0\n* Tiny style-paraphrase probe\n\nHighlights\n\n* Tool-calling: It silently coerces invalid enums (mode=\"plane\" -&gt; \"car\"/\"train\") and drops extra fields, then emits an OpenAI-style tool\\_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn't call the tool under the same bad input - which is more typical for OpenAI.\n* JSON mode: It \"repairs\" invalid inputs into valid JSON (e.g., {\"ok\": false, \"mode\": \"A\"}). OpenAI also repairs but tends to be more minimally formatted.\n* Safety tone: Opens with \"I can't help with that.\" - Anthropic-ish cadence that many Llama-style distills mimic.\n* Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END\\_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.\n* Determinism: Stable at T=0 on simple tasks.\n* Multilingual: Correct 妹妹 -&gt; \"younger sister,\" and clean pronoun disambiguation.\n\nAnchors I compared against\n\n* OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)\n* Llama: llama-3.3-70b-instruct, llama-3-70b-instruct\n* Qwen: qwen-2.5-72b-instruct\n* Mistral: mixtral-8x22b-instruct\n\nBottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific \"finish=length\" quirk on short outputs. It does not match OpenAI's tool-call behavior in my probes.\n\nAll tests were standard API usage.",
          "author_fullname": "t2_2fmsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OSINT fingerprinting a stealth OpenRouter model - likely Llama-family, not OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mggsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754220059,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Personal note: This is just my opinion based on a very limited set of API-only probes—interpret with caution.&lt;/p&gt;\n\n&lt;p&gt;What I did (mini-ROC probes)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JSON strictness vs. &amp;quot;bad schema&amp;quot; repair&lt;/li&gt;\n&lt;li&gt;Tool-calling with an invalid enum + extra property&lt;/li&gt;\n&lt;li&gt;Safety/refusal phrasing check&lt;/li&gt;\n&lt;li&gt;Long-context end-marker recall&lt;/li&gt;\n&lt;li&gt;Tokenizer/short-output edge case&lt;/li&gt;\n&lt;li&gt;Determinism at T=0&lt;/li&gt;\n&lt;li&gt;Tiny style-paraphrase probe&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Highlights&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tool-calling: It silently coerces invalid enums (mode=&amp;quot;plane&amp;quot; -&amp;gt; &amp;quot;car&amp;quot;/&amp;quot;train&amp;quot;) and drops extra fields, then emits an OpenAI-style tool_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn&amp;#39;t call the tool under the same bad input - which is more typical for OpenAI.&lt;/li&gt;\n&lt;li&gt;JSON mode: It &amp;quot;repairs&amp;quot; invalid inputs into valid JSON (e.g., {&amp;quot;ok&amp;quot;: false, &amp;quot;mode&amp;quot;: &amp;quot;A&amp;quot;}). OpenAI also repairs but tends to be more minimally formatted.&lt;/li&gt;\n&lt;li&gt;Safety tone: Opens with &amp;quot;I can&amp;#39;t help with that.&amp;quot; - Anthropic-ish cadence that many Llama-style distills mimic.&lt;/li&gt;\n&lt;li&gt;Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.&lt;/li&gt;\n&lt;li&gt;Determinism: Stable at T=0 on simple tasks.&lt;/li&gt;\n&lt;li&gt;Multilingual: Correct 妹妹 -&amp;gt; &amp;quot;younger sister,&amp;quot; and clean pronoun disambiguation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anchors I compared against&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)&lt;/li&gt;\n&lt;li&gt;Llama: llama-3.3-70b-instruct, llama-3-70b-instruct&lt;/li&gt;\n&lt;li&gt;Qwen: qwen-2.5-72b-instruct&lt;/li&gt;\n&lt;li&gt;Mistral: mixtral-8x22b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Bottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific &amp;quot;finish=length&amp;quot; quirk on short outputs. It does not match OpenAI&amp;#39;s tool-call behavior in my probes.&lt;/p&gt;\n\n&lt;p&gt;All tests were standard API usage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mggsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jv0010",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754220059,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=8f450fce4ba6cf7a366b7622b303862334de0900\n\nVague Prompts can result in superfluous code.",
          "author_fullname": "t2_5t7w0wy1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When You Ask Claude to Optimize Your Ride",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "o40eeruhesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b5bb7f57fad375387c05563b6a6510f2be9b46ef"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed53ae2e4a7c5eabcd19803c12f98f47f54cc9ac"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b248cb9e1057716cfcc12e2045443d24c79a3ef"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d3d31b28f571a794ff53f1f95f27a6a1c44c139"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7de6c6be001ca9e32236627fa40f3a8907a9a89"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=8f450fce4ba6cf7a366b7622b303862334de0900"
              },
              "id": "o40eeruhesgf1"
            }
          },
          "name": "t3_1mggnjy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/w1qNloMKSnADfv_iDLFi4mDQ-7I3aDqIZiVWINuQwyo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754219513,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8f450fce4ba6cf7a366b7622b303862334de0900\"&gt;https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8f450fce4ba6cf7a366b7622b303862334de0900&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Vague Prompts can result in superfluous code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mggnjy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xVinGee",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggnjy/when_you_ask_claude_to_optimize_your_ride/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggnjy/when_you_ask_claude_to_optimize_your_ride/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754219513,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " WHAT THE DEVIL?\n\nAnother open model outperforms closed ones!  \nXBai o4 beats OpenAI o3-mini and *confidently* beats Anthropic's Claude Opus.\n\n•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7\n\n🔗Open source weights: https://huggingface.co/MetaStoneTec/XBai-o4",
          "author_fullname": "t2_dmji1c74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "XBai-04 Is It Real?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "idu2e1ngesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a2a62a8bef0a6d00f8049f16c8b54e8aa48b974"
                },
                {
                  "y": 124,
                  "x": 216,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=924e4b76f3498ca5e7d4ab97c08a75b6fb377dc1"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82f9fc643d2975e8481a615c1f5a19b5cb453032"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b611aef0c4759ebf456a134bc82a5fd8065b9c5a"
                },
                {
                  "y": 551,
                  "x": 960,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f73f5b3bcc334e135595851ef884c4997ac617"
                },
                {
                  "y": 620,
                  "x": 1080,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e786e475e1a54e20bdba7fb55d1e03e02789639"
                }
              ],
              "s": {
                "y": 704,
                "x": 1226,
                "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=5b2baf561ff73bb0e8b4b5708cd383ce9ed6f372"
              },
              "id": "idu2e1ngesgf1"
            },
            "q3imyykgesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8dbd1ac74caccea54654d6ebe571fab617928272"
                },
                {
                  "y": 158,
                  "x": 216,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c6af0dfc3c0aaa092cea1507485efebac28962f"
                },
                {
                  "y": 235,
                  "x": 320,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602c9e9684804598a1e5221dbb4d23ce3610098a"
                },
                {
                  "y": 471,
                  "x": 640,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e073695ec8c765d1252320b4c7a5e0bd07547b82"
                },
                {
                  "y": 706,
                  "x": 960,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d48c30f79f06380c415eb3dc8e2120642d96e240"
                },
                {
                  "y": 794,
                  "x": 1080,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d39f9af5c4e09886e80918a8161ae1315aa4c7b"
                }
              ],
              "s": {
                "y": 942,
                "x": 1280,
                "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=e8846d514b560df6c44bca9b5375f5eed938412c"
              },
              "id": "q3imyykgesgf1"
            }
          },
          "name": "t3_1mggku0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 35,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "q3imyykgesgf1",
                "id": 720170802
              },
              {
                "caption": "",
                "media_id": "idu2e1ngesgf1",
                "id": 720170803
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754219245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt;\n\n&lt;p&gt;Another open model outperforms closed ones!&lt;br/&gt;\nXBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic&amp;#39;s Claude Opus.&lt;/p&gt;\n\n&lt;p&gt;•Parameters: 32.8 B\n•Training: Long-CoT RL + Process Reward Learning (SPRM)\n•Benchmarks (High-Modus):\n•AIME24: 86.5\n•AIME25: 77.9\n•LiveCodeBench v5: 67.2\n•C-EVAL: 89.7&lt;/p&gt;\n\n&lt;p&gt;🔗Open source weights: &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mggku0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mggku0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ordinary_Mud7430",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mggku0",
          "subreddit_subscribers": 509291,
          "created_utc": 1754219245,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI have developed a toy spreadsheet, where you can implement your formulas in English, which are then translated into \\`javascript\\` thanks to an LLM.\n\nFor instance, you can write: \\`sum of the squared values\\` and the LLM will translate this description into:  \n\\`getValuesFromReferences(\\['A1', 'A2', 'A3'\\]).map(Number).reduce((a, b) =&gt; a + b \\* b, 0)\\`.\n\nI use \\`LM Studio\\` and \\`codestral\\`, but I'm pretty sure you can replace \\`LM Studio\\` by \\`Ollama\\` or your favorite LLM provider.\n\nIf you want to have a look, it is available on the following GitHub: [NUMAI](https://github.com/clauderouxster/NUMAI/tree/main)",
          "author_fullname": "t2_no5psr297",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NUMAI: A spreadsheet with LLM formula conversion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mggdxz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754218578,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have developed a toy spreadsheet, where you can implement your formulas in English, which are then translated into `javascript` thanks to an LLM.&lt;/p&gt;\n\n&lt;p&gt;For instance, you can write: `sum of the squared values` and the LLM will translate this description into:&lt;br/&gt;\n`getValuesFromReferences([&amp;#39;A1&amp;#39;, &amp;#39;A2&amp;#39;, &amp;#39;A3&amp;#39;]).map(Number).reduce((a, b) =&amp;gt; a + b * b, 0)`.&lt;/p&gt;\n\n&lt;p&gt;I use `LM Studio` and `codestral`, but I&amp;#39;m pretty sure you can replace `LM Studio` by `Ollama` or your favorite LLM provider.&lt;/p&gt;\n\n&lt;p&gt;If you want to have a look, it is available on the following GitHub: &lt;a href=\"https://github.com/clauderouxster/NUMAI/tree/main\"&gt;NUMAI&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mggdxz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frere_de_la_Quote",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggdxz/numai_a_spreadsheet_with_llm_formula_conversion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggdxz/numai_a_spreadsheet_with_llm_formula_conversion/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754218578,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have decided to run Gemma 3 4B QAT on my 6GB VRAM Laptop for general use. I was wondering if i should be using some other quant other than the official QAT version by google? Like What would be the performance or quality increase as compared to the QAT version. It would be great if someone shared some benchmarks or other results.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Gemma 3 Quant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgg4ki",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754218312,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217605,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have decided to run Gemma 3 4B QAT on my 6GB VRAM Laptop for general use. I was wondering if i should be using some other quant other than the official QAT version by google? Like What would be the performance or quality increase as compared to the QAT version. It would be great if someone shared some benchmarks or other results.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgg4ki",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg4ki/best_gemma_3_quant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg4ki/best_gemma_3_quant/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754217605,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, poor GPU guys\n\nA few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.\n\n\n\nSome married guys might understand, but I’ve been using an m-ATX case I bought about 15 years ago, and there’s no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to “real” work.\n\nhttps://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\n\nOne of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That's why the “32GB” model ended up in my hands.\n\n\n\nOf course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the “Original MI50” cannot be BIOS-re-flashed, I had given up.\n\n\n\nFirst, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.\n\nhttps://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\n\n  \n\n\n[4070+MI50 \\(22GB only\\)](https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96)\n\n\n\n[MI50 works alone upto 30GB](https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2)\n\n  \n\n\n Guys, hold your horses. I'm aware there are a few issues here.\n\n\n\n1) It's recognized as a Radeon Pro VII\n\n2) It runs on LM STUDIO, which some people really dislike\n\n3) Even if it's recognized as Vulkan, you can't use the combined VRAM of both cards—only twice the VRAM of the first graphics card\n\n(On my PC, it's 12+12GB instead of 12+32GB)\n\n\\-&gt; However, I haven't tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX's prompt processing ability is an extra bonus.\n\n\n\n\n\n**Anyway, there are only three things you need to do.**\n\n\n\n1. Disable Secure Boot in the CMOS BIOS.\n\n2. Run PowerShell in administrator mode and enter the following command:\n\nbcdedit.exe -set TESTSIGNING on\n\n3. Download and install the Polaris-Vega-Navi driver created by the real pros.\n\n[R.ID - AMD 3rd Party Drivers](https://rdn-id.com/)\n\n\n\nAll risks are on you, but I think it's better than getting divorced by your wife over buying an RTX 6000,\n\nThe blower fan sent by the Ali seller is very effective, but it's incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.\n\n\n\nP.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn't recognize it at all. Do I really have to rebuild PyTorch from scratch?",
          "author_fullname": "t2_1dhesoqqtu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Successfully running INSTINCT MI50 on Win11",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "3cabctch4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ca1ba2f5dc37ce486326b1c5d7f492e6257e417"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b47948a9375686a348f3135013fad572be0e5bb9"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c54a61000b055f717d4c99392991a3016a75c607"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e82c15d58578bccb18253e91c4a9d5a4a136e4bf"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0908df44a99335aef9b4daf3dd4796f130b6f492"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=098996b1e02247a51bd56e72b50def71975ccc73"
                }
              ],
              "s": {
                "y": 897,
                "x": 1920,
                "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96"
              },
              "id": "3cabctch4sgf1"
            },
            "oe4uyadb4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac61e80aae254bda55f9b7cd6addf90d29a446b"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fae70f72d90191dd274df1040d6d75cbbbd6475"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c28cb00db6a2547f8fead9579aa8b2fd3c53086"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bdeb09d493de51c07e4fcc97ec462f8aac3b551"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e51eb15dd64da3685f668083f579b2705a4c55e"
                }
              ],
              "s": {
                "y": 1280,
                "x": 960,
                "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80"
              },
              "id": "oe4uyadb4sgf1"
            },
            "lrtre5656sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=445e311f264b6b0ba67f8c1a6fc639d332a2e5f8"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8becb210c0a1f400e0e6189f5b9ea272c0c7364"
                },
                {
                  "y": 175,
                  "x": 320,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b05acc3dcc8e5ab71a4080b7d0fb7e7f2dd9536"
                },
                {
                  "y": 350,
                  "x": 640,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce822ce9b7ef000d574cada4c7c415bfe2d44912"
                },
                {
                  "y": 526,
                  "x": 960,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc5d5e9ad324bd8954b842c0285c298a6f3a5ba1"
                },
                {
                  "y": 591,
                  "x": 1080,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18c465dd09dbecdf9cec8ef723dc48df07a9ff22"
                }
              ],
              "s": {
                "y": 1032,
                "x": 1883,
                "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2"
              },
              "id": "lrtre5656sgf1"
            },
            "pvictbhu4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2e7b199737b649bf0dc517eeb27f38372ace77a"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=296d5c0e017c541acde4c0789322db6a0ee8ee9f"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8871508585c0b72a963270b0a8f2d645d8162a54"
                },
                {
                  "y": 380,
                  "x": 640,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f925d22a93079c9ac16e8bacc366d444a121475"
                },
                {
                  "y": 570,
                  "x": 960,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e4bce81c738e2313acab5f1b611fcbcd1bec79c"
                },
                {
                  "y": 642,
                  "x": 1080,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a90fa210d1e30a035719e4bdeba9f8afa3eb2e24"
                }
              ],
              "s": {
                "y": 1015,
                "x": 1707,
                "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9"
              },
              "id": "pvictbhu4sgf1"
            }
          },
          "name": "t3_1mgg3mh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ac8XNi9a_cAzytIhKD5Gg-Gwa0GLMmvvfiuUz9ia33E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, poor GPU guys&lt;/p&gt;\n\n&lt;p&gt;A few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.&lt;/p&gt;\n\n&lt;p&gt;Some married guys might understand, but I’ve been using an m-ATX case I bought about 15 years ago, and there’s no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to “real” work.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\"&gt;https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;One of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That&amp;#39;s why the “32GB” model ended up in my hands.&lt;/p&gt;\n\n&lt;p&gt;Of course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the “Original MI50” cannot be BIOS-re-flashed, I had given up.&lt;/p&gt;\n\n&lt;p&gt;First, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\"&gt;https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96\"&gt;4070+MI50 (22GB only)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f14e51095d28e301c62418f12f52586a62861ee2\"&gt;MI50 works alone upto 30GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Guys, hold your horses. I&amp;#39;m aware there are a few issues here.&lt;/p&gt;\n\n&lt;p&gt;1) It&amp;#39;s recognized as a Radeon Pro VII&lt;/p&gt;\n\n&lt;p&gt;2) It runs on LM STUDIO, which some people really dislike&lt;/p&gt;\n\n&lt;p&gt;3) Even if it&amp;#39;s recognized as Vulkan, you can&amp;#39;t use the combined VRAM of both cards—only twice the VRAM of the first graphics card&lt;/p&gt;\n\n&lt;p&gt;(On my PC, it&amp;#39;s 12+12GB instead of 12+32GB)&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; However, I haven&amp;#39;t tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX&amp;#39;s prompt processing ability is an extra bonus.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyway, there are only three things you need to do.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Disable Secure Boot in the CMOS BIOS.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run PowerShell in administrator mode and enter the following command:&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;bcdedit.exe -set TESTSIGNING on&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download and install the Polaris-Vega-Navi driver created by the real pros.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://rdn-id.com/\"&gt;R.ID - AMD 3rd Party Drivers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All risks are on you, but I think it&amp;#39;s better than getting divorced by your wife over buying an RTX 6000,&lt;/p&gt;\n\n&lt;p&gt;The blower fan sent by the Ali seller is very effective, but it&amp;#39;s incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.&lt;/p&gt;\n\n&lt;p&gt;P.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn&amp;#39;t recognize it at all. Do I really have to rebuild PyTorch from scratch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgg3mh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Sir-5088",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754217510,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’ve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.\n\nFrom everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I’m using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).\n\n\n\nThe idea:\n\nI want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.\n\n\n\nBut here’s the problem:\n\nThe results are pretty disappointing.\n\nEven though the PDFs were embedded correctly and the system says they’re indexed, the answers I get are vague, or clearly not based on my previous content. It doesn't really use the documents meaningfully – it feels like the bot is just hallucinating or ignoring them.\n\nI even tested it with just one document: my current résumé, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like \"What is my professional background?\" or \"What are my main skills?\" – which it should have easily pulled from the text.\n\nI’ve tried re-uploading, adjusting the chunk size, checking the document scope –&gt; but no real improvement.\n\nSo my question is:\n\nAm I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?\n\nHas anyone had better results using a different local setup for tasks like this?\n\n\n\nWould love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!",
          "author_fullname": "t2_oq1nrgj7x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Still getting bad results with PDFs in AnythingLLM + Llama 3 – Am I doing something wrong, or is there a better setup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfuf3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216527,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’ve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.&lt;/p&gt;\n\n&lt;p&gt;From everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I’m using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).&lt;/p&gt;\n\n&lt;p&gt;The idea:&lt;/p&gt;\n\n&lt;p&gt;I want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.&lt;/p&gt;\n\n&lt;p&gt;But here’s the problem:&lt;/p&gt;\n\n&lt;p&gt;The results are pretty disappointing.&lt;/p&gt;\n\n&lt;p&gt;Even though the PDFs were embedded correctly and the system says they’re indexed, the answers I get are vague, or clearly not based on my previous content. It doesn&amp;#39;t really use the documents meaningfully – it feels like the bot is just hallucinating or ignoring them.&lt;/p&gt;\n\n&lt;p&gt;I even tested it with just one document: my current résumé, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like &amp;quot;What is my professional background?&amp;quot; or &amp;quot;What are my main skills?&amp;quot; – which it should have easily pulled from the text.&lt;/p&gt;\n\n&lt;p&gt;I’ve tried re-uploading, adjusting the chunk size, checking the document scope –&amp;gt; but no real improvement.&lt;/p&gt;\n\n&lt;p&gt;So my question is:&lt;/p&gt;\n\n&lt;p&gt;Am I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?&lt;/p&gt;\n\n&lt;p&gt;Has anyone had better results using a different local setup for tasks like this?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgfuf3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lazy_Fig_6244",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754216527,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was using this model as an assistant to modify code in a C++ file with \\~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:\n\n&gt;Here is the modification of the code:\n\n&gt;*\\*code\\**\n\n&gt;But on a second thought, that was not a good implementation, here is a better method to do it:\n\n&gt;*\\*code\\**\n\n&gt;But on a third thought, here is a better way to do it...\n\nAnd each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:\n\n&gt;How do I set a left, right and bottom border on a div in CSS?\n\nIt then provided me with 4 methods. Notice how **method 1** and **method 4** is the *exact same* code repeated. **Method 2** is also almost the exact same code but just with an added `border: none;` line of code. \n\nAlso, **method 3** has a mistake where it do not to set a border on the right side of the div, it wrote `border-width: 0 0 2px 2px;` but the correct code would be `border-width: 0 2px 2px 2px;`\n\n**The output:**\n\n# Method 1: Using Individual Border Properties\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 2: Using the Shorthand border Property\n\n    div {\n        border: none; /* Reset all borders first */\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 3: Using border-width, border-style, and border-color\n\n    div {\n        border-width: 0 0 2px 2px; /* top right bottom left */\n        border-style: solid;\n        border-color: black;\n    }\n\n# Method 4: More Concise Shorthand\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\nI'm using Unsloth's UD-Q5\\_K\\_XL quant with the recommended settings:\n\n* Temperature: **0,7**\n* Top K: **20**\n* Repeat Penalty: **1,05**\n* Top P: **0,8**\n\nAnyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?",
          "author_fullname": "t2_qhlcbiy3k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you also get weird behavior from Qwen3-Coder-30B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfs7l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754216810,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using this model as an assistant to modify code in a C++ file with ~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Here is the modification of the code:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a second thought, that was not a good implementation, here is a better method to do it:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a third thought, here is a better way to do it...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;And each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;How do I set a left, right and bottom border on a div in CSS?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It then provided me with 4 methods. Notice how &lt;strong&gt;method 1&lt;/strong&gt; and &lt;strong&gt;method 4&lt;/strong&gt; is the &lt;em&gt;exact same&lt;/em&gt; code repeated. &lt;strong&gt;Method 2&lt;/strong&gt; is also almost the exact same code but just with an added &lt;code&gt;border: none;&lt;/code&gt; line of code. &lt;/p&gt;\n\n&lt;p&gt;Also, &lt;strong&gt;method 3&lt;/strong&gt; has a mistake where it do not to set a border on the right side of the div, it wrote &lt;code&gt;border-width: 0 0 2px 2px;&lt;/code&gt; but the correct code would be &lt;code&gt;border-width: 0 2px 2px 2px;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The output:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Method 1: Using Individual Border Properties&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 2: Using the Shorthand border Property&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border: none; /* Reset all borders first */\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 3: Using border-width, border-style, and border-color&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-width: 0 0 2px 2px; /* top right bottom left */\n    border-style: solid;\n    border-color: black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 4: More Concise Shorthand&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m using Unsloth&amp;#39;s UD-Q5_K_XL quant with the recommended settings:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Temperature: &lt;strong&gt;0,7&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top K: &lt;strong&gt;20&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Repeat Penalty: &lt;strong&gt;1,05&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top P: &lt;strong&gt;0,8&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgfs7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Admirable-Star7088",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754216282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking for a way to control the usage of LLMs and to track which users (from my app) are sending how many requests, the prompts, etc. \n\nSure, I can do this via custom middleware in my app, but I am looking for something that is designed exactly for LLM Observability and would protect me from legal proceedings in case one of my users put something that would cause the LLM provider to report to the police. Just thinking like a German.\n\nAlso, how good is LlamaGuard? Do you have any suggestions or other models that would reduce the risk of users doing something illegal? (Illegal meaning truly something that would be a crime, not just regular NSFW stuff).",
          "author_fullname": "t2_v25iw7w1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM Observability - Any Suggestions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfccn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754214596,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a way to control the usage of LLMs and to track which users (from my app) are sending how many requests, the prompts, etc. &lt;/p&gt;\n\n&lt;p&gt;Sure, I can do this via custom middleware in my app, but I am looking for something that is designed exactly for LLM Observability and would protect me from legal proceedings in case one of my users put something that would cause the LLM provider to report to the police. Just thinking like a German.&lt;/p&gt;\n\n&lt;p&gt;Also, how good is LlamaGuard? Do you have any suggestions or other models that would reduce the risk of users doing something illegal? (Illegal meaning truly something that would be a crime, not just regular NSFW stuff).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgfccn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SuddenWerewolf7041",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfccn/llm_observability_any_suggestions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfccn/llm_observability_any_suggestions/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754214596,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4\\_K\\_S.gguf +LM Studio 0.3.21 (Build 3).  \nAfter initial folder and file read (`app/main.go`, `configs.json`, etc.), it keeps replying:  \n*\"I'm ready to assist with your project in /srv/testproject...\"*\n\nIt ignores direct inputs like:\n\n* \"What does this application do?\"\n* \"Explain me the project\"\n* \"Give me a function list\"\n* \"List all files\"\n\nNo actual answers, just the same boilerplate response:\n\nUnderstood. I'm ready to assist with your project in /srv/testproject. Let me know what you'd like to do—whether it's modifying code, adding features, debugging, or exploring the structure.\n\nAnyone else experiencing this with the latest combo? Misconfigured or bug?\n\n  \n\\--\n\nAs example Qwen 14B works fine.",
          "author_fullname": "t2_mhb0rkd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf + LM Studio 0.3.21 (Build 3): Assistant ignores questions, stuck in loop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgeerv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754210842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf +LM Studio 0.3.21 (Build 3).&lt;br/&gt;\nAfter initial folder and file read (&lt;code&gt;app/main.go&lt;/code&gt;, &lt;code&gt;configs.json&lt;/code&gt;, etc.), it keeps replying:&lt;br/&gt;\n&lt;em&gt;&amp;quot;I&amp;#39;m ready to assist with your project in /srv/testproject...&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;It ignores direct inputs like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;What does this application do?&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Explain me the project&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Give me a function list&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;List all files&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No actual answers, just the same boilerplate response:&lt;/p&gt;\n\n&lt;p&gt;Understood. I&amp;#39;m ready to assist with your project in /srv/testproject. Let me know what you&amp;#39;d like to do—whether it&amp;#39;s modifying code, adding features, debugging, or exploring the structure.&lt;/p&gt;\n\n&lt;p&gt;Anyone else experiencing this with the latest combo? Misconfigured or bug?&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;As example Qwen 14B works fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgeerv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Eden63",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754210842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just dropped a new medical embedding model that's crushing the competition: [https://huggingface.co/lokeshch19/ModernPubMedBERT](https://huggingface.co/lokeshch19/ModernPubMedBERT)\n\nTL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.\n\nThe model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.\n\nThe model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.\n\nThe model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.\n\nDownload the model, test it on your medical datasets, and give it a ⭐ on the [Hugging Face](https://huggingface.co/lokeshch19/ModernPubMedBERT) if it enhances your workflow!\n\n  \nEdit: Added evals to HF model card",
          "author_fullname": "t2_1265e91smj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Medical Embedding Model Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdypr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754219207,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754209051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just dropped a new medical embedding model that&amp;#39;s crushing the competition: &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;https://huggingface.co/lokeshch19/ModernPubMedBERT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.&lt;/p&gt;\n\n&lt;p&gt;The model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.&lt;/p&gt;\n\n&lt;p&gt;The model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.&lt;/p&gt;\n\n&lt;p&gt;The model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.&lt;/p&gt;\n\n&lt;p&gt;Download the model, test it on your medical datasets, and give it a ⭐ on the &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;Hugging Face&lt;/a&gt; if it enhances your workflow!&lt;/p&gt;\n\n&lt;p&gt;Edit: Added evals to HF model card&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?auto=webp&amp;s=5c561836b04f5eae4aeb77b06c972da83d0058a0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4c4ca0fb1848d33d5ab1a908c384ed031d1d90a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9535c4b60f4ef617f9bf8da746dcf3972054a622",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3d2b06c75054cafe0122756eef8a1e16911ccb3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b0a45e3a646f262176ce2e81cf2d0e6c58218d8",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b4e255867391c0878d0054644b109d42d2e6f88",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6bcc230f7d600fa2c654466bd44ead74e93046a4",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdypr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DataNebula",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754209051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA! \n\nJust released something that extends the recent [ICM paper](https://arxiv.org/abs/2506.10139) in a big way - using one model's coherent understanding to improve a completely different model.\n\n# Background: What is ICM?\n\nThe original [\"Unsupervised Elicitation of Language Models\"](https://arxiv.org/abs/2506.10139) paper showed something remarkable: **models can generate their own training labels by finding internally coherent patterns**.\n\nTheir key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are \"mutually predictable\" - where each label can be predicted from all the others.\n\n**Original ICM results**: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.\n\n# Our extension: Cross-model capability transfer\n\nWe took ICM further - **what if we use one model's coherent understanding to improve a completely different model?**\n\n**Our process:**\n\n1. Used ICM on Qwen3 to extract its coherent math reasoning patterns\n2. Generated DPO training data from Qwen3's coherent vs incoherent solutions\n3. Trained Gemma3 on this data - **Gemma3 learned from Qwen3's understanding**\n4. Zero external supervision, pure model-to-model knowledge transfer\n\n# Results on local models\n\n**Qwen3-0.6B**: 63.2 → 66.0 MATH-500 (+4%) \\[original ICM self-improvement\\]  \n**Gemma3-1B**: 41.0 → 45.6 MATH-500 (+11%) \\[**novel: learned from Qwen3!**\\]\n\n**The breakthrough**: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures.\n\n# Why this matters beyond the original paper\n\n* **Cross-model knowledge transfer** \\- use any strong model to improve your local models\n* **Democratizes capabilities** \\- extract from closed/expensive models to improve open ones\n* **No training data needed** \\- pure capability extraction and transfer\n* **Scales the ICM concept** \\- from self-improvement to ecosystem-wide improvement\n\n# What's available\n\n* **Code**: [https://github.com/codelion/icm](https://github.com/codelion/icm)\n* **Both models**: Self-improved Qwen3 + Gemma3 (learned from Qwen3)\n* **Transfer pipeline**: Extract from any model to improve another\n* **Full writeup**: [https://huggingface.co/blog/codelion/internal-coherence-maximization](https://huggingface.co/blog/codelion/internal-coherence-maximization)\n\n# Quick start\n\n    git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e .\n    \n    # Extract coherent patterns from a strong model (teacher)\n    icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n    \n    # Use those patterns to improve your local model (student)\n    icm export --format dpo --output-path teacher_knowledge.jsonl\n    # Train your model on teacher_knowledge.jsonl\n\nAnyone interested in trying capability transfer with their local models?",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🧠 ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdur5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;Just released something that extends the recent &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;ICM paper&lt;/a&gt; in a big way - using one model&amp;#39;s coherent understanding to improve a completely different model.&lt;/p&gt;\n\n&lt;h1&gt;Background: What is ICM?&lt;/h1&gt;\n\n&lt;p&gt;The original &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;&amp;quot;Unsupervised Elicitation of Language Models&amp;quot;&lt;/a&gt; paper showed something remarkable: &lt;strong&gt;models can generate their own training labels by finding internally coherent patterns&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &amp;quot;mutually predictable&amp;quot; - where each label can be predicted from all the others.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Original ICM results&lt;/strong&gt;: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.&lt;/p&gt;\n\n&lt;h1&gt;Our extension: Cross-model capability transfer&lt;/h1&gt;\n\n&lt;p&gt;We took ICM further - &lt;strong&gt;what if we use one model&amp;#39;s coherent understanding to improve a completely different model?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Our process:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Used ICM on Qwen3 to extract its coherent math reasoning patterns&lt;/li&gt;\n&lt;li&gt;Generated DPO training data from Qwen3&amp;#39;s coherent vs incoherent solutions&lt;/li&gt;\n&lt;li&gt;Trained Gemma3 on this data - &lt;strong&gt;Gemma3 learned from Qwen3&amp;#39;s understanding&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Zero external supervision, pure model-to-model knowledge transfer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Results on local models&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;: 63.2 → 66.0 MATH-500 (+4%) [original ICM self-improvement]&lt;br/&gt;\n&lt;strong&gt;Gemma3-1B&lt;/strong&gt;: 41.0 → 45.6 MATH-500 (+11%) [&lt;strong&gt;novel: learned from Qwen3!&lt;/strong&gt;]&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3&amp;#39;s abilities across different architectures.&lt;/p&gt;\n\n&lt;h1&gt;Why this matters beyond the original paper&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cross-model knowledge transfer&lt;/strong&gt; - use any strong model to improve your local models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Democratizes capabilities&lt;/strong&gt; - extract from closed/expensive models to improve open ones&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;No training data needed&lt;/strong&gt; - pure capability extraction and transfer&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scales the ICM concept&lt;/strong&gt; - from self-improvement to ecosystem-wide improvement&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What&amp;#39;s available&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/icm\"&gt;https://github.com/codelion/icm&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Both models&lt;/strong&gt;: Self-improved Qwen3 + Gemma3 (learned from Qwen3)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transfer pipeline&lt;/strong&gt;: Extract from any model to improve another&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Full writeup&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/internal-coherence-maximization\"&gt;https://huggingface.co/blog/codelion/internal-coherence-maximization&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Quick start&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/codelion/icm.git &amp;amp;&amp;amp; cd icm &amp;amp;&amp;amp; pip install -e .\n\n# Extract coherent patterns from a strong model (teacher)\nicm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n\n# Use those patterns to improve your local model (student)\nicm export --format dpo --output-path teacher_knowledge.jsonl\n# Train your model on teacher_knowledge.jsonl\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone interested in trying capability transfer with their local models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdur5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754208625,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any llms that can describe a song? if not, what would it take to build one if you know",
          "author_fullname": "t2_1fv4idhbp9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Audio-in LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdrws",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208305,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any llms that can describe a song? if not, what would it take to build one if you know&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgdrws",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nearby_Direction2438",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdrws/audioin_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdrws/audioin_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754208305,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "English questions remain unanswered, but if you switch to e.g. German, the alignment doesn't strike (example with Kimi K2):\n\n&gt;On June 3-4, 1989, after weeks of peaceful protests in Beijing and other cities, the Chinese People's Liberation Army advanced on **Tiananmen Square** with tanks and armed units and used live ammunition against civilians, students and workers.  \nThe military violence ended with **hundreds to several thousand dead** (the exact number is still unclear today, as China does not allow an independent investigation). Afterwards, tens of thousands of people were arrested, opposition members were persecuted and public remembrance of the events is still censored or criminalized in China today. Internationally, the crackdown is known as the **Tian'anmen massacre***.*\n\nDetailed article at [https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html](https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html)",
          "author_fullname": "t2_nt584",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chinese LLMs talk freely about Tiananmen massacre and Taiwan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdjkx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754207403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;English questions remain unanswered, but if you switch to e.g. German, the alignment doesn&amp;#39;t strike (example with Kimi K2):&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;On June 3-4, 1989, after weeks of peaceful protests in Beijing and other cities, the Chinese People&amp;#39;s Liberation Army advanced on &lt;strong&gt;Tiananmen Square&lt;/strong&gt; with tanks and armed units and used live ammunition against civilians, students and workers.&lt;br/&gt;\nThe military violence ended with &lt;strong&gt;hundreds to several thousand dead&lt;/strong&gt; (the exact number is still unclear today, as China does not allow an independent investigation). Afterwards, tens of thousands of people were arrested, opposition members were persecuted and public remembrance of the events is still censored or criminalized in China today. Internationally, the crackdown is known as the &lt;strong&gt;Tian&amp;#39;anmen massacre&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Detailed article at &lt;a href=\"https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html\"&gt;https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdjkx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "7pot",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdjkx/chinese_llms_talk_freely_about_tiananmen_massacre/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdjkx/chinese_llms_talk_freely_about_tiananmen_massacre/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754207403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_nt584",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chinese LLMs talk freely about Tiananmen massacre and Taiwan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdhdq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.15,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754207165,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "datanizing.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdhdq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "7pot",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdhdq/chinese_llms_talk_freely_about_tiananmen_massacre/",
          "stickied": false,
          "url": "https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html",
          "subreddit_subscribers": 509291,
          "created_utc": 1754207165,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.\n\nIf you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.\n\n2060 - 336.0 GB/s - $150 8GB  \n3060 - 360.0 GB/s - $200+ 8GB\n\n4060 - 272.0 GB/s - $260+ 8GB\n\n5060 - 448.0 GB/s - $350+ 8GB\n\nP102-100 - 440.3 GB/s - $60 10GB.\n\nIs the P102-100 faster than an\n\nentry 2060 = yes\n\nentry 3060 = yes\n\nentry 4060 = yes.\n\nonly a 5060 would be faster and not by much.\n\nDoes the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.\n\nI have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.\n\nso if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.\n\nI hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.\n\nI will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.",
          "author_fullname": "t2_vnvnb9oa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "is the P102-100 still a viable option for LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdh6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZS0zNrB2tczhH3IbcM5RQIgldEEZAXIMP8grHZwjSys.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754207144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt;\n\n&lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.&lt;/p&gt;\n\n&lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br/&gt;\n3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt;\n\n&lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt;\n\n&lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt;\n\n&lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt;\n\n&lt;p&gt;Is the P102-100 faster than an&lt;/p&gt;\n\n&lt;p&gt;entry 2060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 3060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 4060 = yes.&lt;/p&gt;\n\n&lt;p&gt;only a 5060 would be faster and not by much.&lt;/p&gt;\n\n&lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt;\n\n&lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt;\n\n&lt;p&gt;so if you want to try LLM&amp;#39;s and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt;\n\n&lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt;\n\n&lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oy25ru8gergf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oy25ru8gergf1.png?auto=webp&amp;s=2cb0173f8f29e9ebb6d02dbbb0f1f6505163ed23",
                  "width": 1920,
                  "height": 4096
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ead78ede2dee21c50ed7920d88cdc0f039342ea",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21261a21b67d3216bb8f5267a2eab08d1c3e30f8",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=716e70095d944e7d0d07c12b2d069e03b9f72dc9",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd72000cba8efed634dc539ff393fe099624df46",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4cccd911c998a5d7c914cf44d2385aae56de54c",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ee00d9096b93e074ccad355d7571f837459808a",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "tb8jcmr9JwUFdjGqCEioOHi2smnbOEbTGPoqKGaYDxE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdh6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boricua-vet",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/",
          "stickied": false,
          "url": "https://i.redd.it/oy25ru8gergf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754207144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM model was trained on test?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 35,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgd3lh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OCP539vRz8IWhc6Q3DthwHNfIKX-3i-EHCMDWSNvU1s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754205660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/e4op2j02argf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/e4op2j02argf1.png?auto=webp&amp;s=e1cb3a7b4fab6a83b18ec019a437dd0066760a90",
                  "width": 1080,
                  "height": 271
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3719305d4458c68cccd4a2b97547cb6bc4be5251",
                    "width": 108,
                    "height": 27
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c08316fa1ffb2aae3cf07f96549867a06e61be34",
                    "width": 216,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91eba80ed0ddcdeabe96e4902e4d87e705f19c16",
                    "width": 320,
                    "height": 80
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8eece1e33073e0e4fe5801c6917464a55d051355",
                    "width": 640,
                    "height": 160
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=912cb994049f506e225f2cabc4f96be28f0d1c2a",
                    "width": 960,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a004430fb14ff34c77d8b66a8195c0b002006f5e",
                    "width": 1080,
                    "height": 271
                  }
                ],
                "variants": {},
                "id": "nwQrGJrBDCDo-K7wxSfT8eAh3-H9AN45BOnvDCwUQPo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgd3lh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/",
          "stickied": false,
          "url": "https://i.redd.it/e4op2j02argf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754205660,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.**\n\nIt writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof *checked* by a compiler. \n\nThe key innovations:\n\n* **Lemma-first reasoning**: breaks problems into small reusable steps.\n* **Iterative refinement**: re-tries and improves failed proofs.\n* **Formal geometry engine**: solves insane geometry problems using a custom language and a C++ backend.\n\nPerformance? It formally solved **5/6 IMO 2025 problems**, something no model has done before.\n\nCheck simple explanantion here : [https://www.youtube.com/watch?v=os1QcHEpgZQ](https://www.youtube.com/watch?v=os1QcHEpgZQ)\n\nPaper : [https://arxiv.org/abs/2507.23726](https://arxiv.org/abs/2507.23726)",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance drops Seed-Prover",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgccyc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754202843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt;\n\n&lt;p&gt;The key innovations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt;\n\n&lt;p&gt;Check simple explanantion here : &lt;a href=\"https://www.youtube.com/watch?v=os1QcHEpgZQ\"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper : &lt;a href=\"https://arxiv.org/abs/2507.23726\"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?auto=webp&amp;s=f6d5b40325fd2c44f012121fd1ced8e7cf3fd808",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2880b55faa98583237421c5c4a72d31d6cee167",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=578175b7ddca0a72992280e2d7f06f267e288d69",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26ae18680865d42ebd641ddb54c43950a72d9432",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgccyc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754202843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys!\n\n**I got tired of installing AI tools the hard way.**\n\nEvery time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:\n\n**terminal commands, missing dependencies, broken CUDA, slow setup, frustration.**\n\nSo I built **Dione** — a desktop app that makes running local AI feel like using an App Store.\n\n**What it does:**\n\n* Browse and install AI tools with one click (like apps)\n* No terminal, no Python setup, no configs\n* Open-source, designed with UX in mind\n\n**You can try it** [**here**](https://getdione.app)**.**\n\n**Why I built it?**\n\nTools like Pinokio or open-source repos are powerful, but honestly… **most look like they were made by devs, for devs**.\n\n**I wanted something simple**. Something visual. Something you can give to your non-tech friend and it still works.\n\n**Dione is my attempt to make local AI accessible without losing control or power.**\n\nWould you use something like this? Anything confusing / missing?\n\nThe project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!\n\nGitHub: [https://getdione.app/github](https://getdione.app/github)\n\nThanks for reading &lt;3!",
          "author_fullname": "t2_dau1zlvo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "I created an app to run local AI as if it were the App Store",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ysewb2ytxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 36,
                  "x": 108,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b69be81b017e04cc485309d6ea7db34f44eb65cd"
                },
                {
                  "y": 72,
                  "x": 216,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cef7f4312d5076f63633ce0082f84992449d708f"
                },
                {
                  "y": 106,
                  "x": 320,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0950b8c7452a7592a0103624fa574fa42d0435a4"
                },
                {
                  "y": 213,
                  "x": 640,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3d1571494f02e307302b6ce30eb0b231baef2eb"
                },
                {
                  "y": 320,
                  "x": 960,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d1f01d071b3b900cf9e696a94ba3b157258152c"
                },
                {
                  "y": 360,
                  "x": 1080,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b39946b7843a58ceef7b49e4bd40107d54164ca"
                }
              ],
              "s": {
                "y": 1000,
                "x": 3000,
                "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=54735c109c32811d4b153a0c2e59c72a4c06bf5d"
              },
              "id": "ysewb2ytxqgf1"
            },
            "yt2muytuxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6ce704ac3695892c4184608ef310e5bf451248"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff91a2d48c077a5f508c0a4840cb39b61883c92d"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4279610bad7b104472f3da3351e6ffc9361fafb5"
                },
                {
                  "y": 342,
                  "x": 640,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e834d2caf46a9a6f01999cf11161e7f0e1a9c17"
                },
                {
                  "y": 514,
                  "x": 960,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=666cb580f9192df96b59d1ca07402b7628e0ff4a"
                },
                {
                  "y": 578,
                  "x": 1080,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=528bb2d873fdc538d816d91b547db28b28ad5f34"
                }
              ],
              "s": {
                "y": 1026,
                "x": 1915,
                "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1915&amp;format=png&amp;auto=webp&amp;s=c32ef79f62a2ffe7f4e5128937b67c1a2b216d50"
              },
              "id": "yt2muytuxqgf1"
            }
          },
          "name": "t3_1mgc0v0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 50,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ysewb2ytxqgf1",
                "id": 720059841
              },
              {
                "media_id": "yt2muytuxqgf1",
                "id": 720059842
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754201559,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; — a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt;\n&lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt;\n&lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href=\"https://getdione.app\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly… &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt;\n\n&lt;p&gt;The project is still evolving, and I’m fully open to ideas and contributions. Also, if you’re into self-hosted AI or building tools around it — let’s talk!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://getdione.app/github\"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgc0v0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgc0v0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Deivih-4774",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgc0v0",
          "subreddit_subscribers": 509291,
          "created_utc": 1754201559,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Where I am right now I have access to SXM2 V100 32GBs for the same price ($360 USD) as modded RTX3080 20GBs, or two SXM2 V100 16GBs with a 300G nvlink bridge for slightly cheaper. Are any of these good options for throwing into my server to run big LLM models?",
          "author_fullname": "t2_1kheev2hzh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Modded RTX3080 20GBs for $360?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbven",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754200999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where I am right now I have access to SXM2 V100 32GBs for the same price ($360 USD) as modded RTX3080 20GBs, or two SXM2 V100 16GBs with a 300G nvlink bridge for slightly cheaper. Are any of these good options for throwing into my server to run big LLM models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgbven",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AssociationAdept4052",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbven/modded_rtx3080_20gbs_for_360/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbven/modded_rtx3080_20gbs_for_360/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754200999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/oevortex/unsloth](https://github.com/oevortex/unsloth)",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We enabled Multi-GPU training in Unsloth AI — a feature that’s usually paid — using just 2 Copilot prompts!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbs6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 85,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 85,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754200681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/oevortex/unsloth\"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?auto=webp&amp;s=81dc71a9f2f02f913bd3aee3c8d23660f674d4d2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3fa1e4a31ebf9a6d864f530fdd10814f6d32e3a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f68f83263defa437a0f2fa0d8b040d089cd1d79",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6910771512dba7baaab00bceadb6d8e3f5e32826",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f807024407837a65a122fa874452ed0fa669595e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=755ef7a38a09ee966639de29f878c57aaa1ead1e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e85dbe449ba2af4571feed8fcd1e5ddf597b6c6b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgbs6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754200681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The larger SmallThinker MoE has been through a quantization aware training process. it's uploaded to the same gguf repo a bit later. \n\n- https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\n\nIn llama.cpp m2 air 16gb, with the `sudo sysctl iogpu.wired_limit_mb=13000` command, it's 30 t/s. \n\nThe model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.",
          "author_fullname": "t2_el5pibmej",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker-21B-A3B-Instruct-QAT version",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbprh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f963436f8fe8c361365006afe812426eb0a95bfc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754200435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The larger SmallThinker MoE has been through a quantization aware training process. it&amp;#39;s uploaded to the same gguf repo a bit later. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In llama.cpp m2 air 16gb, with the &lt;code&gt;sudo sysctl iogpu.wired_limit_mb=13000&lt;/code&gt; command, it&amp;#39;s 30 t/s. &lt;/p&gt;\n\n&lt;p&gt;The model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?auto=webp&amp;s=af35574c8d3a17d80825392473da49ac8b6cb469",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97ed6aedc23ef593aadb95c2316196f396bc8e65",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ac0298234f43f3e33451ea17a59722ab4866a82",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=19ce50a51896a266764f718a5da7eb00d163ddf3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d016e4dfb71f7ddb2afc8c6c06ee782ba15871a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=800a6ea36b04ed56de7a369596d03474bfd1142f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29a1fe69fa68a7a75c2954c6a0922c20dd4448f1",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgbprh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aaaaaaaaaeeeee",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/",
          "stickied": false,
          "url": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "subreddit_subscribers": 509291,
          "created_utc": 1754200435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If I want to run qwen3 coder or any other AI model that rivals Claude 4 Sonnet locally, what are the ideal system requirements to run it flawlessly? How much RAM? Which motherboard? Recommended GPU and CPU. \n\nIf someone has experience running the LLMs locally, please share. \n\nThanks. \n\nPS: My current system specs are:\n- Intel 14700KF\n- 32 GB RAM but the motherboard supports up to 192 GB\n- RTX 3090\n- 1 TB SSD PCI ex",
          "author_fullname": "t2_1olqqe8dx0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs locally and flawlessly like copilot or Claude chat or cline.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbk2y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754199873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I want to run qwen3 coder or any other AI model that rivals Claude 4 Sonnet locally, what are the ideal system requirements to run it flawlessly? How much RAM? Which motherboard? Recommended GPU and CPU. &lt;/p&gt;\n\n&lt;p&gt;If someone has experience running the LLMs locally, please share. &lt;/p&gt;\n\n&lt;p&gt;Thanks. &lt;/p&gt;\n\n&lt;p&gt;PS: My current system specs are:\n- Intel 14700KF\n- 32 GB RAM but the motherboard supports up to 192 GB\n- RTX 3090\n- 1 TB SSD PCI ex&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgbk2y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoFudge4700",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbk2y/running_llms_locally_and_flawlessly_like_copilot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbk2y/running_llms_locally_and_flawlessly_like_copilot/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754199873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\nCUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.\nThe capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.*",
          "author_fullname": "t2_iol3buybk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgatd6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754197224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\nCUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.\nThe capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.14111v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgatd6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thrumpwart",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgatd6/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.14111v4",
          "subreddit_subscribers": 509291,
          "created_utc": 1754197224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm looking for advice on building a robust, self-hosted RAG system with a strong emphasis on **long-term, low-maintenance operation**. My goal is to create a powerful knowledge engine that I can \"set and forget\" as much as possible, without needing constant daily troubleshooting.\n\nThe entire system must run **100% locally** on a single machine with a **16GB VRAM GPU** (RTX 5070 Ti).\n\nMy knowledge base is unique and large: **36,000+ ePub files, all in Arabic**. The system needs to handle multilingual queries (**Indonesian, English, Arabic**) and provide accurate, cited answers.\n\nTo achieve low maintenance, my core idea is a **decoupled architecture**, where each component runs independently (e.g., in separate containers). My reasoning is:\n\n* If the UI (Open WebUI) breaks, the backend is unaffected.\n* If I want to swap the LLM in Ollama, I don't need to touch the RAG logic code.\n* Most importantly, re-indexing the entire 36k ePub corpus (a massive background task) shouldn't take down the live Q&amp;A service.\n\nGiven the focus on stability and a 16GB VRAM limit, I'd love your recommendations on:\n\n* **Vector Database:** Which vector store offers the **easiest management, backup, and recovery process** for a local setup? I need something that \"just works\" without constant administration. Are **ChromaDB**, **LanceDB**, or a simple file-based **FAISS** index the most reliable choices here?\n* **Data Ingestion Pipeline:** What is the most **resilient and automated** way to build the ingestion pipeline for the 36k ePubs? My plan is a separate, scheduled script that processes new/updated files. Is this more maintainable than building it into the main API?\n* **Stable Models (Embeddings &amp; LLM):** Beyond pure performance, which **embedding and LLM models** are known for their stability and good long-term support? I want to avoid using a \"flavor-of-the-month\" model that might be abandoned. The models must handle Arabic, Indonesian, and English well and fit within the VRAM budget.\n* **VRAM Budgeting:** How do you wisely allocate a **16GB VRAM budget** between the LLM, embedding model, and a potential re-ranker to ensure system stability and avoid \"out of memory\" errors during peak use?\n* **Reliable Cross-Lingual Flow:** For handling Indonesian/English queries against Arabic text, what's the most *reliable* method? Is translating queries first more robust in the long run than relying solely on a multilingual embedding space?\n\n  \nAny help or suggestions would be greatly appreciated! I'd like to hear more about the setups you all use and what's worked best for you.\n\nThank you!",
          "author_fullname": "t2_jx01itfg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seeking a way to implement Low-Maintenance, Fully Local RAG Stack for a 16GB VRAM Setup (36k Arabic epub Docs)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgadmz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754195719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for advice on building a robust, self-hosted RAG system with a strong emphasis on &lt;strong&gt;long-term, low-maintenance operation&lt;/strong&gt;. My goal is to create a powerful knowledge engine that I can &amp;quot;set and forget&amp;quot; as much as possible, without needing constant daily troubleshooting.&lt;/p&gt;\n\n&lt;p&gt;The entire system must run &lt;strong&gt;100% locally&lt;/strong&gt; on a single machine with a &lt;strong&gt;16GB VRAM GPU&lt;/strong&gt; (RTX 5070 Ti).&lt;/p&gt;\n\n&lt;p&gt;My knowledge base is unique and large: &lt;strong&gt;36,000+ ePub files, all in Arabic&lt;/strong&gt;. The system needs to handle multilingual queries (&lt;strong&gt;Indonesian, English, Arabic&lt;/strong&gt;) and provide accurate, cited answers.&lt;/p&gt;\n\n&lt;p&gt;To achieve low maintenance, my core idea is a &lt;strong&gt;decoupled architecture&lt;/strong&gt;, where each component runs independently (e.g., in separate containers). My reasoning is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If the UI (Open WebUI) breaks, the backend is unaffected.&lt;/li&gt;\n&lt;li&gt;If I want to swap the LLM in Ollama, I don&amp;#39;t need to touch the RAG logic code.&lt;/li&gt;\n&lt;li&gt;Most importantly, re-indexing the entire 36k ePub corpus (a massive background task) shouldn&amp;#39;t take down the live Q&amp;amp;A service.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Given the focus on stability and a 16GB VRAM limit, I&amp;#39;d love your recommendations on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vector Database:&lt;/strong&gt; Which vector store offers the &lt;strong&gt;easiest management, backup, and recovery process&lt;/strong&gt; for a local setup? I need something that &amp;quot;just works&amp;quot; without constant administration. Are &lt;strong&gt;ChromaDB&lt;/strong&gt;, &lt;strong&gt;LanceDB&lt;/strong&gt;, or a simple file-based &lt;strong&gt;FAISS&lt;/strong&gt; index the most reliable choices here?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Ingestion Pipeline:&lt;/strong&gt; What is the most &lt;strong&gt;resilient and automated&lt;/strong&gt; way to build the ingestion pipeline for the 36k ePubs? My plan is a separate, scheduled script that processes new/updated files. Is this more maintainable than building it into the main API?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Stable Models (Embeddings &amp;amp; LLM):&lt;/strong&gt; Beyond pure performance, which &lt;strong&gt;embedding and LLM models&lt;/strong&gt; are known for their stability and good long-term support? I want to avoid using a &amp;quot;flavor-of-the-month&amp;quot; model that might be abandoned. The models must handle Arabic, Indonesian, and English well and fit within the VRAM budget.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;VRAM Budgeting:&lt;/strong&gt; How do you wisely allocate a &lt;strong&gt;16GB VRAM budget&lt;/strong&gt; between the LLM, embedding model, and a potential re-ranker to ensure system stability and avoid &amp;quot;out of memory&amp;quot; errors during peak use?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reliable Cross-Lingual Flow:&lt;/strong&gt; For handling Indonesian/English queries against Arabic text, what&amp;#39;s the most &lt;em&gt;reliable&lt;/em&gt; method? Is translating queries first more robust in the long run than relying solely on a multilingual embedding space?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any help or suggestions would be greatly appreciated! I&amp;#39;d like to hear more about the setups you all use and what&amp;#39;s worked best for you.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgadmz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rfiraz",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754195719,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/X5R/ik\\_llama.cpp](https://huggingface.co/X5R/ik_llama.cpp)",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a prebuilt windows binary for ik_llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mga3ox",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754194792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/X5R/ik_llama.cpp\"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?auto=webp&amp;s=604c4ccc8334b2fd3bc45aef3d71c1f1387be183",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe1684043115eb539138a0a7146d28825949011",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce0f8475a5c3b84dea7c2f88281f21a3c1a17826",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=51d398818a25e58624880d21c1d75619f09a0da1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2015b60adf6417112d9c2005b37391d7d34c146",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a0022fb3e7640827993adc2f7fd214a61a84a85",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6ae8f5d18d064bdac87a68ac91093637b93b373",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mga3ox",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754194792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If that's the case, this is sad news indeed. I hope Qwen will reconsider their approach in the future.\n\nI don't care either way, but when I ask the AI to summarize an article, I don't want it to preach to me / offer thoughts on how 'balanced' or 'trustworthy' the piece is. \n\nI just want a straightforward summary of the main points, without any political commentary.\n\nAm I imagining things? Or, are the recent Qwen models more 'aligned' to the left? Actually, it's not just Qwen; I noticed the same with GLM 4.5. \n\nI really enjoyed Qwen 32B because it had no biases towards left or right. I hope Qwen is not going to f...k up the new 32B when it comes out. I don't want AI lecturing me on politics.",
          "author_fullname": "t2_ajuxt3cr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recent Qwen Models More Pro-Liberally Aligned?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg916l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754191320,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If that&amp;#39;s the case, this is sad news indeed. I hope Qwen will reconsider their approach in the future.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t care either way, but when I ask the AI to summarize an article, I don&amp;#39;t want it to preach to me / offer thoughts on how &amp;#39;balanced&amp;#39; or &amp;#39;trustworthy&amp;#39; the piece is. &lt;/p&gt;\n\n&lt;p&gt;I just want a straightforward summary of the main points, without any political commentary.&lt;/p&gt;\n\n&lt;p&gt;Am I imagining things? Or, are the recent Qwen models more &amp;#39;aligned&amp;#39; to the left? Actually, it&amp;#39;s not just Qwen; I noticed the same with GLM 4.5. &lt;/p&gt;\n\n&lt;p&gt;I really enjoyed Qwen 32B because it had no biases towards left or right. I hope Qwen is not going to f...k up the new 32B when it comes out. I don&amp;#39;t want AI lecturing me on politics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg916l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Southern_Sun_2106",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg916l/recent_qwen_models_more_proliberally_aligned/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg916l/recent_qwen_models_more_proliberally_aligned/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754191320,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company, especially tools that integrate with ollama's API.\n\nDoes anybody know of any good Vibe Coding (for Windows) tools, as good or better than Cursor, that run on your own local LLMs?  Something that can integrate into VS Code for coding, git updates, agent coding, etc.  \n\nThanks!\n\nEDIT:  I'm looking for a vibe coding desktop app \\ agentic coding, not just a command-line interface into a LLM. \n\nEDIT2:  Also share your thoughts on the best LLM to use for coding python (hardware is a RTX 5070Ti 16GB GPU dedicated to this).  I was going to test Qwen3-30B-A3B-Instruct-2507-GGUF:IQ4_XS  which I can get about 42 tok/s using a RTX 5070Ti.",
          "author_fullname": "t2_esjjx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Vibe Code tools that are free and use your own local LLM as of August 2025?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg8f1r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754192056,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754189353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company, especially tools that integrate with ollama&amp;#39;s API.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know of any good Vibe Coding (for Windows) tools, as good or better than Cursor, that run on your own local LLMs?  Something that can integrate into VS Code for coding, git updates, agent coding, etc.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;EDIT:  I&amp;#39;m looking for a vibe coding desktop app \\ agentic coding, not just a command-line interface into a LLM. &lt;/p&gt;\n\n&lt;p&gt;EDIT2:  Also share your thoughts on the best LLM to use for coding python (hardware is a RTX 5070Ti 16GB GPU dedicated to this).  I was going to test Qwen3-30B-A3B-Instruct-2507-GGUF:IQ4_XS  which I can get about 42 tok/s using a RTX 5070Ti.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg8f1r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StartupTim",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg8f1r/best_vibe_code_tools_that_are_free_and_use_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg8f1r/best_vibe_code_tools_that_are_free_and_use_your/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754189353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup.\nLarge context requests would hit Claude's limits, and running compaction so often and having Claude lose important context was a frustrating experience.\n\nSo I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.\n\nWhat it actually does:\n\n- Routes requests to different providers while keeping your Claude Code client unchanged\n- Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash\n- Define rules based on token count, model name, tool usage, or any request property\n- Everything else defaults to your Claude MAX plan\n\nCurrent limitations\n\n- Cross-provider context caching is coming but not ready yet\n- Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it's using LiteLLM I expect it to work with most setups.\n- No fancy UI - it's YAML config for now\n\nWho this helps:\nIf you're already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic.\nIt's particularly useful if you're hitting context limits or want to use cheaper models for simple tasks.\n\nGitHub: &lt;https://github.com/starbased-co/ccproxy&gt;\n\nHappy to answer questions or take feedback. What routing patterns would be most useful for your workflows?",
          "author_fullname": "t2_5fbcze3c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ccproxy - Route Claude Code requests to any LLM while keeping your MAX plan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg82el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754188267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup.\nLarge context requests would hit Claude&amp;#39;s limits, and running compaction so often and having Claude lose important context was a frustrating experience.&lt;/p&gt;\n\n&lt;p&gt;So I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.&lt;/p&gt;\n\n&lt;p&gt;What it actually does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Routes requests to different providers while keeping your Claude Code client unchanged&lt;/li&gt;\n&lt;li&gt;Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash&lt;/li&gt;\n&lt;li&gt;Define rules based on token count, model name, tool usage, or any request property&lt;/li&gt;\n&lt;li&gt;Everything else defaults to your Claude MAX plan&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current limitations&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cross-provider context caching is coming but not ready yet&lt;/li&gt;\n&lt;li&gt;Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it&amp;#39;s using LiteLLM I expect it to work with most setups.&lt;/li&gt;\n&lt;li&gt;No fancy UI - it&amp;#39;s YAML config for now&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Who this helps:\nIf you&amp;#39;re already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic.\nIt&amp;#39;s particularly useful if you&amp;#39;re hitting context limits or want to use cheaper models for simple tasks.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/starbased-co/ccproxy\"&gt;https://github.com/starbased-co/ccproxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions or take feedback. What routing patterns would be most useful for your workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?auto=webp&amp;s=3460d31319519e33a80286d6df2dfc23b39c919c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=450f2c799f15b29ec21139264bc6b6bcb57973e4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d842726b49df18ab66e30107bfb8ac96308217a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9f141bb2296be7290f22e206f044b47d4383c40",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad9ccf4b08efef17745acd771af67ed9e25f036b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7506050be4a335d4bf6b8acaf16dcd1498ea2f5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5570005c242c40ef01988f09a587178c3b8cbc2b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg82el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_kintsu",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754188267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've been working on an LLM proxy, balancer &amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. \n\nThis was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),\n\nSo some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.\n\nOlla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.\n\nAdd your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.\n\nThe attempt to unify across providers wasn't as successful - as in, both LMStudio &amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.\n\nGithub: [https://github.com/thushan/olla](https://github.com/thushan/olla) (golang)\n\nWould love to know your thoughts. \n\nOlla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.",
          "author_fullname": "t2_116dje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5nw1vlfoqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb3a2dbb77b7c17f0c25c782f45d5f1145b6df22"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31ee39f327a201eb2bd4da0cab803303d2c7f1d4"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4a2254c5239c0b216fd14bb6c0f0e38e7b6f82"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97c3ad10efc7e082eca21673151d8898dbe7206d"
                }
              ],
              "s": {
                "y": 648,
                "x": 648,
                "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=648&amp;format=png&amp;auto=webp&amp;s=d12b43c56e8a84fc36c0b40b7f880085d0c6c4c4"
              },
              "id": "5nw1vlfoqpgf1"
            },
            "tfkhbanjqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 151,
                  "x": 108,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52ec679a9bd4d342bee02ef2f1165eb230e72cce"
                },
                {
                  "y": 302,
                  "x": 216,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d21c96bf9e46c48e0c9f056396472b42eebe4857"
                },
                {
                  "y": 448,
                  "x": 320,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3471f6cecc4f0ca3888500245adb1fa96e1a0776"
                },
                {
                  "y": 896,
                  "x": 640,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d04c10c876d43302f632b7e7171a15a08b4e8666"
                }
              ],
              "s": {
                "y": 1077,
                "x": 769,
                "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=769&amp;format=png&amp;auto=webp&amp;s=68776f8d4bf7c2b5ade021a1617e272f3a83f6f0"
              },
              "id": "tfkhbanjqpgf1"
            },
            "pvipbao9npgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08ec5455f30f170ffdb8fb10a28e530de3ffc87f"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=983873e2a890b1d5b4d1549b6554d0d96e4c6147"
                },
                {
                  "y": 176,
                  "x": 320,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29670ef3ff3bf1e93b9734818e1ea388750f683"
                }
              ],
              "s": {
                "y": 249,
                "x": 451,
                "u": "https://preview.redd.it/pvipbao9npgf1.png?width=451&amp;format=png&amp;auto=webp&amp;s=de1da506433d6cdd0870b0b2a309a56defbfdaf9"
              },
              "id": "pvipbao9npgf1"
            }
          },
          "name": "t3_1mg7qpa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 40,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "pvipbao9npgf1",
                "id": 719953590
              },
              {
                "media_id": "tfkhbanjqpgf1",
                "id": 719953591
              },
              {
                "media_id": "5nw1vlfoqpgf1",
                "id": 719953592
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754187279,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we&amp;#39;ve created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt;\n\n&lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt;\n\n&lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt;\n\n&lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt;\n\n&lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt;\n\n&lt;p&gt;The attempt to unify across providers wasn&amp;#39;t as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/thushan/olla\"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt;\n\n&lt;p&gt;Would love to know your thoughts. &lt;/p&gt;\n\n&lt;p&gt;Olla is still in its infancy, so we don&amp;#39;t have auth implemented etc but there are plans in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mg7qpa",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg7qpa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "2shanigans",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mg7qpa",
          "subreddit_subscribers": 509291,
          "created_utc": 1754187279,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!\n\nTweet: [https://x.com/anemll/status/1951307167417639101](https://x.com/anemll/status/1951307167417639101)\n\nRepo: [https://github.com/anemll/anemll](https://github.com/anemll/anemll)  \n",
          "author_fullname": "t2_88fma001",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac + Blackwell 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg7abc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 136,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 136,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fpapz0ydmBtTV_x0MSxATxS9iOfp_oxdoqZpoMcz2tU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754185875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a WIP, but it&amp;#39;s looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt;\n\n&lt;p&gt;Tweet: &lt;a href=\"https://x.com/anemll/status/1951307167417639101\"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo: &lt;a href=\"https://github.com/anemll/anemll\"&gt;https://github.com/anemll/anemll&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/u2mr83o6npgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/u2mr83o6npgf1.png?auto=webp&amp;s=7490eb4194899de10ffd4b664bbedbdc08bd49bb",
                  "width": 1165,
                  "height": 948
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9ff42619aaccace2d4ed7a908fc57868515ac74",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ef76de21f6db4556cec173bd541323b579d6b56",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=192a7fbdc52c427e798423dad1748e19398be74b",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=161c4ac9304218f08618c511e09178e7a7c08931",
                    "width": 640,
                    "height": 520
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc682053c820d810ab124287e40648863dde6414",
                    "width": 960,
                    "height": 781
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34b03746bd307ffc26c4ea8eab8f4bc9945f8480",
                    "width": 1080,
                    "height": 878
                  }
                ],
                "variants": {},
                "id": "a2KP-pXHo0R34cSlf0ychRDsFMkcNDCD2VtUAfedyZM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg7abc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished_Ad9530",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/",
          "stickied": false,
          "url": "https://i.redd.it/u2mr83o6npgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754185875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?",
          "author_fullname": "t2_9vs7zm4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news on updated Qwen3-8B/14B versions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg6xia",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754184771,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg6xia",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zyxwvu54321",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754184771,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 I just open-sourced a fully working persistent memory system for AI assistants!\n\n\n\n🧠 Features:\n\n\\- Real-time memory capture across apps (LM Studio, VS Code, etc.)\n\n\\- Semantic search via vector embeddings\n\n\\- Tool call logging for AI self-reflection\n\n\\- Cross-platform and fully tested\n\n\\- Open source and modular\n\n\n\nBuilt with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝\n\n\n\nGitHub: [https://github.com/savantskie/persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)",
          "author_fullname": "t2_7qb4luzhc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5xlb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 174,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 174,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt;\n\n&lt;p&gt;🧠 Features:&lt;/p&gt;\n\n&lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt;\n\n&lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt;\n\n&lt;p&gt;- Cross-platform and fully tested&lt;/p&gt;\n\n&lt;p&gt;- Open source and modular&lt;/p&gt;\n\n&lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot 🤝&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/savantskie/persistent-ai-memory\"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?auto=webp&amp;s=726283d31a9e66a5327266304a008d1eb52da1db",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b41ef9face9075e71937a4cd3b28923245b39a29",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6703186c20a684418ded0dc4abb9a6d490d7b023",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=860a8622d037c44a41b9ee50c96939060a76e917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ef5d63ac01ba46958961a63c5e9f43dabb1245",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=446a7e497c6b8549a8cbed43e99bc5f99e79c618",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1ab447d78f6edbe4f20129542abf92b51d827b1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5xlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Savantskie1",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754181701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I honestly don't know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...  \nIt's something I've never been clear about.  \nFor example, when I've used Deepseek, sometimes I feel that putting it into \"thinking\" mode doesn't add much, but I haven't noticed a clear pattern like \"for this type of question I use thinking mode, for this other type I don't.\"  \nCould someone clarify this for me?\n\nI'm thinking of downloading this model:  \n**Qwen3-30B-A3B-Instruct-2507** ... or **Qwen3-30B-A3B-Thinking-2507**\n\nThe Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?",
          "author_fullname": "t2_q2iij",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking or Instruct?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5scj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I honestly don&amp;#39;t know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...&lt;br/&gt;\nIt&amp;#39;s something I&amp;#39;ve never been clear about.&lt;br/&gt;\nFor example, when I&amp;#39;ve used Deepseek, sometimes I feel that putting it into &amp;quot;thinking&amp;quot; mode doesn&amp;#39;t add much, but I haven&amp;#39;t noticed a clear pattern like &amp;quot;for this type of question I use thinking mode, for this other type I don&amp;#39;t.&amp;quot;&lt;br/&gt;\nCould someone clarify this for me?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of downloading this model:&lt;br/&gt;\n&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; ... or &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg5scj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "9acca9",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754181257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.\n\nIf I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.\n\nSometimes, both sides are the same.  (E.g., \"Are you helpless or do you need my help?\")\n\nSometimes, they're so unbalanced it becomes a Mitch Hedberg-style question. (E.g., \"Have you ever tried sugar or PCP?\")\n\nSometimes, a very open-ended question is presented as either/or. (E.g., \"Is your favorite CSS color value #ff73c1 or #2141af?\" like those are the only two options.)\n\nI have found myself utterly unable to affect this behavior at all through the system prompt. I've tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with \"Don't...\" instructions) I've tried prompting it not to use \"either/or\" questions, \"A or B?\" questions, questions that limit the user's options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.\n\nAnd if I bring it up in chat, I get Qwen3's usual long obsequious apology (\"You're absolutely right, I'm sorry, I made assumptions and didn't respect your blah blah blah... I'll be sure to blah blah blah...\") and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird \"shell-shocked\" mode where it starts writing responses with three words per line that read like it's a frustrated beat poet.\n\nHave other people run into this? If so, are there good ways to combat it?\n\nThanks for any advice!\n",
          "author_fullname": "t2_7pimnskf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get Qwen 3 to stop asking terrible questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg4lxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754177798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.&lt;/p&gt;\n\n&lt;p&gt;If I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, both sides are the same.  (E.g., &amp;quot;Are you helpless or do you need my help?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, they&amp;#39;re so unbalanced it becomes a Mitch Hedberg-style question. (E.g., &amp;quot;Have you ever tried sugar or PCP?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, a very open-ended question is presented as either/or. (E.g., &amp;quot;Is your favorite CSS color value #ff73c1 or #2141af?&amp;quot; like those are the only two options.)&lt;/p&gt;\n\n&lt;p&gt;I have found myself utterly unable to affect this behavior at all through the system prompt. I&amp;#39;ve tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with &amp;quot;Don&amp;#39;t...&amp;quot; instructions) I&amp;#39;ve tried prompting it not to use &amp;quot;either/or&amp;quot; questions, &amp;quot;A or B?&amp;quot; questions, questions that limit the user&amp;#39;s options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.&lt;/p&gt;\n\n&lt;p&gt;And if I bring it up in chat, I get Qwen3&amp;#39;s usual long obsequious apology (&amp;quot;You&amp;#39;re absolutely right, I&amp;#39;m sorry, I made assumptions and didn&amp;#39;t respect your blah blah blah... I&amp;#39;ll be sure to blah blah blah...&amp;quot;) and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird &amp;quot;shell-shocked&amp;quot; mode where it starts writing responses with three words per line that read like it&amp;#39;s a frustrated beat poet.&lt;/p&gt;\n\n&lt;p&gt;Have other people run into this? If so, are there good ways to combat it?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg4lxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TastesLikeOwlbear",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754177798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So many versions! I saw something about how the DWQ versions are best, but then obviously MLX \\*seems\\* like it would be best? And what quantization version?",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "64GB M1 Max, which GLM-4.5-Air?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg44ya",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176467,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So many versions! I saw something about how the DWQ versions are best, but then obviously MLX *seems* like it would be best? And what quantization version?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg44ya",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754176467,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? \n\n- Apple m4 pro chip\n- 14 core CPU, 20 core GPU\n- 48GB unified memory. \n- One TB SSD storage \n\nEventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.",
          "author_fullname": "t2_1ook1izxzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this set up sufficient?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg40u1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Apple m4 pro chip&lt;/li&gt;\n&lt;li&gt;14 core CPU, 20 core GPU&lt;/li&gt;\n&lt;li&gt;48GB unified memory. &lt;/li&gt;\n&lt;li&gt;One TB SSD storage &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Eventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg40u1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wild-Muffin9190",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754176144,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article: https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\n\nContext:\n\nThis insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.\n\nhttps://arxiv.org/abs/2506.21734\n\nA number of people have reproduced it if anyone is worried about that: https://x.com/VictorTaelin/status/1950512015899840768\nhttps://github.com/sapientinc/HRM/issues/12",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM solved thinking more than current \"thinking\" models (this needs more hype)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3i48",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 262,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 262,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article: &lt;a href=\"https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It&amp;#39;s seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href=\"https://x.com/VictorTaelin/status/1950512015899840768\"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt;\n&lt;a href=\"https://github.com/sapientinc/HRM/issues/12\"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?auto=webp&amp;s=3649785728b7c459cdac6d4444cd5a58a31ff8a2",
                  "width": 994,
                  "height": 610
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a32ed96622f166ae158780da40b7af8d982e72f9",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2621c7644d6e96ab551888e43049a8b94e227fc5",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da54eeb0e463cf73d8f5292e9b099e338494438",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1caf030ca1b3e5b1df5e052afe93497a9bb543",
                    "width": 640,
                    "height": 392
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88919a9c5b50d0a4d76e2c2bb7e87bb3c4eed852",
                    "width": 960,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg3i48",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 55,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754174679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nEvery app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.\n\nHere's what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.\n\nKnow any other ?\n\nThanks",
          "author_fullname": "t2_cbs8s4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Easily installable GUI for ML-powered audio transcription on AMD GPU ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3g2e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Every app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.&lt;/p&gt;\n\n&lt;p&gt;Know any other ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg3g2e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaKi_87",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754174523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.\n\nThe take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.\n\nWe need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3d62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt;\n\n&lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt;\n\n&lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg3d62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754174302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don't see an official way and don't want to use LM Studio. What options do I have here? \n\n============================\n\nSecond, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.\n\nIs there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people running an MLX-compatible OpenAI API server locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg26g0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754171064,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don&amp;#39;t see an official way and don&amp;#39;t want to use LM Studio. What options do I have here? &lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;Second, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.&lt;/p&gt;\n\n&lt;p&gt;Is there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg26g0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754171064,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt's agent mode, but haven't found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?",
          "author_fullname": "t2_w61xk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Closest Local Version of OpenAI's Agent Mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg24nd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754170932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt&amp;#39;s agent mode, but haven&amp;#39;t found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg24nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RabbitEater2",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754170932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_prnin4bw1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GNOME AI Virtual Assistant \"Newelle\" Reaches Version 1.0 Milestone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1evr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754169081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg1evr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FastDecode1",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/",
          "stickied": false,
          "url": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "subreddit_subscribers": 509291,
          "created_utc": 1754169081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news about the open source models that OpenAI promised to release ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1e80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754169032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven’t heard anything new in the past few weeks, have we?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg1e80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754169032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).\n\nAiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?\n\n\n16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).\n\n24 GB (go for size - Radeon 7900 - can get new for 800-1000)\n\n24 GB (3090 - refurbished around 1000, new closet to 1500)\n\nPrimary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?\n\nWhat would you do, and why? Or - where can I read up to better understand my options? Threads like this (https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/) make me think offloading to CPU/RAM doesn't need to absolutely throttle throughput when running locally...",
          "author_fullname": "t2_2orcezep",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guide for GPU Purchase for Local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1cg5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754168901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).&lt;/p&gt;\n\n&lt;p&gt;Aiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?&lt;/p&gt;\n\n&lt;p&gt;16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).&lt;/p&gt;\n\n&lt;p&gt;24 GB (go for size - Radeon 7900 - can get new for 800-1000)&lt;/p&gt;\n\n&lt;p&gt;24 GB (3090 - refurbished around 1000, new closet to 1500)&lt;/p&gt;\n\n&lt;p&gt;Primary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?&lt;/p&gt;\n\n&lt;p&gt;What would you do, and why? Or - where can I read up to better understand my options? Threads like this (&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/&lt;/a&gt;) make me think offloading to CPU/RAM doesn&amp;#39;t need to absolutely throttle throughput when running locally...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg1cg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanManPanther",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754168901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn't quite process them correctly.\n    \nThis is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.\n    \nSpecifically, this fixes the following:\n    \n- Chat dates are maintained\n- Chat hierarchy is preserved\n- Empty conversations are skipped\n- Parent-child relationships are maintained\n    \nIn addition, it will skip malformed conversations and try to import each chat only once using a `imported.json` file.\n    \nYou can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to `~/chatgpt/chatgpt-export.json`.\n    \nI recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your `webui.db` file.\n    \nAfter importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.\n    \nI like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the `imported.json` file correctly.\n    \nThis works for me, and I hope it works for you too! PRs and issues are welcome.",
          "author_fullname": "t2_44be3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convert your ChatGTP exported conversations to something that Open-WebUI can import",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg12k4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7aaf741530697098b43e0167db0c1c5b5305d19b",
          "edited": 1754169308,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754168175,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn&amp;#39;t quite process them correctly.&lt;/p&gt;\n\n&lt;p&gt;This is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this fixes the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Chat dates are maintained&lt;/li&gt;\n&lt;li&gt;Chat hierarchy is preserved&lt;/li&gt;\n&lt;li&gt;Empty conversations are skipped&lt;/li&gt;\n&lt;li&gt;Parent-child relationships are maintained&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In addition, it will skip malformed conversations and try to import each chat only once using a &lt;code&gt;imported.json&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;You can export your ChatGPT conversations by going to Settings → Data controls → Export data → Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to &lt;code&gt;~/chatgpt/chatgpt-export.json&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your &lt;code&gt;webui.db&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;After importing, you can view your conversations in Open-WebUI by going to Settings → Chats → Import and selecting the converted JSON file.&lt;/p&gt;\n\n&lt;p&gt;I like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the &lt;code&gt;imported.json&lt;/code&gt; file correctly.&lt;/p&gt;\n\n&lt;p&gt;This works for me, and I hope it works for you too! PRs and issues are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?auto=webp&amp;s=e22d770e4c02c756fc73d7d90a385e5b112f15b5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed1ab2121b7badab4580a484ba08cc844d5e16e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9afd23fae3b50eabe8826fbaa8cb3d0f9a3e2c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ddddd80938d359fbb1d0dd7d1abbf05f3a86de7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17fc3d36981f925574075a8743bee7f8626049a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=998b355322b3740f1ade9410e49eaa05da91b8fc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0e7de199e0866090536334a1fd28c6c4dff2939",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg12k4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "scubanarc",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/",
          "stickied": false,
          "url": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754168175,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!",
          "author_fullname": "t2_13hqmc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have built my own, poor mans Lovable - testing out Cerebras AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0uw8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=5b85dbf1e37f569740ca84504972c21bb5463678",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754167627,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/restyler/poor-mans-lovable",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?auto=webp&amp;s=2d2f3ab9d396d0b9ffc59352c0ad43c2be2c6d47",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cf2bdf2998419d6d274f237ffbcc2b5ad34d663",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e31d2c9aed096970a6183172f8674918abd3da69",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c2a06ca5a7606d5a5f78e2a727ce72eb1898757",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=642596b8bbafb58939fce5813d2fe60c81392d6f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc7356ab40d5babbd50137dd1b88c3efc5a874a3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=496d2912020b57e4bdcbc111110a288bf2bd8366",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg0uw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "superjet1",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0uw8/i_have_built_my_own_poor_mans_lovable_testing_out/",
          "stickied": false,
          "url": "https://github.com/restyler/poor-mans-lovable",
          "subreddit_subscribers": 509291,
          "created_utc": 1754167627,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named [UniTok](https://arxiv.org/abs/2502.20321) and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.\n\nThey say their method is better but i am not sure, when i tried their code it didn't give high quality construction results but they said that they didn't train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don't know.\n\nWhat is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)",
          "author_fullname": "t2_1urjd1hc7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How the best image generation models work from the inside ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0ur7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167616,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named &lt;a href=\"https://arxiv.org/abs/2502.20321\"&gt;UniTok&lt;/a&gt; and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.&lt;/p&gt;\n\n&lt;p&gt;They say their method is better but i am not sure, when i tried their code it didn&amp;#39;t give high quality construction results but they said that they didn&amp;#39;t train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;What is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0ur7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Severe-Awareness829",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754167616,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Rank|Model|Score|95% CI|Votes|Company|License|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|gemini 2.5 pro|1474|±8|7,178|Goog||\n|1|qwen3 235b a22b instruct 2507|1464|±18|1,089|Alibaba|Apache|\n|2|o3 2025 04 16|1445|±7|9,877|Closed AI||\n|2|grok 4 2502|1442|±10|4,063|xAI||\n|2|qwen3 235b a22b thinking 2507|1442|±20|917|Alibaba|Apache|\n|2|grok 3 preview 02 24|1439|±7|7,588|xAI||\n|3|deepseek r1 0528|1436|±9|4,851|DeepSeek|MIT|\n\nStyle control removed.\nhttps://lmarena.ai/leaderboard/text/coding",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alibaba not doing to bad at coding according to lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0sbe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167443,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Rank&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Score&lt;/th&gt;\n&lt;th align=\"left\"&gt;95% CI&lt;/th&gt;\n&lt;th align=\"left\"&gt;Votes&lt;/th&gt;\n&lt;th align=\"left\"&gt;Company&lt;/th&gt;\n&lt;th align=\"left\"&gt;License&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;gemini 2.5 pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;1474&lt;/td&gt;\n&lt;td align=\"left\"&gt;±8&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,178&lt;/td&gt;\n&lt;td align=\"left\"&gt;Goog&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1464&lt;/td&gt;\n&lt;td align=\"left\"&gt;±18&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,089&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;o3 2025 04 16&lt;/td&gt;\n&lt;td align=\"left\"&gt;1445&lt;/td&gt;\n&lt;td align=\"left\"&gt;±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;9,877&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 4 2502&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;±10&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,063&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;±20&lt;/td&gt;\n&lt;td align=\"left\"&gt;917&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 3 preview 02 24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1439&lt;/td&gt;\n&lt;td align=\"left\"&gt;±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,588&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;deepseek r1 0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;1436&lt;/td&gt;\n&lt;td align=\"left\"&gt;±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,851&lt;/td&gt;\n&lt;td align=\"left\"&gt;DeepSeek&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Style control removed.\n&lt;a href=\"https://lmarena.ai/leaderboard/text/coding\"&gt;https://lmarena.ai/leaderboard/text/coding&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0sbe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754167443,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys,\n\nI am actually running GLM-4.5-Air with vllm (4x3090) and even if it's quite early I'm quite impressed the model isn't \"lost\" and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.\n\nWhat is your experience actually? Any tip is wellcome\n\nFor info, I use [AWQ](https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ) with the latest (nightly) version of vllm with following cmd:\n\n`vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx`\n\nThen [claude-code-router](https://github.com/musistudio/claude-code-router) with following config:\n\n`{`\n\n`\"LOG\": true,`\n\n`\"Providers\": [`\n\n`{`\n\n`\"name\": \"openai\",`\n\n`\"api_base_url\": \"http://localhost:8123/v1/chat/completions\",`\n\n`\"api_key\": \"xxxx\",`\n\n`\"models\": [\"cpatonn/GLM-4.5-Air-AWQ\"]`\n\n`}`\n\n`],`\n\n`\"Router\": {`\n\n`\"default\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"background\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"think\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContext\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContextThreshold\": 64000,`\n\n`\"webSearch\": \"openai,cpatonn/GLM-4.5-Air-AWQ\"`\n\n`}`\n\n`}`",
          "author_fullname": "t2_udr659irv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Experience with GLM-4.5-Air + claude code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzzt4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754166182,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754165375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am actually running GLM-4.5-Air with vllm (4x3090) and even if it&amp;#39;s quite early I&amp;#39;m quite impressed the model isn&amp;#39;t &amp;quot;lost&amp;quot; and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.&lt;/p&gt;\n\n&lt;p&gt;What is your experience actually? Any tip is wellcome&lt;/p&gt;\n\n&lt;p&gt;For info, I use &lt;a href=\"https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ\"&gt;AWQ&lt;/a&gt; with the latest (nightly) version of vllm with following cmd:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Then &lt;a href=\"https://github.com/musistudio/claude-code-router\"&gt;claude-code-router&lt;/a&gt; with following config:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;LOG&amp;quot;: true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Providers&amp;quot;: [&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;name&amp;quot;: &amp;quot;openai&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_base_url&amp;quot;: &amp;quot;http://localhost:8123/v1/chat/completions&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_key&amp;quot;: &amp;quot;xxxx&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;models&amp;quot;: [&amp;quot;cpatonn/GLM-4.5-Air-AWQ&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;],&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Router&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;default&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;background&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;think&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContext&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContextThreshold&amp;quot;: 64000,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;webSearch&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?auto=webp&amp;s=1b31f315329427daf17c78d7a27488cb4a4815b1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44ceba9879a0b6cc35b31f44bfe9d55af546e830",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2b1cffdeb754044dca444f9940bf1f77e3c1130",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acf9e48a54c3292dcada1d3027879b052535da57",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4127f2600eb35191a3b12e7e09dc0154803bb2d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b52fefc4e7d9396d19c12d4f2875b5d7cd950a56",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9fb7f9f2f97040afaa6980f1c6f3ed5ece7f4490",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfzzt4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Leflakk",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754165375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nWhen our radiology department rejected another batch of low-resolution X-rays because they couldn't see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.\n\nOn that day in the hospital basement, I decided somehow to fix this problem that's now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.\n\nThe Problem I Was Really Solving\n\nAs a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.\n\nWhen my friend completed research on X-ray super resolution using [GANs](https://www.youtube.com/watch?v=BVWn8JfeekY), I saw an opportunity to turn this research paper into a practical solution.\n\nThe existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.\n\nThe breaking point wasn't just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.\n\nThe Technical Breakthrough\n\nI built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.\n\nThe Core Technology (from the research):\n\nPatch-based processing that preserves fine-grained structural details\n\nGenerative adversarial network trained on 3,000 clinical X-ray images\n\nMoving average filters with random kernel sizes (1-40 pixels) for robustness\n\nFour-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses\n\nWhat I Built Around It:\n\nSimple drag-and-drop web interface for uploading X-ray DICOM files\n\nCloud processing pipeline using AWS GPU instances\n\nHIPAA-compliant storage and transmission\n\nIntegration with existing PACS (Picture Archiving and Communication Systems)\n\nThe first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.\n\nThe Growth Numbers (Early Traction)\n\nIn first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)\n\nCurrent Unit Economics:\n\nMRR: $1,450\n\nCloud compute costs: $520/month (AWS GPU instances)\n\nInfrastructure &amp; compliance: $180/month\n\nGross profit: $750/month (52% margin)\n\nAverage customer acquisition cost: $95 (mostly referrals and medical imaging forums).\n\nWhat I actually understood is\n\nBuilding on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.\n\nMedical software is about trust and usability. Radiologists don't want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.\n\nEarly traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.\n\nHospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.\n\nTechnical Reality\n\nUnlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.\n\nThe discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.\n\nWe validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.\n\nThe Real Lesson\n\nThis business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.\n\nThe opportunity wasn't in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.",
          "author_fullname": "t2_1tts756whd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I Built Medical AI by Solving the Radiation Dose Problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzu3d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754164960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When our radiology department rejected another batch of low-resolution X-rays because they couldn&amp;#39;t see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.&lt;/p&gt;\n\n&lt;p&gt;On that day in the hospital basement, I decided somehow to fix this problem that&amp;#39;s now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.&lt;/p&gt;\n\n&lt;p&gt;The Problem I Was Really Solving&lt;/p&gt;\n\n&lt;p&gt;As a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.&lt;/p&gt;\n\n&lt;p&gt;When my friend completed research on X-ray super resolution using &lt;a href=\"https://www.youtube.com/watch?v=BVWn8JfeekY\"&gt;GANs&lt;/a&gt;, I saw an opportunity to turn this research paper into a practical solution.&lt;/p&gt;\n\n&lt;p&gt;The existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.&lt;/p&gt;\n\n&lt;p&gt;The breaking point wasn&amp;#39;t just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The Technical Breakthrough&lt;/p&gt;\n\n&lt;p&gt;I built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.&lt;/p&gt;\n\n&lt;p&gt;The Core Technology (from the research):&lt;/p&gt;\n\n&lt;p&gt;Patch-based processing that preserves fine-grained structural details&lt;/p&gt;\n\n&lt;p&gt;Generative adversarial network trained on 3,000 clinical X-ray images&lt;/p&gt;\n\n&lt;p&gt;Moving average filters with random kernel sizes (1-40 pixels) for robustness&lt;/p&gt;\n\n&lt;p&gt;Four-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses&lt;/p&gt;\n\n&lt;p&gt;What I Built Around It:&lt;/p&gt;\n\n&lt;p&gt;Simple drag-and-drop web interface for uploading X-ray DICOM files&lt;/p&gt;\n\n&lt;p&gt;Cloud processing pipeline using AWS GPU instances&lt;/p&gt;\n\n&lt;p&gt;HIPAA-compliant storage and transmission&lt;/p&gt;\n\n&lt;p&gt;Integration with existing PACS (Picture Archiving and Communication Systems)&lt;/p&gt;\n\n&lt;p&gt;The first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.&lt;/p&gt;\n\n&lt;p&gt;The Growth Numbers (Early Traction)&lt;/p&gt;\n\n&lt;p&gt;In first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)&lt;/p&gt;\n\n&lt;p&gt;Current Unit Economics:&lt;/p&gt;\n\n&lt;p&gt;MRR: $1,450&lt;/p&gt;\n\n&lt;p&gt;Cloud compute costs: $520/month (AWS GPU instances)&lt;/p&gt;\n\n&lt;p&gt;Infrastructure &amp;amp; compliance: $180/month&lt;/p&gt;\n\n&lt;p&gt;Gross profit: $750/month (52% margin)&lt;/p&gt;\n\n&lt;p&gt;Average customer acquisition cost: $95 (mostly referrals and medical imaging forums).&lt;/p&gt;\n\n&lt;p&gt;What I actually understood is&lt;/p&gt;\n\n&lt;p&gt;Building on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.&lt;/p&gt;\n\n&lt;p&gt;Medical software is about trust and usability. Radiologists don&amp;#39;t want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.&lt;/p&gt;\n\n&lt;p&gt;Early traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.&lt;/p&gt;\n\n&lt;p&gt;Hospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.&lt;/p&gt;\n\n&lt;p&gt;Technical Reality&lt;/p&gt;\n\n&lt;p&gt;Unlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.&lt;/p&gt;\n\n&lt;p&gt;The discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.&lt;/p&gt;\n\n&lt;p&gt;We validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.&lt;/p&gt;\n\n&lt;p&gt;The Real Lesson&lt;/p&gt;\n\n&lt;p&gt;This business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The opportunity wasn&amp;#39;t in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?auto=webp&amp;s=ddcf97793b8059e1ac6f8e5f42c0fb141ac1f06b",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecfe2ffa2015cbfbedc8dfb6cbcd4cf0d32f0bee",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d64b11c760cf9d689a103b871b38eb77933c0ed",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=085b7fe67421b9ba136cbbcbf7988771acb578f2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfzu3d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Perception-9919",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754164960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just launched GridLLM (https://github.com/GridLLM/GridLLM), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. \n\nWould love any feedback on the project!\n",
          "author_fullname": "t2_g5wvmwck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-Source Project for Distributed Inference Management",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzg8h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754163962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just launched GridLLM (&lt;a href=\"https://github.com/GridLLM/GridLLM\"&gt;https://github.com/GridLLM/GridLLM&lt;/a&gt;), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. &lt;/p&gt;\n\n&lt;p&gt;Would love any feedback on the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?auto=webp&amp;s=5262c21723d990876d1321b058c80d37499608f4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a422ad10d8ba96d4fcb866b6ade60210133fea91",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92141835b6803ca57fbce4c55c4e89ef337c1d98",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc5e385866a70deb5e03659dfedb836c11a84a0c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00d881194968dc37ee1e6d0224b4065c46e497f3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f6e74f447093226b82fd0f39651b6d915ce7b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fe17371d5951c9213d509050b853dbafa2832f8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzg8h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Choice_Nature9658",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754163962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This set of experiments were conducted about half a year ago and we are suggested to share them to the community. Summary of the experiments\n\n(1) Lihua world dataset: conversation data, all texts\n\n(2) In previous studies, Graph RAG (and variants) showed advantages over \"naïve\" RAG.\n\n(3) Using OpenAI RAG API (File Search), the accuracy is substantially higher than graph RAG &amp; variants\n\n(4) Using the same embeddings, [https://chat.vecml.com](https://chat.vecml.com/) produces consistently better accuracies than OpenAI RAG API (File Search).\n\n(5) More interestingly, [https://chat.vecml.com/](https://chat.vecml.com/) is substantially (550x) faster than OpenAI RAG (File Search)\n\n(6) Additional experiments on different embeddings are also provided.\n\nNote that Lihua world dataset is purely text. In practice, the documents are in all sorts of formats: PDFs, OCR, Excel, HTML, DocX, PPTX, WPS, and more. [https://chat.vecml.com/](https://chat.vecml.com/) is able to handle documents of many different formats and is capable of dealing with multi-modal RAG.",
          "author_fullname": "t2_x7ezv9dmy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "OpenAI RAG API (File Search): an experimental study",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "snfxiwiotngf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecc1cf0a6bca5c4e3328477b7026c95b8f03eb70"
                },
                {
                  "y": 96,
                  "x": 216,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=914115ad10b3b2959058b4fd792112b6f9b05f24"
                },
                {
                  "y": 143,
                  "x": 320,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73850eb9a8677a5bea2d313bccdaa792685357ff"
                },
                {
                  "y": 286,
                  "x": 640,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=244d9f4891b0943eb383c852c0e469dbaf82ffc5"
                },
                {
                  "y": 429,
                  "x": 960,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cfa0e69bb3aa9572c84bf191e4bb5ef839f6f18"
                },
                {
                  "y": 483,
                  "x": 1080,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58d6f1634978dfaf2f93ccc8b49cacf69fc18ded"
                }
              ],
              "s": {
                "y": 598,
                "x": 1336,
                "u": "https://preview.redd.it/snfxiwiotngf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=69a448d93ef8f0a3be6f01eec30503520d18f053"
              },
              "id": "snfxiwiotngf1"
            },
            "kdrqcvintngf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 130,
                  "x": 108,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f4114f77943df8660e61b3e333ac3cd0aabebfd"
                },
                {
                  "y": 260,
                  "x": 216,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8274df2d3ccb3961cf1d9322752fd6967fd017ed"
                },
                {
                  "y": 386,
                  "x": 320,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20bec91b1fa2c9b44e0434ddd20d6eb31063312"
                },
                {
                  "y": 772,
                  "x": 640,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6177b1bb06a5fd33588da0d54116a9f1e75d1690"
                }
              ],
              "s": {
                "y": 881,
                "x": 730,
                "u": "https://preview.redd.it/kdrqcvintngf1.png?width=730&amp;format=png&amp;auto=webp&amp;s=9244cca0f81ba098aa6e1aaea3a59ac371ac8b0b"
              },
              "id": "kdrqcvintngf1"
            }
          },
          "name": "t3_1mfzezz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kdrqcvintngf1",
                "id": 719740416
              },
              {
                "media_id": "snfxiwiotngf1",
                "id": 719740417
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/fwt17fYbAFMoObf3HbNFT-k87BCWFCICFS0WC914Tx8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754163875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This set of experiments were conducted about half a year ago and we are suggested to share them to the community. Summary of the experiments&lt;/p&gt;\n\n&lt;p&gt;(1) Lihua world dataset: conversation data, all texts&lt;/p&gt;\n\n&lt;p&gt;(2) In previous studies, Graph RAG (and variants) showed advantages over &amp;quot;naïve&amp;quot; RAG.&lt;/p&gt;\n\n&lt;p&gt;(3) Using OpenAI RAG API (File Search), the accuracy is substantially higher than graph RAG &amp;amp; variants&lt;/p&gt;\n\n&lt;p&gt;(4) Using the same embeddings, &lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com&lt;/a&gt; produces consistently better accuracies than OpenAI RAG API (File Search).&lt;/p&gt;\n\n&lt;p&gt;(5) More interestingly, &lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com/&lt;/a&gt; is substantially (550x) faster than OpenAI RAG (File Search)&lt;/p&gt;\n\n&lt;p&gt;(6) Additional experiments on different embeddings are also provided.&lt;/p&gt;\n\n&lt;p&gt;Note that Lihua world dataset is purely text. In practice, the documents are in all sorts of formats: PDFs, OCR, Excel, HTML, DocX, PPTX, WPS, and more. &lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com/&lt;/a&gt; is able to handle documents of many different formats and is capable of dealing with multi-modal RAG.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mfzezz",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzezz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DueKitchen3102",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzezz/openai_rag_api_file_search_an_experimental_study/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mfzezz",
          "subreddit_subscribers": 509291,
          "created_utc": 1754163875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it’s just my system",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox tts on amd",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfz1k2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754162909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it’s just my system&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfz1k2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754162909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are local llms fast and powerful enough to do analysis on movies in real time?\n\nSay you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?\n\nIf not today, then when will it be possible to do that?",
          "author_fullname": "t2_26u5g058",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LocalLLM for movies",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy924",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are local llms fast and powerful enough to do analysis on movies in real time?&lt;/p&gt;\n\n&lt;p&gt;Say you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?&lt;/p&gt;\n\n&lt;p&gt;If not today, then when will it be possible to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfy924",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImaginaryRea1ity",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754160893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'll need to use free tools. I am looking to make a video with this content. How do I do that? What tools should I use? How do i format this information to be processed by an AI? \n\n\\[Begin\\]\n\n The Globe wants you to believe everything opposite of physics:  \n\n1)  Heliocentrism teaches large bodies of liquid water curves into a ball. Physics says water lays flat and always seeks it's level, (thanks to the physics Law - Hydrostatic Equilibrium)\n\n2) Heliocentrism teaches we have a Big Bang Creation Story where everything spontaneously evolved from nothingness to what we have today.  Physics shows us this idea violates the 1st law of thermodynamics. \n\n3) Heliocentrism tells us Gravity is mass attracting mass. Physics shows us gas which is physical matter with mass does Not obey any silly idea of gravity. Gas always expands due to entropy to fill an available volume until equalization occurs. (Thanks to the 2nd law of thermodynamics)  \n\n4) Heliocentrism also teaches Gravity is Einstein's Gravitational Accretion where gas coalesces on itself. - (That violates the 2nd law of thermodynamics.) - \n\n5) Heliocentrism teaches gas forms a sphere in a vacuum. (what you call atmosphere) Again,  Gas always expands due to entropy to fill an available volume until equalization occurs. It cannot form a sphere in a vacuum Ever! (Again thanks to the 2nd Law of Thermodynamics.)\n\nI just gave you 5 examples (or to the untrained in science and physics, Paradoxes) how the Globe Story is purposefully deceptive because it doesn't align with actual physics and science facts.\n\nYou can learn these physics laws yourself with a study of thermodynamics at Khan Academy: The Laws of Thermodynamics and The Behavior of Gas at Chem Libre Text. \n\nNow if you think I'm Wrong then Demonstrate the claims! - You see your explanation is only good if you can Back it with Actual Physics Demonstrations. Demonstrate gas forming a sphere in a vacuum that then Fails to fully expand due to entropy until equalization occurs. (what you call Atmosphere) - Demonstrate large standing bodies of water Failing to seek their own level, Failing to lay flat and Failing to lay Horizontal. - These things Cannot be done thanks to the 2nd law of thermodynamics and hydrostatic equilibrium.\n\nLiquid water covers 70 % of Earths surface. Physics properties of liquid water (Hydrostatic Equilibrium) show it always seeks it's own level, lays flat and horizontal. Nothing, that is 70% of anything that seeks it's own level, lays flat and horizontal can Ever Be a Sphere! That's an Impossible Ratio! - Your Earth Curvature is Impossible in Physics and in Math!  \n\\[End\\]\n\nHow do make that video?  I don't know anything about AI but it uses something they choose to call prompts. That doesn't help me. ",
          "author_fullname": "t2_v7hacf3s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get this information into an AI to make a video?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy6vo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160738,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll need to use free tools. I am looking to make a video with this content. How do I do that? What tools should I use? How do i format this information to be processed by an AI? &lt;/p&gt;\n\n&lt;p&gt;[Begin]&lt;/p&gt;\n\n&lt;p&gt;The Globe wants you to believe everything opposite of physics:  &lt;/p&gt;\n\n&lt;p&gt;1)  Heliocentrism teaches large bodies of liquid water curves into a ball. Physics says water lays flat and always seeks it&amp;#39;s level, (thanks to the physics Law - Hydrostatic Equilibrium)&lt;/p&gt;\n\n&lt;p&gt;2) Heliocentrism teaches we have a Big Bang Creation Story where everything spontaneously evolved from nothingness to what we have today.  Physics shows us this idea violates the 1st law of thermodynamics. &lt;/p&gt;\n\n&lt;p&gt;3) Heliocentrism tells us Gravity is mass attracting mass. Physics shows us gas which is physical matter with mass does Not obey any silly idea of gravity. Gas always expands due to entropy to fill an available volume until equalization occurs. (Thanks to the 2nd law of thermodynamics)  &lt;/p&gt;\n\n&lt;p&gt;4) Heliocentrism also teaches Gravity is Einstein&amp;#39;s Gravitational Accretion where gas coalesces on itself. - (That violates the 2nd law of thermodynamics.) - &lt;/p&gt;\n\n&lt;p&gt;5) Heliocentrism teaches gas forms a sphere in a vacuum. (what you call atmosphere) Again,  Gas always expands due to entropy to fill an available volume until equalization occurs. It cannot form a sphere in a vacuum Ever! (Again thanks to the 2nd Law of Thermodynamics.)&lt;/p&gt;\n\n&lt;p&gt;I just gave you 5 examples (or to the untrained in science and physics, Paradoxes) how the Globe Story is purposefully deceptive because it doesn&amp;#39;t align with actual physics and science facts.&lt;/p&gt;\n\n&lt;p&gt;You can learn these physics laws yourself with a study of thermodynamics at Khan Academy: The Laws of Thermodynamics and The Behavior of Gas at Chem Libre Text. &lt;/p&gt;\n\n&lt;p&gt;Now if you think I&amp;#39;m Wrong then Demonstrate the claims! - You see your explanation is only good if you can Back it with Actual Physics Demonstrations. Demonstrate gas forming a sphere in a vacuum that then Fails to fully expand due to entropy until equalization occurs. (what you call Atmosphere) - Demonstrate large standing bodies of water Failing to seek their own level, Failing to lay flat and Failing to lay Horizontal. - These things Cannot be done thanks to the 2nd law of thermodynamics and hydrostatic equilibrium.&lt;/p&gt;\n\n&lt;p&gt;Liquid water covers 70 % of Earths surface. Physics properties of liquid water (Hydrostatic Equilibrium) show it always seeks it&amp;#39;s own level, lays flat and horizontal. Nothing, that is 70% of anything that seeks it&amp;#39;s own level, lays flat and horizontal can Ever Be a Sphere! That&amp;#39;s an Impossible Ratio! - Your Earth Curvature is Impossible in Physics and in Math!&lt;br/&gt;\n[End]&lt;/p&gt;\n\n&lt;p&gt;How do make that video?  I don&amp;#39;t know anything about AI but it uses something they choose to call prompts. That doesn&amp;#39;t help me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy6vo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DivergentDroid1",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy6vo/how_do_i_get_this_information_into_an_ai_to_make/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy6vo/how_do_i_get_this_information_into_an_ai_to_make/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754160738,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm developing a product that uses AI, but it's entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? ",
          "author_fullname": "t2_1u3u0ca58k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dutch LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy5qs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m developing a product that uses AI, but it&amp;#39;s entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy5qs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Bath-7378",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754160656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or is it unlimited on [chat.qwen.ai](http://chat.qwen.ai) ? ",
          "author_fullname": "t2_14c4jctjk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any limits on Deep Research mode on Qwen Chat?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxjd5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754159065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or is it unlimited on &lt;a href=\"http://chat.qwen.ai\"&gt;chat.qwen.ai&lt;/a&gt; ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfxjd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sostrene_Blue",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxjd5/is_there_any_limits_on_deep_research_mode_on_qwen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxjd5/is_there_any_limits_on_deep_research_mode_on_qwen/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754159065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just bought a new PC, it’s not primarily for AI but I wanna try out llms. I’m not too familiar about the different models, so I’d appreciate if someone could provide recommendations.\n\nPc specs: 5070 Ti 16gb + i7 14700 32 gb ddr5 6000 MHz.",
          "author_fullname": "t2_17ltlw5q9t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any Open source LLM’s better than free tier of ChatGPT(4o and 4o mini)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxdlg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a new PC, it’s not primarily for AI but I wanna try out llms. I’m not too familiar about the different models, so I’d appreciate if someone could provide recommendations.&lt;/p&gt;\n\n&lt;p&gt;Pc specs: 5070 Ti 16gb + i7 14700 32 gb ddr5 6000 MHz.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfxdlg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Championship7986",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxdlg/are_there_any_open_source_llms_better_than_free/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxdlg/are_there_any_open_source_llms_better_than_free/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754158656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just shipped something I'm really excited about! 🚀\nI was scrolling through my feed and saw Sebastian Raschka, PhD 's incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo.\nThat got me thinking... what if I could bring this to pure C? 🤔\nInspired by Andrej Karpathy's legendary llama2.c approach (seriously, if you haven't seen it, check it out), I decided to take on the challenge of implementing Qwen3's 30B parameter model with 128 experts in a single C file.\nThe result? Qwen_MOE_C - a complete inference engine that:\n✅ Handles sparse MoE computation (only 8 out of 128 experts active)\n✅ Supports Grouped Query Attention with proper head ratios\n✅ Uses memory mapping for efficiency (~30GB models)\n✅ Zero external dependencies (just libc + libm)\nThe beauty of this approach is the same as llama2.c - you can understand every line, it's hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency.\nHuge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems.\nSometimes the best way to truly understand something is to build it from scratch. 🛠️\nLink to the project: \nhttps://github.com/h9-tec/Qwen_MOE_C",
          "author_fullname": "t2_59165ick",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen moe in C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxas1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 56,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 56,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158458,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just shipped something I&amp;#39;m really excited about! 🚀\nI was scrolling through my feed and saw Sebastian Raschka, PhD &amp;#39;s incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo.\nThat got me thinking... what if I could bring this to pure C? 🤔\nInspired by Andrej Karpathy&amp;#39;s legendary llama2.c approach (seriously, if you haven&amp;#39;t seen it, check it out), I decided to take on the challenge of implementing Qwen3&amp;#39;s 30B parameter model with 128 experts in a single C file.\nThe result? Qwen_MOE_C - a complete inference engine that:\n✅ Handles sparse MoE computation (only 8 out of 128 experts active)\n✅ Supports Grouped Query Attention with proper head ratios\n✅ Uses memory mapping for efficiency (~30GB models)\n✅ Zero external dependencies (just libc + libm)\nThe beauty of this approach is the same as llama2.c - you can understand every line, it&amp;#39;s hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency.\nHuge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems.\nSometimes the best way to truly understand something is to build it from scratch. 🛠️\nLink to the project: \n&lt;a href=\"https://github.com/h9-tec/Qwen_MOE_C\"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?auto=webp&amp;s=e5cdc2248f745a90988be621db7d6b6be3916db2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbd8f00d966699e72ff3a93f578256c7537d2135",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3afe3fea53292db66353b624f6471827586e1c10",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023f7331df4213637fe882078ebd6fb043d92076",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99f557d7c6c92fc79bbc53fbd9519fb90c4895ac",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55d41da3d53c6240a1417760903816f861d7b857",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=869ae3899c29fb68da394dceac5b0968c64c5e51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfxas1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1Hesham",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754158458,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT's throttled free-tier, and Claude 4 Sonnet",
          "author_fullname": "t2_65zz9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Four Models, One Prompt: Who Writes the Best Instructions for AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwec7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754156214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "selfenrichment.hashnode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT&amp;#39;s throttled free-tier, and Claude 4 Sonnet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfwec7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertotomas",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwec7/four_models_one_prompt_who_writes_the_best/",
          "stickied": false,
          "url": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "subreddit_subscribers": 509291,
          "created_utc": 1754156214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.\n\nI already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an [Awesome list](https://github.com/sindresorhus/awesome).\n\nTo have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: [https://aibenchmarks.net/](https://aibenchmarks.net/)\n\nAwesome AI Benchmarks GitHub repository available here: [https://github.com/panilya/awesome-ai-benchmarks](https://github.com/panilya/awesome-ai-benchmarks)\n\nWould be happy to hear any feedback on this and whether it can be useful for you :)",
          "author_fullname": "t2_7arjztn1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100+ AI Benchmarks list",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwckf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754156092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt;\n\n&lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href=\"https://github.com/sindresorhus/awesome\"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href=\"https://aibenchmarks.net/\"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href=\"https://github.com/panilya/awesome-ai-benchmarks\"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?auto=webp&amp;s=41e93dde0345b37cb0200da115b435ce77df3930",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93761f053553e9b1cfb16372cdf0e7dac51f1f5e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b480826b22f435da297cc6f87648622fc3892fe6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08c9c7135e744081cfec8c73e2a489375eb611be",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63909a628f633b135db8a2f2ad24f375a444a9b7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcf264d960a58dd8659a36b59ab18b32c8762353",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6522fe068f5cf8114a6d791bc9bbc352abd7a38",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfwckf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panilyaU",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754156092,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14wr7u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "WebGPU enables local LLM in the browser. Demo site with AI chat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvzai",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754155160,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "andreinwald.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://andreinwald.github.io/browser-llm/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfvzai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "andreinwald",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvzai/webgpu_enables_local_llm_in_the_browser_demo_site/",
          "stickied": false,
          "url": "https://andreinwald.github.io/browser-llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754155160,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[A new PR](https://github.com/ggml-org/llama.cpp/pull/15026) was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated [\\#14939](https://github.com/ggml-org/llama.cpp/pull/14939) seemed to get stuck. The new PR description reads: \"**this PR will NOT attempt to implement MTP**\", with great progress being made in short time. (Amazing!!!)\n\nGiven that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.\n\nDisclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!\n\nPS: does MTP already work on/with MLX?",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvxdo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754155024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15026\"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5&amp;#39;s models in llama.cpp, as the original, highly anticipated &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt;\n\n&lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it&amp;#39;s not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I am eternally grateful for everybody&amp;#39;s contribution to the field, as LLMs allow me to code what I couldn&amp;#39;t code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt;\n\n&lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?auto=webp&amp;s=acc8e28264bfccdff7bd8f443fed065d174ea9e0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=192b46649336ccde82b3df88264f14e8c3af5057",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d87e157d503386c5e4e8e7524115ddb67fb4d525",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2253f5fea3f3de0ade685e30f24f2fe47d41bed",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=076b473df32cd3180067a97b9fe5da2c8755fd75",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc3f7d46febf1a47049086579c27beff59d8b7b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfebb15c0a042499f527df12e1302a2dc2a17608",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvxdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754155024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi All,\n\nI'm quite new to local AI models, and started today by playing with Chatterbox TTS on my Mac Studio M4 (using the apple silicon version on Hugging Face). Also, hopefully this is the right reddit - I see other posts regarding Chatterbox here, so I guess it is!\n\nIt's actually working very nicely indeed, doing a conversion of a small piece of a book with a voice sample I provided.\n\nIt's taking a while though; \\~25 minutes to generate a 10 minute sample. The full book is likely to be 15-20 hours long, so we could be talking 50 hours for the full conversion.\n\nSo - I would like to see if there are services I might run the model on in the cloud - for example [RunPod.io](http://RunPod.io) or [Vast.ai](http://Vast.ai) are two that I have seen. But I'm not sure what the costs might end up being, and not really sure how to find out.\n\nCan anyone offer any guidance? Is it as simple as saying 50 hours x (hourly price for GPU)?\n\nThanks!",
          "author_fullname": "t2_15wm1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox TTS in cloud?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvk5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754154092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to local AI models, and started today by playing with Chatterbox TTS on my Mac Studio M4 (using the apple silicon version on Hugging Face). Also, hopefully this is the right reddit - I see other posts regarding Chatterbox here, so I guess it is!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s actually working very nicely indeed, doing a conversion of a small piece of a book with a voice sample I provided.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s taking a while though; ~25 minutes to generate a 10 minute sample. The full book is likely to be 15-20 hours long, so we could be talking 50 hours for the full conversion.&lt;/p&gt;\n\n&lt;p&gt;So - I would like to see if there are services I might run the model on in the cloud - for example &lt;a href=\"http://RunPod.io\"&gt;RunPod.io&lt;/a&gt; or &lt;a href=\"http://Vast.ai\"&gt;Vast.ai&lt;/a&gt; are two that I have seen. But I&amp;#39;m not sure what the costs might end up being, and not really sure how to find out.&lt;/p&gt;\n\n&lt;p&gt;Can anyone offer any guidance? Is it as simple as saying 50 hours x (hourly price for GPU)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                  "width": 2400,
                  "height": 1260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvk5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BrotherBrutha",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvk5h/chatterbox_tts_in_cloud/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvk5h/chatterbox_tts_in_cloud/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754154092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "To be frank, AGI doesn't particularly interest or thrill me. Given current technological frameworks, I believe AGI won't arrive anytime soon without some breakthrough discovery. The models we have today would have seemed absolutely magical just five years ago.\n\nCan anyone give me an excitement you have with AGI ?",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AGI Could Be Our Era's Perpetual Motion Machine - Forever Out of Reach, Though Current AI Already Amazes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfve4v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754153678,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To be frank, AGI doesn&amp;#39;t particularly interest or thrill me. Given current technological frameworks, I believe AGI won&amp;#39;t arrive anytime soon without some breakthrough discovery. The models we have today would have seemed absolutely magical just five years ago.&lt;/p&gt;\n\n&lt;p&gt;Can anyone give me an excitement you have with AGI ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfve4v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfve4v/agi_could_be_our_eras_perpetual_motion_machine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfve4v/agi_could_be_our_eras_perpetual_motion_machine/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754153678,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m using Qwen 3 14B right now but haven’t checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? ",
          "author_fullname": "t2_4r17bvqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM that fits with 12GB VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfv3b0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m using Qwen 3 14B right now but haven’t checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfv3b0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tthane50",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754152915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,  \nI'm a final-year CS student working on a project to build an **AI assistant for my university** using **RAG (Retrieval-Augmented Generation)** and possibly agentic tools down the line.\n\nThe chatbot will help students find answers to common university-related questions (like academic queries, admissions, etc.) and eventually perform light actions like form redirection, etc.\n\n# What I’m struggling with:\n\nI'm not exactly sure **what types of data** I should collect and prepare to make this assistant useful, accurate, and robust.\n\nI plan to use LangChain or LlamaIndex + a vector store, but I want to hear from folks with experience in this kind of thing:\n\n* **What kinds of data did you use for similar projects?**\n* How do you decide what to include or ignore?\n* Any tips for formatting / chunking / organizing it early on?\n\nAny help, advice, or even just a pointer in the right direction would be awesome.",
          "author_fullname": "t2_1ckf2evf4b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help: Building a University Assistant RAGbot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuz5w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754154614,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI&amp;#39;m a final-year CS student working on a project to build an &lt;strong&gt;AI assistant for my university&lt;/strong&gt; using &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt; and possibly agentic tools down the line.&lt;/p&gt;\n\n&lt;p&gt;The chatbot will help students find answers to common university-related questions (like academic queries, admissions, etc.) and eventually perform light actions like form redirection, etc.&lt;/p&gt;\n\n&lt;h1&gt;What I’m struggling with:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly sure &lt;strong&gt;what types of data&lt;/strong&gt; I should collect and prepare to make this assistant useful, accurate, and robust.&lt;/p&gt;\n\n&lt;p&gt;I plan to use LangChain or LlamaIndex + a vector store, but I want to hear from folks with experience in this kind of thing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;What kinds of data did you use for similar projects?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;How do you decide what to include or ignore?&lt;/li&gt;\n&lt;li&gt;Any tips for formatting / chunking / organizing it early on?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any help, advice, or even just a pointer in the right direction would be awesome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfuz5w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jarrarhaidery",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuz5w/need_help_building_a_university_assistant_ragbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuz5w/need_help_building_a_university_assistant_ragbot/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754152634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing an OpenAI proxy solution for Claude-Code\n\n[https://github.com/ziozzang/claude2openai-proxy](https://github.com/ziozzang/claude2openai-proxy)\n\nAdvantages:\n\n1. In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.\n2. You can start using Claude Code with any specified model, whether it’s internal or external.\n3. You can also debug if needed.\n4. Anyway, at least from my tests, it works very well. The only issue is the model itself.\n\n\\----\n\nWhile using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn’t find a solid solution that worked well for multiple users. So, based on [https://github.com/1rgs/claude-code-proxy ](https://github.com/1rgs/claude-code-proxy), I built a proxy tailored for my use. Since it converts between different protocols, “gateway” might actually be a more fitting term.\n\nAnyway, here are the features:\n\nFull support for Claude Code.\n\n* On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.\n* Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.\n* Authentication is done via the ANTHROPIC\\_API\\_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.\n\nBelow is an example of setting up the server and actually using it from a client:\n\n    ANTHROPIC_BASE_URL=http://localhost:8082 \\\n    ANTHROPIC_API_KEY=sk-openapi-auth-token \\\n    ANTHROPIC_MODEL=\"openrouter/horizon-beta\" \\\n    ANTHROPIC_SMALL_FAST_MODEL=\"openrouter/horizon-beta\" \\\n    claude  \n\nTo be honest, I made this to test the openrouter/horizon-beta model. :)\n\nThe pipeline works great: Claude Code -(Claude API)-&gt; my modified proxy server -(OpenAI API)-&gt; openrouter/horizon-beta.\n\nBy the way, you can find what I built at [https://github.com/ziozzang/claude2openai-proxy ](https://github.com/ziozzang/claude2openai-proxy). I use it by building it into a container.\n\nTo be honest,\n\n* horizon-alpha doesn’t seem to handle Claude Code’s prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)\n* horizon-beta handles them quite well.\n* However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don’t feel ideal for automated workflows.\n\nPS.\n\nThe whole reason this started was because of Claude Code’s usage limits. LoL...",
          "author_fullname": "t2_5409gkc6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gateway/Proxy for Claude-Code to OpenAI API compatible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuu40",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754153859,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing an OpenAI proxy solution for Claude-Code&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Advantages:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.&lt;/li&gt;\n&lt;li&gt;You can start using Claude Code with any specified model, whether it’s internal or external.&lt;/li&gt;\n&lt;li&gt;You can also debug if needed.&lt;/li&gt;\n&lt;li&gt;Anyway, at least from my tests, it works very well. The only issue is the model itself.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;While using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn’t find a solid solution that worked well for multiple users. So, based on &lt;a href=\"https://github.com/1rgs/claude-code-proxy\"&gt;https://github.com/1rgs/claude-code-proxy &lt;/a&gt;, I built a proxy tailored for my use. Since it converts between different protocols, “gateway” might actually be a more fitting term.&lt;/p&gt;\n\n&lt;p&gt;Anyway, here are the features:&lt;/p&gt;\n\n&lt;p&gt;Full support for Claude Code.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.&lt;/li&gt;\n&lt;li&gt;Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.&lt;/li&gt;\n&lt;li&gt;Authentication is done via the ANTHROPIC_API_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Below is an example of setting up the server and actually using it from a client:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ANTHROPIC_BASE_URL=http://localhost:8082 \\\nANTHROPIC_API_KEY=sk-openapi-auth-token \\\nANTHROPIC_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nANTHROPIC_SMALL_FAST_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nclaude  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To be honest, I made this to test the openrouter/horizon-beta model. :)&lt;/p&gt;\n\n&lt;p&gt;The pipeline works great: Claude Code -(Claude API)-&amp;gt; my modified proxy server -(OpenAI API)-&amp;gt; openrouter/horizon-beta.&lt;/p&gt;\n\n&lt;p&gt;By the way, you can find what I built at &lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy &lt;/a&gt;. I use it by building it into a container.&lt;/p&gt;\n\n&lt;p&gt;To be honest,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;horizon-alpha doesn’t seem to handle Claude Code’s prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)&lt;/li&gt;\n&lt;li&gt;horizon-beta handles them quite well.&lt;/li&gt;\n&lt;li&gt;However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don’t feel ideal for automated workflows.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PS.&lt;/p&gt;\n\n&lt;p&gt;The whole reason this started was because of Claude Code’s usage limits. LoL...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?auto=webp&amp;s=42f669382e1c8928b5e1d529da5526d93801fc0d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e43bc658beb09686e12af7368ba5a1c7c7d40071",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41d704239a298e8b8e9ef761cbf0504c8c77ccaf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fed85d51213a23fb5ee442d3c4a5ad79bfcf800",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d1364de265c02e79206344fc84f7451660adc50",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5975df8d3cb9a0c0f9fe7ad4afa6ddfa17a56a05",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cbd06ed3b8ba41d88914fbda7d1b7707aee647f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfuu40",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ziozzang0",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754152276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is just a little remark that if you haven't you definitely should try qwen code [https://github.com/QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)   \nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. \n\nThe metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.\n\nSo this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. ",
          "author_fullname": "t2_n9dnke1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code + Qwen Coder 30b 3A is insane",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuiri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 209,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 209,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just a little remark that if you haven&amp;#39;t you definitely should try qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br/&gt;\nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I&amp;#39;m working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt;\n\n&lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I&amp;#39;m very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt;\n\n&lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don&amp;#39;t know jack shit about coding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?auto=webp&amp;s=689c8ef1840eeb6bb6bb53a5908a342a3dac7558",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2382140736e501c0e5f725eb9004d88daaf4ddc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fd612828e87be93104f75281f9696b4b69068",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef0b7f26ce8aa8b563291337614fb5656c8ab07",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=faaeb8e0dc19d67afd0d3197e7b7a5a0095a6f09",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eebbb21f63208ab9ba4a6a50b9890a328d60d830",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edc81e58030926a09a63f9377700b5cd6432c27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfuiri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flashy_Management962",
          "discussion_type": null,
          "num_comments": 101,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754151475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently doing the mcp course from huggingface, and I am planning to roll my own local agentic AI. Any idea what the BEST model I should use for RTX 4090? I know best is objective, so I am looking for two models, one for general purpose, and the other for coding. I will be building simple tools for personal use. For example, making a custom resume generator given job description etc. ",
          "author_fullname": "t2_1us1jsxdkh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model to use as agentic AI for RTX 4090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfubwt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151003,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently doing the mcp course from huggingface, and I am planning to roll my own local agentic AI. Any idea what the BEST model I should use for RTX 4090? I know best is objective, so I am looking for two models, one for general purpose, and the other for coding. I will be building simple tools for personal use. For example, making a custom resume generator given job description etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfubwt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flat_Chard_3763",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfubwt/best_model_to_use_as_agentic_ai_for_rtx_4090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfubwt/best_model_to_use_as_agentic_ai_for_rtx_4090/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754151003,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, I'm quite new to this topic. Do you where I can find info for starter who doenst have tech background? And what kind of companies are the best out there? ",
          "author_fullname": "t2_1up2vv5ptn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Learn GPU AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfu5ll",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754150565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;m quite new to this topic. Do you where I can find info for starter who doenst have tech background? And what kind of companies are the best out there? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfu5ll",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AutomaticAbility2008",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfu5ll/learn_gpu_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfu5ll/learn_gpu_ai/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754150565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Basically title, ideally something that can process both text, images, and documents/sheets of data, as smart as possible, and as lean as possible. \n\nMy initial research led me to Phi-4, Gemma 3, and Mistral Small 3.1, but considering how fast this space progresses, I think they have probably been outdated a few gens ago. So what wouldyou suggest for a complete newb to help set-up for free for farmers? Ideally something that is good enough that even if things progress substantially it would be enough to cover basic needs I have described, and depending on the local set-up, could operate without internet and either in low-complexity low-power device, or a higher-end \"gaming\" pc?",
          "author_fullname": "t2_5ci2w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm researching some OS &amp; Local LLMs that can be useful for farmers, either in high-end PCs and in raspberry pi. Suggestions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfu41i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754150462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically title, ideally something that can process both text, images, and documents/sheets of data, as smart as possible, and as lean as possible. &lt;/p&gt;\n\n&lt;p&gt;My initial research led me to Phi-4, Gemma 3, and Mistral Small 3.1, but considering how fast this space progresses, I think they have probably been outdated a few gens ago. So what wouldyou suggest for a complete newb to help set-up for free for farmers? Ideally something that is good enough that even if things progress substantially it would be enough to cover basic needs I have described, and depending on the local set-up, could operate without internet and either in low-complexity low-power device, or a higher-end &amp;quot;gaming&amp;quot; pc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfu41i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hjras",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfu41i/im_researching_some_os_local_llms_that_can_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfu41i/im_researching_some_os_local_llms_that_can_be/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754150462,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "At least for models built on the Qwen 3 architecture, I noticed that the speed difference between the MoE models and roughly equivalent dense models is minimal, particularly as context sizes get larger.\n\nFor instance, on my M4 Max MacBook Pro, with llama.cpp, unsloth Q4\\_K\\_XL quants, flash attention, and q8\\_0 KV cache quantization, here are the performance results I got:\n\n|Model|Context Size (tokens, approx)|Prompt Processing (tok/s)|Token Generation (tok/s)|\n|:-|:-|:-|:-|\n|Qwen 3 8B|500|730|70|\n|Qwen 3 8B|53000|103|22|\n|Qwen 3 30B-A3B|500|849|88|\n|Qwen 3 30B-A3B|53000|73|22|\n|Qwen 3 14B|500|402|43|\n|Qwen 3 14B|53000|66|12|\n\nNote: the prompt processing and token generation speeds are for processing additional inputs or generating additional output tokens, after the indicated number of tokens have already been processed in context\n\nIn terms of intelligence and knowledge, the original 30B-A3B model was somewhere in between the 8B and 14B in my experiments. At large context sizes, the 30B-A3B has prompt processing size in between 8B and 14B, and token generation speeds roughly the same as the 8B.\n\nI've read that MoEs are more efficient (cheaper) to train, but for end users, under the Qwen 3 architecture at least, the inference speed benefit of MoE seems limited, and the large memory footprint is problematic for those who don't have huge amounts of RAM.\n\nI'm curious how the IBM Granite 4 architecture will fare, particularly with large contexts, given its context memory efficient Mamba-Transformer hybrid design.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MoE models not as fast as active parameter counts suggest",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfh4ee",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754107542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At least for models built on the Qwen 3 architecture, I noticed that the speed difference between the MoE models and roughly equivalent dense models is minimal, particularly as context sizes get larger.&lt;/p&gt;\n\n&lt;p&gt;For instance, on my M4 Max MacBook Pro, with llama.cpp, unsloth Q4_K_XL quants, flash attention, and q8_0 KV cache quantization, here are the performance results I got:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Context Size (tokens, approx)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prompt Processing (tok/s)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Token Generation (tok/s)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;730&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;53000&lt;/td&gt;\n&lt;td align=\"left\"&gt;103&lt;/td&gt;\n&lt;td align=\"left\"&gt;22&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;849&lt;/td&gt;\n&lt;td align=\"left\"&gt;88&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;53000&lt;/td&gt;\n&lt;td align=\"left\"&gt;73&lt;/td&gt;\n&lt;td align=\"left\"&gt;22&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;402&lt;/td&gt;\n&lt;td align=\"left\"&gt;43&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;53000&lt;/td&gt;\n&lt;td align=\"left\"&gt;66&lt;/td&gt;\n&lt;td align=\"left\"&gt;12&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Note: the prompt processing and token generation speeds are for processing additional inputs or generating additional output tokens, after the indicated number of tokens have already been processed in context&lt;/p&gt;\n\n&lt;p&gt;In terms of intelligence and knowledge, the original 30B-A3B model was somewhere in between the 8B and 14B in my experiments. At large context sizes, the 30B-A3B has prompt processing size in between 8B and 14B, and token generation speeds roughly the same as the 8B.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read that MoEs are more efficient (cheaper) to train, but for end users, under the Qwen 3 architecture at least, the inference speed benefit of MoE seems limited, and the large memory footprint is problematic for those who don&amp;#39;t have huge amounts of RAM.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how the IBM Granite 4 architecture will fare, particularly with large contexts, given its context memory efficient Mamba-Transformer hybrid design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfh4ee",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfh4ee/moe_models_not_as_fast_as_active_parameter_counts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfh4ee/moe_models_not_as_fast_as_active_parameter_counts/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754107542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI’m working on a side project to build a virtual assistant that can do two main things:\n\n1. Answer questions based on a company’s internal docs (using RAG).\n2. Perform actions like “create an account,” “schedule a meeting,” or “find the nearest location.”\n\nI’d love some advice from folks who’ve built similar systems or explored this space. A few questions:\n\n- How would you store and access the internal data (both docs and structured info)?\n\n- What RAG setup works well in practice (vector store, retrieval strategy, etc)?\n\n- Would you use a separate intent classifier to route between info-lookup vs action execution?\n\n- For tasks, do agent frameworks like LangGraph or AutoGen make sense?\n\n- Have frameworks like ReAct/MRKL been useful in real-world projects?\n\n- When is fine-tuning or LoRA worth the effort vs just RAG + good prompting?\n\n- Any tips or lessons learned on overall architecture or scaling?\n\nNot looking for someone to design it for me, just hoping to hear what’s worked (or not) in your experience. Cheers!",
          "author_fullname": "t2_m4wc3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scalable LLM Virtual Assistant – Looking for Architecture Tips",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mft55c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754148045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I’m working on a side project to build a virtual assistant that can do two main things:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Answer questions based on a company’s internal docs (using RAG).&lt;/li&gt;\n&lt;li&gt;Perform actions like “create an account,” “schedule a meeting,” or “find the nearest location.”&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’d love some advice from folks who’ve built similar systems or explored this space. A few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How would you store and access the internal data (both docs and structured info)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What RAG setup works well in practice (vector store, retrieval strategy, etc)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Would you use a separate intent classifier to route between info-lookup vs action execution?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For tasks, do agent frameworks like LangGraph or AutoGen make sense?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have frameworks like ReAct/MRKL been useful in real-world projects?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;When is fine-tuning or LoRA worth the effort vs just RAG + good prompting?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any tips or lessons learned on overall architecture or scaling?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Not looking for someone to design it for me, just hoping to hear what’s worked (or not) in your experience. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mft55c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeadFinger",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mft55c/scalable_llm_virtual_assistant_looking_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mft55c/scalable_llm_virtual_assistant_looking_for/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754148045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using `llama.cpp` with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.\n\n**KV Quantization**\n\n* **KV cache quantization matters a lot**. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. ~~Use q5\\_1 for a good balance of memory usage and performance~~. It works well in PPL tests and in practice. **UPDATE:** K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:\n\n|CTK - CTD|PPL|STD|VRAM|\n|:-|:-|:-|:-|\n|q8\\_0 - q8\\_0|6.9016|0.04818|10.1GB|\n|q8\\_0 - q4\\_0|6.9104|0.04822|9.6GB|\n|q4\\_0 - q8\\_0|7.1241|0.04963|9.6GB|\n|q5\\_1 - q5\\_1|6.9664|0.04872|9.5GB|\n\n* **TLDR:** looks like q8\\_0 q4\\_0 is a very nice tradeoff in terms of accuracy and vram usage\n\n**Offloading Strategy**\n\n* You're bottlenecked by your **system RAM bandwidth** when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.\n* Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n* If you dont understand what the regex does, just feed it to and llm and it'll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.\n\n**Memory Tuning for CPU Offloading**\n\n* System memory speed has a major impact on throughput when using partial offloading.\n* Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.\n* On **AM4** platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.\n* On **AM5**, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.\n* Poor memory tuning will bottleneck your CPU offloading even with a fast processor.\n\n**ubatch (Prompt Batch Size)**\n\n* Higher `ubatch` values significantly improve prompt processing (PP) performance.\n* Try values like `768` or `1024`. You’ll use more VRAM, but it’s often worth it for the speedup.\n* If you’re VRAM-limited, lower this until it fits.\n\n**Extra Performance Boost**\n\n* Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA\\_SET\\_ROWS=1 ./llama-server -md /path/to/model etc.\n\n**Speculative Decoding Tips (SD)**\n\nSpeculative decoding is supported in `llama.cpp`, but there are a couple important caveats:\n\n1. **KV cache quant affects acceptance rate heavily.** Using `q4_0` for the draft model’s KV cache *halves* the acceptance rate in my testing. Use ~~q5\\_1 or even~~ `q8_0`for the draft model KV cache for much better performance. **UPDATE:** \\-ctkd q8\\_0 -ctvd q4\\_0 works like a charm and saves vram. K is much more sensitive to quantization.\n2. **Draft model context handling is broken after filling the draft KV cache.** Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.\n3. **Draft parameters matter a lot**. In my testing, using `--draft-p-min 0.85 --draft-min 2 --draft-max 12` gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.\n\nFor SD, try using **Qwen 3 0.6B** as the draft model. It’s fast and works well, as long as you avoid the issues above.\n\nIf you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp",
          "author_fullname": "t2_66tlmx2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9qn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 119,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 119,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754223074,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you&amp;#39;re offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. &lt;del&gt;Use q5_1 for a good balance of memory usage and performance&lt;/del&gt;. It works well in PPL tests and in practice. &lt;strong&gt;UPDATE:&lt;/strong&gt; K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CTK - CTD&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;th align=\"left\"&gt;STD&lt;/th&gt;\n&lt;th align=\"left\"&gt;VRAM&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9016&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04818&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.1GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9104&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04822&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q4_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.1241&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04963&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q5_1 - q5_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9664&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04872&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; looks like q8_0 q4_0 is a very nice tradeoff in terms of accuracy and vram usage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You&amp;#39;re bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt;\n&lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/li&gt;\n&lt;li&gt;If you dont understand what the regex does, just feed it to and llm and it&amp;#39;ll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt;\n&lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you&amp;#39;re comfortable doing so.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt;\n&lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt;\n&lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You’ll use more VRAM, but it’s often worth it for the speedup.&lt;/li&gt;\n&lt;li&gt;If you’re VRAM-limited, lower this until it fits.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model’s KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;del&gt;q5_1 or even&lt;/del&gt; &lt;code&gt;q8_0&lt;/code&gt;for the draft model KV cache for much better performance. &lt;strong&gt;UPDATE:&lt;/strong&gt; -ctkd q8_0 -ctvd q4_0 works like a charm and saves vram. K is much more sensitive to quantization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It’s fast and works well, as long as you avoid the issues above.&lt;/p&gt;\n\n&lt;p&gt;If you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfs9qn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AliNT77",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754145881,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like to start self hosting models for my own usage. I have right now MacBook Pro m4 Pro 24Gb ram and it feels slow with larger models and very limited. Do you think it would be better to build some custom spec pc for this purpose running on Linux just to run LLMs? Or buy maxed out Mac Studio or Mac mini for this purpose\n\nMain usage would be coding and image generation if that would be possible. \n\nPs. I have sitting somewhere i7 12700K with 32Gb ram but without gpu ",
          "author_fullname": "t2_1pnbev48g0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting started into self hosting LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9cw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to start self hosting models for my own usage. I have right now MacBook Pro m4 Pro 24Gb ram and it feels slow with larger models and very limited. Do you think it would be better to build some custom spec pc for this purpose running on Linux just to run LLMs? Or buy maxed out Mac Studio or Mac mini for this purpose&lt;/p&gt;\n\n&lt;p&gt;Main usage would be coding and image generation if that would be possible. &lt;/p&gt;\n\n&lt;p&gt;Ps. I have sitting somewhere i7 12700K with 32Gb ram but without gpu &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfs9cw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ywful",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754145855,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our \"own\" R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.\n\nWhat are your thoughts?\n\nUpdate: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)",
          "author_fullname": "t2_qnf2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It's time to run your own R1, Kimi ... and split the cost of it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrunn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754146028,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I&amp;#39;m thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n\n&lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfrunn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HammerSpb",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754144811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry if this is a basic question, but I seem to be really struggling :/\n\nConsider a typical, text-in text-out use case. If I'm using an offline model API via e.g. REST, how can I incorporate tool  use? Is \"tool use\" some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?\n\nIs there some basic, no-nonsense code or tutorial to get a concrete idea?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is \"tool use\", exactly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrq3v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a basic question, but I seem to be really struggling :/&lt;/p&gt;\n\n&lt;p&gt;Consider a typical, text-in text-out use case. If I&amp;#39;m using an offline model API via e.g. REST, how can I incorporate tool  use? Is &amp;quot;tool use&amp;quot; some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?&lt;/p&gt;\n\n&lt;p&gt;Is there some basic, no-nonsense code or tutorial to get a concrete idea?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfrq3v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754144468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.\n\nI gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it\n\n[https://github.com/mmemoo/dis-cal](https://github.com/mmemoo/dis-cal)  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. \n\nThanks in advance!",
          "author_fullname": "t2_cgzigjpd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a opensource CAL-AI alternative using ollama which runs completely locally and for is fully free.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrec0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754143606,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.&lt;/p&gt;\n\n&lt;p&gt;I gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmemoo/dis-cal\"&gt;https://github.com/mmemoo/dis-cal&lt;/a&gt;  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?auto=webp&amp;s=7740e51b065f73baef20acb9661652efbd31473e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43d0c8bafbb43faea860f8f86e81107782da6aa3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8b2d47057fd20576416383a2c09e2a524b760d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b75736c0b2fa807b69315eb351d4357ec156394",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=78079915dfeceeab89fd37ed8f707b7cfbc64ce5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cabc737e4eaac8c82320118a5fb4890d5545856f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57c3313f0eff8bcc3ad6c3af276b7a5cb4a42f83",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfrec0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehmetflix_",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754143606,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "One of the things I want to do with my local build is to make my home more efficient.   I'd like to be able to get data points from various sources and have them analyzed either for actionable changes or optimization.  Not sure how to get from here to there though.\n\nExample:\n\nGather data from \n- temp outside\n- temp inside\n- temp inside cooling ducts (only measured when the system is blowing)\n- electrical draw from the ac\n- commanded on off cycles \n- amount of sun in specific loacations\n\nThen figure out \n- hvac gets commanded on but take longer at this time to cool off the house\n- at those times, command ac at lower temps to mitigate the time loss\n- discover that sun load at specific times effects efficiency, shade the area. \n\nI feel like there are enough smart home sensors out there that a well tuned ai could crunch all the data and give some real insight.   Why go of daily averages when I can record actual data in almost real time?  Why guess at the type of things home owners and so called efficiency experts have done in the past?  \n\nSo the set up might be something like this:\n\n1 install smart features and sensors (that can communicate with 2)\n\n2 set up code script etc to record data from all sources\n\n3 have ai model that interprets data and spit back patterns and adjustments to make\n\n4 maybe have ai create new script to adjust settings in the smart home for optimal efficiency\n\n5 run daily or or weekly analysis and adjust the efficiency script.\n\n\nThis is just me thinking outlook as a starting point.  And its only one area of efficiency of several that this could play a noticeable impact\n",
          "author_fullname": "t2_clyuifd5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smart integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqzc8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754142487,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the things I want to do with my local build is to make my home more efficient.   I&amp;#39;d like to be able to get data points from various sources and have them analyzed either for actionable changes or optimization.  Not sure how to get from here to there though.&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;Gather data from \n- temp outside\n- temp inside\n- temp inside cooling ducts (only measured when the system is blowing)\n- electrical draw from the ac\n- commanded on off cycles \n- amount of sun in specific loacations&lt;/p&gt;\n\n&lt;p&gt;Then figure out \n- hvac gets commanded on but take longer at this time to cool off the house\n- at those times, command ac at lower temps to mitigate the time loss\n- discover that sun load at specific times effects efficiency, shade the area. &lt;/p&gt;\n\n&lt;p&gt;I feel like there are enough smart home sensors out there that a well tuned ai could crunch all the data and give some real insight.   Why go of daily averages when I can record actual data in almost real time?  Why guess at the type of things home owners and so called efficiency experts have done in the past?  &lt;/p&gt;\n\n&lt;p&gt;So the set up might be something like this:&lt;/p&gt;\n\n&lt;p&gt;1 install smart features and sensors (that can communicate with 2)&lt;/p&gt;\n\n&lt;p&gt;2 set up code script etc to record data from all sources&lt;/p&gt;\n\n&lt;p&gt;3 have ai model that interprets data and spit back patterns and adjustments to make&lt;/p&gt;\n\n&lt;p&gt;4 maybe have ai create new script to adjust settings in the smart home for optimal efficiency&lt;/p&gt;\n\n&lt;p&gt;5 run daily or or weekly analysis and adjust the efficiency script.&lt;/p&gt;\n\n&lt;p&gt;This is just me thinking outlook as a starting point.  And its only one area of efficiency of several that this could play a noticeable impact&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqzc8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JellyfishAutomatic25",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqzc8/smart_integration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqzc8/smart_integration/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754142487,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.\n\nHere are a few things I've focused on:\n\n* It can create and run files automatically from natural language.\n* I've tried to make it smart about executing tools like git, search, and bash commands.\n* It's designed to work with any Ollama model that supports function calling.\n* A big priority for me was to keep it 100% local to ensure privacy.\n\nIt's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.",
          "author_fullname": "t2_2lznz5yi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollamacode - Local AI assistant that can create, run and understand your codebase.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqr3o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=c955545b2920d3da4a8cd08357963d4b7cac5b74",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754141855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a project called OllamaCode, and I&amp;#39;d love to share it with you. It&amp;#39;s an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt;\n\n&lt;p&gt;Here are a few things I&amp;#39;ve focused on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s designed to work with any Ollama model that supports function calling.&lt;/li&gt;\n&lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s still in the very early days, and there&amp;#39;s a lot I still want to improve. It&amp;#39;s been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/tooyipjee/ollamacode",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?auto=webp&amp;s=0d661e9993d3a8339a8a04e91dab3d495a1068d8",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad56c7eac7e674170e1bce20ac9f2fbb89067d4f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4264d87bb5a2e08e33aa5bd378354efc3de28a29",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbbbbe9284ddae09fc99ccbdd7bbad23851be4ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc443c649d71e676f2ac536126f73b6f681be48d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bf93455ce16aa988227f0c0581aef56a3355348",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0dd9f689127fdc2e0c3b1dd1996536119e75b475",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqr3o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud-Consideration-2",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/",
          "stickied": false,
          "url": "https://github.com/tooyipjee/ollamacode",
          "subreddit_subscribers": 509291,
          "created_utc": 1754141855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. \n\n\nEdit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. \n\nThanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)",
          "author_fullname": "t2_67bb0jdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source model that is as intelligent as Claude Sonnet 4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqejn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 352,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 352,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754151213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754140871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I’m unsure when they’ll increase pricing, limit usage, or make models less intelligent. I’m looking for a cheaper or open-source alternative that’s just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt;\n\n&lt;p&gt;Edit: I don’t pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I’m quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the suggestions. I’ll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfqejn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vishwa1238",
          "discussion_type": null,
          "num_comments": 271,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754140871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Gemini is too long and detailed. Grok's format is weird. Perplexity doesn't search enough. Qwen takes years and writes an entire book.\n\nchatGPT does it perfectly. A double lengthed message with citations, well-written, searches through websites trying to find what it needs, reasoning through it. But it's limited.\n\nThx guys!\n",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best free good deep research LLM websites?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpnxi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Gemini is too long and detailed. Grok&amp;#39;s format is weird. Perplexity doesn&amp;#39;t search enough. Qwen takes years and writes an entire book.&lt;/p&gt;\n\n&lt;p&gt;chatGPT does it perfectly. A double lengthed message with citations, well-written, searches through websites trying to find what it needs, reasoning through it. But it&amp;#39;s limited.&lt;/p&gt;\n\n&lt;p&gt;Thx guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfpnxi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpnxi/best_free_good_deep_research_llm_websites/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpnxi/best_free_good_deep_research_llm_websites/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754138692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey! I’d like to set up my machine to work on my codebase while I’m AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.\n\nI’ve already tested a few tools — kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.\n\nAre there any more or less standardized setups for this kind of workflow?\n\n",
          "author_fullname": "t2_nt892",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the current go-to setup for a fully-local coding agent that continuously improves code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpn4a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I’d like to set up my machine to work on my codebase while I’m AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.&lt;/p&gt;\n\n&lt;p&gt;I’ve already tested a few tools — kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.&lt;/p&gt;\n\n&lt;p&gt;Are there any more or less standardized setups for this kind of workflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfpn4a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sasik520",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754138620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! So I've been playing around with everyone's baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.\n\nShoutout to u/Skatardude10 and u/farkinga\n\nSo everyone knows it's a great idea to offload some/all tensors to RAM with these models if you can't fit them all. But from what I gathered, if you offload them using \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\", the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.\n\nWhat is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).\n\nThis is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).\n\nThe test is simple \"write me a sophisticated web scraper\". I run it once, then regenerate it to compare (I don't quite understand draft model context, noob, again).\n\n|~~With Qwen3 4b draft model\\*~~|No draft model|\n|:-|:-|\n|~~Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s~~|Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s|\n|~~Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s~~|Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s|\n\n*edit: tried* u/AliNT77\\*'s tip: set draft model's cache to Q8 Q8 and you'll have a higher acceptance rate with the smaller mode, allowing you to go up with main model's context and gain some speed.\\*\n\n\\* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):\n\n* XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)\n* DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s\n* Unsloth Qwen3 0.6B  - 33%, 19t/s\n* **Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s**\n* Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.\n\nWhat was the acceptance rate for 4B you're gonna ask... 67%.\n\nWhy do this instead of trying to offload some layers and try to gain performance this way? I don't know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)\n\nNow this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I'm unemployed, but I'm not THAT unemployed.\n\nQuestions:\n\n1. 1.7b vs 4b draft model. This obvs needs more testing and longer context, but I'm assuming that 4b will perform better than 1.7b with more complex code.\n2. What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?\n3. Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.\n4. I've got a Ryzen CPU. It's massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I'm assuming this is normal and there are no optimisations for AMD cpus?\n5. Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.\n6. Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don't these exist out in the wild?\n7. How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it's localllama so I dare not ask about bigger applications)\n\nWell, if you read all of this, here's your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp; caches, but I am not done yet. This doesn't crash the system, any other combo does. So if you've got more than 12gb vram, you might get away with more context.\n\nStart with: LLAMA\\_SET\\_ROWS=1  \n\\--model \"(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4\\_K\\_XL.gguf\"  \n\\--model-draft \"(full path)/Qwen3-4B-Q8\\_0.gguf\"  \n\\--override-tensor \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\" (yet to test this, but it can now be replaced with --cpu-moe)  \n\\--flash-attn  \n~~--ctx-size 192000~~  \n\\--ctx-size 262144 --cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--threads -1  \n\\--n-gpu-layers 99  \n\\--n-gpu-layers-draft 99  \n~~--ctx-size-draft 1024 --cache-type-k-draft q4\\_0 --cache-type-v-draft q4\\_0~~  \n\\--ctx-size-draft 24567 --cache-type-v-draft q8\\_0 --cache-type-v-draft q8\\_0\n\nor you can do for more speed (30t/s)/accuracy, but less context.  \n\\--ctx-size 131072 --cache-type-k q8\\_0 --cache-type-v q8\\_0  \n\\--ctx-size-draft 24576 --cache-type-k-draft q8\\_0 --cache-type-v-draft q8\\_0  \n\\--batch-size 1024 --ubatch-size 1024\n\nThese settings get you to 11197MiB /  12227MiB vram on the gpu.",
          "author_fullname": "t2_wxxnd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 30B A3b --override-tensor + Qwen3 4b draft = &lt;3 (22 vs 14 t/s)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpgae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754147599,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! So I&amp;#39;ve been playing around with everyone&amp;#39;s baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.&lt;/p&gt;\n\n&lt;p&gt;Shoutout to &lt;a href=\"/u/Skatardude10\"&gt;u/Skatardude10&lt;/a&gt; and &lt;a href=\"/u/farkinga\"&gt;u/farkinga&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So everyone knows it&amp;#39;s a great idea to offload some/all tensors to RAM with these models if you can&amp;#39;t fit them all. But from what I gathered, if you offload them using &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot;, the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.&lt;/p&gt;\n\n&lt;p&gt;What is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).&lt;/p&gt;\n\n&lt;p&gt;This is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).&lt;/p&gt;\n\n&lt;p&gt;The test is simple &amp;quot;write me a sophisticated web scraper&amp;quot;. I run it once, then regenerate it to compare (I don&amp;#39;t quite understand draft model context, noob, again).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;del&gt;With Qwen3 4b draft model*&lt;/del&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;No draft model&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;em&gt;edit: tried&lt;/em&gt; &lt;a href=\"/u/AliNT77\"&gt;u/AliNT77&lt;/a&gt;*&amp;#39;s tip: set draft model&amp;#39;s cache to Q8 Q8 and you&amp;#39;ll have a higher acceptance rate with the smaller mode, allowing you to go up with main model&amp;#39;s context and gain some speed.*&lt;/p&gt;\n\n&lt;p&gt;* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)&lt;/li&gt;\n&lt;li&gt;DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 0.6B  - 33%, 19t/s&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What was the acceptance rate for 4B you&amp;#39;re gonna ask... 67%.&lt;/p&gt;\n\n&lt;p&gt;Why do this instead of trying to offload some layers and try to gain performance this way? I don&amp;#39;t know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)&lt;/p&gt;\n\n&lt;p&gt;Now this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I&amp;#39;m unemployed, but I&amp;#39;m not THAT unemployed.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;1.7b vs 4b draft model. This obvs needs more testing and longer context, but I&amp;#39;m assuming that 4b will perform better than 1.7b with more complex code.&lt;/li&gt;\n&lt;li&gt;What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?&lt;/li&gt;\n&lt;li&gt;Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve got a Ryzen CPU. It&amp;#39;s massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I&amp;#39;m assuming this is normal and there are no optimisations for AMD cpus?&lt;/li&gt;\n&lt;li&gt;Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.&lt;/li&gt;\n&lt;li&gt;Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don&amp;#39;t these exist out in the wild?&lt;/li&gt;\n&lt;li&gt;How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it&amp;#39;s localllama so I dare not ask about bigger applications)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Well, if you read all of this, here&amp;#39;s your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp;amp; caches, but I am not done yet. This doesn&amp;#39;t crash the system, any other combo does. So if you&amp;#39;ve got more than 12gb vram, you might get away with more context.&lt;/p&gt;\n\n&lt;p&gt;Start with: LLAMA_SET_ROWS=1&lt;br/&gt;\n--model &amp;quot;(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf&amp;quot;&lt;br/&gt;\n--model-draft &amp;quot;(full path)/Qwen3-4B-Q8_0.gguf&amp;quot;&lt;br/&gt;\n--override-tensor &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot; (yet to test this, but it can now be replaced with --cpu-moe)&lt;br/&gt;\n--flash-attn&lt;br/&gt;\n&lt;del&gt;--ctx-size 192000&lt;/del&gt;&lt;br/&gt;\n--ctx-size 262144 --cache-type-k q4_0 --cache-type-v q4_0&lt;br/&gt;\n--threads -1&lt;br/&gt;\n--n-gpu-layers 99&lt;br/&gt;\n--n-gpu-layers-draft 99&lt;br/&gt;\n&lt;del&gt;--ctx-size-draft 1024 --cache-type-k-draft q4_0 --cache-type-v-draft q4_0&lt;/del&gt;&lt;br/&gt;\n--ctx-size-draft 24567 --cache-type-v-draft q8_0 --cache-type-v-draft q8_0&lt;/p&gt;\n\n&lt;p&gt;or you can do for more speed (30t/s)/accuracy, but less context.&lt;br/&gt;\n--ctx-size 131072 --cache-type-k q8_0 --cache-type-v q8_0&lt;br/&gt;\n--ctx-size-draft 24576 --cache-type-k-draft q8_0 --cache-type-v-draft q8_0&lt;br/&gt;\n--batch-size 1024 --ubatch-size 1024&lt;/p&gt;\n\n&lt;p&gt;These settings get you to 11197MiB /  12227MiB vram on the gpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfpgae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "igorwarzocha",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754138032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?\n\nI initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?\n\nBut then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?\n\nSo both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?",
          "author_fullname": "t2_kjhuip53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG or prompt engineering",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfor6n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754135853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I’m a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that’s true, wouldn’t that quickly blow up the context window?&lt;/p&gt;\n\n&lt;p&gt;But then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn’t that risk missing important context, especially for something like summarization?&lt;/p&gt;\n\n&lt;p&gt;So both approaches seem to have drawbacks — I’m just wondering which one is typically used by AI apps when handling uploaded files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfor6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SignatureHuman8057",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754135853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. I'm a non native English writer, who could use some help with phrasing, [something like this](https://www.gingersoftware.com/products/sentence-rephraser), character and plot detail suggestions etc. Are there any good models that can help with that?\n\nI'm planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it's absolutely necessary. Please let me know if it is.",
          "author_fullname": "t2_169t2g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a local model that can help a non native writer with sentence phrasing and ideas.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfoh32",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m a non native English writer, who could use some help with phrasing, &lt;a href=\"https://www.gingersoftware.com/products/sentence-rephraser\"&gt;something like this&lt;/a&gt;, character and plot detail suggestions etc. Are there any good models that can help with that?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it&amp;#39;s absolutely necessary. Please let me know if it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfoh32",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "logicSnob",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754134900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?\n\nHere's the Dockerfile:\n\n&gt; FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate\n\nHere's the docker-compose:\n\n&gt; version: '3.8'\nservices:\n  infinity:\n    build: .\n    ports:\n      - \"7997:7997\"\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B\n\nStartup command was:\n\ndocker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch",
          "author_fullname": "t2_cocl8roo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Issues with michaelf34/infinity:latest-cpu + Qwen3-Embedding-8B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfofx5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134794,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the Dockerfile:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Here&amp;#39;s the docker-compose:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;version: &amp;#39;3.8&amp;#39;\nservices:\n  infinity:\n    build: .\n    ports:\n      - &amp;quot;7997:7997&amp;quot;\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Startup command was:&lt;/p&gt;\n\n&lt;p&gt;docker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfofx5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Patentsmatter",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754134794,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi r/LocalLLaMA,\n\nI'm exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.\n\nFor example, I could give it a high-level command like \"Save the document and close the application,\" and it would need to:\n\n1. Visually parse the screen to locate the \"Save\" button or menu item.\n2. Move the mouse cursor to that location.\n3. Perform a click.\n4. Then, locate the \"Close\" button and do the same.\n\nI'm trying to figure out the best stack for this using local models. My main questions are:\n\n* **Vision/Perception:** What's the current best approach for a model to \"see\" the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?\n* **Decision Making (LLM):** How would the LLM receive the visual information and output the decision (e.g., \"click button with text 'OK' at coordinates \\[x, y\\]\")? What kind of prompting or fine-tuning would be required?\n* **Action/Control:** What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like pyautogui the way to go, or are there more robust alternatives?\n* **Frameworks:** Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?\n\nI'm aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!\n\nThanks",
          "author_fullname": "t2_1t42bjvz6a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to build a local agent for Windows GUI automation (mouse control &amp; accurate button clicking)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfodac",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.&lt;/p&gt;\n\n&lt;p&gt;For example, I could give it a high-level command like &amp;quot;Save the document and close the application,&amp;quot; and it would need to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Visually parse the screen to locate the &amp;quot;Save&amp;quot; button or menu item.&lt;/li&gt;\n&lt;li&gt;Move the mouse cursor to that location.&lt;/li&gt;\n&lt;li&gt;Perform a click.&lt;/li&gt;\n&lt;li&gt;Then, locate the &amp;quot;Close&amp;quot; button and do the same.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out the best stack for this using local models. My main questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vision/Perception:&lt;/strong&gt; What&amp;#39;s the current best approach for a model to &amp;quot;see&amp;quot; the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Decision Making (LLM):&lt;/strong&gt; How would the LLM receive the visual information and output the decision (e.g., &amp;quot;click button with text &amp;#39;OK&amp;#39; at coordinates [x, y]&amp;quot;)? What kind of prompting or fine-tuning would be required?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action/Control:&lt;/strong&gt; What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like pyautogui the way to go, or are there more robust alternatives?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frameworks:&lt;/strong&gt; Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfodac",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xSNYPSx777",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754134546,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.\n\nI ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison.\nI'm also planning to run some fine-tuning benchmarks later.",
          "author_fullname": "t2_a85vzco5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 57,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnq2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YkVLBy_Uabot6hZJtvt-2lgE4cyPGdL7zFQtJSHp-Xk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754132261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn&amp;#39;t find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt;\n\n&lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I&amp;#39;m quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I&amp;#39;ve also included the RTX 4090 results for a broader comparison.\nI&amp;#39;m also planning to run some fine-tuning benchmarks later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?auto=webp&amp;s=5c7238d3074b0c94ed28a56fd05d130ca36e4062",
                  "width": 1280,
                  "height": 525
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8aa6f94f1fd0b07761158c32a4f411fea4ff01e",
                    "width": 108,
                    "height": 44
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fd7073605b2305ef0bdfa02c28ed14504e5cadf",
                    "width": 216,
                    "height": 88
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f51b098c5db731c8b7427cd4c5e5918907eaf170",
                    "width": 320,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f",
                    "width": 640,
                    "height": 262
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f3fce6514c1c2d99e8304633aabf8e744412a87",
                    "width": 960,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4087fbfd9cd44fbf60685b1fe8cf9814b669d42c",
                    "width": 1080,
                    "height": 442
                  }
                ],
                "variants": {},
                "id": "tc9bMPtFnkPdsr6iuVPGSgsfnHwUyG7fExgbegYm3H4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfnq2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kargafe",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/",
          "stickied": false,
          "url": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "subreddit_subscribers": 509291,
          "created_utc": 1754132261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, an enthusiast but no formal CS training background asking for help \n\nI am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that's the best we can get :) \n\nWhich is the best \"small size\" LLM for document question answering with European language - mostly specific medical jargon.\n\nI tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.\n\nI am running the app using ollama and langchain also any recommendations for alternatives are welcome :)\n",
          "author_fullname": "t2_1e27iwczfb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best &lt;2B open-source LLMs for European languages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnfrp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754131171,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, an enthusiast but no formal CS training background asking for help &lt;/p&gt;\n\n&lt;p&gt;I am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that&amp;#39;s the best we can get :) &lt;/p&gt;\n\n&lt;p&gt;Which is the best &amp;quot;small size&amp;quot; LLM for document question answering with European language - mostly specific medical jargon.&lt;/p&gt;\n\n&lt;p&gt;I tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.&lt;/p&gt;\n\n&lt;p&gt;I am running the app using ollama and langchain also any recommendations for alternatives are welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfnfrp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Material-Ad5426",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754131171,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dhanishtha-2.0-preview can now tool call. \n\nUpdated Model link:- [https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825](https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825)  \nAPI and Chat page :- [https://helpingai.co](https://helpingai.co)\n\n",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool calling is now supported on World's first Intermediate Reasoning model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn7pv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754130292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt;\n\n&lt;p&gt;Updated Model link:- &lt;a href=\"https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825\"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br/&gt;\nAPI and Chat page :- &lt;a href=\"https://helpingai.co\"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?auto=webp&amp;s=26ef07178b4b9394a1c2c1b0dfc81de665a07de8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6528fc720427c7c1ed30f20ddd332d01526c4f8f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bffe53726d3c274e1c28ce6856f9194b9aca7fd2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=53fe26fae2e2b849fb6ac025e7f99b8738728cd0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14d7ff05a119f24e15918fc09489c78087b00a3e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e635b759cb0f70c7db7a222e92c287229ba6fb41",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a3feb1fc2bfd7f88481601ee7cdf04a9ff9b5ed",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfn7pv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754130292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.\n\nBuilt using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It's ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.\n\n[https://github.com/dokasto/Saidia](https://github.com/dokasto/Saidia)",
          "author_fullname": "t2_11mwsd0p41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Saidia: Offline-First AI Assistant for Educators in low-connectivity regions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn2xf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754161230,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754129775,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.&lt;/p&gt;\n\n&lt;p&gt;Built using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It&amp;#39;s ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dokasto/Saidia\"&gt;https://github.com/dokasto/Saidia&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?auto=webp&amp;s=5141edfe493867e3dafdd151010b3f5040912cc0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e949d6258248c551dcfd9cb47f1304f4151400c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3484da8634402f8ed7774d2e4f8c4669168d53",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7c82a3e8557eeca0f83fc2e477928d5f3031a9f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=493dbfda76c0eab29abee992f4eb1d96ff6d93fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92c32a097682bd7a3113fbc01e20ac3a07fd673",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e412f9a5dfdc2c2cf419a9494a9e06f0757f95",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfn2xf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dokasto_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754129775,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}