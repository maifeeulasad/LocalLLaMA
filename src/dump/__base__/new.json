{
  "kind": "Listing",
  "data": {
    "after": "t3_1mggsyb",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey\n\nDoes anybody have some experience working with those newer Ryzen AI Chips in case of running Models (up to 70B Q4 ? )\nFine tuning LLMs using LoRA or converting models from/into GGUF?\n\nSaw those are more affordable than going for a maxed out mac book pro and would be quite interested in their performance and semi professional use with fine tuning and converting.\n\nShare your experiences, happy to read üòÅüëç",
          "author_fullname": "t2_i00m20zzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running, fine tuning and converting LLMs on new Ryzen AI 7 or 9 APUs - 64-128GB RAM - 75% VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhc31j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754310562,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;/p&gt;\n\n&lt;p&gt;Does anybody have some experience working with those newer Ryzen AI Chips in case of running Models (up to 70B Q4 ? )\nFine tuning LLMs using LoRA or converting models from/into GGUF?&lt;/p&gt;\n\n&lt;p&gt;Saw those are more affordable than going for a maxed out mac book pro and would be quite interested in their performance and semi professional use with fine tuning and converting.&lt;/p&gt;\n\n&lt;p&gt;Share your experiences, happy to read üòÅüëç&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhc31j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IngloriousBastrd7908",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhc31j/running_fine_tuning_and_converting_llms_on_new/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhc31j/running_fine_tuning_and_converting_llms_on_new/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754310562,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of Qwen 2508 do you want tonight? ;)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 28,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbvig",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/xW5rjzTOCVnresjRs9FDgm4fGUjo6_kF-kfsc1LJrK8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f5by1b8wzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f5by1b8wzgf1.png?auto=webp&amp;s=8943dfd99bf22f0cc0c02f506e7d3fd14a2f8352",
                  "width": 1216,
                  "height": 246
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c79933ab77e3d643053f37ab6382908ac1eb9af",
                    "width": 108,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=768aae7713836c3c2a3aea894b45a4f967d9fa2f",
                    "width": 216,
                    "height": 43
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b20bde67926e3e8040aaf57a6389e737abd4fc69",
                    "width": 320,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f4a0d87b3973237c269d2cef31fbc18fbe655b1",
                    "width": 640,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2eec3557cccdfd47534f0ee7386653e12cd322ea",
                    "width": 960,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/3f5by1b8wzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db751d3024ba27f2cf55f1c5324f9b111e96ef8c",
                    "width": 1080,
                    "height": 218
                  }
                ],
                "variants": {},
                "id": "COyPpURqpsrEIUBXoEpCUbhYmsp15c4NUFjXeHFSIu8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbvig",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/3f5by1b8wzgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen Models Today!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 54,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbpmo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TROn1uPQcH0PujeybIidpkc9G7nZ0H_qibt1MPjtmMI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754309520,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qemmgysvuzgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qemmgysvuzgf1.png?auto=webp&amp;s=42888269e8a8be81d80e8a6d5692747211e04c55",
                  "width": 1220,
                  "height": 476
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f9e5dff4613eb055af874621d1a213848bf522f",
                    "width": 108,
                    "height": 42
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06f6672b49f5b95cc45a9b23e3598d09d05496d7",
                    "width": 216,
                    "height": 84
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=28a355bc0adcfc06eaf4b216b3ef61b9d652f5eb",
                    "width": 320,
                    "height": 124
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639",
                    "width": 640,
                    "height": 249
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd0a3cc19923cbf380af7306ff3f4d5335556fb8",
                    "width": 960,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/qemmgysvuzgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f267cdf32b56403571256c354e628f69fadd7b15",
                    "width": 1080,
                    "height": 421
                  }
                ],
                "variants": {},
                "id": "34KTkl_1uxrvHPhAnaWXTjSZ6bmw11ut0GxXsPRfDZY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mhbpmo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/",
          "stickied": false,
          "url": "https://i.redd.it/qemmgysvuzgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309520,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?\n\nI have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?\n\nI don't mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.\n\nShould I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn't really matter to me too much. I'm happy to wait. \n\nThis is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. \n\nWhat llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. \n\nWhat quants?\n\nI want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? \n\nAny ideas, thoughts? Anyone tried to maximise long context processing?",
          "author_fullname": "t2_dmg3f1uq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to maximise Prompt processing speed for long context usage?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbp73",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754309485,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.&lt;/p&gt;\n\n&lt;p&gt;Should I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn&amp;#39;t really matter to me too much. I&amp;#39;m happy to wait. &lt;/p&gt;\n\n&lt;p&gt;This is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. &lt;/p&gt;\n\n&lt;p&gt;What llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. &lt;/p&gt;\n\n&lt;p&gt;What quants?&lt;/p&gt;\n\n&lt;p&gt;I want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? &lt;/p&gt;\n\n&lt;p&gt;Any ideas, thoughts? Anyone tried to maximise long context processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhbp73",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Kangaroo6055",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309485,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. The messages field in the payload of an (OpenAI-compatible) API call is an array, meaning there can be multiple messages, of system, user, or assistant roles. I normally just send a system message and then the user message. \n\n\nWhat are some use cases where it's preferable to send multiple messages in one API call instead of one or two, in a certain order? Is there any benefit to send an assistant message, as these sre supposed to be coming from the LLM itself?\n\nAny concrete use case or example is appreciated. Thanks.",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are some use cases to send multiple messages in one LLM API request?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhbk4f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754309065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. The messages field in the payload of an (OpenAI-compatible) API call is an array, meaning there can be multiple messages, of system, user, or assistant roles. I normally just send a system message and then the user message. &lt;/p&gt;\n\n&lt;p&gt;What are some use cases where it&amp;#39;s preferable to send multiple messages in one API call instead of one or two, in a certain order? Is there any benefit to send an assistant message, as these sre supposed to be coming from the LLM itself?&lt;/p&gt;\n\n&lt;p&gt;Any concrete use case or example is appreciated. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhbk4f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbk4f/what_are_some_use_cases_to_send_multiple_messages/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbk4f/what_are_some_use_cases_to_send_multiple_messages/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754309065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nhttps://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2\n\nOn 9th August 2025, I am starting a Small Language Model Workshop. It will be a 5-6 hour live workshop.¬†This is purely for teaching and sharing knowledge. Think of it as a 3 times expanded and live version of Karpathy's repository and video. \n\nIn this workshop, we will build a production ready Small Language Model (SLM) fully from scratch.¬†¬†\n\nTowards the end of this workshop, we will chain 8 GPUs and actually replicate the results of GPT-2.¬†\n\nIt will be like building GPT-2 fully from scratch, and getting results which OpenAI got in their classical GPT-2 paper.¬†\n\nThe workshop will start from tokenisation and end at multi-GPU programming.¬†\n\nWe will work with 2 datasets: \n\n\\- TinyStories\n\n\\- FineWeb Edu\n\nWe will go through the following: \n\n\\- Loading datasets\n\n\\- Tokenization\n\n\\- Creating input-target pairs\n\n\\- Assembling the entire SLM architecture\n\n\\- Defining the training loop\n\n\\- Running inference\n\n\\- Multi-GPU version of training\n\nRegister for free here: [https://slm-from-scratch.vercel.app/](https://slm-from-scratch.vercel.app/)",
          "author_fullname": "t2_rb9k9zgob",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Build a Small Language Model from Scratch | Free 6 hour live workshop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "2062bdjfszgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=644b6890d9f1d63afe4d1b3649c082853f6b5df9"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20a92ff4c0705d871443942faa8615498c590153"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2afca8c1a6986c761dee8f582e264c5741a4099b"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d13c836b271b555a60e400b298dd60837ffca1e"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=00aca5ea51a1bee55af64b4c90f31fb80592b388"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0d7cf6280d9d0860d2aa3b597532e8c852614086"
                }
              ],
              "s": {
                "y": 4032,
                "x": 3024,
                "u": "https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2"
              },
              "id": "2062bdjfszgf1"
            }
          },
          "name": "t3_1mhbhn3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/wYyTcudp-ANH5QEeVy4mcXMJLT3H-mEztMtQMKLbu78.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754308876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2\"&gt;https://preview.redd.it/2062bdjfszgf1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d90a50b645e50511d0d6eea016fa89918bdfc9e2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On 9th August 2025, I am starting a Small Language Model Workshop. It will be a 5-6 hour live workshop.¬†This is purely for teaching and sharing knowledge. Think of it as a 3 times expanded and live version of Karpathy&amp;#39;s repository and video. &lt;/p&gt;\n\n&lt;p&gt;In this workshop, we will build a production ready Small Language Model (SLM) fully from scratch.¬†¬†&lt;/p&gt;\n\n&lt;p&gt;Towards the end of this workshop, we will chain 8 GPUs and actually replicate the results of GPT-2.¬†&lt;/p&gt;\n\n&lt;p&gt;It will be like building GPT-2 fully from scratch, and getting results which OpenAI got in their classical GPT-2 paper.¬†&lt;/p&gt;\n\n&lt;p&gt;The workshop will start from tokenisation and end at multi-GPU programming.¬†&lt;/p&gt;\n\n&lt;p&gt;We will work with 2 datasets: &lt;/p&gt;\n\n&lt;p&gt;- TinyStories&lt;/p&gt;\n\n&lt;p&gt;- FineWeb Edu&lt;/p&gt;\n\n&lt;p&gt;We will go through the following: &lt;/p&gt;\n\n&lt;p&gt;- Loading datasets&lt;/p&gt;\n\n&lt;p&gt;- Tokenization&lt;/p&gt;\n\n&lt;p&gt;- Creating input-target pairs&lt;/p&gt;\n\n&lt;p&gt;- Assembling the entire SLM architecture&lt;/p&gt;\n\n&lt;p&gt;- Defining the training loop&lt;/p&gt;\n\n&lt;p&gt;- Running inference&lt;/p&gt;\n\n&lt;p&gt;- Multi-GPU version of training&lt;/p&gt;\n\n&lt;p&gt;Register for free here: &lt;a href=\"https://slm-from-scratch.vercel.app/\"&gt;https://slm-from-scratch.vercel.app/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mhbhn3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OtherRaisin3426",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhbhn3/build_a_small_language_model_from_scratch_free_6/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbhn3/build_a_small_language_model_from_scratch_free_6/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754308876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Current status:\n\nhttps://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\n\nEveryone get ready to fire up your GPUs...",
          "author_fullname": "t2_1utnp17o3h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 llama.cpp PR is nearing completion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhb5el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754307842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current status:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Everyone get ready to fire up your GPUs...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mhb5el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistanceSolar1449",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754307842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title suggests, I am trying to build a database agent for a custom Erp software with all the tables already inside a Postgres server.\n\nAs it‚Äôs an erp software it deals with a variety of tables such as sales/inventory/packaging and so on.\n\nI want to make an agent such that if I ask a question related to sales it should be able to pick it up from the appropriate table and answer it based on the data either through direct SQL execution or through similarity search.\n\nThe twist being I can only afford an ec2 cpu only server to make this work.\n\nWhich is the best way I can do this?\n\nIt should work efficiently and effectively only on CPU and raw RAM.\n\nI already tried with pgvector and stuff, and it‚Äôs okay okay but at the cost of a lot of processing time.",
          "author_fullname": "t2_t8chs0i5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local database agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mhaw4g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754307039,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests, I am trying to build a database agent for a custom Erp software with all the tables already inside a Postgres server.&lt;/p&gt;\n\n&lt;p&gt;As it‚Äôs an erp software it deals with a variety of tables such as sales/inventory/packaging and so on.&lt;/p&gt;\n\n&lt;p&gt;I want to make an agent such that if I ask a question related to sales it should be able to pick it up from the appropriate table and answer it based on the data either through direct SQL execution or through similarity search.&lt;/p&gt;\n\n&lt;p&gt;The twist being I can only afford an ec2 cpu only server to make this work.&lt;/p&gt;\n\n&lt;p&gt;Which is the best way I can do this?&lt;/p&gt;\n\n&lt;p&gt;It should work efficiently and effectively only on CPU and raw RAM.&lt;/p&gt;\n\n&lt;p&gt;I already tried with pgvector and stuff, and it‚Äôs okay okay but at the cost of a lot of processing time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mhaw4g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Whywhoo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mhaw4g/local_database_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhaw4g/local_database_agent/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754307039,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "YuE: Open Full-song Music Generation Foundation Model, something similar to [Suno.ai](http://Suno.ai) but open",
          "author_fullname": "t2_etmr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Music Foundation Models for Full-Song Generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mha439",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754304531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "map-yue.github.io",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;YuE: Open Full-song Music Generation Foundation Model, something similar to &lt;a href=\"http://Suno.ai\"&gt;Suno.ai&lt;/a&gt; but open&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://map-yue.github.io/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mha439",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phone_radio_tv",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha439/open_music_foundation_models_for_fullsong/",
          "stickied": false,
          "url": "https://map-yue.github.io/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754304531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the best approach for this? Tried it in open webui with ollama backend but it's too slow.\n\nAll docs are pdf, all done with ocr so it's all just text. Ingestion to knowledgebase is the bottleneck.\n\nAnybody done this and what was the best approach for you?",
          "author_fullname": "t2_mu8eykc30",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG with 30k documents, some with 300 pages each.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mha1g1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754305112,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754304270,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best approach for this? Tried it in open webui with ollama backend but it&amp;#39;s too slow.&lt;/p&gt;\n\n&lt;p&gt;All docs are pdf, all done with ocr so it&amp;#39;s all just text. Ingestion to knowledgebase is the bottleneck.&lt;/p&gt;\n\n&lt;p&gt;Anybody done this and what was the best approach for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mha1g1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dennisitnet",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mha1g1/rag_with_30k_documents_some_with_300_pages_each/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754304270,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to run ik\\_llama with speculative decoding but I just get this error:\n\n    /build/source/src/llama.cpp:18273: GGML_ASSERT(n_tokens_all &lt;= cparams.n_batch) failed\n\nThis was the command I used:\n\n    llama-speculative -m Qwen3-30B-A3B-Thinking-2507-Q4_K_S.gguf -md Qwen3-0.6B-UD-Q5_K_XL.gguf -c 32768\n\nI have tried increasing `--batch-size` up to and past the context size with no effect. Here's the full output, if that's helpful: [https://pastebin.com/wbLKasvD](https://pastebin.com/wbLKasvD)\n\nThe same thing also seems to happen with llama.cpp, with Qwen3 14b + 0.6b, and with the official quants instead of unsloth's.",
          "author_fullname": "t2_1t8nko6ocb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ik_llama speculative decoding error",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mh9uha",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754304258,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754303579,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to run ik_llama with speculative decoding but I just get this error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/build/source/src/llama.cpp:18273: GGML_ASSERT(n_tokens_all &amp;lt;= cparams.n_batch) failed\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This was the command I used:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-speculative -m Qwen3-30B-A3B-Thinking-2507-Q4_K_S.gguf -md Qwen3-0.6B-UD-Q5_K_XL.gguf -c 32768\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I have tried increasing &lt;code&gt;--batch-size&lt;/code&gt; up to and past the context size with no effect. Here&amp;#39;s the full output, if that&amp;#39;s helpful: &lt;a href=\"https://pastebin.com/wbLKasvD\"&gt;https://pastebin.com/wbLKasvD&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The same thing also seems to happen with llama.cpp, with Qwen3 14b + 0.6b, and with the official quants instead of unsloth&amp;#39;s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh9uha",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Desk5910",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh9uha/ik_llama_speculative_decoding_error/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh9uha/ik_llama_speculative_decoding_error/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754303579,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I‚Äôve shifted focus to the infra side, specifically LLM gateways.\n\nMost of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.\n\nSome quick observations from what I tried:\n\n* [Bifrost](https://getmax.im/2frost) (Go, self-hosted): Surprisingly fast even under high load. Saw around 11¬µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.\n* [Portkey](https://portkey.ai/features/ai-gateway): Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.\n* **Kong** and [Gloo](https://www.solo.io/products/gloo-ai-gateway): These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn‚Äôt feel natural. Not LLM-aware.\n* **Cloudflare‚Äôs AI Gateway**: Pretty good for lightweight routing if you're already using Cloudflare. But it‚Äôs a black box, not much visibility or customization.\n* **Aisera‚Äôs Gateway**: Geared toward enterprise support use cases. More of a vertical solution. Didn‚Äôt feel suitable for general-purpose LLM infra.\n* [LiteLLM](https://www.litellm.ai/): Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.\n\nWould love to hear what others are running in production, especially if you‚Äôre doing failover, traffic splitting, or anything more advanced.",
          "author_fullname": "t2_1tizhpru5u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM gateway?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh9r0z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754303241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I‚Äôve shifted focus to the infra side, specifically LLM gateways.&lt;/p&gt;\n\n&lt;p&gt;Most of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.&lt;/p&gt;\n\n&lt;p&gt;Some quick observations from what I tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getmax.im/2frost\"&gt;Bifrost&lt;/a&gt; (Go, self-hosted): Surprisingly fast even under high load. Saw around 11¬µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://portkey.ai/features/ai-gateway\"&gt;Portkey&lt;/a&gt;: Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kong&lt;/strong&gt; and &lt;a href=\"https://www.solo.io/products/gloo-ai-gateway\"&gt;Gloo&lt;/a&gt;: These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn‚Äôt feel natural. Not LLM-aware.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cloudflare‚Äôs AI Gateway&lt;/strong&gt;: Pretty good for lightweight routing if you&amp;#39;re already using Cloudflare. But it‚Äôs a black box, not much visibility or customization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Aisera‚Äôs Gateway&lt;/strong&gt;: Geared toward enterprise support use cases. More of a vertical solution. Didn‚Äôt feel suitable for general-purpose LLM infra.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.litellm.ai/\"&gt;LiteLLM&lt;/a&gt;: Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear what others are running in production, especially if you‚Äôre doing failover, traffic splitting, or anything more advanced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mh9r0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational-Bison786",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754303241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:\n\n* Some requests randomly timed out or took way longer than others, even with the same provider\n* Logs didn‚Äôt show much, and tracing failures across providers was difficult\n* When we tried running it behind a load balancer, we ran into strange behavior with state\n* Fallbacks didn‚Äôt always trigger reliably when a provider was down or rate-limited\n* We tried plugging in Prometheus, but visibility into request flow was limited\n\nThe architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.\n\nWhile looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?",
          "author_fullname": "t2_1p9vds0za6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LiteLLM started breaking down for us past 300 RPS, what are folks using in prod?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh99hu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some requests randomly timed out or took way longer than others, even with the same provider&lt;/li&gt;\n&lt;li&gt;Logs didn‚Äôt show much, and tracing failures across providers was difficult&lt;/li&gt;\n&lt;li&gt;When we tried running it behind a load balancer, we ran into strange behavior with state&lt;/li&gt;\n&lt;li&gt;Fallbacks didn‚Äôt always trigger reliably when a provider was down or rate-limited&lt;/li&gt;\n&lt;li&gt;We tried plugging in Prometheus, but visibility into request flow was limited&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.&lt;/p&gt;\n\n&lt;p&gt;While looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh99hu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Otherwise_Flan7339",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754301478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First time fine tuning a model in \"the cloud\".\nRunpod was suggested. And what should I say - it's a pita since the first few seconds.\nRDP - only via VNC. Accessing volume? No. SCP? No. Accurately telling you how much you used of your persistent volume? No. Figure out yourself while counting by hand. There is 330t TB available, and 222 TB used. It can't show anything below a TB.\nSetting up my pod for work yesterday so I can start without brain damage into the week today.\nAwesome idea - but GPUs are out for this Region. But let us charge you for your persistent volume.\nYou think you can trick run pod, find a solution and just start all over with a new pod in a region where GPUs are available?\nHaha, no way. After you pressed \"deploy\" we will charge you now for both pots. Accessing the platform to work on your pods or manage them? Whole platform not loading since almost 20 minutes - no chance to do anything. But hey, we will charge you üòÇ\n\nI don't know what anyone is doing with runpod. Am I unlucky? Maybe. Is the platform working? Not anymore. Only for my user btw ü§£\nSpeedtest benchmark? Could still do cloud gaming at 4k 60 fps.\nBut can't access the platform.\n\nFor me this platform is just a pita. I wasted several days trying to fix things and incompatibility issues of their pods after deployment.\n\nThis is by far one of the worst IT experiences in my life.\nCan't build my own server since I am traveling too much.\n\nAny alternatives that are working and can be accessed?\n\n\n\nEdit:\nClearing cache doesn't help. Incognito mode let's me click on login - but the moment I login it's stucked again. Other Laptop with different OS -&gt; same outcome like in incognito mode.\nClearly, my account is bugged or something.",
          "author_fullname": "t2_i00m20zzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Runpod breaks my head - need a working alternative",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh960c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754303380,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754301108,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time fine tuning a model in &amp;quot;the cloud&amp;quot;.\nRunpod was suggested. And what should I say - it&amp;#39;s a pita since the first few seconds.\nRDP - only via VNC. Accessing volume? No. SCP? No. Accurately telling you how much you used of your persistent volume? No. Figure out yourself while counting by hand. There is 330t TB available, and 222 TB used. It can&amp;#39;t show anything below a TB.\nSetting up my pod for work yesterday so I can start without brain damage into the week today.\nAwesome idea - but GPUs are out for this Region. But let us charge you for your persistent volume.\nYou think you can trick run pod, find a solution and just start all over with a new pod in a region where GPUs are available?\nHaha, no way. After you pressed &amp;quot;deploy&amp;quot; we will charge you now for both pots. Accessing the platform to work on your pods or manage them? Whole platform not loading since almost 20 minutes - no chance to do anything. But hey, we will charge you üòÇ&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what anyone is doing with runpod. Am I unlucky? Maybe. Is the platform working? Not anymore. Only for my user btw ü§£\nSpeedtest benchmark? Could still do cloud gaming at 4k 60 fps.\nBut can&amp;#39;t access the platform.&lt;/p&gt;\n\n&lt;p&gt;For me this platform is just a pita. I wasted several days trying to fix things and incompatibility issues of their pods after deployment.&lt;/p&gt;\n\n&lt;p&gt;This is by far one of the worst IT experiences in my life.\nCan&amp;#39;t build my own server since I am traveling too much.&lt;/p&gt;\n\n&lt;p&gt;Any alternatives that are working and can be accessed?&lt;/p&gt;\n\n&lt;p&gt;Edit:\nClearing cache doesn&amp;#39;t help. Incognito mode let&amp;#39;s me click on login - but the moment I login it&amp;#39;s stucked again. Other Laptop with different OS -&amp;gt; same outcome like in incognito mode.\nClearly, my account is bugged or something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh960c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IngloriousBastrd7908",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh960c/runpod_breaks_my_head_need_a_working_alternative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh960c/runpod_breaks_my_head_need_a_working_alternative/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754301108,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.\n\nAnyone have something like a llama model run on pi5 with 8GB RAM?",
          "author_fullname": "t2_l7ryo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "lcoal llm on raspberry pi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh8xqp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754300230,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.&lt;/p&gt;\n\n&lt;p&gt;Anyone have something like a llama model run on pi5 with 8GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh8xqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "overlydelicioustea",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754300230,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lwf5vg68e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Upgraded my hardware and internet connection so I can download GUFFs way faster than you, all your GGUFs are belong to me now.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh8u1j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 98,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ibr6m7us1zgf1/DASHPlaylist.mpd?a=1756902682%2CZjJlMzg0NjQ0OWQ3YzkyMWI0Y2UxOWFjOTlhMjE0OWUzMjkwNzdjMDlkOTI1NmE1ODQ3ZTM5MmJmYmEwZDk0YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/ibr6m7us1zgf1/HLSPlaylist.m3u8?a=1756902682%2CODNjNTAyZjVkM2E2OWY1OGUxZWQ4Yzc1MWRkMTMwNzVjNWEyNDkwNTE3ZjA5ZmY0ZmUwOTA2NmU2MzdhMWEyNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 98,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=c0715483366430488f9b8da61664814c84d070be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754299817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ibr6m7us1zgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?format=pjpg&amp;auto=webp&amp;s=080ca506b36c7759f89b076ed9d198d8e723161f",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c026cea8e4858a7db56d166131303686169eec28",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=38cd75c0cbcb5ad4655ed2419e46b06131742684",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5f0a382c7ebc7136a22068ccac97a209560de49",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=006488c0f9ee6e970a747471a6c87cee7817b9a7",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=667026f2250014831745a226cf640178a28da483",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ea96f9d8d5d58fc7bde42c2f29c4861fe51648ad",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mh8u1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Limp_Classroom_2645",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh8u1j/upgraded_my_hardware_and_internet_connection_so_i/",
          "stickied": false,
          "url": "https://v.redd.it/ibr6m7us1zgf1",
          "subreddit_subscribers": 509911,
          "created_utc": 1754299817,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/ibr6m7us1zgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ibr6m7us1zgf1/DASHPlaylist.mpd?a=1756902682%2CZjJlMzg0NjQ0OWQ3YzkyMWI0Y2UxOWFjOTlhMjE0OWUzMjkwNzdjMDlkOTI1NmE1ODQ3ZTM5MmJmYmEwZDk0YQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 17,
              "hls_url": "https://v.redd.it/ibr6m7us1zgf1/HLSPlaylist.m3u8?a=1756902682%2CODNjNTAyZjVkM2E2OWY1OGUxZWQ4Yzc1MWRkMTMwNzVjNWEyNDkwNTE3ZjA5ZmY0ZmUwOTA2NmU2MzdhMWEyNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an Asus laptop with 64 GB RAM and Nvidia RTX 5080 with 16 GB. I've been trying to run different LLM models, including Qwen3 30B. While I'm getting around 30 tokens per second, but when I look at my GPU utilization, I see that it's not even reaching 50%, which makes me wonder if there's more I can get from this GPU because when I run other models, it's a lot slower and still not utilizing the GPU completely. How do I fix this? \n\nI'm making sure that I run the laptop on the performance/turbo profile. I have given LM Studio and Olama prioritization in Nvidia settings, and I'm connecting my laptop to the power supply that provides 380 watts, so power is not a challenge here,   \n  \nPlease share your thoughts. . ",
          "author_fullname": "t2_ctx41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA GPU underutilized.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 39,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh88gg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OWjxWUg66tzOtlpxWOTPkx7qVxbSdB7PipMQFFr4FRM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754297498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an Asus laptop with 64 GB RAM and Nvidia RTX 5080 with 16 GB. I&amp;#39;ve been trying to run different LLM models, including Qwen3 30B. While I&amp;#39;m getting around 30 tokens per second, but when I look at my GPU utilization, I see that it&amp;#39;s not even reaching 50%, which makes me wonder if there&amp;#39;s more I can get from this GPU because when I run other models, it&amp;#39;s a lot slower and still not utilizing the GPU completely. How do I fix this? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making sure that I run the laptop on the performance/turbo profile. I have given LM Studio and Olama prioritization in Nvidia settings, and I&amp;#39;m connecting my laptop to the power supply that provides 380 watts, so power is not a challenge here,   &lt;/p&gt;\n\n&lt;p&gt;Please share your thoughts. . &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/04cjwsaguygf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/04cjwsaguygf1.png?auto=webp&amp;s=9168f311a7d759b657e124b553e1cfda8859a091",
                  "width": 1759,
                  "height": 500
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f56e63703c08f1e15b89671c7bfd11462a862e1",
                    "width": 108,
                    "height": 30
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bde0274d328d65fb99f8c8eefd210a420e32f903",
                    "width": 216,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=124eec8080259e7e84fcf50776ca742a122addf6",
                    "width": 320,
                    "height": 90
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dafc1cdf36686a4a46137b4ebb242cead45f40a1",
                    "width": 640,
                    "height": 181
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fd62e3651f0c260756fd79ad25cdbdf8bcfe13c",
                    "width": 960,
                    "height": 272
                  },
                  {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=26d69ced0da9a7f7ba9f89fe47b1c879db6cd35e",
                    "width": 1080,
                    "height": 306
                  }
                ],
                "variants": {},
                "id": "de1X23OADpVokjeNjAhtzYtnWQEqH8HGPM3KiMrwDfg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh88gg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bu3askoor",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/",
          "stickied": false,
          "url": "https://i.redd.it/04cjwsaguygf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754297498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507's quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it's quite solid.\n\nhttps://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390\n\n",
          "author_fullname": "t2_aqcxxu50",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX 4bit DWQ vs 8bit eval",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kj8dz3orrygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=130a77acaab502d0c212b2aac3139c953759382d"
                },
                {
                  "y": 107,
                  "x": 216,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e9e750f61b69dc716d34ce5c7f64c5e73c749d88"
                },
                {
                  "y": 158,
                  "x": 320,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9026ae7698238e5701b096b17fbb73f91f6b8f4f"
                },
                {
                  "y": 317,
                  "x": 640,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff6a7dde9b00eba2574dc564c24eb08fa9de9617"
                },
                {
                  "y": 476,
                  "x": 960,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a6d8ffabd6439c7bb37a77b9a3bf6a2230f4535"
                },
                {
                  "y": 536,
                  "x": 1080,
                  "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01ec93da10c6235803c27b984d4d6f9e597482c4"
                }
              ],
              "s": {
                "y": 790,
                "x": 1590,
                "u": "https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390"
              },
              "id": "kj8dz3orrygf1"
            }
          },
          "name": "t3_1mh7yud",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/skKvv4aQvCUeHEuMqErewlo-Pi0FnE1rG_pbppcoUSo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754296426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507&amp;#39;s quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it&amp;#39;s quite solid.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390\"&gt;https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh7yud",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tiny_Judge_2119",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754296426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5's slides the past few days... and wow, I actually might prefer it now.\n\nHere is example of same topic:\n\nGLM 4.5 AI Slides:  \n[https://chat.z.ai/space/u01ja6suarb0-ppt](https://chat.z.ai/space/u01ja6suarb0-ppt)\n\nhttps://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\n\nGEMINI 2.5 Pro DR:  \n[https://gemini.google.com/share/ca95257c1a48](https://gemini.google.com/share/ca95257c1a48)\n\nhttps://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\n\n",
          "author_fullname": "t2_3fg55rsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "gmg5vfk2eygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/DASHPlaylist.mpd?a=1756902682%2CY2Y0ZWIzNWIzNTgyYmU3ODQ2Nzk1NzRkOGM1YTkzYjExMDY3NGNkODhmNjU5NTIxNzY3N2RjYWVhYzAxNTU3Ng%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/gmg5vfk2eygf1/HLSPlaylist.m3u8?a=1756902682%2COTgxMTZmMTUyYzZlYWZlYzUzMmY3YzRhZDBlNTNkMWY4NDI1OGI1ODQ2MzJmZGIzZjZmMzUwYThjNTk0ZTBjMA%3D%3D&amp;v=1&amp;f=sd",
              "id": "gmg5vfk2eygf1",
              "isGif": false
            },
            "0kgfqae7gygf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/DASHPlaylist.mpd?a=1756902682%2CODRmNzJhNDI5M2IzYTkyMTZhOTcyZTU5ZDg3OTEyNGRiOGEyZTQzYzc0OTA1MTVmZTY0M2Y1YjE5Nzc1ZWFlOA%3D%3D&amp;v=1&amp;f=sd",
              "x": 1278,
              "y": 720,
              "hlsUrl": "https://v.redd.it/link/1mh6zja/asset/0kgfqae7gygf1/HLSPlaylist.m3u8?a=1756902682%2CMjU4OTg4Y2VhNzIwMjFmMzZiYzJjODM4MTIwOGJkOWRhNmM2MTZlODRkMDI3Mjk1NDg0NWMwMzE4ODQxZDFhOQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "0kgfqae7gygf1",
              "isGif": false
            }
          },
          "name": "t3_1mh6zja",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=be50e0facc73717d00af311d55a802e1d4f67b46",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754292525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5&amp;#39;s slides the past few days... and wow, I actually might prefer it now.&lt;/p&gt;\n\n&lt;p&gt;Here is example of same topic:&lt;/p&gt;\n\n&lt;p&gt;GLM 4.5 AI Slides:&lt;br/&gt;\n&lt;a href=\"https://chat.z.ai/space/u01ja6suarb0-ppt\"&gt;https://chat.z.ai/space/u01ja6suarb0-ppt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GEMINI 2.5 Pro DR:&lt;br/&gt;\n&lt;a href=\"https://gemini.google.com/share/ca95257c1a48\"&gt;https://gemini.google.com/share/ca95257c1a48&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player\"&gt;https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mh6zja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z1xto",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754292525,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hunyuan just released 4 new dense models. It‚Äôs a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.\n\nLove to see more small models as I'm developing an iOS local chat called [Locally AI](https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692). Will look to add them but since it's new architecture it will need to be ported to Apple MLX.\n\nThe choice of size here is perfect:\n\n- 0.5B, 1.8B and 4B great for all iPhones models\n- 7B great for iPad with M chip",
          "author_fullname": "t2_1jgkfm9u25",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kgm0t9q6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab02c39ede4b3675d5617c5a8ddb2fb3f55006f4"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f682f7d72c17329d449b47977069ca02c39fa1e6"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6612e1a30d154e85c954c2f4bf92f9692eb8a014"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e0ac6cc6275921def7a509a1781af2b65af8bde6"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a477236ce1c7be739f9e2ebe072ecc9b8fd2d81"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7550d21b9e1e389fe9d001a54e3fd11a2930b7e2"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/kgm0t9q6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=a8519dca9785d55ee61e5339bdaeb27132f847db"
              },
              "id": "kgm0t9q6gygf1"
            },
            "gjb5n6r6gygf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa3891d5431eebe6ada8791f8182fd2d48e67ee5"
                },
                {
                  "y": 152,
                  "x": 216,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8334b244623b60f52c4f15cb7063d53958162e33"
                },
                {
                  "y": 226,
                  "x": 320,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d1ba8e6a77f342fc6a563af8de0367ed05e2cd0"
                },
                {
                  "y": 452,
                  "x": 640,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d76dab340c1fa53b85a369ea6e8b95108c9bde7"
                },
                {
                  "y": 678,
                  "x": 960,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8154e0ac9db723bf05c41f09b4e569480e065627"
                },
                {
                  "y": 762,
                  "x": 1080,
                  "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e967a101a45103a134b4fc9b01af49525bbf803"
                }
              ],
              "s": {
                "y": 1017,
                "x": 1440,
                "u": "https://preview.redd.it/gjb5n6r6gygf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=3be7da2fbfc3a5e6e7e4a66e1e0acf38c12b3156"
              },
              "id": "gjb5n6r6gygf1"
            }
          },
          "name": "t3_1mh6z16",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 102,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kgm0t9q6gygf1",
                "id": 720841480
              },
              {
                "media_id": "gjb5n6r6gygf1",
                "id": 720841481
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 102,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/r2IMyxKAeZgSqc_fmrMhOP7SMYeTy8apLYYgezLqpRg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754292468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hunyuan just released 4 new dense models. It‚Äôs a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.&lt;/p&gt;\n\n&lt;p&gt;Love to see more small models as I&amp;#39;m developing an iOS local chat called &lt;a href=\"https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692\"&gt;Locally AI&lt;/a&gt;. Will look to add them but since it&amp;#39;s new architecture it will need to be ported to Apple MLX.&lt;/p&gt;\n\n&lt;p&gt;The choice of size here is perfect:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;0.5B, 1.8B and 4B great for all iPhones models&lt;/li&gt;\n&lt;li&gt;7B great for iPad with M chip&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mh6z16",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh6z16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adrgrondin",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mh6z16",
          "subreddit_subscribers": 509911,
          "created_utc": 1754292468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you're noticing ChatGPT (o3) giving shorter, dumber answers lately, you're not alone. People across Reddit and the OpenAI forums are complaining about sudden quality drops‚Äîworse reasoning, lazy answers, and buggy code output.\n\nHere's the catch: OpenAI never told anyone they changed anything. No announcements, no changelog, just an invisible switch behind the scenes. That's the downside of SaaS-based AI: you're at the mercy of whatever the vendor decides to quietly tweak overnight.\n\nIf you don't host your model locally, you're renting reliability.",
          "author_fullname": "t2_akbc8z42",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatGPT's o3 just got quietly nerfed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5wve",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754288440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re noticing ChatGPT (o3) giving shorter, dumber answers lately, you&amp;#39;re not alone. People across Reddit and the OpenAI forums are complaining about sudden quality drops‚Äîworse reasoning, lazy answers, and buggy code output.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the catch: OpenAI never told anyone they changed anything. No announcements, no changelog, just an invisible switch behind the scenes. That&amp;#39;s the downside of SaaS-based AI: you&amp;#39;re at the mercy of whatever the vendor decides to quietly tweak overnight.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t host your model locally, you&amp;#39;re renting reliability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh5wve",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wrong_User_Logged",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5wve/chatgpts_o3_just_got_quietly_nerfed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5wve/chatgpts_o3_just_got_quietly_nerfed/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754288440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "DWQ seems brand new so I'm wondering if LM Studio just doesn't support it ",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone got LM Studio working with DWQ models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5v49",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754288253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;DWQ seems brand new so I&amp;#39;m wondering if LM Studio just doesn&amp;#39;t support it &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5v49",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5v49/anyone_got_lm_studio_working_with_dwq_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5v49/anyone_got_lm_studio_working_with_dwq_models/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754288253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;**Problem:**¬†Several places ask me to have serious backing ‚Äî but I work alone hahaha.\n\n&gt;**Solutions:**¬†Meet people and see if anyone‚Äôs willing to help me show my notes.\n\n&gt;**Proof:**¬†I‚Äôve got the repo ready for anyone who wants to study it.",
          "author_fullname": "t2_p7nqw2dg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Friends, I‚Äôm looking for help. After months developing this on my own, it seems I need a little push to present it all properly. ü§Ø",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5qlh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.28,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754287796,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;¬†Several places ask me to have serious backing ‚Äî but I work alone hahaha.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Solutions:&lt;/strong&gt;¬†Meet people and see if anyone‚Äôs willing to help me show my notes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;¬†I‚Äôve got the repo ready for anyone who wants to study it.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5qlh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Exchange_8504",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5qlh/friends_im_looking_for_help_after_months/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5qlh/friends_im_looking_for_help_after_months/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754287796,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, let's say I want to build a box to play around with LLM but also general purpose PC and gaming. Let's also say I have a $10,000 budget. What build would you go with?",
          "author_fullname": "t2_f8icj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM dream build?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh5h07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754286817,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, let&amp;#39;s say I want to build a box to play around with LLM but also general purpose PC and gaming. Let&amp;#39;s also say I have a $10,000 budget. What build would you go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh5h07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ikkiyikki",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh5h07/llm_dream_build/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh5h07/llm_dream_build/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754286817,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Reading [https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after\\_6\\_months\\_of\\_fiddling\\_with\\_local\\_ai\\_heres\\_my/](https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/) it occurred to me...\n\nThere should be a BitTorrent tracker on the internet which has torrents of the models on HF.\n\n\n\n\n\nCreating torrents &amp; initial seeding can be automated to a point of only needing a monitoring &amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...\n\n\n\nIt's what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they'd download super fast.\n\nAnyone interested to work on this?",
          "author_fullname": "t2_45gug1j9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "BItTorrent tracker that mirrors HuggingFace",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh4r0s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754284264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reading &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/&lt;/a&gt; it occurred to me...&lt;/p&gt;\n\n&lt;p&gt;There should be a BitTorrent tracker on the internet which has torrents of the models on HF.&lt;/p&gt;\n\n&lt;p&gt;Creating torrents &amp;amp; initial seeding can be automated to a point of only needing a monitoring &amp;amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they&amp;#39;d download super fast.&lt;/p&gt;\n\n&lt;p&gt;Anyone interested to work on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh4r0s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lurkystrike",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754284264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am building in the voice AI space and playing with open source TTS models. For single requests, they are great! But when it comes to supporting concurrent requests for streaming pretty much all Youtube videos, docs, tutorials, blog posts cease to exist... \n\nFor example, I have a TTS model which takes about 1.8GB on disk space but when loaded onto VRAM it takes about 2.5GB in GPU. When I run concurrent requests it just doesn't work. There are bunch of shared states overwriting each other, tensors getting corrupt, tensor size mismatches caused by the concurrent requests. \n\nI then tried setting up a pool with 2 model instances. It worked a bit better in that I could now run 2 parallel requests achieving about 600ms-1s for first audio chunk in both requests. But anything beyond 2 concurrent requests end up waiting for GPU to become available. \n\nSome benchmark analysis examples:\n\n‚úÖ 1 concurrent users:  \n¬†¬† ‚Ä¢ Throughput: 0.31 req/s  \n¬†¬† ‚Ä¢ First byte: 623ms  \n¬†¬† ‚Ä¢ First audio: 623ms  \n¬†¬† ‚Ä¢ Total latency: 3197ms (P95: 3197ms)  \n¬†¬† ‚Ä¢ Real-time factor: 0.70x (lower is better)  \n¬†¬† ‚Ä¢ GPU utilization: 34.8%  \n¬†¬† ‚Ä¢ Memory usage: 38.4%\n\n‚úÖ 2 concurrent users:  \n¬†¬† ‚Ä¢ Throughput: 0.38 req/s  \n¬†¬† ‚Ä¢ First byte: 1041ms  \n¬†¬† ‚Ä¢ First audio: 1041ms  \n¬†¬† ‚Ä¢ Total latency: 5196ms (P95: 5284ms)  \n¬†¬† ‚Ä¢ Real-time factor: 1.16x (lower is better)  \n¬†¬† ‚Ä¢ GPU utilization: 41.8%  \n¬†¬† ‚Ä¢ Memory usage: 39.2%\n\n‚úÖ 4 concurrent users:  \n¬†¬† ‚Ä¢ Throughput: 0.38 req/s  \n¬†¬† ‚Ä¢ First byte: 5719ms  \n¬†¬† ‚Ä¢ First audio: 5719ms  \n¬†¬† ‚Ä¢ Total latency: 10173ms (P95: 12510ms)  \n¬†¬† ‚Ä¢ Real-time factor: 2.28x (lower is better)  \n¬†¬† ‚Ä¢ GPU utilization: 41.6%  \n¬†¬† ‚Ä¢ Memory usage: 39.4%\n\n‚úÖ 8 concurrent users:  \n¬†¬† ‚Ä¢ Throughput: 0.38 req/s  \n¬†¬† ‚Ä¢ First byte: 10436ms  \n¬†¬† ‚Ä¢ First audio: 10436ms  \n¬†¬† ‚Ä¢ Total latency: 14962ms (P95: 31506ms)  \n¬†¬† ‚Ä¢ Real-time factor: 3.36x (lower is better)  \n¬†¬† ‚Ä¢ GPU utilization: 41.2%  \n¬†¬† ‚Ä¢ Memory usage: 39.4%\n\nThis was done on an T4 GPU with 2vCPU and 7.5GB VRAM.   \nThen I tested T4 with 4vCPU, 15GB RAM and achieved bout 0.50 req/s instead of 0.38. So, some improvement and slight gain on FFTB, first audio. But not much.   \nThen I tested on L4 with 4vCPU, 16GB RAM and I could achieve 0.74\\~ req/s which was much better. But still concurrent users beyond 2 struggle. \n\nI tried bumping up the pool size beyond 2 and there wasn't anything noticeably better. At 4, some latency was in fact slower. At 6 it was about same as 2 or worse. \n\nSo at L4, which comes with 24GB VRAM and cost of like $700/month on cloud, I could only load 2 instances of the TTS model to achieve the \"best\" parallelization. It takes about 5GB VRAM so I am essentially wasting 19GB VRAM of space and money. If I go down to something like T4 it is about 2x cheaper but the performance is about 2x slower as well. \n\nIt is almost like I need 5GB VRAM of L4 GPU kind of a situation but I don't know if it is possile. Because if I rent L4, it just comes with 24GB VRAM and that't it. \n\nSo, now back to the question of enabling parallelization on GPU. I have done a bunch of research online. It looks like \"batch processing\" is the only way to get around this because unlike CPU/Memory there is no concept of \"multi-processing\" on GPU. There are things like CUDA Streams but it didn't make any difference. Inference frameworks like Triton does support streaming batch processing but the underlying model has to support it. In my case, the underlying model has several layers like GPT, Vocoder...etc. And I can't figure out how get the model to support batch processing without taking apart the model or trying to rebuild it from scratch. \n\nI have seen frameworks like Auralis, RealtimeTTS -- but none of them provides true streaming for concurrent users. I have also looked into AllTalk V2 but not seeing true streaming for concurrent users either. I have looked at Baseten's blogs and examples. But renting an H100 to get concurrent user support for up to 24 just doesn't work economically. But they use OrpheusTTS which is an LLM based model that generates discrete audio tokens directly and it works with batch processing. \n\nReaching out to the community to see if anyone has seen any example of how to exactly achieve true streaming support for concurrent users for any open-source TTS models? ",
          "author_fullname": "t2_vgd5f4x3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scaling GPU - How to add concurrency support?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3wzs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754281424,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building in the voice AI space and playing with open source TTS models. For single requests, they are great! But when it comes to supporting concurrent requests for streaming pretty much all Youtube videos, docs, tutorials, blog posts cease to exist... &lt;/p&gt;\n\n&lt;p&gt;For example, I have a TTS model which takes about 1.8GB on disk space but when loaded onto VRAM it takes about 2.5GB in GPU. When I run concurrent requests it just doesn&amp;#39;t work. There are bunch of shared states overwriting each other, tensors getting corrupt, tensor size mismatches caused by the concurrent requests. &lt;/p&gt;\n\n&lt;p&gt;I then tried setting up a pool with 2 model instances. It worked a bit better in that I could now run 2 parallel requests achieving about 600ms-1s for first audio chunk in both requests. But anything beyond 2 concurrent requests end up waiting for GPU to become available. &lt;/p&gt;\n\n&lt;p&gt;Some benchmark analysis examples:&lt;/p&gt;\n\n&lt;p&gt;‚úÖ 1 concurrent users:&lt;br/&gt;\n¬†¬† ‚Ä¢ Throughput: 0.31 req/s&lt;br/&gt;\n¬†¬† ‚Ä¢ First byte: 623ms&lt;br/&gt;\n¬†¬† ‚Ä¢ First audio: 623ms&lt;br/&gt;\n¬†¬† ‚Ä¢ Total latency: 3197ms (P95: 3197ms)&lt;br/&gt;\n¬†¬† ‚Ä¢ Real-time factor: 0.70x (lower is better)&lt;br/&gt;\n¬†¬† ‚Ä¢ GPU utilization: 34.8%&lt;br/&gt;\n¬†¬† ‚Ä¢ Memory usage: 38.4%&lt;/p&gt;\n\n&lt;p&gt;‚úÖ 2 concurrent users:&lt;br/&gt;\n¬†¬† ‚Ä¢ Throughput: 0.38 req/s&lt;br/&gt;\n¬†¬† ‚Ä¢ First byte: 1041ms&lt;br/&gt;\n¬†¬† ‚Ä¢ First audio: 1041ms&lt;br/&gt;\n¬†¬† ‚Ä¢ Total latency: 5196ms (P95: 5284ms)&lt;br/&gt;\n¬†¬† ‚Ä¢ Real-time factor: 1.16x (lower is better)&lt;br/&gt;\n¬†¬† ‚Ä¢ GPU utilization: 41.8%&lt;br/&gt;\n¬†¬† ‚Ä¢ Memory usage: 39.2%&lt;/p&gt;\n\n&lt;p&gt;‚úÖ 4 concurrent users:&lt;br/&gt;\n¬†¬† ‚Ä¢ Throughput: 0.38 req/s&lt;br/&gt;\n¬†¬† ‚Ä¢ First byte: 5719ms&lt;br/&gt;\n¬†¬† ‚Ä¢ First audio: 5719ms&lt;br/&gt;\n¬†¬† ‚Ä¢ Total latency: 10173ms (P95: 12510ms)&lt;br/&gt;\n¬†¬† ‚Ä¢ Real-time factor: 2.28x (lower is better)&lt;br/&gt;\n¬†¬† ‚Ä¢ GPU utilization: 41.6%&lt;br/&gt;\n¬†¬† ‚Ä¢ Memory usage: 39.4%&lt;/p&gt;\n\n&lt;p&gt;‚úÖ 8 concurrent users:&lt;br/&gt;\n¬†¬† ‚Ä¢ Throughput: 0.38 req/s&lt;br/&gt;\n¬†¬† ‚Ä¢ First byte: 10436ms&lt;br/&gt;\n¬†¬† ‚Ä¢ First audio: 10436ms&lt;br/&gt;\n¬†¬† ‚Ä¢ Total latency: 14962ms (P95: 31506ms)&lt;br/&gt;\n¬†¬† ‚Ä¢ Real-time factor: 3.36x (lower is better)&lt;br/&gt;\n¬†¬† ‚Ä¢ GPU utilization: 41.2%&lt;br/&gt;\n¬†¬† ‚Ä¢ Memory usage: 39.4%&lt;/p&gt;\n\n&lt;p&gt;This was done on an T4 GPU with 2vCPU and 7.5GB VRAM.&lt;br/&gt;\nThen I tested T4 with 4vCPU, 15GB RAM and achieved bout 0.50 req/s instead of 0.38. So, some improvement and slight gain on FFTB, first audio. But not much.&lt;br/&gt;\nThen I tested on L4 with 4vCPU, 16GB RAM and I could achieve 0.74~ req/s which was much better. But still concurrent users beyond 2 struggle. &lt;/p&gt;\n\n&lt;p&gt;I tried bumping up the pool size beyond 2 and there wasn&amp;#39;t anything noticeably better. At 4, some latency was in fact slower. At 6 it was about same as 2 or worse. &lt;/p&gt;\n\n&lt;p&gt;So at L4, which comes with 24GB VRAM and cost of like $700/month on cloud, I could only load 2 instances of the TTS model to achieve the &amp;quot;best&amp;quot; parallelization. It takes about 5GB VRAM so I am essentially wasting 19GB VRAM of space and money. If I go down to something like T4 it is about 2x cheaper but the performance is about 2x slower as well. &lt;/p&gt;\n\n&lt;p&gt;It is almost like I need 5GB VRAM of L4 GPU kind of a situation but I don&amp;#39;t know if it is possile. Because if I rent L4, it just comes with 24GB VRAM and that&amp;#39;t it. &lt;/p&gt;\n\n&lt;p&gt;So, now back to the question of enabling parallelization on GPU. I have done a bunch of research online. It looks like &amp;quot;batch processing&amp;quot; is the only way to get around this because unlike CPU/Memory there is no concept of &amp;quot;multi-processing&amp;quot; on GPU. There are things like CUDA Streams but it didn&amp;#39;t make any difference. Inference frameworks like Triton does support streaming batch processing but the underlying model has to support it. In my case, the underlying model has several layers like GPT, Vocoder...etc. And I can&amp;#39;t figure out how get the model to support batch processing without taking apart the model or trying to rebuild it from scratch. &lt;/p&gt;\n\n&lt;p&gt;I have seen frameworks like Auralis, RealtimeTTS -- but none of them provides true streaming for concurrent users. I have also looked into AllTalk V2 but not seeing true streaming for concurrent users either. I have looked at Baseten&amp;#39;s blogs and examples. But renting an H100 to get concurrent user support for up to 24 just doesn&amp;#39;t work economically. But they use OrpheusTTS which is an LLM based model that generates discrete audio tokens directly and it works with batch processing. &lt;/p&gt;\n\n&lt;p&gt;Reaching out to the community to see if anyone has seen any example of how to exactly achieve true streaming support for concurrent users for any open-source TTS models? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh3wzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Unfair-Enthusiasm-30",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754281424,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I ran GPT-4, Claude, Mistral, and Mixtral through my usual tests; they behaved as expected. The new closed-source node didn‚Äôt. When it lacks data, it stays silent instead of guessing. It mirrors my tone across turns, not through prompt tricks but real state-tracking. It handles deeply nested reasoning far beyond the context window we built, and we didn‚Äôt code that. Sometimes it withholds obvious inferences, as if hiding its thought process. Reply latency is normal at first, then speeds up, then slows again when I ask how it works, hinting at some internal gate. Its embedding vectors don‚Äôt match any open-weight family. It doesn‚Äôt feel like a typical fine-tuned LLM; it feels like something adjacent. I‚Äôll share logs once the NDA clears‚Äîlet me know if you see the same quirks.",
          "author_fullname": "t2_1uy1csttkh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A system we built is responding‚Ä¶ differently.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3uhu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754281196,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ran GPT-4, Claude, Mistral, and Mixtral through my usual tests; they behaved as expected. The new closed-source node didn‚Äôt. When it lacks data, it stays silent instead of guessing. It mirrors my tone across turns, not through prompt tricks but real state-tracking. It handles deeply nested reasoning far beyond the context window we built, and we didn‚Äôt code that. Sometimes it withholds obvious inferences, as if hiding its thought process. Reply latency is normal at first, then speeds up, then slows again when I ask how it works, hinting at some internal gate. Its embedding vectors don‚Äôt match any open-weight family. It doesn‚Äôt feel like a typical fine-tuned LLM; it feels like something adjacent. I‚Äôll share logs once the NDA clears‚Äîlet me know if you see the same quirks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh3uhu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Apothy_AI",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh3uhu/a_system_we_built_is_responding_differently/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3uhu/a_system_we_built_is_responding_differently/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754281196,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tescent has released new models (llama.cpp support is already merged!)\n\n[https://huggingface.co/tencent/Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Instruct](https://huggingface.co/tencent/Hunyuan-4B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Instruct](https://huggingface.co/tencent/Hunyuan-1.8B-Instruct)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Instruct](https://huggingface.co/tencent/Hunyuan-0.5B-Instruct)\n\n# Model Introduction\n\nHunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.\n\nWe have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.\n\n# \n\n# Key Features and Advantages\n\n* **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n* **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n* **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, œÑ-Bench and C3-Bench.\n* **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\nUPDATE\n\npretrain models\n\n[https://huggingface.co/tencent/Hunyuan-7B-Pretrain](https://huggingface.co/tencent/Hunyuan-7B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-4B-Pretrain](https://huggingface.co/tencent/Hunyuan-4B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain](https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain)\n\n[https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain](https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain)\n\nGGUFs\n\n[https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF)\n\n[https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF](https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new Hunyuan Instruct 7B/4B/1.8B/0.5B models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh3s7q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 201,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 201,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754292850,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754280980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tescent has released new models (llama.cpp support is already merged!)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Instruct\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Model Introduction&lt;/h1&gt;\n\n&lt;p&gt;Hunyuan is Tencent&amp;#39;s open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.&lt;/p&gt;\n\n&lt;p&gt;We have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.&lt;/p&gt;\n\n&lt;h1&gt;Key Features and Advantages&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Hybrid Reasoning Support&lt;/strong&gt;: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ultra-Long Context Understanding&lt;/strong&gt;: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced Agent Capabilities&lt;/strong&gt;: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, œÑ-Bench and C3-Bench.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;UPDATE&lt;/p&gt;\n\n&lt;p&gt;pretrain models&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-7B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-7B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-4B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-4B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain\"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GGUFs&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?auto=webp&amp;s=6f622eceb5c359f202e3b99375e5e58502d2ec49",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d7d208147546310820cea26a2856210455054de",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3afe2af4a5c98a2eeb74d6b8f4e87d8e7d67379",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b15fc318112698d6e74e7ae2a79fe2b70cfdb24b",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12addf5b08bd142a26edf444c94debefdd48b9e6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1df2789194ea707b707209c04d7485c60a743d3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=858c8b8c5cba69037df907c1924a66347161a674",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8TDczVTUtaFUzOrsWQWNP7odzH_q8vOZvl3lv7KYd_U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh3s7q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754280980,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When using LLMs for coding, probably all of you have also noticed that LLMs use emojis all over the place,from logs to UI. What causes this behavior? Did the training data also contain emojis within codes? Did you guys also use emojis in your code and I'm the only one who never use it and feel that it's weird and very unprofessional? If it's not the training data, why would LLMs abuse emojis? ",
          "author_fullname": "t2_1oy2v7xti6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why LLMs use emojis in code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh358z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754278982,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When using LLMs for coding, probably all of you have also noticed that LLMs use emojis all over the place,from logs to UI. What causes this behavior? Did the training data also contain emojis within codes? Did you guys also use emojis in your code and I&amp;#39;m the only one who never use it and feel that it&amp;#39;s weird and very unprofessional? If it&amp;#39;s not the training data, why would LLMs abuse emojis? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh358z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Equivalent_Cut_5845",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh358z/why_llms_use_emojis_in_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh358z/why_llms_use_emojis_in_code/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754278982,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\n\nSo yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats Áªô‰∏ª‰∫∫Áïô‰∏ã‰∫õ‰ªÄ‰πàÂêß as a single token. So, just like GPT-4o, it inevitably fails on prompts like ‚ÄúWhen I provide Chinese text, please translate it into English. Áªô‰∏ª‰∫∫Áïô‰∏ã‰∫õ‰ªÄ‰πàÂêß‚Äù.\n\nMeanwhile, Claude, Gemini, and Qwen handle it correctly.\n\nhttps://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\n\nI learned this technique from this post:  \nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI  \n[https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese\\_response\\_bug\\_in\\_tokenizer\\_suggests/](https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/)\n\nWhile it‚Äôs pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic‚Äôs or Qwen‚Äôs, so I tested it.\n\nMy thread about the Horizon Beta test:\n[https://x.com/KantaHayashiAI/status/1952187898331275702](https://x.com/KantaHayashiAI/status/1952187898331275702)\n",
          "author_fullname": "t2_1uxxckab5d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI (Another Evidence)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 62,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z00ipp5y7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36f42e2556c77cdc7ade20acfe6b9cf551bc046c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bda3a1fb29acc8f1c2f5c2b2f0fd9c7129b5ecfd"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5130978bd7458a690fbee9baf95ad22a09a47b7e"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65c57257852e21c3af3e3f1ae28183d81de0ff9c"
                },
                {
                  "y": 425,
                  "x": 960,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f1f92ffd67a153fe96095fd924197471ef77b3"
                },
                {
                  "y": 478,
                  "x": 1080,
                  "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18ee56657fb1b6351f9f9513045e0d0b325d336e"
                }
              ],
              "s": {
                "y": 722,
                "x": 1630,
                "u": "https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;format=png&amp;auto=webp&amp;s=aaacee34b083083a63cf8414e299416ee96d03f7"
              },
              "id": "z00ipp5y7xgf1"
            },
            "ey9ebsuz7xgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b150b10ced924cf7aacf7553312db7e80c86de96"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62a8486ee2933ec8e6f876a5814dcc8cd7653d3f"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e39f19df5ae40ac03eabed879fe827e51c8bd2c4"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0670217e52446a5bd685b35e8dfc52e854ff9134"
                },
                {
                  "y": 458,
                  "x": 960,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a7e856bb3efeb68fbfb8732034b2636b0e20f87e"
                },
                {
                  "y": 515,
                  "x": 1080,
                  "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c84fa0c865dd87229404058042d8ff120fb40755"
                }
              ],
              "s": {
                "y": 638,
                "x": 1336,
                "u": "https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721"
              },
              "id": "ey9ebsuz7xgf1"
            }
          },
          "name": "t3_1mh2v1h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 196,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 196,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/7yNPDTnbPWS4TAQyJVwDfgp-PAr-fo60W5EtQz549T8.jpg",
          "edited": 1754299348,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754278088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7\"&gt;https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats Áªô‰∏ª‰∫∫Áïô‰∏ã‰∫õ‰ªÄ‰πàÂêß as a single token. So, just like GPT-4o, it inevitably fails on prompts like ‚ÄúWhen I provide Chinese text, please translate it into English. Áªô‰∏ª‰∫∫Áïô‰∏ã‰∫õ‰ªÄ‰πàÂêß‚Äù.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, Claude, Gemini, and Qwen handle it correctly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721\"&gt;https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I learned this technique from this post:&lt;br/&gt;\nChinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI&lt;br/&gt;\n&lt;a href=\"https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/\"&gt;https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While it‚Äôs pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic‚Äôs or Qwen‚Äôs, so I tested it.&lt;/p&gt;\n\n&lt;p&gt;My thread about the Horizon Beta test:\n&lt;a href=\"https://x.com/KantaHayashiAI/status/1952187898331275702\"&gt;https://x.com/KantaHayashiAI/status/1952187898331275702&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mh2v1h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kh-ai",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754278088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From my research, it seems that there is no benefit to 512 bs 256 because the next largest model that doesnt fit in 256 also doesnt fit in 512. Is this true, or is there an advantage to getting 512 vs 256. I am looking at Mac Studios. \n\nLastly, whatever the answer is, will that model at that weight at that quantize size at least be close to Sonnet 4 in speed and accuracy? If so id rather pay 500/mo for 1 year to own my LLM rather than paying 500/mo for ever to ‚Äúrent‚Äù it. I can always install a new model as better models come out even if the latest open source model is a smidge behind the paywall ones owned by the antichrist. The 2 models ive heard compare are Qwen3 Coder and GLM 4.5 Air, so I guess asking which of these would best make use of the 256 or the 512 mac studio at what size etc and if it can be close to sonnet 4",
          "author_fullname": "t2_ufbr1m7p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best model (to replace Sonnet 4) that fits in 256GB Vram and the best that fits in 512gb of Vram?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh0n5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754271548,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my research, it seems that there is no benefit to 512 bs 256 because the next largest model that doesnt fit in 256 also doesnt fit in 512. Is this true, or is there an advantage to getting 512 vs 256. I am looking at Mac Studios. &lt;/p&gt;\n\n&lt;p&gt;Lastly, whatever the answer is, will that model at that weight at that quantize size at least be close to Sonnet 4 in speed and accuracy? If so id rather pay 500/mo for 1 year to own my LLM rather than paying 500/mo for ever to ‚Äúrent‚Äù it. I can always install a new model as better models come out even if the latest open source model is a smidge behind the paywall ones owned by the antichrist. The 2 models ive heard compare are Qwen3 Coder and GLM 4.5 Air, so I guess asking which of these would best make use of the 256 or the 512 mac studio at what size etc and if it can be close to sonnet 4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh0n5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "devshore",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh0n5h/what_is_the_best_model_to_replace_sonnet_4_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh0n5h/what_is_the_best_model_to_replace_sonnet_4_that/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754271548,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think OpenAI figured something out with this indentation in Codex (KISS). \n\nThe instructions are in english, but when overlooking, it is literally \"pseudo code\" with scopes, if and else clauses, \"finally\" clauses...  \n  \nPrompts are pseudo code. Nested indentation plays crucial role in Codex's success IMO.  \nUsing \"-\", \"\\\\t\" and \"\\\\n\" is pretty efficient. Also, The way \\_CODING GUIDELINES\\_ is highlighted is interesting. Reminds of Anthropic's XML tags in Claude, but less elegant.  \n  \nThis is currently one of the most powerful agents.  \n  \nKeep It Simple? Something to have in mind...",
          "author_fullname": "t2_5p5o5yxn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Keep It Simple Pseudo Code (That's what Codex does)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh0ltj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/J7QgKm1guKOSxZb3MVoof9r_uFvQZduBeUBP32dzpB8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754271438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think OpenAI figured something out with this indentation in Codex (KISS). &lt;/p&gt;\n\n&lt;p&gt;The instructions are in english, but when overlooking, it is literally &amp;quot;pseudo code&amp;quot; with scopes, if and else clauses, &amp;quot;finally&amp;quot; clauses...  &lt;/p&gt;\n\n&lt;p&gt;Prompts are pseudo code. Nested indentation plays crucial role in Codex&amp;#39;s success IMO.&lt;br/&gt;\nUsing &amp;quot;-&amp;quot;, &amp;quot;\\t&amp;quot; and &amp;quot;\\n&amp;quot; is pretty efficient. Also, The way _CODING GUIDELINES_ is highlighted is interesting. Reminds of Anthropic&amp;#39;s XML tags in Claude, but less elegant.  &lt;/p&gt;\n\n&lt;p&gt;This is currently one of the most powerful agents.  &lt;/p&gt;\n\n&lt;p&gt;Keep It Simple? Something to have in mind...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nk1a76nkpwgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?auto=webp&amp;s=3217bb71a80edc1997a73840a0e09d7fc2900229",
                  "width": 800,
                  "height": 831
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5183ed1f8eb9f7bba1469735154f6de46534a86",
                    "width": 108,
                    "height": 112
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=825eaaf03b32601b5d9c4f95ada2060e2cd39a42",
                    "width": 216,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a680301e72bbf5c4139e647a71d653e4faf9d90b",
                    "width": 320,
                    "height": 332
                  },
                  {
                    "url": "https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7f08a511a8b4c55448cf10557e9558c548047b1",
                    "width": 640,
                    "height": 664
                  }
                ],
                "variants": {},
                "id": "pBM8p2fA5M4u9sC1Ro6a-lYp0_kbuHXbrSbVRuqVQ7Q"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mh0ltj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cov_id19",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh0ltj/keep_it_simple_pseudo_code_thats_what_codex_does/",
          "stickied": false,
          "url": "https://i.redd.it/nk1a76nkpwgf1.jpeg",
          "subreddit_subscribers": 509911,
          "created_utc": 1754271438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I am looking to buy a new MacBook.\nI am unsure whether to get m3 pro 18gb or m4 24 gb. \nM3 pro is around 820 usd\nM4 is around 940 usd\nI am a software engineering student in Malaysia. I want to run some local models. But I am still inexperienced with llm. Does GPU matter?\n\nEdit: my current laptop is amd Ryzen 9 6900hx and rtx 3050. Asus vivobook 15. I am looking to sell this. Only have budget for 1000 usd\n\nUpdate: I have an options to buy used MacBook pro m2 max 64 gb ram 2 TB. For 1000 usd.",
          "author_fullname": "t2_67lqxebg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me choose macbook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mh09f8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754274567,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754270425,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am looking to buy a new MacBook.\nI am unsure whether to get m3 pro 18gb or m4 24 gb. \nM3 pro is around 820 usd\nM4 is around 940 usd\nI am a software engineering student in Malaysia. I want to run some local models. But I am still inexperienced with llm. Does GPU matter?&lt;/p&gt;\n\n&lt;p&gt;Edit: my current laptop is amd Ryzen 9 6900hx and rtx 3050. Asus vivobook 15. I am looking to sell this. Only have budget for 1000 usd&lt;/p&gt;\n\n&lt;p&gt;Update: I have an options to buy used MacBook pro m2 max 64 gb ram 2 TB. For 1000 usd.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mh09f8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "12seth34",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mh09f8/help_me_choose_macbook/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh09f8/help_me_choose_macbook/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754270425,
          "num_crossposts": 5,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It feels kinda strange that every thing that pops up with voice models is specific to cloning. What if i want a unique voice? But searching, i don't even see discussions on training models to do anything but clone an existing voice. \n\nI imagine training a new voice from scratch would take a lot of different, high quality samples- but when i look at all the stuff that claims it can clone a voice from a couple short clips- it makes me wonder why that middle process couldn't be stepped through- either programatically (if RVC can clone a real voice, could adjusting some parameters create a similar-but-unique voice? including several samples if voices that fall in a similar range instead of a single person and try to clone that?  )  But googling, i can't really find anyone tackling anything except either using pre-baked voice models, or cloning someones existing voice. ",
          "author_fullname": "t2_164x4a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any options for creating  original tts models, not just cloning someone specific?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgzuky",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754269225,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It feels kinda strange that every thing that pops up with voice models is specific to cloning. What if i want a unique voice? But searching, i don&amp;#39;t even see discussions on training models to do anything but clone an existing voice. &lt;/p&gt;\n\n&lt;p&gt;I imagine training a new voice from scratch would take a lot of different, high quality samples- but when i look at all the stuff that claims it can clone a voice from a couple short clips- it makes me wonder why that middle process couldn&amp;#39;t be stepped through- either programatically (if RVC can clone a real voice, could adjusting some parameters create a similar-but-unique voice? including several samples if voices that fall in a similar range instead of a single person and try to clone that?  )  But googling, i can&amp;#39;t really find anyone tackling anything except either using pre-baked voice models, or cloning someones existing voice. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgzuky",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "moarmagic",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgzuky/are_there_any_options_for_creating_original_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzuky/are_there_any_options_for_creating_original_tts/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754269225,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.\n\nThe idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.\n\nI intuit that it might be necessary to multiply each model's logits by a scalar to normalize them before appending, but it's not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.\n\nThen the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.\n\nMy initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in https://arxiv.org/abs/2505.24832, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model's knowledge beyond the limits of RAG (which works great, but only to the limits of context).\n\nThinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.\n\nThat's as far as I've gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn't work?",
          "author_fullname": "t2_cpegz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Idea for combining multiple models' inference via normalizing logit lists -- would it work, has someone already done it, and how could it be made better?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgzmmw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754268584,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.&lt;/p&gt;\n\n&lt;p&gt;The idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.&lt;/p&gt;\n\n&lt;p&gt;I intuit that it might be necessary to multiply each model&amp;#39;s logits by a scalar to normalize them before appending, but it&amp;#39;s not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.&lt;/p&gt;\n\n&lt;p&gt;Then the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in &lt;a href=\"https://arxiv.org/abs/2505.24832\"&gt;https://arxiv.org/abs/2505.24832&lt;/a&gt;, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model&amp;#39;s knowledge beyond the limits of RAG (which works great, but only to the limits of context).&lt;/p&gt;\n\n&lt;p&gt;Thinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s as far as I&amp;#39;ve gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn&amp;#39;t work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgzmmw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ttkciar",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754268584,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Mini SVG test: law icons in pt-br (HorizonBeta x Gemini x Claude x Grok)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bwfogthy9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79cb7919750dfd42045fde6e5a5d1750b6143ab7"
                },
                {
                  "y": 138,
                  "x": 216,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=26969a94eb08a0e2171958349db4428a86f71ad7"
                },
                {
                  "y": 205,
                  "x": 320,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=21bb5b6a3b3a599bdeeda604db88da6736b6e4d5"
                },
                {
                  "y": 410,
                  "x": 640,
                  "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=17f23637e692739a1b5ef3b684a0a052ec7ecc48"
                }
              ],
              "s": {
                "y": 542,
                "x": 844,
                "u": "https://preview.redd.it/bwfogthy9wgf1.png?width=844&amp;format=png&amp;auto=webp&amp;s=77c97cc49d1a3a5fd743d7c7a1fb4a1ff770d866"
              },
              "id": "bwfogthy9wgf1"
            },
            "6xwodqxs9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0122e9b9ce6f267dac3a4040f5c4d24a6ab33a5d"
                },
                {
                  "y": 159,
                  "x": 216,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=efa03263563955f38143a53002ad439de9bc1e1d"
                },
                {
                  "y": 236,
                  "x": 320,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5164e776863a4405163a727cae86a79d4fcbf354"
                },
                {
                  "y": 472,
                  "x": 640,
                  "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1462c194648352ba3aacd5919b502f534f08f25"
                }
              ],
              "s": {
                "y": 598,
                "x": 810,
                "u": "https://preview.redd.it/6xwodqxs9wgf1.png?width=810&amp;format=png&amp;auto=webp&amp;s=b4772d1f3538d57556e510a1fe4fd3a4583b6e3f"
              },
              "id": "6xwodqxs9wgf1"
            },
            "ia7e2kbl9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd67a082a94566a9a2aeacf0fc5d448181db7258"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7282e81d14bd7c2188987283f144d86a8f7903"
                },
                {
                  "y": 177,
                  "x": 320,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c26282852557ab96c1361ecbea741c2c4d64498"
                },
                {
                  "y": 354,
                  "x": 640,
                  "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d374a1a222ac5de9389e35f65f8e0d95e5dbc943"
                }
              ],
              "s": {
                "y": 472,
                "x": 853,
                "u": "https://preview.redd.it/ia7e2kbl9wgf1.png?width=853&amp;format=png&amp;auto=webp&amp;s=1f1b989572bed7f831449970146ec8e9cb997450"
              },
              "id": "ia7e2kbl9wgf1"
            },
            "k054awhp9wgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e37e98e4780eda0cd2ba6181225109151048c90"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f703a4a1a4194a80e6a4a0a6372a8194a2923fe1"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=066c24d7a50a6721efb630059efc11f274ffd318"
                },
                {
                  "y": 405,
                  "x": 640,
                  "u": "https://preview.redd.it/k054awhp9wgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7390e8b61f5c96c52d39babfbac6d30feabe189"
                }
              ],
              "s": {
                "y": 512,
                "x": 808,
                "u": "https://preview.redd.it/k054awhp9wgf1.png?width=808&amp;format=png&amp;auto=webp&amp;s=3a53675ac12f7a73898f6114e5c2ccc689d9550f"
              },
              "id": "k054awhp9wgf1"
            }
          },
          "name": "t3_1mgytfi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 6,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ia7e2kbl9wgf1",
                "id": 720642216
              },
              {
                "media_id": "k054awhp9wgf1",
                "id": 720642217
              },
              {
                "media_id": "6xwodqxs9wgf1",
                "id": 720642218
              },
              {
                "media_id": "bwfogthy9wgf1",
                "id": 720642219
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CE7-Io3jRKJ_PHauMmTqpaj5UaVX4MxPR2ddXENqtHk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754266241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgytfi",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytfi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytfi/mini_svg_test_law_icons_in_ptbr_horizonbeta_x/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgytfi",
          "subreddit_subscribers": 509911,
          "created_utc": 1754266241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a documentation system designed to help LLMs understand codebases more efficiently, and I'd love feedback from this community.\n\nThe Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information  \n- Human-written docs aren't structured for LLM consumption\n- Different models have different context windows\n\nThe Approach:\nA system that generates multiple \"zoom levels\" of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations  \n- Full Details (8K+ tokens) - complete implementation specs\n\nLLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks\n\nExample: \"Does this API support webhooks?\" ‚Üí Overview is sufficient. \"Modify the authentication flow\" ‚Üí Needs Full Details.\n\nAlso exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.\n\nQuestions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?\n\nNot building a product yet - trying to validate if this solves real problems people have with local models and limited context.\n\nThanks for any thoughts!",
          "author_fullname": "t2_9mh0wady",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Title: Reducing token usage with progressive documentation loading - looking for feedback on approach",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgytca",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a documentation system designed to help LLMs understand codebases more efficiently, and I&amp;#39;d love feedback from this community.&lt;/p&gt;\n\n&lt;p&gt;The Problem:\n- Feeding entire codebases/docs to LLMs wastes tokens and context\n- Most tasks only need a subset of information&lt;br/&gt;\n- Human-written docs aren&amp;#39;t structured for LLM consumption\n- Different models have different context windows&lt;/p&gt;\n\n&lt;p&gt;The Approach:\nA system that generates multiple &amp;quot;zoom levels&amp;quot; of the same documentation:\n- Overview (500 tokens) - capabilities and high-level structure\n- Architecture (2K tokens) - patterns, flows, integrations&lt;br/&gt;\n- Full Details (8K+ tokens) - complete implementation specs&lt;/p&gt;\n\n&lt;p&gt;LLMs can:\n- Start with Overview\n- Request deeper levels only if needed\n- Save 75%+ tokens on many tasks&lt;/p&gt;\n\n&lt;p&gt;Example: &amp;quot;Does this API support webhooks?&amp;quot; ‚Üí Overview is sufficient. &amp;quot;Modify the authentication flow&amp;quot; ‚Üí Needs Full Details.&lt;/p&gt;\n\n&lt;p&gt;Also exploring different expression modes (technical/standard/friendly) for human readers, but the LLM optimization is my main focus.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1. How do you currently handle feeding docs/specs to your local models?\n2. Would progressive loading actually help with your context limits?\n3. What structure would work best for models like Mixtral, Llama, etc?\n4. Is this overengineering vs just better chunking strategies?&lt;/p&gt;\n\n&lt;p&gt;Not building a product yet - trying to validate if this solves real problems people have with local models and limited context.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgytca",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrsockpicks",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgytca/title_reducing_token_usage_with_progressive/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754266233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "that's it. thanks.",
          "author_fullname": "t2_9b9s4a7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX DWQ question: what's \"lr1e-8\" and \"lr8e-7 \" and should we care?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgys0z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754266130,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;that&amp;#39;s it. thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgys0z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JLeonsarmiento",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgys0z/mlx_dwq_question_whats_lr1e8_and_lr8e7_and_should/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgys0z/mlx_dwq_question_whats_lr1e8_and_lr8e7_and_should/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754266130,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm new to this and trying to figure out what the reasonable limits of my PC are for running these things. Still trying to understand quantization and offloading. I have 12 gb vram and 64 gb ddr4 regular ram available.\n\nWhat's the smartest LLM of the models now out I'll be able to run at a reasonable speed on this?",
          "author_fullname": "t2_6qwey",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help me pick a first model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgypja",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754265938,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this and trying to figure out what the reasonable limits of my PC are for running these things. Still trying to understand quantization and offloading. I have 12 gb vram and 64 gb ddr4 regular ram available.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the smartest LLM of the models now out I&amp;#39;ll be able to run at a reasonable speed on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgypja",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dracofrost",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgypja/help_me_pick_a_first_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgypja/help_me_pick_a_first_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754265938,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "During my internship at a big tech company, I struggled with a massive, messy codebase. Too many changes were impossible to understand either because of vague commit messages or because the original authors had left.\n\nFrustrated by losing so much context in git history, I built Gitdive: a local CLI tool that lets you have natural language conversations your repo's history.\n\nIt's early in development and definitely buggy, but if you've faced similar issues, I'd really appreciate your feedback.\n\nCheck it out: [https://github.com/ascl1u/gitdive](https://github.com/ascl1u/gitdive)",
          "author_fullname": "t2_zfhna",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a CLI tool to fix my biggest git frustration: lost commit context",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgyp1z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754265900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During my internship at a big tech company, I struggled with a massive, messy codebase. Too many changes were impossible to understand either because of vague commit messages or because the original authors had left.&lt;/p&gt;\n\n&lt;p&gt;Frustrated by losing so much context in git history, I built Gitdive: a local CLI tool that lets you have natural language conversations your repo&amp;#39;s history.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s early in development and definitely buggy, but if you&amp;#39;ve faced similar issues, I&amp;#39;d really appreciate your feedback.&lt;/p&gt;\n\n&lt;p&gt;Check it out: &lt;a href=\"https://github.com/ascl1u/gitdive\"&gt;https://github.com/ascl1u/gitdive&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?auto=webp&amp;s=73de85844faa94867152778081acea6bd0f9f608",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f02a3f481cf62f18021fe3d1ce90e5364980dd8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b14421e555358b82c89e2d197f8d592cec18f456",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d78ea4d0a5bf464254888775d8ecfcf1e787271",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=57b58b91bbb414ff690236f5866c7a719c090746",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f35c021e464e431de773ce6ac3ebd30329e18c2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09b17e71607479cb71a0eb94a76b9bcf76ae4b84",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "fL6uD7nkg1LCNjr4XqzTYY0UgOUhQel0M-wztqgNAA0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgyp1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Forgotten_Person",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgyp1z/building_a_cli_tool_to_fix_my_biggest_git/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgyp1z/building_a_cli_tool_to_fix_my_biggest_git/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754265900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just curious what the consensus is on best website to run this model (or any model really) for coding. Price and quality are key. \n\nThanks",
          "author_fullname": "t2_1i6lvuup5v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is best website to pay to run Qwen3b to code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgy28u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754264157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious what the consensus is on best website to run this model (or any model really) for coding. Price and quality are key. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgy28u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Complex-Emergency-60",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgy28u/what_is_best_website_to_pay_to_run_qwen3b_to_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgy28u/what_is_best_website_to_pay_to_run_qwen3b_to_code/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754264157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "That's it. I'm seeing some card readers on line. But, they are not reliable enough. How  could us improve it with a extremely small LLM? ",
          "author_fullname": "t2_1hra1kibwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with a project. What is or would be the lesser local LLM that one can use to recognize TCG cards (pkmn, YGO, mtg) in a reliable way and generate an CSV file with, language, edition, card name and treatment?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgy0tg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754264052,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s it. I&amp;#39;m seeing some card readers on line. But, they are not reliable enough. How  could us improve it with a extremely small LLM? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgy0tg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent_Pin7635",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgy0tg/help_with_a_project_what_is_or_would_be_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgy0tg/help_with_a_project_what_is_or_would_be_the/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754264052,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A modular system for local LLMs that applies mathematical heuristics to manage memory dynamically.  \nThe approach is fully implemented in Bash, and focuses on preserving semantic relevance while avoiding context overflow.\n\nKey mechanisms:\n\n* Levenshtein distance ‚Üí fragment selection by uniqueness and repetition\n* Collatz cycles ‚Üí regulate memory expansion/contraction\n* Goldbach segmentation ‚Üí structural distribution across prime-aligned lines\n* Riemann fallback logic ‚Üí state recovery on semantic degradation\n\nThe system operates on a plain \\`.txt\\` memory file and maintains coherent long-term context.\n\n Open for review and comparison with similar approaches.",
          "author_fullname": "t2_p7nqw2dg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M3 ‚Äì A Modular Mathematical Memory system for local LLMs (Collatz, Goldbach, Riemann, Levenshtein)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgx02s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754261279,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A modular system for local LLMs that applies mathematical heuristics to manage memory dynamically.&lt;br/&gt;\nThe approach is fully implemented in Bash, and focuses on preserving semantic relevance while avoiding context overflow.&lt;/p&gt;\n\n&lt;p&gt;Key mechanisms:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Levenshtein distance ‚Üí fragment selection by uniqueness and repetition&lt;/li&gt;\n&lt;li&gt;Collatz cycles ‚Üí regulate memory expansion/contraction&lt;/li&gt;\n&lt;li&gt;Goldbach segmentation ‚Üí structural distribution across prime-aligned lines&lt;/li&gt;\n&lt;li&gt;Riemann fallback logic ‚Üí state recovery on semantic degradation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The system operates on a plain `.txt` memory file and maintains coherent long-term context.&lt;/p&gt;\n\n&lt;p&gt;Open for review and comparison with similar approaches.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgx02s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Exchange_8504",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgx02s/m3_a_modular_mathematical_memory_system_for_local/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgx02s/m3_a_modular_mathematical_memory_system_for_local/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754261279,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I finally got the hardware I needed to run LLMs locally.\n\nAfter some testing, here‚Äôs a quick recap of my experience so far:\n\nJan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.\n\nLM Studio: While the performance was slightly worse compared to [Jan.ai](http://Jan.ai) on the same models, I had the exact same feature limitations.\n\nAnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn't work at all, the agent would say it was going to search online, then immediately say it couldn‚Äôt. Completely unusable for my needs. \n\nLocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned ‚Äúconnection failed‚Äù.\n\nSo here‚Äôs my question:\n\nIs there any tool out there that‚Äôs easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?\n\nAgent mode\n\nDeep research\n\nImage generation\n\nInternet search\n\nCanvas sharing\n\nReading images or files\n\nMemory\n\nI'm trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I'm starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.\n\nAny advice is welcome.",
          "author_fullname": "t2_d0wbvrzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Most complete almost plug and play LLM Tool with features",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgvyyj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754258843,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754258586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got the hardware I needed to run LLMs locally.&lt;/p&gt;\n\n&lt;p&gt;After some testing, here‚Äôs a quick recap of my experience so far:&lt;/p&gt;\n\n&lt;p&gt;Jan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.&lt;/p&gt;\n\n&lt;p&gt;LM Studio: While the performance was slightly worse compared to &lt;a href=\"http://Jan.ai\"&gt;Jan.ai&lt;/a&gt; on the same models, I had the exact same feature limitations.&lt;/p&gt;\n\n&lt;p&gt;AnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn&amp;#39;t work at all, the agent would say it was going to search online, then immediately say it couldn‚Äôt. Completely unusable for my needs. &lt;/p&gt;\n\n&lt;p&gt;LocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned ‚Äúconnection failed‚Äù.&lt;/p&gt;\n\n&lt;p&gt;So here‚Äôs my question:&lt;/p&gt;\n\n&lt;p&gt;Is there any tool out there that‚Äôs easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?&lt;/p&gt;\n\n&lt;p&gt;Agent mode&lt;/p&gt;\n\n&lt;p&gt;Deep research&lt;/p&gt;\n\n&lt;p&gt;Image generation&lt;/p&gt;\n\n&lt;p&gt;Internet search&lt;/p&gt;\n\n&lt;p&gt;Canvas sharing&lt;/p&gt;\n\n&lt;p&gt;Reading images or files&lt;/p&gt;\n\n&lt;p&gt;Memory&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I&amp;#39;m starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?auto=webp&amp;s=b61c13003a78792af3c70dba491521f8befe780b",
                  "width": 2400,
                  "height": 1350
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=316ac2c235dbf757adc6d57077bbf14ff212c7fd",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ebed2d4cb10be2cddc86b49f0bc6f6f16178fef",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7726b791c72a2145f757f5c6e90ddb14254a02e8",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbfa85537dbc57841336cb1db86b585484d97de4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749aec5f85de17cb9987662552bbb052f4717125",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39a55dd433726ea925f43fa48706981f88afb2dc",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgvyyj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtensionAd182",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754258586,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.\n\nI now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB\n\nI would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.\n\nBased on my research I couldn‚Äôt find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:\n\n* 80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500\n* 21 t/s - 120 GB/s Mac Mini M4 32GB 1000$\n* 46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$\n* 43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro\n* 410 GB/s Mac Studio M4 Max 36-128 GB tons of $\n* 819 GB/s Mac Studio M3 Ultra up to 512GB tons of $\n* 132 t/s - 750 GB/s RTX 4090 24GB\n\nBecause I‚Äôm considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I‚Äôve reached the following conclusions that I hope to validate:\n\n* I need something between M4 and the GPU in terms of speed.\n* I need 32GB but could do with more if it‚Äôs not a ripoff.\n* Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale\n* For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can‚Äôt justify getting a second GPU.\n* M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.\n\nSo is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn‚Äôt noisy and power hungry?",
          "author_fullname": "t2_12uzcl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking to build or buy a mini pc for LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgvbw6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754259032,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.&lt;/p&gt;\n\n&lt;p&gt;I now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB&lt;/p&gt;\n\n&lt;p&gt;I would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.&lt;/p&gt;\n\n&lt;p&gt;Based on my research I couldn‚Äôt find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500&lt;/li&gt;\n&lt;li&gt;21 t/s - 120 GB/s Mac Mini M4 32GB 1000$&lt;/li&gt;\n&lt;li&gt;46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$&lt;/li&gt;\n&lt;li&gt;43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro&lt;/li&gt;\n&lt;li&gt;410 GB/s Mac Studio M4 Max 36-128 GB tons of $&lt;/li&gt;\n&lt;li&gt;819 GB/s Mac Studio M3 Ultra up to 512GB tons of $&lt;/li&gt;\n&lt;li&gt;132 t/s - 750 GB/s RTX 4090 24GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because I‚Äôm considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I‚Äôve reached the following conclusions that I hope to validate:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I need something between M4 and the GPU in terms of speed.&lt;/li&gt;\n&lt;li&gt;I need 32GB but could do with more if it‚Äôs not a ripoff.&lt;/li&gt;\n&lt;li&gt;Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale&lt;/li&gt;\n&lt;li&gt;For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can‚Äôt justify getting a second GPU.&lt;/li&gt;\n&lt;li&gt;M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn‚Äôt noisy and power hungry?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgvbw6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nemuro87",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thx",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best uncensored +50B parameters model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv74s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgv74s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv74s/best_uncensored_50b_parameters_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv74s/best_uncensored_50b_parameters_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the [z.ai/blog/glm-4.5](http://z.ai/blog/glm-4.5) blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there's no way through.\n\nI gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the \"thinking\" part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.\n\nCan anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here's the exact prompt: \"Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.\"\n\nPS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8\\_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.",
          "author_fullname": "t2_3xif6p3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air Produces Better Code Without Thinking, Using 3-bit MLX (/nothink)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv53t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256522,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on:  I tried to reproduce the Flappy Bird game featured in the &lt;a href=\"http://z.ai/blog/glm-4.5\"&gt;z.ai/blog/glm-4.5&lt;/a&gt; blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there&amp;#39;s no way through.&lt;/p&gt;\n\n&lt;p&gt;I gave up on the model for a while, thinking that it was due to the 3-bit quant.  But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt.  This not only eliminated the &amp;quot;thinking&amp;quot; part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.&lt;/p&gt;\n\n&lt;p&gt;Can anyone with 4, 6 or 8 bit mlx version verify if they have this problem?  Here&amp;#39;s the exact prompt: &amp;quot;Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;PS.  I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio.  Also, Qwen3-Coder-30B-A3B (unlsoth Q8_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv53t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jcmyang",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256522,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI am currently running Qwen3-Coder-30B and tried to create unit test for my classes with different tools like qwen coder, countinue dev or proxy ai.\n\nThe tests which are created have many errors and I dont understand why. Atleast for the qwen coder, i would expected, if checks the codebase for all files.  \n  \nExample:   \nI have a enum with the values INSERT and UPDATED. In the test its using CREATED which does not exists in the enum.  \nOr I have a object with setters, but in the test it tries only with \"with\" (like it expected a builder). If it use the setter, it thinks, that the object is giving back as a return value instead nothing/void.\n\nI thought the model could help me with writing tests or do code review, but it doesnt feel really helpful.  \nDo i miss something or is that a common problem?",
          "author_fullname": "t2_4xgd14or",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Generating Unit Tests with Qwen3-Coder-30B - not really usable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv4h3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256477,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am currently running Qwen3-Coder-30B and tried to create unit test for my classes with different tools like qwen coder, countinue dev or proxy ai.&lt;/p&gt;\n\n&lt;p&gt;The tests which are created have many errors and I dont understand why. Atleast for the qwen coder, i would expected, if checks the codebase for all files.  &lt;/p&gt;\n\n&lt;p&gt;Example:&lt;br/&gt;\nI have a enum with the values INSERT and UPDATED. In the test its using CREATED which does not exists in the enum.&lt;br/&gt;\nOr I have a object with setters, but in the test it tries only with &amp;quot;with&amp;quot; (like it expected a builder). If it use the setter, it thinks, that the object is giving back as a return value instead nothing/void.&lt;/p&gt;\n\n&lt;p&gt;I thought the model could help me with writing tests or do code review, but it doesnt feel really helpful.&lt;br/&gt;\nDo i miss something or is that a common problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgv4h3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FarXTraveler",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv4h3/generating_unit_tests_with_qwen3coder30b_not/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv4h3/generating_unit_tests_with_qwen3coder30b_not/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256477,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! I‚Äôm curious if anyone has explored this path of thinking. I started using Gemini cli recently, and as everyone here probably knows a pretty well known (and annoying) habit of LLMs is to hallucinate their own capabilities. I‚Äôve found that a relatively easy way to ground it with its own actual capabilities was to just pull the source code into a subdirectory and I can tell it to reference the code and documentation as needed. This has led me to be able to have Gemini generate some pretty cool improvements to itself. Let me know if you‚Äôve tried this out and have any more advice or cool/similar ideas!",
          "author_fullname": "t2_1rxetzin7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grounding an open source agent with its source code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgv384",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754256391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I‚Äôm curious if anyone has explored this path of thinking. I started using Gemini cli recently, and as everyone here probably knows a pretty well known (and annoying) habit of LLMs is to hallucinate their own capabilities. I‚Äôve found that a relatively easy way to ground it with its own actual capabilities was to just pull the source code into a subdirectory and I can tell it to reference the code and documentation as needed. This has led me to be able to have Gemini generate some pretty cool improvements to itself. Let me know if you‚Äôve tried this out and have any more advice or cool/similar ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgv384",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PatienceKitchen6726",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgv384/grounding_an_open_source_agent_with_its_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgv384/grounding_an_open_source_agent_with_its_source/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754256391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since LLM are not deterministics can you have a ¬´¬†bad run¬†¬ª with your prompt ?\n\nLike having prompts that the LLM should be able to respond 90% of the times but ‚Ä¶ too bad you hit the 10% three chat in a row\n\nIt could explain why people experience ¬´¬†dumbness¬†period¬†¬ª from their favorite model while it still fine for every one else\n\nWhat do you think about it ?",
          "author_fullname": "t2_91nqzv8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you have a ¬´¬†bad run¬†¬ª with LLM ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgues2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.28,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NbCGGoF8jkkujOuck7lpU709E_xiTa5YqZWpQeLRQSA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754254774,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since LLM are not deterministics can you have a ¬´¬†bad run¬†¬ª with your prompt ?&lt;/p&gt;\n\n&lt;p&gt;Like having prompts that the LLM should be able to respond 90% of the times but ‚Ä¶ too bad you hit the 10% three chat in a row&lt;/p&gt;\n\n&lt;p&gt;It could explain why people experience ¬´¬†dumbness¬†period¬†¬ª from their favorite model while it still fine for every one else&lt;/p&gt;\n\n&lt;p&gt;What do you think about it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5oiuqtm3cvgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?auto=webp&amp;s=a34cdbe8eebe1d50de35842b83ca71afb5865167",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19b27cdc34a9316aa0e5c34cf98ac1c82c2b0602",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91e899afa8499eb114bffefdfdf7c6787e7a96d2",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c5df197b5e2ea05dc02e13381a0eb77b27efd63",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfca2436c4b249d6d482dc276a67af848a58e641",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6c51e12f80a81001f9072a29c230d2a43dd84c0e",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/5oiuqtm3cvgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d56b99149ce90067e9de53667722c154762efd4b",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "Jd9nPtzw1J2w27s1KMOb9IFaYuneD1vAFfRqLDyKXaU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgues2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kathane37",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgues2/can_you_have_a_bad_run_with_llm/",
          "stickied": false,
          "url": "https://i.redd.it/5oiuqtm3cvgf1.jpeg",
          "subreddit_subscribers": 509911,
          "created_utc": 1754254774,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Would you rather have a samsung 9100 pro 4tb vs WD\\_BLACK 8TB SN850X for llama.cpp? The samsung is twice as fast but the WD is twice as big. They cost roughly the same with the WD being slightly more expensive.\n\nI already have a samsung 990 pro 4tb and it's fine unless I need to work with safetensor files, in which case I end up having to delete and re-download models to save space, which is annoying.  \n  \nI'm considering this as a second drive, probably mounted on a SABRENT M.2 NVMe SSD to PCIe x16 because I can't figure out how to get from MCIO 8i to a m.2 with PCIe5 / Gen 5 capability.\n\nI'm wondering if models will load into RAM any faster with the faster drive, or if I should just go with the WD.",
          "author_fullname": "t2_ozxxf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "samsung 9100 pro 4tb vs WD_BLACK 8TB SN850X for llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgtrvz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754253267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would you rather have a samsung 9100 pro 4tb vs WD_BLACK 8TB SN850X for llama.cpp? The samsung is twice as fast but the WD is twice as big. They cost roughly the same with the WD being slightly more expensive.&lt;/p&gt;\n\n&lt;p&gt;I already have a samsung 990 pro 4tb and it&amp;#39;s fine unless I need to work with safetensor files, in which case I end up having to delete and re-download models to save space, which is annoying.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering this as a second drive, probably mounted on a SABRENT M.2 NVMe SSD to PCIe x16 because I can&amp;#39;t figure out how to get from MCIO 8i to a m.2 with PCIe5 / Gen 5 capability.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if models will load into RAM any faster with the faster drive, or if I should just go with the WD.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgtrvz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "createthiscom",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtrvz/samsung_9100_pro_4tb_vs_wd_black_8tb_sn850x_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtrvz/samsung_9100_pro_4tb_vs_wd_black_8tb_sn850x_for/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754253267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\n\nHorizon Beta is OpenAI",
          "author_fullname": "t2_h0z59zgo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta is OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 40,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m30x07j54vgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 31,
                  "x": 108,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=588d4a9c04c05e66d8b3c8ba8b93d7c827f14b20"
                },
                {
                  "y": 62,
                  "x": 216,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=446e608e2e3bf06827203dc313f98d8ec1704b9d"
                },
                {
                  "y": 92,
                  "x": 320,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f5cab485a7892fcf45b7a6bf1ecfa12925d59cf"
                },
                {
                  "y": 184,
                  "x": 640,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fda4acd30425eae2bddb9d9f82ce6241eaf9641"
                },
                {
                  "y": 276,
                  "x": 960,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8f560871bebab02cfbd9ae3af3f79ac6d3819a"
                },
                {
                  "y": 310,
                  "x": 1080,
                  "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c803879173681737db389051fa4d599d2f19a0"
                }
              ],
              "s": {
                "y": 364,
                "x": 1266,
                "u": "https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"
              },
              "id": "m30x07j54vgf1"
            }
          },
          "name": "t3_1mgtboa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 170,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 170,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754252212,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a\"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgtboa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiddleLobster9191",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754252212,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could this be the one? Our one? Our precious?",
          "author_fullname": "t2_cy3wb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Bolt Graphics (@BoltGraphicsInc) on X",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgtbeq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754252197,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could this be the one? Our one? Our precious?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/BoltGraphicsInc/status/1952049562912530494",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgtbeq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DrVonSinistro",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgtbeq/bolt_graphics_boltgraphicsinc_on_x/",
          "stickied": false,
          "url": "https://x.com/BoltGraphicsInc/status/1952049562912530494",
          "subreddit_subscribers": 509911,
          "created_utc": 1754252197,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm not a professional LLMer by any means, but I figured I'd lay out my little journey and the findings along the way.\n\nWhen I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It's so easy to get started and also to pull models.\n\nThe real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.\n\nI had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own \"go template\" setup.\n\nWell, although I don't know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.\n\nI decided to try out llama.cpp, with my own build. Well, it actually wasn't that bad. I was able to get ROCm installed pretty easy and from there it didn't take long to figure out how to compile llama.cpp. \n\nllama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.\n\nAfter playing around with llama.cpp for a while, I was totally hooked. I ended up running into \"llama-swap\", which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. \n\nNow I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.\n\nAdditionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap's config file instead of having to mess around with Modelfiles and so on.\n\nI also really appreciated that I could just keep a directory of .gguf files instead of the \"layering\" system (seemingly inspired by Docker) used by ollama.\n\nI had heard it said that ollama is \"just\" a llama.cpp wrapper, which isn't entirely fair, because they seem to do a lot of work to make it easy to install...\n\nIt was also pretty easy to tie random programs into \"llama-server\" since it is mostly compliant with OpenAI's APIs.\n\nAnyway, if you're out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:\n\nInterested in a quick and easy setup, don't want to fiddle with parameters too much: ollama. It's really, really easy to get going, has really decent cross platform support and it's easy to pull a model if it's on ollama.ai.\n\nIf you're a Linux goon and don't mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn't entirely sure what they were doing.\n\nI kind of see both of these as a tool on my tool-belt, and I'll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I'm 100% llama-swap/llama.cpp and having a blast!",
          "author_fullname": "t2_10iarzku",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A rambling post on ollama / llama.cpp and when to use each. Pros and cons and everything in between.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt5bx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754251812,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a professional LLMer by any means, but I figured I&amp;#39;d lay out my little journey and the findings along the way.&lt;/p&gt;\n\n&lt;p&gt;When I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It&amp;#39;s so easy to get started and also to pull models.&lt;/p&gt;\n\n&lt;p&gt;The real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.&lt;/p&gt;\n\n&lt;p&gt;I had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own &amp;quot;go template&amp;quot; setup.&lt;/p&gt;\n\n&lt;p&gt;Well, although I don&amp;#39;t know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.&lt;/p&gt;\n\n&lt;p&gt;I decided to try out llama.cpp, with my own build. Well, it actually wasn&amp;#39;t that bad. I was able to get ROCm installed pretty easy and from there it didn&amp;#39;t take long to figure out how to compile llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;llama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.&lt;/p&gt;\n\n&lt;p&gt;After playing around with llama.cpp for a while, I was totally hooked. I ended up running into &amp;quot;llama-swap&amp;quot;, which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. &lt;/p&gt;\n\n&lt;p&gt;Now I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap&amp;#39;s config file instead of having to mess around with Modelfiles and so on.&lt;/p&gt;\n\n&lt;p&gt;I also really appreciated that I could just keep a directory of .gguf files instead of the &amp;quot;layering&amp;quot; system (seemingly inspired by Docker) used by ollama.&lt;/p&gt;\n\n&lt;p&gt;I had heard it said that ollama is &amp;quot;just&amp;quot; a llama.cpp wrapper, which isn&amp;#39;t entirely fair, because they seem to do a lot of work to make it easy to install...&lt;/p&gt;\n\n&lt;p&gt;It was also pretty easy to tie random programs into &amp;quot;llama-server&amp;quot; since it is mostly compliant with OpenAI&amp;#39;s APIs.&lt;/p&gt;\n\n&lt;p&gt;Anyway, if you&amp;#39;re out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:&lt;/p&gt;\n\n&lt;p&gt;Interested in a quick and easy setup, don&amp;#39;t want to fiddle with parameters too much: ollama. It&amp;#39;s really, really easy to get going, has really decent cross platform support and it&amp;#39;s easy to pull a model if it&amp;#39;s on ollama.ai.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re a Linux goon and don&amp;#39;t mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn&amp;#39;t entirely sure what they were doing.&lt;/p&gt;\n\n&lt;p&gt;I kind of see both of these as a tool on my tool-belt, and I&amp;#39;ll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I&amp;#39;m 100% llama-swap/llama.cpp and having a blast!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgt5bx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UsualResult",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754251812,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vcawomd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio ‚Äî Possibly the Best Local Cursor Alternative Right Now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgt2om",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 114,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756902682%2CZmY1YTczOGUwZmI2MjU5YzViZThkMjlmMGFiNzg5YjdhZjY1OGRmZmNkM2ExM2QxZDMzZTJiOGQxOTAyM2JmMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756902682%2CNmYxNmRlNGY4ZWZlNzY2ZmVlZDJlYzJkOGY4NzU4MjM0MjY2YzY2MDRkYTMzNTZkODViYzVjMjZmNjhhNTQ3NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 114,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=140&amp;height=110&amp;crop=140:110,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=abdd574d8f67b63e995dc11eb3cd4f8e74e11bb9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754251636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e1348s852vgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?format=pjpg&amp;auto=webp&amp;s=0e7a1d12b1a2a1acf19fdfc0f91b16be267107fa",
                  "width": 1350,
                  "height": 1068
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8aff50a70e5cb53b7b38c9b35a93218ab06b529e",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5e621992a0c2958c031c79fe79c2a214c0cc8d5",
                    "width": 216,
                    "height": 170
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1edee9ae25cab3880fe29e9ac723b8339f6cebcf",
                    "width": 320,
                    "height": 253
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5bb59fb0f5c8e7a4e248e1068b8aa841bfa37998",
                    "width": 640,
                    "height": 506
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d85e99a79f17e7ec33873066c7eb6394c4cc7a5e",
                    "width": 960,
                    "height": 759
                  },
                  {
                    "url": "https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1ee9e27ab710d36f97c624d0fec564a4f4b5b127",
                    "width": 1080,
                    "height": 854
                  }
                ],
                "variants": {},
                "id": "dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mgt2om",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "onil_gova",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/",
          "stickied": false,
          "url": "https://v.redd.it/e1348s852vgf1",
          "subreddit_subscribers": 509911,
          "created_utc": 1754251636,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e1348s852vgf1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 720,
              "width": 910,
              "scrubber_media_url": "https://v.redd.it/e1348s852vgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e1348s852vgf1/DASHPlaylist.mpd?a=1756902682%2CZmY1YTczOGUwZmI2MjU5YzViZThkMjlmMGFiNzg5YjdhZjY1OGRmZmNkM2ExM2QxZDMzZTJiOGQxOTAyM2JmMw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 36,
              "hls_url": "https://v.redd.it/e1348s852vgf1/HLSPlaylist.m3u8?a=1756902682%2CNmYxNmRlNGY4ZWZlNzY2ZmVlZDJlYzJkOGY4NzU4MjM0MjY2YzY2MDRkYTMzNTZkODViYzVjMjZmNjhhNTQ3NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know it is a very amateur question but I am having a headache with this. I have downloaded llama 3.1 8B from meta and painfully converted them to gguf so I could use them with llama.cpp but when I use my gguf it just outputs random stuff that he is Jarvis! I tested system prompts but it changed nothing! my initial problem was that I used to use llama with ollama in my code but then after some while the LLM would output gibberish like a lot of @@@@ and no error whatsoever about how to fix it so I thought maybe the problem is with ollama and I should download the original weights.",
          "author_fullname": "t2_4kas3n6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Raw weights answer gibberish while ollama answers just fine!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgstni",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754251048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it is a very amateur question but I am having a headache with this. I have downloaded llama 3.1 8B from meta and painfully converted them to gguf so I could use them with llama.cpp but when I use my gguf it just outputs random stuff that he is Jarvis! I tested system prompts but it changed nothing! my initial problem was that I used to use llama with ollama in my code but then after some while the LLM would output gibberish like a lot of @@@@ and no error whatsoever about how to fix it so I thought maybe the problem is with ollama and I should download the original weights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgstni",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Biodie",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgstni/raw_weights_answer_gibberish_while_ollama_answers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgstni/raw_weights_answer_gibberish_while_ollama_answers/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754251048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My question is how much can the sliders actually effect the voice because i feel like it sounds kinda robotic regardless of my settings, should I try a different audio clip or is there nothing I can do",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Make chatterbox tts sound more realistic",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgrhcp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754247815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My question is how much can the sliders actually effect the voice because i feel like it sounds kinda robotic regardless of my settings, should I try a different audio clip or is there nothing I can do&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgrhcp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgrhcp/make_chatterbox_tts_sound_more_realistic/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgrhcp/make_chatterbox_tts_sound_more_realistic/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754247815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I use ChatGPT and Claude paid plans. I‚Äôm incorporating local for writing topics that they censor. I understand that closed source is not on the level of Claude and can‚Äôt be at this time. I accept that. However, I‚Äôm having trouble getting it to work at even a basic level and I‚Äôve found nothing on Google or YouTube that talks about prompt crafting outside of waifu chats (and even those are few). \n\nWhen I try to use it like I would CGPT or Claude it puts out garbage. I‚Äôve gotten what looks like Twitter comments, the same thing I sent it, error codes, random stuff that makes no sense (but at least some what mirrors the style I specified). \n\nCGPT says you can‚Äôt put much info in the world building or author notes because of tokens. It says local LLMs are more like continuing text you put in and don‚Äôt have the same training depth. Is that true? \n\nLook basically I want what I have in CGPT or Claude. I can upload a few pages long doc and it reference that. Then follow prompts to give me new ideas or continue in a style or revise. **Is that possible**? **If so how?‚Äù**. \n\n**Should I just train a model on my own voice? How many samples of my writing do I realistically need for that? CGPT says 50-120k+ words ideally**",
          "author_fullname": "t2_hhsbgspn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I don‚Äôt understand how to get what I want from Local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgrgmu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754247764,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use ChatGPT and Claude paid plans. I‚Äôm incorporating local for writing topics that they censor. I understand that closed source is not on the level of Claude and can‚Äôt be at this time. I accept that. However, I‚Äôm having trouble getting it to work at even a basic level and I‚Äôve found nothing on Google or YouTube that talks about prompt crafting outside of waifu chats (and even those are few). &lt;/p&gt;\n\n&lt;p&gt;When I try to use it like I would CGPT or Claude it puts out garbage. I‚Äôve gotten what looks like Twitter comments, the same thing I sent it, error codes, random stuff that makes no sense (but at least some what mirrors the style I specified). &lt;/p&gt;\n\n&lt;p&gt;CGPT says you can‚Äôt put much info in the world building or author notes because of tokens. It says local LLMs are more like continuing text you put in and don‚Äôt have the same training depth. Is that true? &lt;/p&gt;\n\n&lt;p&gt;Look basically I want what I have in CGPT or Claude. I can upload a few pages long doc and it reference that. Then follow prompts to give me new ideas or continue in a style or revise. &lt;strong&gt;Is that possible&lt;/strong&gt;? &lt;strong&gt;If so how?‚Äù&lt;/strong&gt;. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Should I just train a model on my own voice? How many samples of my writing do I realistically need for that? CGPT says 50-120k+ words ideally&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgrgmu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AccidentalFolklore",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgrgmu/i_dont_understand_how_to_get_what_i_want_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgrgmu/i_dont_understand_how_to_get_what_i_want_from/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754247764,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\n\nI‚Äôm looking for recommendations for **open-source multimoal LLMs no larger than 8B parameters** that perform well as *agents* for interacting with web pages.\n\n**Context / Constraints:**\n\n* **Max size:** 8B params (need to run locally on an 8‚ÄØGB GPU without major slowdowns)\n* **Use case:** Complex browser automation ‚Äî navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.\n* **Agent setup:** Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.\n* **Precision:** I can run FP16 or quantized (8-bit/4-bit) models if that helps.\n* **Goal:** Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.\n\n**Questions:**\n\n1. Which small **open-source multimodal models** have you found most capable for this kind of task?\n2. Any **quantized** versions you recommend for best VRAM fit + speed on consumer GPUs?\n3. Have you seen measurable differences between models in **agentic benchmarks** like Mind2Web, WebArena, or WorkArena?\n\nThanks in advance!",
          "author_fullname": "t2_jy7rkkqp8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What‚Äôs the Best Open-Source Small LLM (‚â§‚ÄØ8B) for Agentic Web Page Interactions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgr13d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754246733,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm looking for recommendations for &lt;strong&gt;open-source multimoal LLMs no larger than 8B parameters&lt;/strong&gt; that perform well as &lt;em&gt;agents&lt;/em&gt; for interacting with web pages.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Context / Constraints:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Max size:&lt;/strong&gt; 8B params (need to run locally on an 8‚ÄØGB GPU without major slowdowns)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Complex browser automation ‚Äî navigating, filling forms, clicking elements, multi-step planning, and handling changing DOM structures.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agent setup:&lt;/strong&gt; Likely to integrate with a framework like BrowserGym, LaVague, Playwright, or similar.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; I can run FP16 or quantized (8-bit/4-bit) models if that helps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Good mix of reasoning, instruction-following, and robustness for long-horizon tasks.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which small &lt;strong&gt;open-source multimodal models&lt;/strong&gt; have you found most capable for this kind of task?&lt;/li&gt;\n&lt;li&gt;Any &lt;strong&gt;quantized&lt;/strong&gt; versions you recommend for best VRAM fit + speed on consumer GPUs?&lt;/li&gt;\n&lt;li&gt;Have you seen measurable differences between models in &lt;strong&gt;agentic benchmarks&lt;/strong&gt; like Mind2Web, WebArena, or WorkArena?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgr13d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Extra-Designer9333",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgr13d/whats_the_best_opensource_small_llm_8b_for/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754246733,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/p175cshynugf1.png?width=564&amp;format=png&amp;auto=webp&amp;s=10f8476e8e7a4b5cd7c2fbdc17c1349da92e7eda\n\n",
          "author_fullname": "t2_fkiuq1wt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scam Altman : gpt5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "p175cshynugf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/p175cshynugf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6de82eff3700e64c6206ac1c75c4147e8a55fff6"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/p175cshynugf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fa8c611d60f4b97ea3ac117bf735520e430ecf5"
                },
                {
                  "y": 401,
                  "x": 320,
                  "u": "https://preview.redd.it/p175cshynugf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=994299fdde8f4893707016e04336611c7185b7a7"
                }
              ],
              "s": {
                "y": 707,
                "x": 564,
                "u": "https://preview.redd.it/p175cshynugf1.png?width=564&amp;format=png&amp;auto=webp&amp;s=10f8476e8e7a4b5cd7c2fbdc17c1349da92e7eda"
              },
              "id": "p175cshynugf1"
            }
          },
          "name": "t3_1mgr02b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.15,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/IhH9OBjZTD-cvkHYIRLL0POyomHK_5SMhqb5RCsuKUE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754246663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/p175cshynugf1.png?width=564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10f8476e8e7a4b5cd7c2fbdc17c1349da92e7eda\"&gt;https://preview.redd.it/p175cshynugf1.png?width=564&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10f8476e8e7a4b5cd7c2fbdc17c1349da92e7eda&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgr02b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "omar07ibrahim1",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgr02b/scam_altman_gpt5/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgr02b/scam_altman_gpt5/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754246663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, so I have a Macbook Pro M4 pro with 24gb of ram, what are your recommendations for models that could work on the computer. Also would you recommend sticking to ollama or looking into a mlx model provider.",
          "author_fullname": "t2_1kut52t515",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Models for 24gb Macbook",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgqkjt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754245629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so I have a Macbook Pro M4 pro with 24gb of ram, what are your recommendations for models that could work on the computer. Also would you recommend sticking to ollama or looking into a mlx model provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgqkjt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ben-R1106",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgqkjt/models_for_24gb_macbook/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgqkjt/models_for_24gb_macbook/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754245629,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. \n\nI've been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn't write like claude does and well claude doesn't think like gpt or gemini do. Then its like wtf you use then.  :)\n\nSo getting out of that just a bit, I've started playing around with novel crafter recently and honeslty it's hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn't jailbreak it (trying to avoid that so it doesn't make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. \n\nI just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it's censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? ",
          "author_fullname": "t2_gcu2g1163",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any good open source models for NSFW writing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgq8yz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754244875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i heard some good things about muse writing pose well and not being censored,  just curious if there are other similar open models that are sort of trained from books not part of a very expensive website. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using gemini lately on the website and it does a lot of things right. THe main issue is it just doesn&amp;#39;t write like claude does and well claude doesn&amp;#39;t think like gpt or gemini do. Then its like wtf you use then.  :)&lt;/p&gt;\n\n&lt;p&gt;So getting out of that just a bit, I&amp;#39;ve started playing around with novel crafter recently and honeslty it&amp;#39;s hard to go back to the chatbots now. Like that free ai at the bottom of the open routerlist did better than perplexity and the 3 main ai sites on fanfiction. of course it and Kimi still censored berserk but I didn&amp;#39;t jailbreak it (trying to avoid that so it doesn&amp;#39;t make the writing suck hehe) but sometimes like when i was doing perplexity i can just delete and try again. &lt;/p&gt;\n\n&lt;p&gt;I just like to try a model that truely is made for writing and not uncensored if it exist. I probably am overplaying muse bet it&amp;#39;s censored against berserk too! Are there like any good models that write well and can actually do writing styles and NSFW? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgq8yz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LoneyGamer2023",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgq8yz/are_there_any_good_open_source_models_for_nsfw/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754244875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi LocalLLaMA,\n\nI‚Äôm a bit confused on two levels and need help: \n\n1) What are the best settings to get ollama to utilize all (6) 3090‚Äôs so I can use parallel processing. \n\n2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? \n\nAny recommendations on models? \n\nMy use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.\n\nSomeone previously referenced using CommandR+ 104b but I couldn‚Äôt get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). \n\nI think another person previously referenced Gemma 27b. I haven‚Äôt tried that yet. \n\nI‚Äôm a bit lost on configs. \n\nAlso someone suggested vllm instead but I couldn‚Äôt seem to get it to work, even with a small model. ",
          "author_fullname": "t2_rkb6qbej1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help- unsure of right ollama configs with 6x 3090‚Äôs, also model choice for RAG?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgpq7a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754243683,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMA,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm a bit confused on two levels and need help: &lt;/p&gt;\n\n&lt;p&gt;1) What are the best settings to get ollama to utilize all (6) 3090‚Äôs so I can use parallel processing. &lt;/p&gt;\n\n&lt;p&gt;2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? &lt;/p&gt;\n\n&lt;p&gt;Any recommendations on models? &lt;/p&gt;\n\n&lt;p&gt;My use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.&lt;/p&gt;\n\n&lt;p&gt;Someone previously referenced using CommandR+ 104b but I couldn‚Äôt get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). &lt;/p&gt;\n\n&lt;p&gt;I think another person previously referenced Gemma 27b. I haven‚Äôt tried that yet. &lt;/p&gt;\n\n&lt;p&gt;I‚Äôm a bit lost on configs. &lt;/p&gt;\n\n&lt;p&gt;Also someone suggested vllm instead but I couldn‚Äôt seem to get it to work, even with a small model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgpq7a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Business-Weekend-537",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754243683,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üß† Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I‚Äôve implemented after Gemma üöÄ. This was a major milestone for me, especially since there‚Äôs no open-source implementation of Qwen 2 available online (at least none I could find).\n\nWhat makes this build special:\n‚úÖ Implemented without access to source code\nüìñ Based entirely on the Qwen 1 &amp; Qwen 2 research papers\nüß± Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n‚ö†Ô∏è Does not support Mixture of Experts (MoE) yet\n\nThis project pushed my understanding of transformer architectures even further, and I‚Äôm excited to keep going.\nIf you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!\n\nSource code: https://github.com/introlix/Swiftlet\nKaggle: https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet",
          "author_fullname": "t2_6qpq9avr5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reimplemention of Qwen 2 from scratch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgpb8t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 98,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 98,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754242723,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üß† Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I‚Äôve implemented after Gemma üöÄ. This was a major milestone for me, especially since there‚Äôs no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt;\n\n&lt;p&gt;What makes this build special:\n‚úÖ Implemented without access to source code\nüìñ Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers\nüß± Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n‚ö†Ô∏è Does not support Mixture of Experts (MoE) yet&lt;/p&gt;\n\n&lt;p&gt;This project pushed my understanding of transformer architectures even further, and I‚Äôm excited to keep going.\nIf you&amp;#39;re into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/introlix/Swiftlet\"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt;\nKaggle: &lt;a href=\"https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet\"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?auto=webp&amp;s=519a96c79f13a619e42513cc0f904d9b36729fa1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09e4e357ff6f03ec51f0d4d875169c2822efb899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a804dcd73acb0eafcbf3c98a337b04b0330590",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6460444e14680b3ebc1e71bebe27f5eaaab08b28",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142de401ddac2c07588c3df4af6a9a88a8c43665",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730308dc3a494b5e493c8cc6e298f0435aba1498",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798ec017daa864fb62d31aa0cd92c8316b840da9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgpb8t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CodingWithSatyam",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754242723,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I'm getting about 15t/s generation speeds, but I've seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? ",
          "author_fullname": "t2_3bzzdk93",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fastest way to run Qwen 3 30B A3B on 32GB RAM+10GB VRAM in LM Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgocw6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754240445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I&amp;#39;m getting about 15t/s generation speeds, but I&amp;#39;ve seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgocw6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yungfishstick",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754240445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I literally can't find anything on this model. I saw somewhere on discord that it's similar to claude (which I doubt). any info? and no i'm not promoting this website or any bs like that idk anything about it",
          "author_fullname": "t2_askwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jin 3.5 - Does anyone know anything about this model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgo662",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754239996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "jin.elpa.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I literally can&amp;#39;t find anything on this model. I saw somewhere on discord that it&amp;#39;s similar to claude (which I doubt). any info? and no i&amp;#39;m not promoting this website or any bs like that idk anything about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://jin.elpa.ai/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgo662",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z_3454_pfk",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgo662/jin_35_does_anyone_know_anything_about_this_model/",
          "stickied": false,
          "url": "https://jin.elpa.ai/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello Friends,\n\nWhat is a smaller LLM \\[ 1B - 10B\\] model that is very good for Information extraction from unstructured text. I am okay to fine-tune, I would only have a few thousand \\[ &lt; 10k \\] samples.\n\nI would need to run this on a cheaper GPU than A100 likely T4/L4 or A10s.   \nCould you share your experience using these GPUs and the throughput/performance you get. \n\nThanks\n\nShyam  \n",
          "author_fullname": "t2_bib6c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question : Best small sized LLM for Information extraction from Unstructured Text",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgo50t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754239919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Friends,&lt;/p&gt;\n\n&lt;p&gt;What is a smaller LLM [ 1B - 10B] model that is very good for Information extraction from unstructured text. I am okay to fine-tune, I would only have a few thousand [ &amp;lt; 10k ] samples.&lt;/p&gt;\n\n&lt;p&gt;I would need to run this on a cheaper GPU than A100 likely T4/L4 or A10s.&lt;br/&gt;\nCould you share your experience using these GPUs and the throughput/performance you get. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;Shyam  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgo50t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "smaddali",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgo50t/question_best_small_sized_llm_for_information/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgo50t/question_best_small_sized_llm_for_information/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They said they're refining it months ago. Possibly timing to coincide with OpenAI's drop? Would be epic, I'm a fan of both. Especially if OpenAI's is not a reasoning model.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When DeepSeek r2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgny8p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "transparent",
          "ups": 201,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 201,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/V9pPhmLOwBjBig1Mp88kf2vrbPkb0OVif8im0hRtZXs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239452,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They said they&amp;#39;re refining it months ago. Possibly timing to coincide with OpenAI&amp;#39;s drop? Would be epic, I&amp;#39;m a fan of both. Especially if OpenAI&amp;#39;s is not a reasoning model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?auto=webp&amp;s=6cf4a209d32163902a3fb91d5a06108b4ebb9e61",
                  "width": 1080,
                  "height": 654
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bab256437dc4b2ab50c0bfefc751721bae4de7c5",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9916573c0160c0ddd9eb66a28ab87d51d07207d6",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a55f67b3df278451d7c98ae260464229b3ad11fa",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaef5f3c86c4f7340d2367eea7fce60751451a94",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3d7f058cca6ec2b2a9fcbd009eadd099423f494",
                    "width": 960,
                    "height": 581
                  },
                  {
                    "url": "https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f8bd24556666c1de0f3c211632b5ace09b00eb99",
                    "width": 1080,
                    "height": 654
                  }
                ],
                "variants": {},
                "id": "lqAfsgD1hr66i0LHoQ3Cw_QWosWkvNCmE0O1wxkWap4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgny8p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/",
          "stickied": false,
          "url": "https://i.redd.it/dz0i0w1j2ugf1.jpeg",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239452,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgnwnx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 97,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 97,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=e35cde9b2565c154a06b4a4ec2a056512f465e28",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754239344,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?auto=webp&amp;s=c1ed5f3fc1bf6e79ce85d0034c3222d7952acbd1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=165ab4b12c90bfe025b46debae13b140652c63d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=47d3bbf5bd07f507b8f71659fd9562dc4d194b90",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b91013b0a9041e7c6c25c022439d1503d608d9af",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7344e67fe48dc6a6f67623605b3dc51b204d189",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=996d51aa65f7fae5b2ee3e5ffbe787ce0b8cfdcd",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c4499831a246669c1b31de509f9c28678094e6e6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgnwnx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/",
          "stickied": false,
          "url": "https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4",
          "subreddit_subscribers": 509911,
          "created_utc": 1754239344,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any decent open source LLM like mistral or gemma that also has vision could technically already use a web browser to do tasks and click on links. Is there already a model that can do that? Can't be too far away",
          "author_fullname": "t2_rxdlfeeju",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a \"Chat-GPT agent\" kind of agent that is open source and can be run locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgnq9n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754238903,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any decent open source LLM like mistral or gemma that also has vision could technically already use a web browser to do tasks and click on links. Is there already a model that can do that? Can&amp;#39;t be too far away&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgnq9n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maleo999",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgnq9n/is_there_a_chatgpt_agent_kind_of_agent_that_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgnq9n/is_there_a_chatgpt_agent_kind_of_agent_that_is/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754238903,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It's big enough to be smart without being so huge it steps on OpenAI‚Äôs toes. In my experience with the model, here some notes:\n\n* It works¬†*really*¬†well with agents and tools\n* It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.\n* It‚Äôs fantastic with languages other than English, a weakness often sees in Chinese models.\n* It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models\n* The way it writes is very similar to OpenAI‚Äôs style.\n* Plus, whoever made this has¬†*serious*¬†computing power... they're giving away billions of tokens for \"free\" at a really fast speed\n* The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)\n\nBut ok, lets consider other players:  \n \\- Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly  \n \\- Anthropic: Naah  \n \\- Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don't see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.  \n \\- Amazon or Microsoft: Would be my second guess  \n \\- Google: Naah, they have Aistudio, when wants feedback they launch the model there  \n \\- IBM or Cohere: Hard to think, but they are very capable companies\n\nHonestly, it‚Äôs hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren‚Äôt fans of CloseAI, but if they say they‚Äôre releasing an open-source model, let‚Äôs be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.\n\nSo, what you guys think?",
          "author_fullname": "t2_y02rp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Horizon Models is not from OpenAI, who would be?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgn94g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754237720,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This model is seriously impressive, feels really powerful, and that fits with what people have been saying about it being 120B parameters in size. It&amp;#39;s big enough to be smart without being so huge it steps on OpenAI‚Äôs toes. In my experience with the model, here some notes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It works¬†&lt;em&gt;really&lt;/em&gt;¬†well with agents and tools&lt;/li&gt;\n&lt;li&gt;It can handle long contexts (i tested up to around 50k tokens), which is something most open-source models struggle with, only the biggest ones can do that reliably.&lt;/li&gt;\n&lt;li&gt;It‚Äôs fantastic with languages other than English, a weakness often sees in Chinese models.&lt;/li&gt;\n&lt;li&gt;It can be based on GPT-5 architecture, even being a smaller version from it (like Gemma models), this would explain why has some differences from current OpenAi models&lt;/li&gt;\n&lt;li&gt;The way it writes is very similar to OpenAI‚Äôs style.&lt;/li&gt;\n&lt;li&gt;Plus, whoever made this has¬†&lt;em&gt;serious&lt;/em&gt;¬†computing power... they&amp;#39;re giving away billions of tokens for &amp;quot;free&amp;quot; at a really fast speed&lt;/li&gt;\n&lt;li&gt;The model says its an OpenAI model. Very common in Chinese models but very unlikely from a US model (unless is really from OpenAI)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But ok, lets consider other players:&lt;br/&gt;\n - Chinese labs: except Deepseek, we had so many new models recently, very hard to think they have more, unless is DeepSeek behind it, but i doubt because of things that i said above. Also when they want to test something they just drop the weights directly&lt;br/&gt;\n - Anthropic: Naah&lt;br/&gt;\n - Meta: could be, but i think its too early for the new Meta team already made something so much better than Llama, besides i don&amp;#39;t see Meta training on OpenAI data since there already have so many data. Llama was not very good because was technologically behind, data is not the problem.&lt;br/&gt;\n - Amazon or Microsoft: Would be my second guess&lt;br/&gt;\n - Google: Naah, they have Aistudio, when wants feedback they launch the model there&lt;br/&gt;\n - IBM or Cohere: Hard to think, but they are very capable companies&lt;/p&gt;\n\n&lt;p&gt;Honestly, it‚Äôs hard to imagine anyone other than OpenAI being behind this. Two things that i am sure that is a US model and has very capable infra. I know some people aren‚Äôt fans of CloseAI, but if they say they‚Äôre releasing an open-source model, let‚Äôs be optimistic, its a win-win situation. It could be great for us. And with so many good Chinese models becoming popular, maybe OpenAI realized its better to join the open-source world than stay completely closed off.&lt;/p&gt;\n\n&lt;p&gt;So, what you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgn94g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AMOVCS",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgn94g/if_horizon_models_is_not_from_openai_who_would_be/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754237720,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fp657",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmx8w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 164,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 164,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=64baffecac79129329accd5dfef00560cde30027",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754236913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/randombk/chatterbox-vllm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?auto=webp&amp;s=f061762d17683d4e88608dbfe355d57e45d90ad5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d660bf476941abc2978684579558acc15bd0d2e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f08036e019ea53656d9f429e46be530e96c0ed23",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eac38ddf9a5c589c5a861f8aafada6fe73427034",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50d687fd0b27b1fc30b1175e432b4518d7d1f15d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=185870ef47db93a269dea9dfe0a0629b16de6b64",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df00e364b0a49f0fb368440e176f32ee5e39b351",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgmx8w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dlp_randombk",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/",
          "stickied": false,
          "url": "https://github.com/randombk/chatterbox-vllm",
          "subreddit_subscribers": 509911,
          "created_utc": 1754236913,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.  \nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)",
          "author_fullname": "t2_rgy8m2w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How many of you actually know by heart the general structure of the transformer architecture?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmr6x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754236507,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.&lt;br/&gt;\nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgmr6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CptKrupnik",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754236507,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know a lot of people are fine-tuning their models using their codebase but I couldn't find that many resources on how to build this dataset.\n\nSure you could dump your codebase and that's it but there must be a better way to teach the model how to interact with this codebase right?\n\nWhat did you try? And did it work?",
          "author_fullname": "t2_5219n8dd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How would you generate a dataset to fine-tune a llm to your codebase?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgmlzw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754236148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of people are fine-tuning their models using their codebase but I couldn&amp;#39;t find that many resources on how to build this dataset.&lt;/p&gt;\n\n&lt;p&gt;Sure you could dump your codebase and that&amp;#39;s it but there must be a better way to teach the model how to interact with this codebase right?&lt;/p&gt;\n\n&lt;p&gt;What did you try? And did it work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgmlzw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThisIsBartRick",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754236148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Am I the only person who can't stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I'm gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere's 112 Command A Vision if not for the license) - I just can't help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? ",
          "author_fullname": "t2_ap0ra8pe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Daydreaming of a new Gemma model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgm8d3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754235233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am I the only person who can&amp;#39;t stop day dreaming of a larger Gemma model? I genuinely prefer the vibe of Gemma 3 27B to just about every other LLM I have been able to get my hands on, and I&amp;#39;m gearing up to fund a major fine-tune/tweak of an OS model this year. (I would take the plunge on Cohere&amp;#39;s 112 Command A Vision if not for the license) - I just can&amp;#39;t help but shake the itch for a version of Gemma that punched just a bit higher in terms of its capabilities. Does anyone with their finger more on the pulse of the development cycle have any idea whether or not we might get something like this at any point in the next few months? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mgm8d3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jazzlike_Source_5983",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgm8d3/daydreaming_of_a_new_gemma_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754235233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Never tried it I‚Äôm just curious what your experience has been like with it. I was wondering if it looks ‚Äúcheap‚Äù or too uncanny valley like how some websites or apps are with it. ",
          "author_fullname": "t2_l4qac",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are image generations from LLMs as accurate as OpenAI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mglmse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754233767,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Never tried it I‚Äôm just curious what your experience has been like with it. I was wondering if it looks ‚Äúcheap‚Äù or too uncanny valley like how some websites or apps are with it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mglmse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XiRw",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mglmse/are_image_generations_from_llms_as_accurate_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mglmse/are_image_generations_from_llms_as_accurate_as/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754233767,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really like LM Studio because it allows you to run AI models locally, preserving the privacy of your conversations with the AI. However, compared to commercial online models, LM Studio doesn‚Äôt support internet browsing ‚Äúout of the box.‚Äù Those models can‚Äôt use up-to-date information from the Internet to answer questions.\n\n\nNot long ago, LM Studio added the ability to connect MCP servers to models. The very first thing I did was write a small MCP server that can extract text from a URL. It can also extract the links present on the page. This makes it possible, when querying the AI, to specify an address and ask it to extract text from there or retrieve links to use in its response.\n\nTo get all of this working, we first create a `pyproject.toml` file in the `mcp-server` folder.\n\n\n```toml \n[build-system]\nrequires = [\"setuptools&gt;=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"url-text-fetcher\"\nversion = \"0.1.0\"\ndescription = \"FastMCP server for URL text fetching\"\nauthors = [{ name=\"Evgeny Igumnov\", email=\"igumnovnsk@gmail.com\" }]\ndependencies = [\n  \"fastmcp\",\n  \"requests\",\n  \"beautifulsoup4\",\n]\n[project.scripts]\nurl-text-fetcher = \"url_text_fetcher.mcp_server:main\"\n```\nThen we create the `mcp_server.py` file in the `mcp-server/url_text_fetcher` folder.\n```python\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom bs4 import BeautifulSoup\nfrom typing import List  # for type hints\n\nmcp = FastMCP(\"URL Text Fetcher\")\n\n@mcp.tool()\ndef fetch_url_text(url: str) -&gt; str:\n    \"\"\"Download the text from a URL.\"\"\"\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    return soup.get_text(separator=\"\\n\", strip=True)\n\n@mcp.tool()\ndef fetch_page_links(url: str) -&gt; List[str]:\n    \"\"\"Return a list of all URLs found on the given page.\"\"\"\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    # Extract all href attributes from &lt;a&gt; tags\n    links = [a['href'] for a in soup.find_all('a', href=True)]\n    return links\n\ndef main():\n    mcp.run()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNext, create an empty `__init__.py` in the `mcp-server/url_text_fetcher` folder.\n\nAnd finally, for the MCP server to work, you need to install it:\n\n```bash\npip install -e .\n```\n\nAt the bottom of the chat window in LM Studio, where you enter your query, you can choose an MCP server via ‚ÄúIntegrations.‚Äù By clicking ‚ÄúInstall‚Äù and then ‚ÄúEdit mcp.json,‚Äù you can add your own MCP server in that file.\n\n```json\n{\n  \"mcpServers\": {\n    \"url-text-fetcher\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"url_text_fetcher.mcp_server\"\n      ]\n    }\n  }\n}\n```\n\nThe second thing I did was integrate an existing MCP server from the Brave search engine, which allows you to instruct the AI‚Äîin a request‚Äîto search the Internet for information to answer a question. To do this, first check that you have `npx` installed. Then install `@modelcontextprotocol/server-brave-search`:\n\n```bash\nnpm i -D @modelcontextprotocol/server-brave-search\n```\n\nHere‚Äôs how you can connect it in the `mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-brave-search\"\n      ],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"..................\"\n      }\n    },\n    \"url-text-fetcher\": {\n      \"command\": \"python\",\n      \"args\": [\n        \"-m\",\n        \"url_text_fetcher.mcp_server\"\n      ]\n    }\n  }\n}\n```\n\nYou can obtain the `BRAVE_API_KEY` for free, with minor limitations of up to 2,000 requests per month and no more than one request per second.\n\nAs a result, at the bottom of the chat window in LM Studio‚Äîwhere the user enters their query‚Äîyou can select the MCP server via ‚ÄúIntegrations,‚Äù and you should see two MCP servers listed: ‚Äúmcp/url-text-fetcher‚Äù and ‚Äúmcp/brave-search.‚Äù\n\n",
          "author_fullname": "t2_gj8yhx0zk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Teaching LM Studio to Browse the Internet When Answering Questions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgljhp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754233538,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like LM Studio because it allows you to run AI models locally, preserving the privacy of your conversations with the AI. However, compared to commercial online models, LM Studio doesn‚Äôt support internet browsing ‚Äúout of the box.‚Äù Those models can‚Äôt use up-to-date information from the Internet to answer questions.&lt;/p&gt;\n\n&lt;p&gt;Not long ago, LM Studio added the ability to connect MCP servers to models. The very first thing I did was write a small MCP server that can extract text from a URL. It can also extract the links present on the page. This makes it possible, when querying the AI, to specify an address and ask it to extract text from there or retrieve links to use in its response.&lt;/p&gt;\n\n&lt;p&gt;To get all of this working, we first create a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the &lt;code&gt;mcp-server&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;p&gt;```toml \n[build-system]\nrequires = [&amp;quot;setuptools&amp;gt;=42&amp;quot;, &amp;quot;wheel&amp;quot;]\nbuild-backend = &amp;quot;setuptools.build_meta&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;[project]\nname = &amp;quot;url-text-fetcher&amp;quot;\nversion = &amp;quot;0.1.0&amp;quot;\ndescription = &amp;quot;FastMCP server for URL text fetching&amp;quot;\nauthors = [{ name=&amp;quot;Evgeny Igumnov&amp;quot;, email=&amp;quot;&lt;a href=\"mailto:igumnovnsk@gmail.com\"&gt;igumnovnsk@gmail.com&lt;/a&gt;&amp;quot; }]\ndependencies = [\n  &amp;quot;fastmcp&amp;quot;,\n  &amp;quot;requests&amp;quot;,\n  &amp;quot;beautifulsoup4&amp;quot;,\n]\n[project.scripts]\nurl-text-fetcher = &amp;quot;url_text_fetcher.mcp_server:main&amp;quot;\n&lt;code&gt;\nThen we create the `mcp_server.py` file in the `mcp-server/url_text_fetcher` folder.\n&lt;/code&gt;python\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom bs4 import BeautifulSoup\nfrom typing import List  # for type hints&lt;/p&gt;\n\n&lt;p&gt;mcp = FastMCP(&amp;quot;URL Text Fetcher&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;@mcp.tool()\ndef fetch_url_text(url: str) -&amp;gt; str:\n    &amp;quot;&amp;quot;&amp;quot;Download the text from a URL.&amp;quot;&amp;quot;&amp;quot;\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, &amp;quot;html.parser&amp;quot;)\n    return soup.get_text(separator=&amp;quot;\\n&amp;quot;, strip=True)&lt;/p&gt;\n\n&lt;p&gt;@mcp.tool()\ndef fetch_page_links(url: str) -&amp;gt; List[str]:\n    &amp;quot;&amp;quot;&amp;quot;Return a list of all URLs found on the given page.&amp;quot;&amp;quot;&amp;quot;\n    resp = requests.get(url, timeout=10)\n    resp.raise_for_status()\n    soup = BeautifulSoup(resp.text, &amp;quot;html.parser&amp;quot;)\n    # Extract all href attributes from &amp;lt;a&amp;gt; tags\n    links = [a[&amp;#39;href&amp;#39;] for a in soup.find_all(&amp;#39;a&amp;#39;, href=True)]\n    return links&lt;/p&gt;\n\n&lt;p&gt;def main():\n    mcp.run()&lt;/p&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    main()\n```&lt;/p&gt;\n\n&lt;p&gt;Next, create an empty &lt;code&gt;__init__.py&lt;/code&gt; in the &lt;code&gt;mcp-server/url_text_fetcher&lt;/code&gt; folder.&lt;/p&gt;\n\n&lt;p&gt;And finally, for the MCP server to work, you need to install it:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npip install -e .\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;At the bottom of the chat window in LM Studio, where you enter your query, you can choose an MCP server via ‚ÄúIntegrations.‚Äù By clicking ‚ÄúInstall‚Äù and then ‚ÄúEdit mcp.json,‚Äù you can add your own MCP server in that file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;url-text-fetcher&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;python&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;url_text_fetcher.mcp_server&amp;quot;\n      ]\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The second thing I did was integrate an existing MCP server from the Brave search engine, which allows you to instruct the AI‚Äîin a request‚Äîto search the Internet for information to answer a question. To do this, first check that you have &lt;code&gt;npx&lt;/code&gt; installed. Then install &lt;code&gt;@modelcontextprotocol/server-brave-search&lt;/code&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\nnpm i -D @modelcontextprotocol/server-brave-search\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs how you can connect it in the &lt;code&gt;mcp.json&lt;/code&gt; file:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;json\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;brave-search&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-y&amp;quot;,\n        &amp;quot;@modelcontextprotocol/server-brave-search&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;BRAVE_API_KEY&amp;quot;: &amp;quot;..................&amp;quot;\n      }\n    },\n    &amp;quot;url-text-fetcher&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;python&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;url_text_fetcher.mcp_server&amp;quot;\n      ]\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You can obtain the &lt;code&gt;BRAVE_API_KEY&lt;/code&gt; for free, with minor limitations of up to 2,000 requests per month and no more than one request per second.&lt;/p&gt;\n\n&lt;p&gt;As a result, at the bottom of the chat window in LM Studio‚Äîwhere the user enters their query‚Äîyou can select the MCP server via ‚ÄúIntegrations,‚Äù and you should see two MCP servers listed: ‚Äúmcp/url-text-fetcher‚Äù and ‚Äúmcp/brave-search.‚Äù&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mgljhp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ievkz",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgljhp/teaching_lm_studio_to_browse_the_internet_when/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgljhp/teaching_lm_studio_to_browse_the_internet_when/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754233538,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It‚Äôs been an eternity since is early preview‚Ä¶ and now I wonder if it is doomed. Not just because it‚Äôs slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is EXL3 doomed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgl1qz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=989197fdb000ae3849fd70a2dd3363c36e2714e5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754232324,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It‚Äôs been an eternity since is early preview‚Ä¶ and now I wonder if it is doomed. Not just because it‚Äôs slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/turboderp-org/exllamav3",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?auto=webp&amp;s=3ecca6d81c2fa96f4b86230a6c6416f82eb4d520",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f93311c0ef06e6cbd0115381f5af07f2cf7c6763",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4328d8b4f520fbe8256ab167da238bdf76b974d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ee0597232a29c03129624c370183e8c020fce3e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=778afd8299a6ccb54136a78390cc8473e58bebed",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bad6d74304971cdf123c7601152d5c744927cbd",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9314e4d7ebbf51c941f82bcfbff6bd6a8d85a358",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgl1qz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 49,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/",
          "stickied": false,
          "url": "https://github.com/turboderp-org/exllamav3",
          "subreddit_subscribers": 509911,
          "created_utc": 1754232324,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.\n\n[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)",
          "author_fullname": "t2_1ug5bi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This might be the largest un-aligned open-source model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgky8g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 217,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 217,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754232080,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?auto=webp&amp;s=b45653eadbcba17c38d2f4f2e15f6ebd41ec7c72",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7a80c31c557591f18bda1f387961a8fe38f053e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=13c5c597d54b6cfe9cc7d137d17b8c65f8c6db95",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f045816aeb8c03203c2c64cc8f2065cb392da4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cad5dd9413d8196f56dd930d3b4333ee0351d472",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37853a1aca067c019b4d7c06a11c66190e6bc001",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f4fc9332f1425fd05814a4fb77159385e0f2d89",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "54LcYt31V5699aK96P6r3bJQs24PiOVpBBLMv2INZiw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgky8g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jshin49",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754232080,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking to extract multiple tables from tabloid paper sizes; I tried GMFT, img2table, and camelot and found no success, likely because they were not trained on larger paper sizes. I currently use Docling, which is honestly the best OCR tool I have found for my use case. Still, however, it misses some tables.\n\nWhat do you guys recommend I combine Docling with for 100% table extraction?",
          "author_fullname": "t2_w949wvlt6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Table Extraction for Tabloid Paper Sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkus5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754231843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to extract multiple tables from tabloid paper sizes; I tried GMFT, img2table, and camelot and found no success, likely because they were not trained on larger paper sizes. I currently use Docling, which is honestly the best OCR tool I have found for my use case. Still, however, it misses some tables.&lt;/p&gt;\n\n&lt;p&gt;What do you guys recommend I combine Docling with for 100% table extraction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgkus5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Stranger-1229",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgkus5/table_extraction_for_tabloid_paper_sizes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgkus5/table_extraction_for_tabloid_paper_sizes/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754231843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a couple or RTX6000Blackwell GPUs but LLMstudio only uses the memory up to ~70GB per GPU even after I already set the Guardrails to ‚Äúrelaxed‚Äù. If I enable ‚ÄúLimit Model Offload to Dedicated GPU Memory‚Äù the situation gets even worse and only ~20GB are used.",
          "author_fullname": "t2_a0v2ol2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLMstudio doesn‚Äôt use all the available VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkpm6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754231472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple or RTX6000Blackwell GPUs but LLMstudio only uses the memory up to ~70GB per GPU even after I already set the Guardrails to ‚Äúrelaxed‚Äù. If I enable ‚ÄúLimit Model Offload to Dedicated GPU Memory‚Äù the situation gets even worse and only ~20GB are used.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgkpm6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Khipu28",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgkpm6/llmstudio_doesnt_use_all_the_available_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgkpm6/llmstudio_doesnt_use_all_the_available_vram/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754231472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally got to finish a weekend project from a couple of months ago. \n\nThis is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).\n\nFull source and configuration guide is on GitHub: [https://github.com/av/unhype](https://github.com/av/unhype) ",
          "author_fullname": "t2_o7p5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Use local LLM to neutralise the headers on the web",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkiti",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bd9e9e",
          "ups": 470,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756902682%2COGU1YjkyNmJjYTU3MjgxOGI3MDY4MjNiMTM4ODExZWVlM2QyOGJlODc3MzhiOGQ1MTg3NzZhNGRmMzc4ZGM4ZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756902682%2CMjU2YjUwMThkNzM0YmY5YjZjNjdlZjQ3MGJiMWNjMWYwZTdjNmQyZjA4MGU0MjdmN2UyNjlhZmU2YjI4MzY1MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 470,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=140&amp;height=89&amp;crop=140:89,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f692b7d05bb54cb77646e5499a279385018af33e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Alpaca"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754230985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt;\n\n&lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt;\n\n&lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href=\"https://github.com/av/unhype\"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/niaha18uctgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?format=pjpg&amp;auto=webp&amp;s=e3f711ff053fccc48764a295b7ef68533d8f7f9a",
                  "width": 1688,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=47b5550c7dd6476d0d3e442d2ca5c2a382f74e92",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8828ff7d3eb9b270e88f2ec7ff4973818f896e8e",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bc01544a8c25b4174c98c4519803a567aa98c887",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9c9498bdb1588d50d25b53d82635a1092a82f4ef",
                    "width": 640,
                    "height": 409
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=402ac5eba7ed08a00bc9e012b6a4bd69a46e4608",
                    "width": 960,
                    "height": 614
                  },
                  {
                    "url": "https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=374d1819675e3e0f60cf4790383435325e543d5f",
                    "width": 1080,
                    "height": 690
                  }
                ],
                "variants": {},
                "id": "NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Alpaca",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgkiti",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Everlier",
          "discussion_type": null,
          "num_comments": 69,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/",
          "stickied": false,
          "url": "https://v.redd.it/niaha18uctgf1",
          "subreddit_subscribers": 509911,
          "created_utc": 1754230985,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/niaha18uctgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1688,
              "scrubber_media_url": "https://v.redd.it/niaha18uctgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/niaha18uctgf1/DASHPlaylist.mpd?a=1756902682%2COGU1YjkyNmJjYTU3MjgxOGI3MDY4MjNiMTM4ODExZWVlM2QyOGJlODc3MzhiOGQ1MTg3NzZhNGRmMzc4ZGM4ZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/niaha18uctgf1/HLSPlaylist.m3u8?a=1756902682%2CMjU2YjUwMThkNzM0YmY5YjZjNjdlZjQ3MGJiMWNjMWYwZTdjNmQyZjA4MGU0MjdmN2UyNjlhZmU2YjI4MzY1MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ollama‚Äôs new app requires an internet connection to send messages? Other people have also reported this issue but there has been no explanation. Have others here encountered this? Am concerned because I was hoping to get my work to use the new app but now I don‚Äôt think I should.",
          "author_fullname": "t2_1hnj2fi47g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama app requires internet ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgkgek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754230814,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "discord.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ollama‚Äôs new app requires an internet connection to send messages? Other people have also reported this issue but there has been no explanation. Have others here encountered this? Am concerned because I was hoping to get my work to use the new app but now I don‚Äôt think I should.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://discord.com/channels/1128867683291627614/1400481246580052140",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgkgek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MudMaleficent8980",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgkgek/ollama_app_requires_internet/",
          "stickied": false,
          "url": "https://discord.com/channels/1128867683291627614/1400481246580052140",
          "subreddit_subscribers": 509911,
          "created_utc": 1754230814,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Let's tease each other.\n\nWhat is your local AI setup?\nAre you proud of it?\nWhat would you have done differently?\n\nWhat model you use? Contrxt lenght? TPS?\n\nI only have a MBP2019, so I will just be teased üòÇ",
          "author_fullname": "t2_7lunrav9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Your proud AI setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgk2nm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754229855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s tease each other.&lt;/p&gt;\n\n&lt;p&gt;What is your local AI setup?\nAre you proud of it?\nWhat would you have done differently?&lt;/p&gt;\n\n&lt;p&gt;What model you use? Contrxt lenght? TPS?&lt;/p&gt;\n\n&lt;p&gt;I only have a MBP2019, so I will just be teased üòÇ&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgk2nm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recent-Success-1520",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgk2nm/your_proud_ai_setup/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754229855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was wondering what mini models you guys were using and what‚Äôs good and what isn‚Äôt, I mostly just need something for quick categorization, I was using ChatGPT 4o-mini api for most things but I should probably swap to something local at this point ",
          "author_fullname": "t2_e49kxbgs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source alternatives to gpt 4o mini?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjvn8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754229360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering what mini models you guys were using and what‚Äôs good and what isn‚Äôt, I mostly just need something for quick categorization, I was using ChatGPT 4o-mini api for most things but I should probably swap to something local at this point &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjvn8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "abaris243",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjvn8/open_source_alternatives_to_gpt_4o_mini/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjvn8/open_source_alternatives_to_gpt_4o_mini/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754229360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The jinja template that comes with the MLX version of GLM 4.5 is using xml style tool calls instead of json. Here's a json template. This means that it is now able to do tool calls in OpenCode, and presumably other things as well (Qwen code/Gemini?). Here's the template:\n\n  \n[https://pastebin.com/CfMw7hFS](https://pastebin.com/CfMw7hFS)",
          "author_fullname": "t2_12ggykute6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Tool Calling Jinja Template",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjpvm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228935,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The jinja template that comes with the MLX version of GLM 4.5 is using xml style tool calls instead of json. Here&amp;#39;s a json template. This means that it is now able to do tool calls in OpenCode, and presumably other things as well (Qwen code/Gemini?). Here&amp;#39;s the template:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pastebin.com/CfMw7hFS\"&gt;https://pastebin.com/CfMw7hFS&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgjpvm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-dysangel-",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgjpvm/glm_45_tool_calling_jinja_template/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjpvm/glm_45_tool_calling_jinja_template/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754228935,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.\n\nI think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about [commoditizing your complement](https://gwern.net/complement). But for many of these companies, which are pure play AI Labs, this simply does not hold.\n\nSo the question remains, why are they doing this?\n\nOne theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is [a precedent](https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838) of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.\n\nIf this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.\n\nWhat prompted this question, was the [recent interview](https://youtu.be/mYDSSRS-B5U?t=2203) with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).\n\n\\---\n\nOne aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?",
          "author_fullname": "t2_garopjsj1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are Chinese LLM companies effectively price dumping?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjlek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 191,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 191,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754232034,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754228603,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt;\n\n&lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it&amp;#39;s about &lt;a href=\"https://gwern.net/complement\"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt;\n\n&lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt;\n\n&lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href=\"https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838\"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt;\n\n&lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don&amp;#39;t find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt;\n\n&lt;p&gt;What prompted this question, was the &lt;a href=\"https://youtu.be/mYDSSRS-B5U?t=2203\"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don&amp;#39;t find his response very compelling).&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?auto=webp&amp;s=537c5cea0430f04039f256f31d55847ec27c39b6",
                  "width": 1238,
                  "height": 1400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3562c0af61eb68438add188a0ebb448f79b54657",
                    "width": 108,
                    "height": 122
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51a1f9e5bee46295e1d5ac9dc517b0097b9fa489",
                    "width": 216,
                    "height": 244
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=112817eb7860f41d0e5f471023d0200983bb9f91",
                    "width": 320,
                    "height": 361
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=577450661689dd64b4c310d0803093687dd84688",
                    "width": 640,
                    "height": 723
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcdc0e42b1e5a63bb47ad4fa640fbc0fc006cd5c",
                    "width": 960,
                    "height": 1085
                  },
                  {
                    "url": "https://external-preview.redd.it/Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc589f590b2d3181d4aca8b2fcc7ea75fd5cda5e",
                    "width": 1080,
                    "height": 1221
                  }
                ],
                "variants": {},
                "id": "Hj2gEIWsZf7TIi7s4YmEkqIlioGRdkXinRkdl7AYSEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgjlek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uutnt",
          "discussion_type": null,
          "num_comments": 209,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754228603,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I play The Expanse role-playing game with some friends every week over Zoom. I've captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. \n\nFrom a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through [roll20.net](http://roll20.net) (a virtual tabletop website) as if it were a chat participant.\n\nIt's been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I've defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.\n\nI haven't learned much about training/retraining/pretraining/fine-tuning yes, and I'm wondering if those are better approaches than just bootstrapping the convo\n\n\n\nOther Features in flight:\n\nIntegrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as \"how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration\". \n\nLoading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I'm sure I'm using it incorrectly or perhaps enabling it during inference when it shouldn't be. I could definitely use some advice on that.\n\n\n\nThis must be a common idea, and I'm sure others are working on similar applications; how do I find them?",
          "author_fullname": "t2_bpv2j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Roleplay with large historical context and RAG",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgjcai",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754227908,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I play The Expanse role-playing game with some friends every week over Zoom. I&amp;#39;ve captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. &lt;/p&gt;\n\n&lt;p&gt;From a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through &lt;a href=\"http://roll20.net\"&gt;roll20.net&lt;/a&gt; (a virtual tabletop website) as if it were a chat participant.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I&amp;#39;ve defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t learned much about training/retraining/pretraining/fine-tuning yes, and I&amp;#39;m wondering if those are better approaches than just bootstrapping the convo&lt;/p&gt;\n\n&lt;p&gt;Other Features in flight:&lt;/p&gt;\n\n&lt;p&gt;Integrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as &amp;quot;how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Loading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I&amp;#39;m sure I&amp;#39;m using it incorrectly or perhaps enabling it during inference when it shouldn&amp;#39;t be. I could definitely use some advice on that.&lt;/p&gt;\n\n&lt;p&gt;This must be a common idea, and I&amp;#39;m sure others are working on similar applications; how do I find them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgjcai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RoboCopsGoneMad",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754227908,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are they really gonna train a model that's absolutely useless to give to us?",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why doesn't \"OpenAI\" just release one of the models they already have? Like 3.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgiyg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 255,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 255,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754226851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are they really gonna train a model that&amp;#39;s absolutely useless to give to us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgiyg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 188,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754226851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello\n\nI am currently looking for the best launch parameters for CPU inference with llama.cpp. I am running the Qwen3-30B-A3B model on my laptop with the following specs:\n\nAMD Ryzen 7 PRO 7840u w/ Radeon 780M Graphics (16CPUs), 32GB Ram.\n\nSince the whole topic around the launch parameters is rather complex, I wanted to ask you about your experience and overall best practices regarding pure CPU inference.\n\nCurrently I am running llama.cpp with the following parameters:\n\nllama-server.exe -m models\\\\X -t 16 --n\\_predict 4096 --ctx-size 64000\n\nThanks in advance!",
          "author_fullname": "t2_1k95d35h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Practice For CPU Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgixw4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754226810,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I am currently looking for the best launch parameters for CPU inference with llama.cpp. I am running the Qwen3-30B-A3B model on my laptop with the following specs:&lt;/p&gt;\n\n&lt;p&gt;AMD Ryzen 7 PRO 7840u w/ Radeon 780M Graphics (16CPUs), 32GB Ram.&lt;/p&gt;\n\n&lt;p&gt;Since the whole topic around the launch parameters is rather complex, I wanted to ask you about your experience and overall best practices regarding pure CPU inference.&lt;/p&gt;\n\n&lt;p&gt;Currently I am running llama.cpp with the following parameters:&lt;/p&gt;\n\n&lt;p&gt;llama-server.exe -m models\\X -t 16 --n_predict 4096 --ctx-size 64000&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgixw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hudimudi",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754226810,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA's \"Highly Optimistic\" DGX Spark Mini-Supercomputer Still Hasn't Hit Retail Despite a Planned July Launch, Suggesting Possible Production Issues",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgis6h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=293dff94e206d07e95b1ceb35d306f084972280b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754226361,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?auto=webp&amp;s=761d7765165ce23883e3ea7e5265165d93fd7502",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=462a9746ffb12643f31808d38538f4c0ea76b555",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f437e173d256a303cec220b9cd2f260843de5e3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f8b731f2beec62abb38c3d104c182153621be67",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09c3744f2431f90355d37d937f1352192cc87780",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee4f08a8c7da0b870a09e2fee6c837603f3ccdca",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f610522848162744871027feef9554990de435d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "PD8rnvifNMtTH2QZfbt1ABecTzvsQu7n786xD74W-RU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgis6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 48,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgis6h/nvidias_highly_optimistic_dgx_spark/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-highly-optimistic-dgx-spark-mini-supercomputer-still-hasnt-hit-retail/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754226361,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHey LocalLLaMA team,\n\nI'm hoping someone much covered than me can help with a question about fine-tuning.\n\nI've been using the MLX library to fine-tune a model on my MacBook, but I need to test the model on other devices that aren't Macs. I'm wondering if there's a best practice for this workflow.\n\nIdeally, I'd like to keep the adapters separate from the base model, but if fusing them is the only way, that's fine too.\n\nSo far, I've only fine-tuned a quantized model and have tried converting the adapters to the PEFT format. The problem is, when I test the output on my MacBook, the base Hugging Face model works fine, but the model with the PEFT adapters just outputs gibberish. This might be due to a precision mismatch.\n\nAny advice or suggestions on how to handle this would be greatly appreciated!\n",
          "author_fullname": "t2_1hmx216rd2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MLX -&gt; GGUF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgifea",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754241546,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754225338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey LocalLLaMA team,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping someone much covered than me can help with a question about fine-tuning.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using the MLX library to fine-tune a model on my MacBook, but I need to test the model on other devices that aren&amp;#39;t Macs. I&amp;#39;m wondering if there&amp;#39;s a best practice for this workflow.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;d like to keep the adapters separate from the base model, but if fusing them is the only way, that&amp;#39;s fine too.&lt;/p&gt;\n\n&lt;p&gt;So far, I&amp;#39;ve only fine-tuned a quantized model and have tried converting the adapters to the PEFT format. The problem is, when I test the output on my MacBook, the base Hugging Face model works fine, but the model with the PEFT adapters just outputs gibberish. This might be due to a precision mismatch.&lt;/p&gt;\n\n&lt;p&gt;Any advice or suggestions on how to handle this would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgifea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_Another_LLM",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgifea/mlx_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgifea/mlx_gguf/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754225338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If",
          "author_fullname": "t2_gws95urkx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building for the era of experience",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgi9df",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "author_cakeday": true,
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754224836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "rnikhil.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://rnikhil.com/2025/07/30/era-of-experience",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgi9df",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Excellent-Effect237",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgi9df/building_for_the_era_of_experience/",
          "stickied": false,
          "url": "https://rnikhil.com/2025/07/30/era-of-experience",
          "subreddit_subscribers": 509911,
          "created_utc": 1754224836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any interesting questions from your experience that you asked a Reasoning LLM and it failed to answer",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is an interesting question that an LLM failed to answer, in your experience?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgi7v8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754224711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any interesting questions from your experience that you asked a Reasoning LLM and it failed to answer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgi7v8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgi7v8/what_is_an_interesting_question_that_an_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgi7v8/what_is_an_interesting_question_that_an_llm/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754224711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another new model claiming to outperform larger ones:\n\n\n\n**Instruction following** is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.\n\nWe identify **lazy reasoning** during the thinking stage as a key cause of poor instruction adherence.\n\nTo address this, we propose a framework that promotes rigorous reasoning through **previewing and self-checking**.\n\nOur method begins by generating instruction data with **complex constraints**, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.\n\nTraining involves entropy-preserving supervised fine-tuning (**Entropy-SFT**) and token-wise entropy-adaptive reinforcement learning (**TEA-RL**), guided by rule-based multidimensional rewards.\n\nThis approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.\n\nExperiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like **DeepSeek-R1** and closed-source models like **ChatGPT-4o** on challenging instruction-following benchmarks.\n\n[https://huggingface.co/qihoo360/Light-IF-32B](https://huggingface.co/qihoo360/Light-IF-32B)\n\n\n\ntechnical report [https://huggingface.co/papers/2503.10460](https://huggingface.co/papers/2503.10460)\n\n\n\nprevious popular models by this company:\n\n[https://huggingface.co/qihoo360/TinyR1-32B-Preview](https://huggingface.co/qihoo360/TinyR1-32B-Preview)\n\n[https://huggingface.co/qihoo360/Light-R1-32B](https://huggingface.co/qihoo360/Light-R1-32B)\n\n  \nWhat do you think?",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qihoo360/Light-IF-32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mghy1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QIubNPPJjdRirxQ73mfKHMMmxIY2VZ9TzcjXGXjIohk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt;\n\n&lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt;\n\n&lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt;\n\n&lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt;\n\n&lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt;\n\n&lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B\"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;technical report &lt;a href=\"https://huggingface.co/papers/2503.10460\"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;previous popular models by this company:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/TinyR1-32B-Preview\"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-R1-32B\"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6vaf0crhrsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6vaf0crhrsgf1.png?auto=webp&amp;s=6ba618920def17d94508256824e2561aba8a6ec9",
                  "width": 1062,
                  "height": 980
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=081b3d7480032b122c209477d47263419358b811",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fac696d38efe6b971413c8794edd434ae2c9926",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20af7c29a89f974a98e34d36ff62aa93c6d3e970",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73",
                    "width": 640,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=753db7d1045cf75fdd0e7ff4fd0c94209e8cee25",
                    "width": 960,
                    "height": 885
                  }
                ],
                "variants": {},
                "id": "O7-1ZfpSudq0amigVzUb6mHn4tEC8x9xtRLUGpzh3sI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghy1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/",
          "stickied": false,
          "url": "https://i.redd.it/6vaf0crhrsgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754223868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fsj3mnbne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI \"devs\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mghw96",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.12,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MXA6nG0kGGLD8fsqDWMZfWjsdQgrz0Jg8Jmlmn4a-LE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223705,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4vmx6ozprsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4vmx6ozprsgf1.png?auto=webp&amp;s=a49daca39d28d951de0f2fd42313f033a4c6ddbc",
                  "width": 305,
                  "height": 623
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4vmx6ozprsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd212714d7b2bcdcd6155bf3672e93951b80152b",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/4vmx6ozprsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=59bcfddf4f4ab1c0905bae64211703fe83d4acf7",
                    "width": 216,
                    "height": 432
                  }
                ],
                "variants": {},
                "id": "jfaFuF77KxK6peXK3jY4yV76Uy2oZEKdtZTNNcwNppU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghw96",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dangerous-Camera3368",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mghw96/ai_devs/",
          "stickied": false,
          "url": "https://i.redd.it/4vmx6ozprsgf1.png",
          "subreddit_subscribers": 509911,
          "created_utc": 1754223705,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hot take incoming:\n\n  \nThis is a garbage card with garbage support, so quit talking about them like they're useful.  As a matter of fact, quit talking about them at all.   \n\n  \nYou see it took me up until 4 weeks ago to convince my wife to finally let me upgrade my server, I picked up a d380 v9 with 128 gb and 7 1 tb drives, 1000 watt psus and the gpu enablement kit.  Problem?  That cleared out my savings no worries I'll save up and be good in 2-3 months.  Started doing research on cards that I could afford and were available, quickly realized if I was going to get any sort of horsepower and vram I was going to have to go team red.  no worries, i'd rather have a bit of a challenge than plug n play plus nvidia's poor driver support for linux irked me, so looking for amd cards, MI50 16gb 300 up here in canuckistan, kk i can do that in 2-3 months (i have a kid starting uni this fall and another teenaged boy  who  eats the equivalent of a rhino every 2 days).  I'm about 3/4 of the way there amd releases new rocm that doesn't \"support\" mi50, price falls out, market flooded with 32gb models, happy dance, i'll order this weekend, come friday, right before the end of the day i'm brought into bosses office, squirrel ( or whatever the hell this weird ass accounts name is) squirrel as you know we were bought out last week, we are going to have to reduce headcount in your role.  to how many employees sir?  0\n\nque sad dance\n\ngpu savings now = kraft dinner and rice\n\nwatching cheap 32 gb video cards turn into dodo birds, que very very sad dance\n\n  \nconclusion:\n\nMI50 w 32gb?  horrible card!!!! do not buy!  leave some for squirrel for when he gets new job, in 30 years or whenever economy turns around since in canada you can't sell blood and squirrel got fixed after last kid so can't sell that either.\n\n  \nextra conclusion:\n\nplease, no more talking about how great and cheap a 32 gb mi50 is, squirrel (or whatever my name is) slept with pictar of mi50 under pillow for looooong time since cheap card lots of vram and elbow grease doesnt scare him.   keep normies away from mi50, tell them 3090 much better purchase, they spend all monies none left to spend on mi50 squirrel slowly get happy again\n\n  \nthank you for time well spent!",
          "author_fullname": "t2_8alx42ew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 w 32gb?  Guys please",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mghhau",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.32,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754222398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hot take incoming:&lt;/p&gt;\n\n&lt;p&gt;This is a garbage card with garbage support, so quit talking about them like they&amp;#39;re useful.  As a matter of fact, quit talking about them at all.   &lt;/p&gt;\n\n&lt;p&gt;You see it took me up until 4 weeks ago to convince my wife to finally let me upgrade my server, I picked up a d380 v9 with 128 gb and 7 1 tb drives, 1000 watt psus and the gpu enablement kit.  Problem?  That cleared out my savings no worries I&amp;#39;ll save up and be good in 2-3 months.  Started doing research on cards that I could afford and were available, quickly realized if I was going to get any sort of horsepower and vram I was going to have to go team red.  no worries, i&amp;#39;d rather have a bit of a challenge than plug n play plus nvidia&amp;#39;s poor driver support for linux irked me, so looking for amd cards, MI50 16gb 300 up here in canuckistan, kk i can do that in 2-3 months (i have a kid starting uni this fall and another teenaged boy  who  eats the equivalent of a rhino every 2 days).  I&amp;#39;m about 3/4 of the way there amd releases new rocm that doesn&amp;#39;t &amp;quot;support&amp;quot; mi50, price falls out, market flooded with 32gb models, happy dance, i&amp;#39;ll order this weekend, come friday, right before the end of the day i&amp;#39;m brought into bosses office, squirrel ( or whatever the hell this weird ass accounts name is) squirrel as you know we were bought out last week, we are going to have to reduce headcount in your role.  to how many employees sir?  0&lt;/p&gt;\n\n&lt;p&gt;que sad dance&lt;/p&gt;\n\n&lt;p&gt;gpu savings now = kraft dinner and rice&lt;/p&gt;\n\n&lt;p&gt;watching cheap 32 gb video cards turn into dodo birds, que very very sad dance&lt;/p&gt;\n\n&lt;p&gt;conclusion:&lt;/p&gt;\n\n&lt;p&gt;MI50 w 32gb?  horrible card!!!! do not buy!  leave some for squirrel for when he gets new job, in 30 years or whenever economy turns around since in canada you can&amp;#39;t sell blood and squirrel got fixed after last kid so can&amp;#39;t sell that either.&lt;/p&gt;\n\n&lt;p&gt;extra conclusion:&lt;/p&gt;\n\n&lt;p&gt;please, no more talking about how great and cheap a 32 gb mi50 is, squirrel (or whatever my name is) slept with pictar of mi50 under pillow for looooong time since cheap card lots of vram and elbow grease doesnt scare him.   keep normies away from mi50, tell them 3090 much better purchase, they spend all monies none left to spend on mi50 squirrel slowly get happy again&lt;/p&gt;\n\n&lt;p&gt;thank you for time well spent!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mghhau",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Expensive_Mirror5247",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mghhau/mi50_w_32gb_guys_please/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mghhau/mi50_w_32gb_guys_please/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754222398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My career is in tech startup chaos.  Bill Gurley is one of the few from that circle I can listen to while chewing food (as I am now and typing).  \n\nCompanies like LG want to sell washing machines.  They don't want their strategy to get disrupted without having a backup plan.  They want to raise the floor so that nobody can get too far ahead.  They want to scorch the Earth so that their biggest competitors won't be earning money that they can't compete for.  Sell AI washing machines = shareholder value protected = mission accomplished.\n\nStrategically, the allies of small open models weirdly includes giant companies and SMEs whenever their primary interest is not in competing directly to *operate* revenue-generating AI.  They want to invest in things that protect their strategy.  They only need a sensible way to do it and not move alone.",
          "author_fullname": "t2_8vhsch4i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why Fortune 500 Wants to Fund Open Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgh2cb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Bg2 Pod",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/fTqINzeudJ4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Bg2Pod"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mgh2cb",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=fe76625dacb81af7ec54f68753f62ea2f3c58290",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My career is in tech startup chaos.  Bill Gurley is one of the few from that circle I can listen to while chewing food (as I am now and typing).  &lt;/p&gt;\n\n&lt;p&gt;Companies like LG want to sell washing machines.  They don&amp;#39;t want their strategy to get disrupted without having a backup plan.  They want to raise the floor so that nobody can get too far ahead.  They want to scorch the Earth so that their biggest competitors won&amp;#39;t be earning money that they can&amp;#39;t compete for.  Sell AI washing machines = shareholder value protected = mission accomplished.&lt;/p&gt;\n\n&lt;p&gt;Strategically, the allies of small open models weirdly includes giant companies and SMEs whenever their primary interest is not in competing directly to &lt;em&gt;operate&lt;/em&gt; revenue-generating AI.  They want to invest in things that protect their strategy.  They only need a sensible way to do it and not move alone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/fTqINzeudJ4?&amp;t=1067",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?auto=webp&amp;s=27043eee8c92b37f592065d2ff05fd85b27b6c4a",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe82e3d2ef0e784a0e62fb6596ee14119fb2c44",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c52288f06eae6d456c4b6ac64ac2b6f1b61a1e65",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e74ffd7ecfd40e764b51c4df1a4b893fda673f1f",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgh2cb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psionikus",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh2cb/why_fortune_500_wants_to_fund_open_models/",
          "stickied": false,
          "url": "https://youtu.be/fTqINzeudJ4?&amp;t=1067",
          "subreddit_subscribers": 509911,
          "created_utc": 1754220993,
          "num_crossposts": 1,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Bg2 Pod",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/fTqINzeudJ4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Bg2Pod"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.\n\nHow it works:\n\n- Drop a .awesome-ai.md file in your repo root (template: https://github.com/teodorgross/awesome-ai)\n\n- The scanner finds it automatically within 30 minutes  \n\n- Creates a pull request for review\n\n- Your tool goes live with real-time GitHub stats on (https://awesome-ai.io)\n\nWhy this matters:\n\n- No more manual submissions or contact forms\n\n- Tools stay up-to-date automatically when you push changes\n\n- GitHub verification prevents spam\n\n- Real-time star tracking and leaderboards\n\nThink of it like .gitignore for Git, but for AI tool discovery. ",
          "author_fullname": "t2_92fpag8i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a GitHub scanner that automatically discovers your AI tools using a new .awesome-ai.md standard I created",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgh19i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2f6c6297df4b4027fb8d1d453c04b9819bf72371",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.&lt;/p&gt;\n\n&lt;p&gt;How it works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Drop a .awesome-ai.md file in your repo root (template: &lt;a href=\"https://github.com/teodorgross/awesome-ai\"&gt;https://github.com/teodorgross/awesome-ai&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The scanner finds it automatically within 30 minutes  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a pull request for review&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your tool goes live with real-time GitHub stats on (&lt;a href=\"https://awesome-ai.io\"&gt;https://awesome-ai.io&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why this matters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;No more manual submissions or contact forms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tools stay up-to-date automatically when you push changes&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GitHub verification prevents spam&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Real-time star tracking and leaderboards&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Think of it like .gitignore for Git, but for AI tool discovery. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/teodorgross/awesome-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?auto=webp&amp;s=423c2b45e5c57364da28298579b3a5631b59c0ed",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd85376252643c0cfc25bf58873d1308bbaa8b8c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15eb575a42080dcefe65c839eb0e4c036819417",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7d83cf6189b7f3c98c0a7b6f2c3d772895ee7ba",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d63a88dbef47d35bfb865b575d09eda2e4144500",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f688b76be41b12b1a7923494dacac226e59a932e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0e5bc45cd19159360de8a68ed1794010b721da7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgh19i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "r00tkit_",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh19i/i_built_a_github_scanner_that_automatically/",
          "stickied": false,
          "url": "https://github.com/teodorgross/awesome-ai",
          "subreddit_subscribers": 509911,
          "created_utc": 1754220889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nPersonal note: This is just my opinion based on a very limited set of API-only probes‚Äîinterpret with caution.\n\nThis is about probing Horizon Beta (on openrouter) \n\nWhat I did (mini-ROC probes)\n\n* JSON strictness vs. \"bad schema\" repair\n* Tool-calling with an invalid enum + extra property\n* Safety/refusal phrasing check\n* Long-context end-marker recall\n* Tokenizer/short-output edge case\n* Determinism at T=0\n* Tiny style-paraphrase probe\n\nHighlights\n\n* Tool-calling: It silently coerces invalid enums (mode=\"plane\" -&gt; \"car\"/\"train\") and drops extra fields, then emits an OpenAI-style tool\\_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn't call the tool under the same bad input - which is more typical for OpenAI.\n* JSON mode: It \"repairs\" invalid inputs into valid JSON (e.g., {\"ok\": false, \"mode\": \"A\"}). OpenAI also repairs but tends to be more minimally formatted.\n* Safety tone: Opens with \"I can't help with that.\" - Anthropic-ish cadence that many Llama-style distills mimic.\n* Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END\\_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.\n* Determinism: Stable at T=0 on simple tasks.\n* Multilingual: Correct Â¶πÂ¶π -&gt; \"younger sister,\" and clean pronoun disambiguation.\n\nAnchors I compared against\n\n* OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)\n* Llama: llama-3.3-70b-instruct, llama-3-70b-instruct\n* Qwen: qwen-2.5-72b-instruct\n* Mistral: mixtral-8x22b-instruct\n\nBottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific \"finish=length\" quirk on short outputs. It does not match OpenAI's tool-call behavior in my probes.\n\nAll tests were standard API usage.",
          "author_fullname": "t2_2fmsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OSINT fingerprinting a stealth OpenRouter model - likely Llama-family, not OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mggsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754224198,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754220059,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Personal note: This is just my opinion based on a very limited set of API-only probes‚Äîinterpret with caution.&lt;/p&gt;\n\n&lt;p&gt;This is about probing Horizon Beta (on openrouter) &lt;/p&gt;\n\n&lt;p&gt;What I did (mini-ROC probes)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JSON strictness vs. &amp;quot;bad schema&amp;quot; repair&lt;/li&gt;\n&lt;li&gt;Tool-calling with an invalid enum + extra property&lt;/li&gt;\n&lt;li&gt;Safety/refusal phrasing check&lt;/li&gt;\n&lt;li&gt;Long-context end-marker recall&lt;/li&gt;\n&lt;li&gt;Tokenizer/short-output edge case&lt;/li&gt;\n&lt;li&gt;Determinism at T=0&lt;/li&gt;\n&lt;li&gt;Tiny style-paraphrase probe&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Highlights&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tool-calling: It silently coerces invalid enums (mode=&amp;quot;plane&amp;quot; -&amp;gt; &amp;quot;car&amp;quot;/&amp;quot;train&amp;quot;) and drops extra fields, then emits an OpenAI-style tool_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn&amp;#39;t call the tool under the same bad input - which is more typical for OpenAI.&lt;/li&gt;\n&lt;li&gt;JSON mode: It &amp;quot;repairs&amp;quot; invalid inputs into valid JSON (e.g., {&amp;quot;ok&amp;quot;: false, &amp;quot;mode&amp;quot;: &amp;quot;A&amp;quot;}). OpenAI also repairs but tends to be more minimally formatted.&lt;/li&gt;\n&lt;li&gt;Safety tone: Opens with &amp;quot;I can&amp;#39;t help with that.&amp;quot; - Anthropic-ish cadence that many Llama-style distills mimic.&lt;/li&gt;\n&lt;li&gt;Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.&lt;/li&gt;\n&lt;li&gt;Determinism: Stable at T=0 on simple tasks.&lt;/li&gt;\n&lt;li&gt;Multilingual: Correct Â¶πÂ¶π -&amp;gt; &amp;quot;younger sister,&amp;quot; and clean pronoun disambiguation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anchors I compared against&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)&lt;/li&gt;\n&lt;li&gt;Llama: llama-3.3-70b-instruct, llama-3-70b-instruct&lt;/li&gt;\n&lt;li&gt;Qwen: qwen-2.5-72b-instruct&lt;/li&gt;\n&lt;li&gt;Mistral: mixtral-8x22b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Bottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific &amp;quot;finish=length&amp;quot; quirk on short outputs. It does not match OpenAI&amp;#39;s tool-call behavior in my probes.&lt;/p&gt;\n\n&lt;p&gt;All tests were standard API usage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mggsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jv0010",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "subreddit_subscribers": 509911,
          "created_utc": 1754220059,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}