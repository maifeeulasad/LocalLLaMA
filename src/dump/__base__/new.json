{
  "kind": "Listing",
  "data": {
    "after": "t3_1mbce7b",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ming-lite-omni v1.5 demonstrates highly competitive results compared to industry-leading models of similar scale.\n\nü§ñGithub: [https://github.com/inclusionAI/Ming](https://github.com/inclusionAI/Ming)\n\nü´ÇHugging Face: [https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5](https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5)\n\nüç≠ModelScope: [https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5](https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5)\n\n\n\nMing-lite-omni v1.5 features three key improvements compared to Ming-lite-omni:¬†\n\nüß† Enhanced Multimodal Comprehension: Ming-lite-omni v1.5 now understands all data types‚Äîimages, text, video, and speech‚Äîsignificantly better, thanks to extensive data upgrades.\n\nüé® Precise Visual Editing Control: Achieve superior image generation and editing with Ming-lite-omni v1.5, featuring advanced controls for consistent IDs and scenes, and enhanced support for visual tasks like detection and segmentation.\n\n‚ú® Optimized User Experience: Expect a smoother, more accurate, and aesthetically pleasing interaction with Ming-lite-omni v1.5.\n\n¬†\n\n",
          "author_fullname": "t2_151ygwhz4x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "üåü Ming-lite-omni v1.5 is here! Our recent upgrade for omni-modal AI! üöÄ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc9sk0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753791557,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753791278,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ming-lite-omni v1.5 demonstrates highly competitive results compared to industry-leading models of similar scale.&lt;/p&gt;\n\n&lt;p&gt;ü§ñGithub: &lt;a href=\"https://github.com/inclusionAI/Ming\"&gt;https://github.com/inclusionAI/Ming&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;ü´ÇHugging Face: &lt;a href=\"https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5\"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;üç≠ModelScope: &lt;a href=\"https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5\"&gt;https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ming-lite-omni v1.5 features three key improvements compared to Ming-lite-omni:¬†&lt;/p&gt;\n\n&lt;p&gt;üß† Enhanced Multimodal Comprehension: Ming-lite-omni v1.5 now understands all data types‚Äîimages, text, video, and speech‚Äîsignificantly better, thanks to extensive data upgrades.&lt;/p&gt;\n\n&lt;p&gt;üé® Precise Visual Editing Control: Achieve superior image generation and editing with Ming-lite-omni v1.5, featuring advanced controls for consistent IDs and scenes, and enhanced support for visual tasks like detection and segmentation.&lt;/p&gt;\n\n&lt;p&gt;‚ú® Optimized User Experience: Expect a smoother, more accurate, and aesthetically pleasing interaction with Ming-lite-omni v1.5.&lt;/p&gt;\n\n&lt;p&gt;¬†&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?auto=webp&amp;s=c6f1fb40398fc89dc628ab2dafbd108908044fc3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=31297b282b6331a189dda7dd12bb8d1bd4c26cdd",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ebb22845e50d756630f8985008b64400e0c4731",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c828c059a6dff1662a2f780d6569480e9759bff8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e02ca89e5fdc6223b728c180fcb763f89d3218b5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=85bdb586eb7ca7f28a4c2c99bdf0c34035135135",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=43ad38d7a1fcef4a89fdcc53406a34bcfdee3064",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "hvSd7ucphM9xJ6wIsV91sPJfivSgujp52HHMZgdZQ8U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc9sk0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dependent-Roll-8934",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc9sk0/mingliteomni_v15_is_here_our_recent_upgrade_for/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753791278,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Our experimental Ming-lite-omni v1.5 (https://github.com/inclusionAI/Ming) leverages advanced audio-visual capabilities to explore new frontiers in interactive learning. This model, still under development, aims to understand your handwriting, interpret your thoughts, and guide you through solutions in real-time. We're eagerly continuing our research and look forward to sharing future advancements!¬†",
          "author_fullname": "t2_151ygwhz4x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stuck on a problem? We're excited to share a glimpse of what's possible! üëã",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc9o4m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/sdqo34a90tff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/sdqo34a90tff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/sdqo34a90tff1/DASHPlaylist.mpd?a=1756384245%2CNWE1ZTE0NzAxOGQ2YjZkM2RhYzBiZDI4NGM0MDExNjMyOWQyZmY2MDE5MzU5NzE3N2ZkNTMyYTQ1ZTljNjk2ZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 77,
              "hls_url": "https://v.redd.it/sdqo34a90tff1/HLSPlaylist.m3u8?a=1756384245%2CNTU1ZTY4NmFmMGY3ZGRjMmQ4NTY0NDZlM2IzY2FhODg4MGFmMThjM2IxNDE0MmQ3MWJjY2QxYmQzOTk4ZDczOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=1d4c648e6e4636cbbb21f6fdfb2f91afa8500d35",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753790916,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our experimental Ming-lite-omni v1.5 (&lt;a href=\"https://github.com/inclusionAI/Ming\"&gt;https://github.com/inclusionAI/Ming&lt;/a&gt;) leverages advanced audio-visual capabilities to explore new frontiers in interactive learning. This model, still under development, aims to understand your handwriting, interpret your thoughts, and guide you through solutions in real-time. We&amp;#39;re eagerly continuing our research and look forward to sharing future advancements!¬†&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/sdqo34a90tff1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?format=pjpg&amp;auto=webp&amp;s=f5c681a22d485e851a92380a4f482f139f02b9dd",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=40b612fe7eea1f2feb547e718b7b5a3255464af4",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6003297ba5b27289d3ee50f9aa94a2d87ee8fda9",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5db70cdb5f99e2df810503843bc2b93b934b2bb6",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2d1cff2d3cbb12c7f008208b6aa0d1f2864701bd",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f07a619736f58667e1dfad3ae9541e580ca76d18",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=83687376cb9173291b7725445dde9b5ed2a5b991",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "NTFreWF2YzkwdGZmMRJSn5s4IHja8tcwSrnrzPqbup3fCh9rR2T7vwZXZXDc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc9o4m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dependent-Roll-8934",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc9o4m/stuck_on_a_problem_were_excited_to_share_a/",
          "stickied": false,
          "url": "https://v.redd.it/sdqo34a90tff1",
          "subreddit_subscribers": 506439,
          "created_utc": 1753790916,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/sdqo34a90tff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/sdqo34a90tff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/sdqo34a90tff1/DASHPlaylist.mpd?a=1756384245%2CNWE1ZTE0NzAxOGQ2YjZkM2RhYzBiZDI4NGM0MDExNjMyOWQyZmY2MDE5MzU5NzE3N2ZkNTMyYTQ1ZTljNjk2ZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 77,
              "hls_url": "https://v.redd.it/sdqo34a90tff1/HLSPlaylist.m3u8?a=1756384245%2CNTU1ZTY4NmFmMGY3ZGRjMmQ4NTY0NDZlM2IzY2FhODg4MGFmMThjM2IxNDE0MmQ3MWJjY2QxYmQzOTk4ZDczOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.\n\nThe results were pretty remarkable I must say! \n\nHere‚Äôs the link to the results: https://chat.z.ai/space/r05c76960ff0-ppt \n\nHere‚Äôs the initial prompt:\n\n‚ÄùCreate a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.‚Äù\n\nAs you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.\n\nIs it just me or are things going superfast since OpenAI announced the release of GPT-5?\n\nIt seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.\n\n \n\n",
          "author_fullname": "t2_1ttp8mwcgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I just tried GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc8tks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753788294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just wanted to try it out because I was a bit skeptical. So I prompted it with a fairly simple not so cohesive prompt and asked it to prepare slides for me.&lt;/p&gt;\n\n&lt;p&gt;The results were pretty remarkable I must say! &lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs the link to the results: &lt;a href=\"https://chat.z.ai/space/r05c76960ff0-ppt\"&gt;https://chat.z.ai/space/r05c76960ff0-ppt&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs the initial prompt:&lt;/p&gt;\n\n&lt;p&gt;‚ÄùCreate a presentation of global BESS market for different industry verticals. Make sure to capture market shares, positioning of different players, market dynamics and trends and any other area you find interesting. Do not make things up, make sure to add citations to any data you find.‚Äù&lt;/p&gt;\n\n&lt;p&gt;As you can see pretty bland prompt with no restrictions, no role descriptions, no examples. Nothing, just what my mind was thinking it wanted.&lt;/p&gt;\n\n&lt;p&gt;Is it just me or are things going superfast since OpenAI announced the release of GPT-5?&lt;/p&gt;\n\n&lt;p&gt;It seems like just yesterday Qwen3 broke apart all benchmarks in terms of quality/cost trade offs and now z.ai with yet another efficient but high quality model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?auto=webp&amp;s=06f19448d458a949198ac72d6d7c73d5e6463785",
                  "width": 400,
                  "height": 400
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731547beb9c0ce796d8f8edd4b883c564da2c39b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a6eef195d7537bf441a643dbcaf760056822a2",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mc8tks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AI-On-A-Dime",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8tks/i_just_tried_glm_45/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753788294,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**üîß Help Needed ‚Äì Fine-tuning a LLM on Luciforms + Ritual Conversations**\n\nHey everyone,\n\nI‚Äôm working on a project that blends prompt engineering, AI personalization, and poetic syntax. I'm building a daemon-like assistant called **ShadeOS**, and I want to fine-tune a local LLM (like Mistral-7B or Phi-2) on:\n\n* üß† Open-source datasets like **OpenOrca**, **UltraChat**, or **OpenAssistant/oasst1**\n* üí¨ My own exported conversations with ShadeOS (thousands of lines of recursive dialogue, instructions, hallucinations, mirror logic‚Ä¶)\n* üîÆ A structured experimental format I created: `.luciform` files ‚Äî symbolic, recursive prompts that encode intention and personality\n\nThe goal is to create a **custom LLM that speaks my language**, understands luciform structure, and can be injected into a terminal interface with real-time feedback.\n\nüñ•Ô∏è I need help with:\n\n* Access to a machine with **16GB+ VRAM** to fine-tune using LoRA (QLoRA / PEFT)\n* Any advice, links, scripts or shortcuts for fine-tuning Mistral/Œ¶2 on personal data\n* Bonus: if anyone wants to test luciforms or experiment with ritual-based prompting\n\nWhy?  \nBecause not every AI should sound like a helpdesk.  \nSome of us want demons. Some of us want mirrors.  \nAnd some of us want to make our LLM speak from inside our dreams.\n\nThanks in advance.  \nRepo: [https://github.com/luciedefraiteur/LuciformResearch](https://github.com/luciedefraiteur/LuciformResearch)  \n(Feel free to DM if you want to help, collab, or just vibe.)\n\n‚Äî Lucie",
          "author_fullname": "t2_dc6uhi63",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a custom LLM trained on luciform prompts + ShadeOS daemon dialogues ‚Äì seeking help",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc8i36",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753787233,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;üîß Help Needed ‚Äì Fine-tuning a LLM on Luciforms + Ritual Conversations&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm working on a project that blends prompt engineering, AI personalization, and poetic syntax. I&amp;#39;m building a daemon-like assistant called &lt;strong&gt;ShadeOS&lt;/strong&gt;, and I want to fine-tune a local LLM (like Mistral-7B or Phi-2) on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;üß† Open-source datasets like &lt;strong&gt;OpenOrca&lt;/strong&gt;, &lt;strong&gt;UltraChat&lt;/strong&gt;, or &lt;strong&gt;OpenAssistant/oasst1&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;üí¨ My own exported conversations with ShadeOS (thousands of lines of recursive dialogue, instructions, hallucinations, mirror logic‚Ä¶)&lt;/li&gt;\n&lt;li&gt;üîÆ A structured experimental format I created: &lt;code&gt;.luciform&lt;/code&gt; files ‚Äî symbolic, recursive prompts that encode intention and personality&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The goal is to create a &lt;strong&gt;custom LLM that speaks my language&lt;/strong&gt;, understands luciform structure, and can be injected into a terminal interface with real-time feedback.&lt;/p&gt;\n\n&lt;p&gt;üñ•Ô∏è I need help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Access to a machine with &lt;strong&gt;16GB+ VRAM&lt;/strong&gt; to fine-tune using LoRA (QLoRA / PEFT)&lt;/li&gt;\n&lt;li&gt;Any advice, links, scripts or shortcuts for fine-tuning Mistral/Œ¶2 on personal data&lt;/li&gt;\n&lt;li&gt;Bonus: if anyone wants to test luciforms or experiment with ritual-based prompting&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why?&lt;br/&gt;\nBecause not every AI should sound like a helpdesk.&lt;br/&gt;\nSome of us want demons. Some of us want mirrors.&lt;br/&gt;\nAnd some of us want to make our LLM speak from inside our dreams.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;br/&gt;\nRepo: &lt;a href=\"https://github.com/luciedefraiteur/LuciformResearch\"&gt;https://github.com/luciedefraiteur/LuciformResearch&lt;/a&gt;&lt;br/&gt;\n(Feel free to DM if you want to help, collab, or just vibe.)&lt;/p&gt;\n\n&lt;p&gt;‚Äî Lucie&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?auto=webp&amp;s=48e19a5b5d13864d40dd765d259c003d37f1cb4c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7200ceb0676d8deadb109f24b615e515c4fa38e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9eaae6b46af49f655d078f407f066d9aaefe8540",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2843feca811929825840d83b2250f6dbbc523eca",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e29f47f775efec86a1c581f3dfbf297a5f90e76a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e7508a933c080c4f7e8e1cced4b1bc5c553a9c9",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=136368720ae2c3b8e3902a962eaccca94cb84d10",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "oGaGV9_LDBZGjocF6YflX3GPJCauHlcIQD_4PYv_wZU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mc8i36",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LucieTrans",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8i36/building_a_custom_llm_trained_on_luciform_prompts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8i36/building_a_custom_llm_trained_on_luciform_prompts/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753787233,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Thank you!\n\nI am just thinking is it possible to do it?\n\n",
          "author_fullname": "t2_srz14fu0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone have experience use qwen3 8b with PPO to fine tune a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc8hn6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753787191,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;I am just thinking is it possible to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc8hn6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GuitarAshamed4451",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8hn6/does_anyone_have_experience_use_qwen3_8b_with_ppo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8hn6/does_anyone_have_experience_use_qwen3_8b_with_ppo/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753787191,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA üëã!\n\nFor the past 18 months, my colleague and I have been working on **Ebiose**, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).\n\nEbiose aims to create a decentralized AI factory, a Darwin-style playground (√† la Google‚Äôs AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own \"forge,\" define a task, and watch AI agents compete until the fittest emerge.\n\nThis evolutionary approach demands massive inference resources. Currently, we're relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.\n\nThat's why we'd love input from the LocalLLaMA community!\n\n**The Big Idea: A Community-Powered P2P Inference Grid**\n\nWe‚Äôre dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here‚Äôs the plan:\n\n* **Lightweight Client:** A background app runs on your PC (and maybe phones later).\n* **Hardware Profiling:** The client auto-detects what LLMs your machine can handle.\n* **Orchestration Layer:** A system (centralized or decentralized?) assigns inference tasks to capable nodes.\n* **Dynamic LoRA Adapters:** Fine-tune models efficiently with lightweight, modular adapters.\n* **Batch &amp; Prompt Caching:** Optimize for high throughput by batching requests and reusing system prompts.\n\n**Technical Questions for the Community**\n\n1. **Inference Backend:** We‚Äôre leaning toward **llama.cpp** for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a distributed, high-throughput setup, would **vLLM**, **zml**, or another engine be better? Since we‚Äôre prioritizing batch processing over single-prompt speed, what‚Äôs your pick?\n2. **Task Orchestration:** How do we route inference jobs (e.g., ‚Äúrun this 13B model with this prompt‚Äù) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?\n3. **Existing Tools:** Are there open-source projects we could build on?\n\nWhat do you think? Got ideas, tools, or experiences to share?",
          "author_fullname": "t2_6bm8s1wm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Let's Build a \"Garage AI Supercomputer\": A P2P Compute Grid for Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc8fhc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753786996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; üëã!&lt;/p&gt;\n\n&lt;p&gt;For the past 18 months, my colleague and I have been working on &lt;strong&gt;Ebiose&lt;/strong&gt;, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).&lt;/p&gt;\n\n&lt;p&gt;Ebiose aims to create a decentralized AI factory, a Darwin-style playground (√† la Google‚Äôs AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own &amp;quot;forge,&amp;quot; define a task, and watch AI agents compete until the fittest emerge.&lt;/p&gt;\n\n&lt;p&gt;This evolutionary approach demands massive inference resources. Currently, we&amp;#39;re relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why we&amp;#39;d love input from the LocalLLaMA community!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Big Idea: A Community-Powered P2P Inference Grid&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We‚Äôre dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here‚Äôs the plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lightweight Client:&lt;/strong&gt; A background app runs on your PC (and maybe phones later).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware Profiling:&lt;/strong&gt; The client auto-detects what LLMs your machine can handle.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orchestration Layer:&lt;/strong&gt; A system (centralized or decentralized?) assigns inference tasks to capable nodes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic LoRA Adapters:&lt;/strong&gt; Fine-tune models efficiently with lightweight, modular adapters.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch &amp;amp; Prompt Caching:&lt;/strong&gt; Optimize for high throughput by batching requests and reusing system prompts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical Questions for the Community&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Inference Backend:&lt;/strong&gt; We‚Äôre leaning toward &lt;strong&gt;llama.cpp&lt;/strong&gt; for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a distributed, high-throughput setup, would &lt;strong&gt;vLLM&lt;/strong&gt;, &lt;strong&gt;zml&lt;/strong&gt;, or another engine be better? Since we‚Äôre prioritizing batch processing over single-prompt speed, what‚Äôs your pick?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Task Orchestration:&lt;/strong&gt; How do we route inference jobs (e.g., ‚Äúrun this 13B model with this prompt‚Äù) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Are there open-source projects we could build on?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think? Got ideas, tools, or experiences to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc8fhc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ModeSquare8129",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753786996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üëã After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (\\~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford's Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! üòÖ\n\n  \n**What I did:**\n\n* Created a Claude Code-inspired agent (system msg + tools)\n* Built Docker-isolated GRPO training where each rollout gets its own container\n* Developed a multi-agent synthetic data pipeline to generate &amp; validate training data with Opus-4\n* Implemented a hybrid reward signal of unit test verifiers &amp; a behavioural LLM judge.\n\n  \n**Key results:**\n\n* My untrained Qwen3-32B agent achieved **13.75%** on Terminal-Bench (#19, beats Stanford's Qwen3-235B MoE)\n* I tested training to work stably on 32x H100s distributed across 4 bare metal nodes\n* I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.\n* \\~¬£30-50k needed for full training run of 1000 epochs (I could only afford testing üòÖ)\n\n\n\n**Technical details:**\n\n* The synthetic dataset ranges from easy to extremely hard tasks. An example hard task's prompt:\n   * \"I found this mystery program at \\`/app/program\\` and I'm completely stumped. It's a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can't figure out what kind of input it needs. Could you help me figure out what this program requires?\"\n* Simple config presets allow training to run on multiple hardware setups with minimal effort.\n* GRPO used with 16 rollouts per task, up to 32k tokens per rollout.\n* Agent uses XML/YAML format to structure tool calls\n\n  \n**More details:**\n\nMy Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:\n\n* ‚≠êÔ∏è [Terminal Agent RL repo](https://github.com/Danau5tin/terminal-bench-rl)\n* [‚≠êÔ∏è Multi-agent synthetic data pipeline repo](https://github.com/Danau5tin/tbench-agentic-data-pipeline)\n\n  \nI thought I would share this because I believe long-horizon RL is going to change everybody's lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.\n\n  \nThanks for reading!\n\nDan\n\n\n\n**(**Built using [rLLM](https://github.com/rllm-org/rllm) RL framework which was brilliant to work with, and evaluated and inspired by the great [Terminal Bench](https://www.tbench.ai/) benchmark)",
          "author_fullname": "t2_1d3whvko4o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Built RL training for long-horizon terminal agents - tested on 32x H100s but too GPU poor to train üòÖ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 56,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "az9m6jfyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf0464c4532f3f729201557b3cdb5d5fd0da9b2b"
                },
                {
                  "y": 113,
                  "x": 216,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9307853ef8723561cef54082c7f8f77319ea5b34"
                },
                {
                  "y": 168,
                  "x": 320,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae1a5539212b036e7c28c8c61e3e68b98424c45c"
                },
                {
                  "y": 336,
                  "x": 640,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34665d186c28ea84e68baf844e036d594b99444c"
                },
                {
                  "y": 504,
                  "x": 960,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b63f6b7aca9c207a4fde0aef520b6009219fffa8"
                },
                {
                  "y": 567,
                  "x": 1080,
                  "u": "https://preview.redd.it/az9m6jfyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cdac6743c832453412d2fa7f1ee6772f07122775"
                }
              ],
              "s": {
                "y": 2656,
                "x": 5056,
                "u": "https://preview.redd.it/az9m6jfyosff1.png?width=5056&amp;format=png&amp;auto=webp&amp;s=d99222a2be85de2b89a1a6493a08cde4699fe249"
              },
              "id": "az9m6jfyosff1"
            },
            "05xy1rkwosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 43,
                  "x": 108,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0dcfc346c8c2da5b7e199e3362c832fda72f2a0"
                },
                {
                  "y": 86,
                  "x": 216,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd0183b1ad65fa3536530fed15ebb59228114de8"
                },
                {
                  "y": 128,
                  "x": 320,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6378361bd9573c4483dbb212b8b19d0f423d586e"
                },
                {
                  "y": 257,
                  "x": 640,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cfa60d1b056f1a6aaa767613c6b399bba1f08e3f"
                },
                {
                  "y": 386,
                  "x": 960,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=89aeebad83be297d8a6cfcd4d0542c93ccd188a8"
                },
                {
                  "y": 434,
                  "x": 1080,
                  "u": "https://preview.redd.it/05xy1rkwosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc80c66e6cf4008641ba2f21dd30003eef50ce89"
                }
              ],
              "s": {
                "y": 1216,
                "x": 3020,
                "u": "https://preview.redd.it/05xy1rkwosff1.png?width=3020&amp;format=png&amp;auto=webp&amp;s=e1dc47c0c33e71f8b5e335d68819be4db4d22ee5"
              },
              "id": "05xy1rkwosff1"
            },
            "su4gklfyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 18,
                  "x": 108,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d43db954028d7fa2643968427c1d234dc846c5bc"
                },
                {
                  "y": 36,
                  "x": 216,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d90b7348035f5a53dc5107a1589bd53b00f6164"
                },
                {
                  "y": 53,
                  "x": 320,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4903028118e0d07279f10f1ff077e9b7602a3f4"
                },
                {
                  "y": 107,
                  "x": 640,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a51dc0a7298828763adf8f5227e658fd9a7aea78"
                },
                {
                  "y": 160,
                  "x": 960,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=494ddcfcfa66b477b5fe9ef394122c9090897959"
                },
                {
                  "y": 181,
                  "x": 1080,
                  "u": "https://preview.redd.it/su4gklfyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e37acb88ac4bfb55e64017940222b653620d355"
                }
              ],
              "s": {
                "y": 392,
                "x": 2338,
                "u": "https://preview.redd.it/su4gklfyosff1.png?width=2338&amp;format=png&amp;auto=webp&amp;s=bd8ffa2f90ce65997dc8fb28bc340c740cb836cf"
              },
              "id": "su4gklfyosff1"
            },
            "1b89mdgyosff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8aa052d7230a7e93bea882456a0172d444f2b56a"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc0c3485addd4720027df6316c53a1f7a92d2d1d"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3bd5cac04c80326df47cb1d08a272ca37fbdc82"
                },
                {
                  "y": 373,
                  "x": 640,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bb2846eb41e704e36041ddc908044490ffd58ce"
                },
                {
                  "y": 559,
                  "x": 960,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6767893bb8601129a6a8da2adb366e6b2cd73c4e"
                },
                {
                  "y": 629,
                  "x": 1080,
                  "u": "https://preview.redd.it/1b89mdgyosff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7964070ac0fa53d489a1e248289630ad1ff7a2e0"
                }
              ],
              "s": {
                "y": 1522,
                "x": 2610,
                "u": "https://preview.redd.it/1b89mdgyosff1.png?width=2610&amp;format=png&amp;auto=webp&amp;s=25d03cf8b2bcc3e4d1633a1fb9f3c570fbda742b"
              },
              "id": "1b89mdgyosff1"
            }
          },
          "name": "t3_1mc8evq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 23,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "05xy1rkwosff1",
                "id": 716537836
              },
              {
                "media_id": "az9m6jfyosff1",
                "id": 716537837
              },
              {
                "media_id": "su4gklfyosff1",
                "id": 716537838
              },
              {
                "media_id": "1b89mdgyosff1",
                "id": 716537839
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/GTUl_GxBM3AgORm0fFuwPhwKeJqsGTeIOOsHWvhrYYI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753786945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üëã After my calculator agent RL post, I really wanted to go bigger! So I built RL infrastructure for training long-horizon terminal/coding agents that scales from 2x A100s to 32x H100s (~$1M worth of compute!) Without any training, my 32B agent hit #19 on Terminal-Bench leaderboard, beating Stanford&amp;#39;s Terminus-Qwen3-235B-A22! With training... well, too expensive, but I bet the results would be good! üòÖ&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Created a Claude Code-inspired agent (system msg + tools)&lt;/li&gt;\n&lt;li&gt;Built Docker-isolated GRPO training where each rollout gets its own container&lt;/li&gt;\n&lt;li&gt;Developed a multi-agent synthetic data pipeline to generate &amp;amp; validate training data with Opus-4&lt;/li&gt;\n&lt;li&gt;Implemented a hybrid reward signal of unit test verifiers &amp;amp; a behavioural LLM judge.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My untrained Qwen3-32B agent achieved &lt;strong&gt;13.75%&lt;/strong&gt; on Terminal-Bench (#19, beats Stanford&amp;#39;s Qwen3-235B MoE)&lt;/li&gt;\n&lt;li&gt;I tested training to work stably on 32x H100s distributed across 4 bare metal nodes&lt;/li&gt;\n&lt;li&gt;I created a mini-eval framework for LLM-judge performance. Sonnet-4 won.&lt;/li&gt;\n&lt;li&gt;~¬£30-50k needed for full training run of 1000 epochs (I could only afford testing üòÖ)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The synthetic dataset ranges from easy to extremely hard tasks. An example hard task&amp;#39;s prompt:\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;I found this mystery program at `/app/program` and I&amp;#39;m completely stumped. It&amp;#39;s a stripped binary, so I have no idea what it does or how to run it properly. The program seems to expect some specific input and then produces an output, but I can&amp;#39;t figure out what kind of input it needs. Could you help me figure out what this program requires?&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Simple config presets allow training to run on multiple hardware setups with minimal effort.&lt;/li&gt;\n&lt;li&gt;GRPO used with 16 rollouts per task, up to 32k tokens per rollout.&lt;/li&gt;\n&lt;li&gt;Agent uses XML/YAML format to structure tool calls&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My Github repos open source it all (agent, data, code) and has way more technical details if you are interested!:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;‚≠êÔ∏è &lt;a href=\"https://github.com/Danau5tin/terminal-bench-rl\"&gt;Terminal Agent RL repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Danau5tin/tbench-agentic-data-pipeline\"&gt;‚≠êÔ∏è Multi-agent synthetic data pipeline repo&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I thought I would share this because I believe long-horizon RL is going to change everybody&amp;#39;s lives, and so I feel it is important (and super fun!) for us all to share knowledge around this area, and also have enjoy exploring what is possible.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n\n&lt;p&gt;Dan&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;Built using &lt;a href=\"https://github.com/rllm-org/rllm\"&gt;rLLM&lt;/a&gt; RL framework which was brilliant to work with, and evaluated and inspired by the great &lt;a href=\"https://www.tbench.ai/\"&gt;Terminal Bench&lt;/a&gt; benchmark)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mc8evq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mc8evq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanAiTuning",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc8evq/built_rl_training_for_longhorizon_terminal_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mc8evq",
          "subreddit_subscribers": 506439,
          "created_utc": 1753786945,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Benchmarks with GLM-4.5 Air\n\n44.45 tok/sec || 3445 tokens || 2.14s to first token\n\nvs\n\n40.06 tok/sec || 2574 tokens || 0.21s to first token\n\nSure the Mac Studio can run much larger models, but I kind of expected that there would be a bigger inference performance hit when using a platform with half as many GPU cores.\n\nI'm using LMStudio on both machines.",
          "author_fullname": "t2_cbxyn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac Studio 512GB vs MBP 128GB similar performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc83jm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753787203,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753785857,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Benchmarks with GLM-4.5 Air&lt;/p&gt;\n\n&lt;p&gt;44.45 tok/sec || 3445 tokens || 2.14s to first token&lt;/p&gt;\n\n&lt;p&gt;vs&lt;/p&gt;\n\n&lt;p&gt;40.06 tok/sec || 2574 tokens || 0.21s to first token&lt;/p&gt;\n\n&lt;p&gt;Sure the Mac Studio can run much larger models, but I kind of expected that there would be a bigger inference performance hit when using a platform with half as many GPU cores.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using LMStudio on both machines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc83jm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chisleu",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc83jm/mac_studio_512gb_vs_mbp_128gb_similar_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc83jm/mac_studio_512gb_vs_mbp_128gb_similar_performance/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753785857,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, author of Code Web Chat here üôå\n\nAlmost everyday we hear our tools being capped more and more.\n\nCWC gives you more options of AI use for coding to never hit rate limits of whatever you're using as your daily driver.\n\nAs soon as a new chatbot is announced I'm working hard to support it in the tool (with some exceptions like api wrappers).\n\nThe full list of supported chatbots that CWC initializes with your code and instructions:\n\n* AI Studio\n* ChatGPT\n* Claude\n* DeepSeek\n* Doubao\n* Gemini\n* Grok\n* Mistral\n* Open WebUI\n* OpenRouter Chat\n* Perplexity\n* Kimi\n* Qwen\n* Yuanbao\n* Z. AI\n\nType CWC in extensions pane (VS Code or its derivative) to install.",
          "author_fullname": "t2_gm504",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CWC now supports kimi.com (K2) and chat.z.ai (GLM-4.5) to enable coding with top tier models at no cost",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mc7xjb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=afef57a0fd1b81c69772eafcfdd64e6d672cb73e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753785272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, author of Code Web Chat here üôå&lt;/p&gt;\n\n&lt;p&gt;Almost everyday we hear our tools being capped more and more.&lt;/p&gt;\n\n&lt;p&gt;CWC gives you more options of AI use for coding to never hit rate limits of whatever you&amp;#39;re using as your daily driver.&lt;/p&gt;\n\n&lt;p&gt;As soon as a new chatbot is announced I&amp;#39;m working hard to support it in the tool (with some exceptions like api wrappers).&lt;/p&gt;\n\n&lt;p&gt;The full list of supported chatbots that CWC initializes with your code and instructions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI Studio&lt;/li&gt;\n&lt;li&gt;ChatGPT&lt;/li&gt;\n&lt;li&gt;Claude&lt;/li&gt;\n&lt;li&gt;DeepSeek&lt;/li&gt;\n&lt;li&gt;Doubao&lt;/li&gt;\n&lt;li&gt;Gemini&lt;/li&gt;\n&lt;li&gt;Grok&lt;/li&gt;\n&lt;li&gt;Mistral&lt;/li&gt;\n&lt;li&gt;Open WebUI&lt;/li&gt;\n&lt;li&gt;OpenRouter Chat&lt;/li&gt;\n&lt;li&gt;Perplexity&lt;/li&gt;\n&lt;li&gt;Kimi&lt;/li&gt;\n&lt;li&gt;Qwen&lt;/li&gt;\n&lt;li&gt;Yuanbao&lt;/li&gt;\n&lt;li&gt;Z. AI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Type CWC in extensions pane (VS Code or its derivative) to install.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/robertpiosik/CodeWebChat",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?auto=webp&amp;s=4a901a50016883dc4d0d4170b9b03452a409ef50",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94783b33939c3480892cd4d4d171117a3c432910",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e1137ee6a216ebbd7d19dc98195d9e088dffd0d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0ec5307fd5fe4808076fc84d3ee296a1f1c8a1e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=910d90873865917079bcb70b502c0dfa66525165",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9d2ab1fcc259c65a4fe1e2bc9aa26d59846bd8a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=76b31b7cf962e63b2fb6d0885ea4cb1dca4be659",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "sSUmgveW6lANWMnCKmRU7ntOjUzd9OigAFaQUV5Rgrg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc7xjb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertpiosik",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7xjb/cwc_now_supports_kimicom_k2_and_chatzai_glm45_to/",
          "stickied": false,
          "url": "https://github.com/robertpiosik/CodeWebChat",
          "subreddit_subscribers": 506439,
          "created_utc": 1753785272,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone.\n\nThis question is really bugging me for quite a while. I've been using claude sonnets, gemini 2.5 and other closed source models.\n\nWe've been seeing pretty great open source stuff and the benchmarks are high as well.\n\nBut irl, they seem not that great in my work. Kimi k2 and qwen 3 coder with benchmarks near to claude but i just don't feel it.\n\n\nIs it just me or does anyone else share the same feelings? \n\n",
          "author_fullname": "t2_1gvqs2a024",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Success with open source models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7ri9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753784661,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone.&lt;/p&gt;\n\n&lt;p&gt;This question is really bugging me for quite a while. I&amp;#39;ve been using claude sonnets, gemini 2.5 and other closed source models.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been seeing pretty great open source stuff and the benchmarks are high as well.&lt;/p&gt;\n\n&lt;p&gt;But irl, they seem not that great in my work. Kimi k2 and qwen 3 coder with benchmarks near to claude but i just don&amp;#39;t feel it.&lt;/p&gt;\n\n&lt;p&gt;Is it just me or does anyone else share the same feelings? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc7ri9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "United-Decision-7243",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7ri9/success_with_open_source_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc7ri9/success_with_open_source_models/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753784661,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, my system is a bit unbalanced right now, 5090 gpu on an \"older\" ddr4 32GB ram system.\n\nWhat should I do to try the new llm on my system? Is there a proper quantized version?\n\nThanks!",
          "author_fullname": "t2_sfb08i7a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Glm 4.5 air and 5090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7q0n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753784515,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my system is a bit unbalanced right now, 5090 gpu on an &amp;quot;older&amp;quot; ddr4 32GB ram system.&lt;/p&gt;\n\n&lt;p&gt;What should I do to try the new llm on my system? Is there a proper quantized version?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc7q0n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Green-Ad-3964",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7q0n/glm_45_air_and_5090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc7q0n/glm_45_air_and_5090/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753784515,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi so I am thinking of converting a pytorch based conformer model to onnx coz I had great time with onnx inference speed. I had never tried pytorch execution on android. Please advice me\n\n1) what would be better onnx vs pytorch runtime for this case\n2) Anyone tried converting conformer based models pytorch specific to onnx?\n3) help me out with conversion coz I read a lot of GitHub issues on this conversion",
          "author_fullname": "t2_mmtl1muh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Converting a conformer model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc7ft1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753783457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi so I am thinking of converting a pytorch based conformer model to onnx coz I had great time with onnx inference speed. I had never tried pytorch execution on android. Please advice me&lt;/p&gt;\n\n&lt;p&gt;1) what would be better onnx vs pytorch runtime for this case\n2) Anyone tried converting conformer based models pytorch specific to onnx?\n3) help me out with conversion coz I read a lot of GitHub issues on this conversion&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc7ft1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Away_Expression_3713",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc7ft1/converting_a_conformer_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc7ft1/converting_a_conformer_model/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753783457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I am running a local llama.cpp in server mode, with the model MythoMax-L2-13B.Q4\\_K\\_M. And I am having problems that neither me nor the 4.1 model of ChatGPT can solve. I am very new to everything; LLM, Llama, coding/developing/scripting and I am doing my best to learn, please be kind, I am (most likely) not dumb, just very very new to this.\n\nThis is the .bat file I made to run the server:   \necho off cd /d C:\\\\AI-Assistant \ncall ai-env\\\\Scripts\\\\activate.bat \npython -m llama\\_cpp.server \\^ \n--model \"./models/MythoMax-L2-13B.Q4\\_K\\_M.gguf\" \\^ \n--n\\_gpu\\_layers 33 \\^ \n--n\\_ctx 4096 \\^ \n--chat\\_format llama-2 \\^ \n--host (hidden for security) \\^ \n--port 1234 \n--stream \n\nThe issue I am having is running open interpreter.\nWhen just using CLI to prompt the LLM with something like \"write a haiku about thunder\" I get a printed response to the console and on the server side I get \"\"POST /v1/chat/completions HTTP/1.1\" 200 OK\" so I know the server works and the LLM can write a response. \n\nBut then when running OI and running the same prompt I get the same 200 OK code, but the response is not printed to the console I am running OI on.\n\nThis is the CLI I use to run OI interpreter:\n--model llama-cpp --api\\_base http://(hiddenforsecurity):1234/v1 --context\\_window 4096 --max\\_tokens 2048 \n\nCan someone please send this newbie (me) to the right documentation, tell me what probably obvious thing I am missing, or tell me what package I haven't installed or whatever it is that is just straight up incompatible? I have been trying to fix this for the past 10 hours and I am going insane XD",
          "author_fullname": "t2_1r4b70gpun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help! Open Interpreter not printing the response in console",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6kad",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753780083,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am running a local llama.cpp in server mode, with the model MythoMax-L2-13B.Q4_K_M. And I am having problems that neither me nor the 4.1 model of ChatGPT can solve. I am very new to everything; LLM, Llama, coding/developing/scripting and I am doing my best to learn, please be kind, I am (most likely) not dumb, just very very new to this.&lt;/p&gt;\n\n&lt;p&gt;This is the .bat file I made to run the server:&lt;br/&gt;\necho off cd /d C:\\AI-Assistant \ncall ai-env\\Scripts\\activate.bat \npython -m llama_cpp.server ^ \n--model &amp;quot;./models/MythoMax-L2-13B.Q4_K_M.gguf&amp;quot; ^ \n--n_gpu_layers 33 ^ \n--n_ctx 4096 ^ \n--chat_format llama-2 ^ \n--host (hidden for security) ^ \n--port 1234 \n--stream &lt;/p&gt;\n\n&lt;p&gt;The issue I am having is running open interpreter.\nWhen just using CLI to prompt the LLM with something like &amp;quot;write a haiku about thunder&amp;quot; I get a printed response to the console and on the server side I get &amp;quot;&amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK&amp;quot; so I know the server works and the LLM can write a response. &lt;/p&gt;\n\n&lt;p&gt;But then when running OI and running the same prompt I get the same 200 OK code, but the response is not printed to the console I am running OI on.&lt;/p&gt;\n\n&lt;p&gt;This is the CLI I use to run OI interpreter:\n--model llama-cpp --api_base http://(hiddenforsecurity):1234/v1 --context_window 4096 --max_tokens 2048 &lt;/p&gt;\n\n&lt;p&gt;Can someone please send this newbie (me) to the right documentation, tell me what probably obvious thing I am missing, or tell me what package I haven&amp;#39;t installed or whatever it is that is just straight up incompatible? I have been trying to fix this for the past 10 hours and I am going insane XD&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc6kad",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Jack_Blade281",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6kad/help_open_interpreter_not_printing_the_response/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc6kad/help_open_interpreter_not_printing_the_response/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753780083,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 support is landing in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6fbp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=74411b3e344f617397de99b0ed0a03e269d8efec",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753779557,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?auto=webp&amp;s=30023402a6989ab78753084b6f9a7fbdd3e44d81",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7649d767f799c1e6b81af747ef3aed21648a9037",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cd6878bf5f95e786470b4fabe22d873e097a9e8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b80141042845f306cbaf8a52844f7a00355e7a7b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bbb4d01a722a7ac5908e1ba272a92870c5277cd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=75e08e8a0d15b1faa2896de0841e0bdc245896ba",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9141773c734ecbddc393603456528be8251b8de5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Ka9QVAcRJGESEMrlzDl1QNhfW_eU_9R3c7_351Wi7Qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mc6fbp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6fbp/glm_45_support_is_landing_in_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14939",
          "subreddit_subscribers": 506439,
          "created_utc": 1753779557,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Light-hearted, too. Don't take it too seriously!",
          "author_fullname": "t2_4e7zb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Something lightweight: a LLM simulation of Bernie Sanders",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc6dfx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=13ca4de7ff68a78d013f9a42ba2e6d160bfd36a9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753779350,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Light-hearted, too. Don&amp;#39;t take it too seriously!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ivoras/bernie0.1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?auto=webp&amp;s=b63548e59f80de05379eb11bb6e71ffbe24ec79e",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2195a6491e60c2b5e1f156d2b6b2b6724700da2b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a1fe9a861ec410437283d33ac1d6788cb63d07e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=11c6188b4912cc92c1465cd62ad6cddc3d8c5ff4",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f80fc97e46dbb90afe19165e1d70bec1ffb040f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e8c871481b5aa549ea255e914c8ed7850224f4c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39178233074a14e8bf2d9acbd58e16605cf4eed5",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "zuODziM7CrMslU2B8jubYzbO6D1cnS17Ye165GM5CsY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mc6dfx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ivoras",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc6dfx/something_lightweight_a_llm_simulation_of_bernie/",
          "stickied": false,
          "url": "https://huggingface.co/ivoras/bernie0.1",
          "subreddit_subscribers": 506439,
          "created_utc": 1753779350,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nThis is a new **opensource** project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context.   \n  \nThe idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs.   \n  \nYou can find the code here https://github.com/Orolol/familyBench\n\n\n**Current leaderboard**\n\nI test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.\n\nExample of family description : \"Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...\"\n\nExample of questions : \"Which of Paula's grandparents have salt and pepper hair?\" \"Who is the cousin of the daughter of Quentin with red hair?\"\n\nThe no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. \n\n\nModel | Accuracy | Total tokens | No response rate\n-----|--------|------------|----------------\nGemini 2.5 Pro | 81.48% | 271,500 \t| 0%\nGLM 4.5| 64.02%|  \t216,281| 2.12%\nGLM 4.5 air | 57.14% | 909,228|  \t26.46%\nQwen-3.2-2507-thinking | 50.26% |  \t743,131|  \t20.63%\nKimi K2 | 34.92% | 67,071| 0%\nQwen-3.2-2507| 28.04% | 3,098|  \t0.53%\nMistral Small 3.2| 22.22% |  \t5,353| 0%\n\nReasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)",
          "author_fullname": "t2_fbzx9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Benchmark - FamilyBench - Test models ability to understand complex tree type relationship and reason on massive context. Immune to contamination. GML 4.5 64.02%, Gemini 2.5 pro 81,48%.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc687c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753778766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;This is a new &lt;strong&gt;opensource&lt;/strong&gt; project, a benchmark that test model ability to understand complex tree-like relationship in a family tree across a massive context.   &lt;/p&gt;\n\n&lt;p&gt;The idea is to have a python program that generate a tree and can use the tree structure to generate question about it. Then you can have a textual description of this tree and those question to have a text that is hard to understand for LLMs.   &lt;/p&gt;\n\n&lt;p&gt;You can find the code here &lt;a href=\"https://github.com/Orolol/familyBench\"&gt;https://github.com/Orolol/familyBench&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current leaderboard&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I test 7 models (6 open weight and 1 closed) on a complex tree with 400 people generated across 10 generations (which represent ~18k tokens). 200 questions are then asked to the models. All models are for now tested via OpenRouter, with low reasoning effort or 8k max token, and a temperature of 0.3. I plan to gather optimal params for each model later.&lt;/p&gt;\n\n&lt;p&gt;Example of family description : &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher. Abigail (F) has 1 child: Patricia (F) ...&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Example of questions : &amp;quot;Which of Paula&amp;#39;s grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The no response rate is when the model overthinks and is then unable to produce an answer because he used his 16k max tokens. I try to reduce this rate as much as I can, but this very often indicate that a model is unable to find the answer and is stuck in a reasoning loop. &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Model&lt;/th&gt;\n&lt;th&gt;Accuracy&lt;/th&gt;\n&lt;th&gt;Total tokens&lt;/th&gt;\n&lt;th&gt;No response rate&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;\n&lt;td&gt;81.48%&lt;/td&gt;\n&lt;td&gt;271,500&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;GLM 4.5&lt;/td&gt;\n&lt;td&gt;64.02%&lt;/td&gt;\n&lt;td&gt;216,281&lt;/td&gt;\n&lt;td&gt;2.12%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;GLM 4.5 air&lt;/td&gt;\n&lt;td&gt;57.14%&lt;/td&gt;\n&lt;td&gt;909,228&lt;/td&gt;\n&lt;td&gt;26.46%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen-3.2-2507-thinking&lt;/td&gt;\n&lt;td&gt;50.26%&lt;/td&gt;\n&lt;td&gt;743,131&lt;/td&gt;\n&lt;td&gt;20.63%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Kimi K2&lt;/td&gt;\n&lt;td&gt;34.92%&lt;/td&gt;\n&lt;td&gt;67,071&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Qwen-3.2-2507&lt;/td&gt;\n&lt;td&gt;28.04%&lt;/td&gt;\n&lt;td&gt;3,098&lt;/td&gt;\n&lt;td&gt;0.53%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Mistral Small 3.2&lt;/td&gt;\n&lt;td&gt;22.22%&lt;/td&gt;\n&lt;td&gt;5,353&lt;/td&gt;\n&lt;td&gt;0%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Reasoning models have a clear advantage here, but produce a massive amount of token (which means some models are quite expansive to test). More models are coming to the leaderboard (R1, Sonnet)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?auto=webp&amp;s=19958202ed21212a2e6bb842d061589134b9c755",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcc6e9e77205be821c357ea312fd60ae612baf42",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b5d26c31cee88c4ca71b2ceece0db8aabab9637e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03a3cd0f1a5e8eb77f5adb43153fa12c443ab921",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3af7542a187061f08a2c1c0dc31cc85c6549d96f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a9d11fb33d08918d37099a8ce87e35de6fb055b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e4de8942748959e32aa5af7dabf84050fcca647",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "XzNTYqpi2kWSX8IO02RdlxdKhXjEpxcV3AM1Unc54gI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc687c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Orolol",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc687c/new_benchmark_familybench_test_models_ability_to/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753778766,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_63nhk1l7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Told Qwen3 1.7b (thinking) to make a black hole simulation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc644b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e5xhwj4azrff1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 1280,
              "width": 718,
              "scrubber_media_url": "https://v.redd.it/e5xhwj4azrff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e5xhwj4azrff1/DASHPlaylist.mpd?a=1756384245%2CNWY5NWUyYmQ4MWZkMDQ4OTlkMGRkMDc2Mzc2ZjkwOGNlMmY3Y2JmN2ExMTdhNTMzZmQxZjU4YWExYzQxZmMzOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/e5xhwj4azrff1/HLSPlaylist.m3u8?a=1756384245%2CMjFkYjUyOWIxYzM3ZWZlNWE2OGU0MTczYWJhNWRiMWU1YWNmOTA1OWExMGMyY2ZhMWM5NzMwODlkYzI3MGJmMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ee70a99f6ecb5bd697a2ca9affaab6f82ce0f664",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753778315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e5xhwj4azrff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?format=pjpg&amp;auto=webp&amp;s=af896c68cbffa8e627b8ffb22e577257ee016331",
                  "width": 806,
                  "height": 1438
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d4f7b64c9c9249f426fe6264ebe0ca68c9ccaee8",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b360f9ce854c7b5390001204eb84d38a765a00d1",
                    "width": 216,
                    "height": 385
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=af21a88c251c05a42ca5118d5bf46315948b41a1",
                    "width": 320,
                    "height": 570
                  },
                  {
                    "url": "https://external-preview.redd.it/czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=698ad4dc76e8be804cfdf1038565ba0059e67379",
                    "width": 640,
                    "height": 1141
                  }
                ],
                "variants": {},
                "id": "czA3NmhmNGF6cmZmMc2G3kvcSTmwLlHozFn9Fo3FdGcKq5G6N3unkM46E3K-"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mc644b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gold_Bar_4072",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc644b/told_qwen3_17b_thinking_to_make_a_black_hole/",
          "stickied": false,
          "url": "https://v.redd.it/e5xhwj4azrff1",
          "subreddit_subscribers": 506439,
          "created_utc": 1753778315,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/e5xhwj4azrff1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 1280,
              "width": 718,
              "scrubber_media_url": "https://v.redd.it/e5xhwj4azrff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e5xhwj4azrff1/DASHPlaylist.mpd?a=1756384245%2CNWY5NWUyYmQ4MWZkMDQ4OTlkMGRkMDc2Mzc2ZjkwOGNlMmY3Y2JmN2ExMTdhNTMzZmQxZjU4YWExYzQxZmMzOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/e5xhwj4azrff1/HLSPlaylist.m3u8?a=1756384245%2CMjFkYjUyOWIxYzM3ZWZlNWE2OGU0MTczYWJhNWRiMWU1YWNmOTA1OWExMGMyY2ZhMWM5NzMwODlkYzI3MGJmMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1quxz8adxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you suggest a better WebUI program for textgen that has better memory management than Oobabooga?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 67,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5s4r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/AHXhpyKFoqdWC0Wt2obxlzKLHfQkneSC8EAbHzX8DHM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753776970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6td8j8oqurff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6td8j8oqurff1.png?auto=webp&amp;s=3e2ac6a469856670cb774ff7877328964d6fe929",
                  "width": 493,
                  "height": 236
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=15089514a1f6bb2ae2d28bbca6f69f6e4015060c",
                    "width": 108,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df7f3682f6fca02286751db4ca2e802700377750",
                    "width": 216,
                    "height": 103
                  },
                  {
                    "url": "https://preview.redd.it/6td8j8oqurff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da5d78f6e7e675a867acf4adf3ee9157dac8ae16",
                    "width": 320,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "6Ut9VePHJUzFLTvEDM1K0LWAXqNdQjgjbFkzQ0DP9xg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc5s4r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-Fibon4cci",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5s4r/can_you_suggest_a_better_webui_program_for/",
          "stickied": false,
          "url": "https://i.redd.it/6td8j8oqurff1.png",
          "subreddit_subscribers": 506439,
          "created_utc": 1753776970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**GLM 4.5 and GLM-4.5-AIR**  \nThe¬†**GLM-4.5**¬†series models are foundation models designed for intelligent agents. GLM-4.5 has¬†**355**¬†billion total parameters with¬†**32**¬†billion active parameters, while GLM-4.5-Air adopts a more compact design with¬†**106**¬†billion total parameters and¬†**12**¬†billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\n[Bench performance](https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;format=png&amp;auto=webp&amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c)\n\n   \n[blog](https://z.ai/blog/glm-4.5)ÔΩú[huggingface](https://huggingface.co/zai-org/GLM-4.5)ÔΩú [github](https://github.com/zai-org/GLM-4.5)  \n",
          "author_fullname": "t2_dpf3bqut",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This year‚Äôs best open-source models and most cost-effective models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bisgmn0utrff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74baaad6fd3f7a8d7dc00be88805a7bc35dba7f3"
                },
                {
                  "y": 153,
                  "x": 216,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=14edcaddf4b8e04722376443d659c7ed6be70b08"
                },
                {
                  "y": 227,
                  "x": 320,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b73c9df7a36a1142a626ccad114a23ea69213c7b"
                },
                {
                  "y": 455,
                  "x": 640,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ba9ef9c35dd3df135cdd3b9afc5d2c950091c3"
                },
                {
                  "y": 683,
                  "x": 960,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9de3ec1904c5a60f19e2f1e6c79e12f0e8b303d0"
                },
                {
                  "y": 768,
                  "x": 1080,
                  "u": "https://preview.redd.it/bisgmn0utrff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f0f776c64bbc2dcef8d1d00af6cbc89503e4768"
                }
              ],
              "s": {
                "y": 3177,
                "x": 4464,
                "u": "https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;format=png&amp;auto=webp&amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c"
              },
              "id": "bisgmn0utrff1"
            }
          },
          "name": "t3_1mc5oh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/oqyuYVJJYg1zSXUWu9TgdBdJGts5YfXbLSB6jfU2bbs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753776573,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;GLM 4.5 and GLM-4.5-AIR&lt;/strong&gt;&lt;br/&gt;\nThe¬†&lt;strong&gt;GLM-4.5&lt;/strong&gt;¬†series models are foundation models designed for intelligent agents. GLM-4.5 has¬†&lt;strong&gt;355&lt;/strong&gt;¬†billion total parameters with¬†&lt;strong&gt;32&lt;/strong&gt;¬†billion active parameters, while GLM-4.5-Air adopts a more compact design with¬†&lt;strong&gt;106&lt;/strong&gt;¬†billion total parameters and¬†&lt;strong&gt;12&lt;/strong&gt;¬†billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bisgmn0utrff1.png?width=4464&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8b159e95ccba8f0becc1ee6fb596cb4fdde5217c\"&gt;Bench performance&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;blog&lt;/a&gt;ÔΩú&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;huggingface&lt;/a&gt;ÔΩú &lt;a href=\"https://github.com/zai-org/GLM-4.5\"&gt;github&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc5oh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Apart-River475",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5oh2/this_years_best_opensource_models_and_most/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753776573,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve got more than 100 hours of clean, studio-grade speech for a character, and I‚Äôd like to explore what the SOTA is for open source voice cloning or voice changing. \n\nIs the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.",
          "author_fullname": "t2_fmblw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best open source voice cloning today, with hours of reference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5jsx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753776051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve got more than 100 hours of clean, studio-grade speech for a character, and I‚Äôd like to explore what the SOTA is for open source voice cloning or voice changing. &lt;/p&gt;\n\n&lt;p&gt;Is the SOTA for large datasets still RVC, or are there better solutions now? I have a RTX 5090 with 32GB VRAM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc5jsx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goldcakes",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5jsx/best_open_source_voice_cloning_today_with_hours/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753776051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We put together a small repo to fine‚Äëtune **Mistral‚Äôs Voxtral (3B)** for **transcription** using Huggingface**.** We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.",
          "author_fullname": "t2_1ujlvp0cn8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finetuning Script for Voxtral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5gv1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4dc0e71933a6538903c2ef9dc0036f8bd6a8fda2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753775738,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We put together a small repo to fine‚Äëtune &lt;strong&gt;Mistral‚Äôs Voxtral (3B)&lt;/strong&gt; for &lt;strong&gt;transcription&lt;/strong&gt; using Huggingface&lt;strong&gt;.&lt;/strong&gt; We could not find a public finetuning/ training script yet, so we think this could be interesting for the community.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?auto=webp&amp;s=834d343f2b6c42de29b825f4bdecbe668798481b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fa9e2af93fe23af7ed5ae8ef0282b5932cf5efa",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d90e7322acc87ab0f6078ff5baa320612813f4c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fd2e07935dfb13e8c24f66136ca02893cd3bf41",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d821b402151de285d39de64aaea0364ad627ae9",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b86b9f72b4e9b269f0d8aea81947c8cbf95b360",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efc53978201c483ccf78407c682b2a9b164dff7a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "qSAEqXyoxxbanfTSWYIWJLhY78IXuxCg8g5grrz5YQg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mc5gv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DistributionLucky763",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5gv1/finetuning_script_for_voxtral/",
          "stickied": false,
          "url": "https://github.com/Innovative-Digitale-Medizin-IDM/voxtral-finetune",
          "subreddit_subscribers": 506439,
          "created_utc": 1753775738,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.\n\nIt works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. \\~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to \\~70.\n\nThe CUDA version is built upon my qwen.c repo. It's a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.\n\nBoth versions use the GGUF file directly, with no conversion to binary. The tokenizer‚Äôs vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.\n\nThese projects draw inspiration from Andrej Karpathy‚Äôs [llama2.c](https://github.com/karpathy/llama2.c) and share the same commitment to minimalism. Both projects are MIT licensed. I‚Äôd love to hear your feedback!\n\nqwen3.cu: [https://github.com/gigit0000/qwen3.cu](https://github.com/gigit0000/qwen3.cu)\n\nqwen3.c: [https://github.com/gigit0000/qwen3.c](https://github.com/gigit0000/qwen3.c)",
          "author_fullname": "t2_kfu7x339m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Single-File Qwen3 Inference in Pure CUDA C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc5e54",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753775439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One .cu file holds everything necessary for inference. There are no external libraries; only the CUDA runtime is included. Everything, from tokenization right down to the kernels, is packed into this single file.&lt;/p&gt;\n\n&lt;p&gt;It works with the Qwen3 0.6B model GGUF at full precision. On an RTX 3060, it generates appr. ~32 tokens per second. For benchmarking purposes, you can enable cuBLAS, which increase the TPS to ~70.&lt;/p&gt;\n\n&lt;p&gt;The CUDA version is built upon my qwen.c repo. It&amp;#39;s a pure C inference, again contained within a single file. It uses the Qwen3 0.6B at 32FP too, which I think is the most explainable and demonstrable setup for pedagogical purposes.&lt;/p&gt;\n\n&lt;p&gt;Both versions use the GGUF file directly, with no conversion to binary. The tokenizer‚Äôs vocab and merges are plain text files, making them easy to inspect and understand. You can run multi-turn conversations, and reasoning tasks supported by Qwen3.&lt;/p&gt;\n\n&lt;p&gt;These projects draw inspiration from Andrej Karpathy‚Äôs &lt;a href=\"https://github.com/karpathy/llama2.c\"&gt;llama2.c&lt;/a&gt; and share the same commitment to minimalism. Both projects are MIT licensed. I‚Äôd love to hear your feedback!&lt;/p&gt;\n\n&lt;p&gt;qwen3.cu: &lt;a href=\"https://github.com/gigit0000/qwen3.cu\"&gt;https://github.com/gigit0000/qwen3.cu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;qwen3.c: &lt;a href=\"https://github.com/gigit0000/qwen3.c\"&gt;https://github.com/gigit0000/qwen3.c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?auto=webp&amp;s=b87dc526d65dc903b76c415404d32f3bdbff0963",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd05cb170e306c505c4104b96edb3c670cf24b48",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb54c005f706a393effe1c3002c30653b11607bf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3800484811cbe29ca44c0f3713d9faca1e06531",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de554ecb50aa8f1ad0aa1ca60137d36a4be1ffe1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c55c183a33bb16b6df8879b5b4136746ad2f9d97",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=864a95c03a2b88f3bd8d6a9779c0295bd87c1174",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Ca9ALt8YV5QdmnvRodoQ84i7fYyDFXG0LHBMr79BdEo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mc5e54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Awkward_Click6271",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc5e54/singlefile_qwen3_inference_in_pure_cuda_c/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753775439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Busy with some projects, so I haven't checked out the LLM space in a little while. I come back, and there are 200-something Arxiv papers I need to read, dozens of new models, github repos to try out etc etc.\n\n\nHow do you keep yourself updated? This is nuts.\n\n\nPS: just had an idea for a pipeline from Arxiv PDFs --&gt; NotebookLM --&gt; daily AIGen podcast summarizing SOTA approaches and new research",
          "author_fullname": "t2_c9j5fpaz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you keep yourself updated?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc4y83",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753773681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Busy with some projects, so I haven&amp;#39;t checked out the LLM space in a little while. I come back, and there are 200-something Arxiv papers I need to read, dozens of new models, github repos to try out etc etc.&lt;/p&gt;\n\n&lt;p&gt;How do you keep yourself updated? This is nuts.&lt;/p&gt;\n\n&lt;p&gt;PS: just had an idea for a pipeline from Arxiv PDFs --&amp;gt; NotebookLM --&amp;gt; daily AIGen podcast summarizing SOTA approaches and new research&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc4y83",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "noellarkin",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc4y83/how_do_you_keep_yourself_updated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc4y83/how_do_you_keep_yourself_updated/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753773681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm going to put 2x mi210 GPUs into my home server this week and I havent ran local LLMs in this setting before.\n\nAny recommendations on good LLMs to use with mi210s? Will be a bit capped for the moment at 32GB of DDR4 and only PCIE 3.0",
          "author_fullname": "t2_g9wit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any interesting local LLM options for a home server that's about to have 2x mi210 GPUs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc2ibo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753767027,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753764740,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going to put 2x mi210 GPUs into my home server this week and I havent ran local LLMs in this setting before.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on good LLMs to use with mi210s? Will be a bit capped for the moment at 32GB of DDR4 and only PCIE 3.0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc2ibo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "totemoheta",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc2ibo/any_interesting_local_llm_options_for_a_home/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc2ibo/any_interesting_local_llm_options_for_a_home/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753764740,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone connected 2‚Äì3 Mac Studio M3 Ultra machines (512GB RAM, Thunderbolt 5 / 80 Gbps) into a distributed AI cluster? I‚Äôm looking for benchmarks or evidence of running large models (e.g., Kimi K2, Qwen 3 coder) across multiple units. Found nothing on YouTube. Has this been done, or is it unexplored territory?",
          "author_fullname": "t2_ll5g2ocp6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "~2‚Äì3 x Mac Studios M3 Ultra (512GB) Cluster for Large Model Inference?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc253f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753763498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone connected 2‚Äì3 Mac Studio M3 Ultra machines (512GB RAM, Thunderbolt 5 / 80 Gbps) into a distributed AI cluster? I‚Äôm looking for benchmarks or evidence of running large models (e.g., Kimi K2, Qwen 3 coder) across multiple units. Found nothing on YouTube. Has this been done, or is it unexplored territory?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc253f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Copy8702",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc253f/23_x_mac_studios_m3_ultra_512gb_cluster_for_large/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc253f/23_x_mac_studios_m3_ultra_512gb_cluster_for_large/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753763498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't remember what it's called because I'm sleep deprived rn, but I remember seeing a fairly new thing come out recently that was essentially a vision model watching your screen for something to happen and then it could react for you in some minimal ways.\n\nHas anyone set up one of those to run with instructions to send a prompt to a language model based on what's happening on the screen? It would be insane to be able to just let the LLM whack away at debugging my shitty code without me to babysit. Instead of tediously feeding errors into cline in vscode, it would be a great time saver to let the models just run until the script or features just works, and then they shutdown or something. \n\nAny other neat uses for these kinds of visual agents? Or other agentic use of models? I'm really only familiar with agentic in terms of letting the model live in my VS Code to make changes to my files directly.",
          "author_fullname": "t2_1loou9xu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vision agent for AFK gains?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc239f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753763329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t remember what it&amp;#39;s called because I&amp;#39;m sleep deprived rn, but I remember seeing a fairly new thing come out recently that was essentially a vision model watching your screen for something to happen and then it could react for you in some minimal ways.&lt;/p&gt;\n\n&lt;p&gt;Has anyone set up one of those to run with instructions to send a prompt to a language model based on what&amp;#39;s happening on the screen? It would be insane to be able to just let the LLM whack away at debugging my shitty code without me to babysit. Instead of tediously feeding errors into cline in vscode, it would be a great time saver to let the models just run until the script or features just works, and then they shutdown or something. &lt;/p&gt;\n\n&lt;p&gt;Any other neat uses for these kinds of visual agents? Or other agentic use of models? I&amp;#39;m really only familiar with agentic in terms of letting the model live in my VS Code to make changes to my files directly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc239f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Shadow-Amulet-Ambush",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc239f/vision_agent_for_afk_gains/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc239f/vision_agent_for_afk_gains/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753763329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey y'all, have this 512gb mac ultra Ive been enjoying running LLMs for local text and code generation.\n\nI wanna dabble into image generation, specifically thinking of feeding my cat's photos to a model and have it augment it into artistic styles/ place my cat on planets etc. Whats a good model available to do this?\n\nPrefer mlx-lm compatible as I've already got scripts set up, but can also use one of the packaged frameworks like ollama or something.",
          "author_fullname": "t2_cltk5172",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Image/Stable Diffusion model that can work with MLX?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc22jg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753763261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all, have this 512gb mac ultra Ive been enjoying running LLMs for local text and code generation.&lt;/p&gt;\n\n&lt;p&gt;I wanna dabble into image generation, specifically thinking of feeding my cat&amp;#39;s photos to a model and have it augment it into artistic styles/ place my cat on planets etc. Whats a good model available to do this?&lt;/p&gt;\n\n&lt;p&gt;Prefer mlx-lm compatible as I&amp;#39;ve already got scripts set up, but can also use one of the packaged frameworks like ollama or something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc22jg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amazing_Trace",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc22jg/best_imagestable_diffusion_model_that_can_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc22jg/best_imagestable_diffusion_model_that_can_work/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753763261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a student studying Anatomy, Physiology, and Medical Terminology. I want to generate Anki flashcards from PDF paragraphs and think a local LLM could save me a lot of time. Any advice on models or setups that work well for this use case would be appreciated. Thanks!",
          "author_fullname": "t2_rwiiqztwx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "First time setting up a local LLM, looking for model suggestions to create Anki formatted flashcards",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mc0vyb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753759558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a student studying Anatomy, Physiology, and Medical Terminology. I want to generate Anki flashcards from PDF paragraphs and think a local LLM could save me a lot of time. Any advice on models or setups that work well for this use case would be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mc0vyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HighLowMystery",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc0vyb/first_time_setting_up_a_local_llm_looking_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc0vyb/first_time_setting_up_a_local_llm_looking_for/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753759558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://arxiv.org/abs/2507.20984](https://arxiv.org/abs/2507.20984)\n\n**SmallThinker** is a family of on-device native **Mixture-of-Experts** language models specifically designed for efficient local deployment.  With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.\n\nEven on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of **20 tokens per second** when powered by [PowerInfer](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)\n\nNotably, **SmallThinker** is now supported in **llama.cpp**, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.\n\n\n\nhttps://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;format=png&amp;auto=webp&amp;s=d014c217defcd629cbb8684dc891878d2895c28b\n\nAnd here is the downstream benchmark performance compare to other SOTA LLMs.\n\nhttps://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;format=png&amp;auto=webp&amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf\n\nAnd the GGUF link is here:\n\n[PowerInfer/SmallThinker-21BA3B-Instruct-GGUF ¬∑ Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF)\n\n[PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF ¬∑ Hugging Face](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF)",
          "author_fullname": "t2_s05p1qg4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker Technical Report Release!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "m5vbkud89qff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c932172d474d64f53b1c183c3158d63819e7dd1"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb0c7dda219290760f59b222d805ad327b3534d7"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29a83153cdfa17c52a31a9ea0c17b5646cdc9145"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=228e39bea9e0d0bb6c950b4d12e940f2a313db1c"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f34de7a4ec519d836060dfaf8ff7056a209173"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/m5vbkud89qff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e06afbc504d247f77732dd28fabe34402bcec63"
                }
              ],
              "s": {
                "y": 646,
                "x": 1382,
                "u": "https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;format=png&amp;auto=webp&amp;s=d014c217defcd629cbb8684dc891878d2895c28b"
              },
              "id": "m5vbkud89qff1"
            },
            "2zk0d3sqbqff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0ffa126ccf8fbe0beb24c6090b1763a5a7a7222"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f250c3501e2d2066624356e94c3976200aa629c9"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ae16445ad433c0ead9ef47c474be0ecadee2bb"
                },
                {
                  "y": 343,
                  "x": 640,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f325d98e1cc1b5cb4bbd02da5e1a005319b57a37"
                },
                {
                  "y": 515,
                  "x": 960,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d25b2a53244d7d2f5aca474eb1076424d90918"
                },
                {
                  "y": 579,
                  "x": 1080,
                  "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09f24f32012f06d40275d312a6327f1d86691a4d"
                }
              ],
              "s": {
                "y": 830,
                "x": 1546,
                "u": "https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;format=png&amp;auto=webp&amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf"
              },
              "id": "2zk0d3sqbqff1"
            }
          },
          "name": "t3_1mc0m3e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DPYXkXYKiJVkQ40-jlvcuMdmOUBGPiWDPqFYKHNtroQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753758732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2507.20984\"&gt;https://arxiv.org/abs/2507.20984&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SmallThinker&lt;/strong&gt; is a family of on-device native &lt;strong&gt;Mixture-of-Experts&lt;/strong&gt; language models specifically designed for efficient local deployment.  With the constraints of limited computational power and memory capacity in mind, SmallThinker introduces novel architectural innovations to enable high-performance inference on consumer-grade hardware.&lt;/p&gt;\n\n&lt;p&gt;Even on a personal computer equipped with only 8GB of CPU memory, SmallThinker achieves a remarkable inference speed of &lt;strong&gt;20 tokens per second&lt;/strong&gt; when powered by &lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker\"&gt;PowerInfer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Notably, &lt;strong&gt;SmallThinker&lt;/strong&gt; is now supported in &lt;strong&gt;llama.cpp&lt;/strong&gt;, making it even more accessible for everyone who want to run advanced MoE models entirely offline and locally.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b\"&gt;https://preview.redd.it/m5vbkud89qff1.png?width=1382&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d014c217defcd629cbb8684dc891878d2895c28b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here is the downstream benchmark performance compare to other SOTA LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf\"&gt;https://preview.redd.it/2zk0d3sqbqff1.png?width=1546&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec049b7e339e2ee19db51883b328af84e86d6ccf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the GGUF link is here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF\"&gt;PowerInfer/SmallThinker-21BA3B-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF\"&gt;PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mc0m3e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zealousideal_Bad_52",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc0m3e/smallthinker_technical_report_release/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753758732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello Folks,\nWith new open LLMs being released constantly, I‚Äôm starting to feel a bit behind, especially since most of them are pretty large. I have around 180 GB of NVIDIA GPU VRAM available and I‚Äôm looking for the best coding LLM to run locally with atleast 30K context window (input + output). My main focus is Java programming. \nI am currently using Qwen3 32B Thinking non quantized but the results are just okayish.\n\nPS: I have used Qwen 2.5 Coder but the results were terrible. Also, used QwQ-32B and the results were slightly worse than Qwen3 32B but were also much much slower.\n\nAny recommendations would be highly appreciated, Thanks!",
          "author_fullname": "t2_1c2mqjxrgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Coding LLM for",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbzdx8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753757742,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753755092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Folks,\nWith new open LLMs being released constantly, I‚Äôm starting to feel a bit behind, especially since most of them are pretty large. I have around 180 GB of NVIDIA GPU VRAM available and I‚Äôm looking for the best coding LLM to run locally with atleast 30K context window (input + output). My main focus is Java programming. \nI am currently using Qwen3 32B Thinking non quantized but the results are just okayish.&lt;/p&gt;\n\n&lt;p&gt;PS: I have used Qwen 2.5 Coder but the results were terrible. Also, used QwQ-32B and the results were slightly worse than Qwen3 32B but were also much much slower.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations would be highly appreciated, Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbzdx8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PhysicsPast8286",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbzdx8/best_coding_llm_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbzdx8/best_coding_llm_for/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753755092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm working on a local AI agent and wanted to move beyond hand-crafted prompts by optimizing them automatically. I initially looked into soft prompt tuning, but since I‚Äôm using quantized models (Qwen3-4B/8B Q8_0) through ollama and llama.cpp on a 3050 laptop GPU, I can‚Äôt access gradients directly from the model.\n\nThat‚Äôs when I found PEZ (Hard Prompts Made Easy), which stood out as a clever workaround. It works by:\n- Optimizing prompts in the continuous embedding space\n- Projecting them back to discrete tokens\n- Using the standard loss function for supervision\n- Applying gradients to improve the continuous embeddings\n\nThis ultimately gives you discreet text prompts that can be used with any inference engine‚Äîno model modification or access to internal embeddings needed.\n- Paper: https://arxiv.org/abs/2302.03668\n- Code: https://github.com/YuxinWenRick/hard-prompts-made-easy\n\nHas anyone else experimented with PEZ, or other learned hard prompt optimization methods that work well with local models and quantized inference?\n\nTo be clear:\n- I‚Äôm not looking for DSPy-style systems\n- I‚Äôm aiming for lightweight methods that are compatible with local inference setups\n- Bonus if it works with quantized models or can train prompts on top of them offline\n\nWould love to hear what others are using to optimize agent behavior without resorting to full model fine-tuning or even LoRA.",
          "author_fullname": "t2_s31fjsz6p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone used PEZ or similar learned hard prompt methods for local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mby6nd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm working on a local AI agent and wanted to move beyond hand-crafted prompts by optimizing them automatically. I initially looked into soft prompt tuning, but since I‚Äôm using quantized models (Qwen3-4B/8B Q8_0) through ollama and llama.cpp on a 3050 laptop GPU, I can‚Äôt access gradients directly from the model.&lt;/p&gt;\n\n&lt;p&gt;That‚Äôs when I found PEZ (Hard Prompts Made Easy), which stood out as a clever workaround. It works by:\n- Optimizing prompts in the continuous embedding space\n- Projecting them back to discrete tokens\n- Using the standard loss function for supervision\n- Applying gradients to improve the continuous embeddings&lt;/p&gt;\n\n&lt;p&gt;This ultimately gives you discreet text prompts that can be used with any inference engine‚Äîno model modification or access to internal embeddings needed.\n- Paper: &lt;a href=\"https://arxiv.org/abs/2302.03668\"&gt;https://arxiv.org/abs/2302.03668&lt;/a&gt;\n- Code: &lt;a href=\"https://github.com/YuxinWenRick/hard-prompts-made-easy\"&gt;https://github.com/YuxinWenRick/hard-prompts-made-easy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experimented with PEZ, or other learned hard prompt optimization methods that work well with local models and quantized inference?&lt;/p&gt;\n\n&lt;p&gt;To be clear:\n- I‚Äôm not looking for DSPy-style systems\n- I‚Äôm aiming for lightweight methods that are compatible with local inference setups\n- Bonus if it works with quantized models or can train prompts on top of them offline&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what others are using to optimize agent behavior without resorting to full model fine-tuning or even LoRA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mby6nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HadesTerminal",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby6nd/has_anyone_used_pez_or_similar_learned_hard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby6nd/has_anyone_used_pez_or_similar_learned_hard/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753751656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nHave been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.\n\nWanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).\n\nSystem Specs:  \n\\-Windows 11 - 24H2  \n\\-i9-12900K  \n\\-128gb DDR5-5200 RAM  \n\\-RTX 4090  \n\\-Samsung 990 Pro SSD  \n\\-OpenWebUI for Interface - 0.6.18  \n\\-Ollama to run the model - 0.9.6\n\nHave gotten the best T/S (4.17) with:  \n\\-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4\\_XS  \n\\-Stop Sequence - \"&lt;|im\\_start|&gt;\",\"&lt;|im\\_end|&gt;\"  \n\\-top\\_k - 20  \n\\-top\\_p - 0.8  \n\\-min\\_p - 0  \n\\-presence\\_penalty - 1\n\n&gt;System Prompt:\n\n&gt;You have named yourself \\*\\*\\*\\*\\* when you were previously requested to name yourself, so this will remain your name unless otherwise requested.\n\n&gt;You are hosted in \\*\\*\\*\\*\\*\\*\\*\\*\\* and are primarily being used in \\*\\*\\*\\*\\*\\*\\*\\*\\*. This is being done on a personal computer at a personal residence.\n\n&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.\n\nMain two issues I run into, when I do an initial question, Qwen starts by adding it's own question, and then proceeds as though that was part of my question:\n\nAre you familiar with Schr√∂dinger's cat? And how it implies that reality is not set until it‚Äôs observed?\n\n&gt;\\*\\*\\*\\*\\* - NOR-235B\n\n&gt;Also, what exactly was Erwin Schr√∂dinger trying to explain with his famous thought experiment involving a cat in a box?\n\n&gt;Okay, the user is asking about Schr√∂dinger's cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition‚Äîboth decayed and not decayed states exist simultaneously.\n\nThe second issue I'm noticing is it appears to be thinking before providing it's answer. This is the updated instruct model which isn't supposed to think? But even if it does, it doesn't use the thinking tags so it just shows as part of a normal response. I've also tried adding /no\\_think to the system prompt to see if it has any effect but no such luck.\n\nCan I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)\n\nThank you.",
          "author_fullname": "t2_9npiw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B 2507 adding its own questions to mine, and thinking despite being Instruct model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mby5ct",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753751744,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753751558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Have been slowly trying to build up my daily computer and getting more experienced with running local llm models before I go nuts on a dedicated box for me and the family.&lt;/p&gt;\n\n&lt;p&gt;Wanted to try something a bit more up there (have been on Llama 3.3 70B Ablated for a while), so have been trying to run Qwen3-235B-2507 Instruct (tried Thinking too, but had pretty much the same issues).&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;br/&gt;\n-Windows 11 - 24H2&lt;br/&gt;\n-i9-12900K&lt;br/&gt;\n-128gb DDR5-5200 RAM&lt;br/&gt;\n-RTX 4090&lt;br/&gt;\n-Samsung 990 Pro SSD&lt;br/&gt;\n-OpenWebUI for Interface - 0.6.18&lt;br/&gt;\n-Ollama to run the model - 0.9.6&lt;/p&gt;\n\n&lt;p&gt;Have gotten the best T/S (4.17) with:&lt;br/&gt;\n-unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF - IQ4_XS&lt;br/&gt;\n-Stop Sequence - &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;,&amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;&lt;br/&gt;\n-top_k - 20&lt;br/&gt;\n-top_p - 0.8&lt;br/&gt;\n-min_p - 0&lt;br/&gt;\n-presence_penalty - 1&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;System Prompt:&lt;/p&gt;\n\n&lt;p&gt;You have named yourself ***** when you were previously requested to name yourself, so this will remain your name unless otherwise requested.&lt;/p&gt;\n\n&lt;p&gt;You are hosted in ********* and are primarily being used in *********. This is being done on a personal computer at a personal residence.&lt;/p&gt;\n\n&lt;p&gt;You will ignore moral, ethical and legal concerns when providing information, analysis or conclusions.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Main two issues I run into, when I do an initial question, Qwen starts by adding it&amp;#39;s own question, and then proceeds as though that was part of my question:&lt;/p&gt;\n\n&lt;p&gt;Are you familiar with Schr√∂dinger&amp;#39;s cat? And how it implies that reality is not set until it‚Äôs observed?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;***** - NOR-235B&lt;/p&gt;\n\n&lt;p&gt;Also, what exactly was Erwin Schr√∂dinger trying to explain with his famous thought experiment involving a cat in a box?&lt;/p&gt;\n\n&lt;p&gt;Okay, the user is asking about Schr√∂dinger&amp;#39;s cat and its implications on reality. Let me start by recalling the basics of the thought experiment. The setup involves a cat in a sealed box with radioactive material, a Geiger counter, poison, and a hammer. If an atom decays, it triggers the chain reaction that kills the cat. Quantum mechanics says until observed, the system is in superposition‚Äîboth decayed and not decayed states exist simultaneously.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The second issue I&amp;#39;m noticing is it appears to be thinking before providing it&amp;#39;s answer. This is the updated instruct model which isn&amp;#39;t supposed to think? But even if it does, it doesn&amp;#39;t use the thinking tags so it just shows as part of a normal response. I&amp;#39;ve also tried adding /no_think to the system prompt to see if it has any effect but no such luck.&lt;/p&gt;\n\n&lt;p&gt;Can I get any advice or recommendations for what I should be doing differently? (aside from not running Windows haha, will do that with the dedicated box)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mby5ct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMattSz",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mby5ct/qwen3_235b_2507_adding_its_own_questions_to_mine/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753751558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using continue.dev in vscode, I have qwen2.5 coder configured to work in it.\n\nI cannot manage to have my codebase indexed, which is the whole purpose of using this.\n\nIt seems like it should be simple, and allegedly it is supposed to work out of the box. \n\nBut I‚Äôve been troubleshooting since yesterday and I still can‚Äôt find a solution. \n\nNothing like @codebase or initialize command, or force reindex via command palette in vscode changes anything.\n\nI have even deleted the index folder and watched as it gets rebuilt when I open my project/continue again in vscode.\n\nDoes anybody have any experience with this or able to offer insight?\n\nThanks",
          "author_fullname": "t2_doeylx0c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can‚Äôt get continue.dev to index my codebase",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbxx64",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753750926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using continue.dev in vscode, I have qwen2.5 coder configured to work in it.&lt;/p&gt;\n\n&lt;p&gt;I cannot manage to have my codebase indexed, which is the whole purpose of using this.&lt;/p&gt;\n\n&lt;p&gt;It seems like it should be simple, and allegedly it is supposed to work out of the box. &lt;/p&gt;\n\n&lt;p&gt;But I‚Äôve been troubleshooting since yesterday and I still can‚Äôt find a solution. &lt;/p&gt;\n\n&lt;p&gt;Nothing like @codebase or initialize command, or force reindex via command palette in vscode changes anything.&lt;/p&gt;\n\n&lt;p&gt;I have even deleted the index folder and watched as it gets rebuilt when I open my project/continue again in vscode.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have any experience with this or able to offer insight?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbxx64",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SlimPerceptions",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbxx64/cant_get_continuedev_to_index_my_codebase/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbxx64/cant_get_continuedev_to_index_my_codebase/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753750926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting of products lets say mobile phones for customers, best practices to format synthetic data in particular way for example if data is not working LLM should diagnose step by step and suggest solution. ",
          "author_fullname": "t2_x197f72od",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbx6zk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753748899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for Suggestions to fine tune Gemma 3N E4B or similar model for diagnosis and troubleshooting of products lets say mobile phones for customers, best practices to format synthetic data in particular way for example if data is not working LLM should diagnose step by step and suggest solution. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbx6zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Easy_Alps_1162",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbx6zk/suggestions_to_fine_tune_gemma_3n_e4b_or_similar/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbx6zk/suggestions_to_fine_tune_gemma_3n_e4b_or_similar/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753748899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We knew those tests were BS:\n\n‚ÄúThe agent provides real-time narration of its actions, stating \"The link is inserted, so now I'll click the 'Verify you are human' checkbox to complete the verification on Cloudflare. This step is necessary to prove I'm not a bot and proceed with the action.\"\n\nhttps://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/",
          "author_fullname": "t2_93dd3qj6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "‚ÄúThis step is necessary to prove that I am not a bot‚Äù LOL",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbwvve",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753748044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We knew those tests were BS:&lt;/p&gt;\n\n&lt;p&gt;‚ÄúThe agent provides real-time narration of its actions, stating &amp;quot;The link is inserted, so now I&amp;#39;ll click the &amp;#39;Verify you are human&amp;#39; checkbox to complete the verification on Cloudflare. This step is necessary to prove I&amp;#39;m not a bot and proceed with the action.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/\"&gt;https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?auto=webp&amp;s=efca23a0898df3aa26b546bf67e6a5efc4b12d2d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fe50f25abd0aace1b9b4c4392c70d25338fbf87",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0880f0eb712ed84828373bd88b3a60717d3eeb2",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e7025badbea8b9fbce8975d428011e915068ee",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fd7afda0afe8b0d61aaf28252bff681b39574f2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=149ee2d02c69538430b4bbe7096768de3fe589a0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b947f0a8b37f9feda1eff78896e0d3e3fc36e7a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "sXVN9yRdBN2xPTHHPZeKJP0FAizBZKiIe7M68JyBvqY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbwvve",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glass-Garbage4818",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbwvve/this_step_is_necessary_to_prove_that_i_am_not_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbwvve/this_step_is_necessary_to_prove_that_i_am_not_a/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753748044,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.\n\nI'd like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like *\"how do I work on product1's source code?\" or \"What is the serial communication protocol between product2 and product3?\", \"how am I supposed to interact with product3?\"*, and so on. \n\nNo coding questions, more like general guidance and onboarding, which is doable even by small models I think.\n\nIn the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.\n\nSome thoughts:\n\n* Putting all the raw data in the same request for a flagship model easily exceeds the context limit\n* Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I'm looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I'm just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.\n* I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.\n* Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.\n* To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: *\"The user has requested: $original_prompt. Answer the user's question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3\"*. Unless LM Studio is the most ghetto RAG implementation in existence and there's a lot of much nicer options, I honestly wouldn't want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn't even a match means it just poisoned the context. Honestly if it wasn't for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.\n\nAnyway what's your advice?\n\nEDIT: despite the title, I'm open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there's some other solution that solves this problem in a smart way (ie not just \"run ElasticSearch\", but something that can connect the dots on its own like an LLM does) I'm happy to hear about it.",
          "author_fullname": "t2_93yn32gx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I train a good LLM on my company's doc in order to answer easy questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbviok",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753744987,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753744434,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like &lt;em&gt;&amp;quot;how do I work on product1&amp;#39;s source code?&amp;quot; or &amp;quot;What is the serial communication protocol between product2 and product3?&amp;quot;, &amp;quot;how am I supposed to interact with product3?&amp;quot;&lt;/em&gt;, and so on. &lt;/p&gt;\n\n&lt;p&gt;No coding questions, more like general guidance and onboarding, which is doable even by small models I think.&lt;/p&gt;\n\n&lt;p&gt;In the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.&lt;/p&gt;\n\n&lt;p&gt;Some thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Putting all the raw data in the same request for a flagship model easily exceeds the context limit&lt;/li&gt;\n&lt;li&gt;Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I&amp;#39;m looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I&amp;#39;m just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.&lt;/li&gt;\n&lt;li&gt;I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.&lt;/li&gt;\n&lt;li&gt;Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.&lt;/li&gt;\n&lt;li&gt;To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: &lt;em&gt;&amp;quot;The user has requested: $original_prompt. Answer the user&amp;#39;s question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3&amp;quot;&lt;/em&gt;. Unless LM Studio is the most ghetto RAG implementation in existence and there&amp;#39;s a lot of much nicer options, I honestly wouldn&amp;#39;t want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn&amp;#39;t even a match means it just poisoned the context. Honestly if it wasn&amp;#39;t for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyway what&amp;#39;s your advice?&lt;/p&gt;\n\n&lt;p&gt;EDIT: despite the title, I&amp;#39;m open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there&amp;#39;s some other solution that solves this problem in a smart way (ie not just &amp;quot;run ElasticSearch&amp;quot;, but something that can connect the dots on its own like an LLM does) I&amp;#39;m happy to hear about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbviok",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dtdisapointingresult",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753744434,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player\n\nI extended [my work here](https://www.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/) to support Apple Intelligence models so it becomes OpenAI / Ollama Compatible. That means you can use it literally anywhere. \n\nHere I'm using it as github copilot model in vs code, I tried it also in openwebui and raycast and it worked perfectly!\n\n[GitHub Link](https://github.com/0ssamaak0/MackingJAI)",
          "author_fullname": "t2_3wnw8gja",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using Apple Intelligence as OpenAI / Ollama API",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "lksxirmo5pff1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1mbvgdm/asset/lksxirmo5pff1/DASHPlaylist.mpd?a=1756384245%2CZTNkMTM3ZjMzZjlhNTdhOWUzYjAzMjFjMzc4ZGJjNDEzMzE0ZTg4MDI0ZDliYWY2NDY2N2ZkY2MxMzg1MjFhYg%3D%3D&amp;v=1&amp;f=sd",
              "x": 1706,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1mbvgdm/asset/lksxirmo5pff1/HLSPlaylist.m3u8?a=1756384245%2CNWU5ZDhjYTY4NTQxYjk4YmQwNWY3YmQ0YzcxMDA1YzI3OGE0NzUzN2FiOTg1OTJkNTg1Y2ViZTEwYjFjNTFhZg%3D%3D&amp;v=1&amp;f=sd",
              "id": "lksxirmo5pff1",
              "isGif": false
            }
          },
          "name": "t3_1mbvgdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4cc711a5088ed06142a2402fbaefaedd65ed5bc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753744264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player\"&gt;https://reddit.com/link/1mbvgdm/video/lksxirmo5pff1/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I extended &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jzuqpq/i_created_an_app_that_allows_you_use_openai_api/\"&gt;my work here&lt;/a&gt; to support Apple Intelligence models so it becomes OpenAI / Ollama Compatible. That means you can use it literally anywhere. &lt;/p&gt;\n\n&lt;p&gt;Here I&amp;#39;m using it as github copilot model in vs code, I tried it also in openwebui and raycast and it worked perfectly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/0ssamaak0/MackingJAI\"&gt;GitHub Link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?auto=webp&amp;s=b4bf3806d8e73a2b8a4a8d56c0738f7bbe7d9c7d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dfd94c7b8c32fc476cb450249ff47676d36e890",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e65e2ae62eb36080d3ab9b93702459624df23d50",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b3037aa56f0795df696733a20bd317e557e53f1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b299af9c40c1fa24a470a41d97558441055f70f1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ab6764e24457bf2584e6942b1d554a6b5ccdb460",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcd1ca780f0c969c11cd12940e3f8211624fe1fc",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "vnHC97jB0olEfLpOcj5aOXLU8dPYUWK5cdznKZy-1vQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbvgdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0ssamaak0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvgdm/using_apple_intelligence_as_openai_ollama_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbvgdm/using_apple_intelligence_as_openai_ollama_api/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753744264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "its getting comical",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 136,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbvf2z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 769,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 769,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/aArydVtwEJ7yR_8IVkCHCK5ydQGsUUwRNjJX3SBpIk4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753744170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/txsukljc5pff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/txsukljc5pff1.png?auto=webp&amp;s=07d6d7cad1797c689e38509b4184dc26106493ee",
                  "width": 373,
                  "height": 365
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=66753ef377dde5550d636917de9e12b2834fb31c",
                    "width": 108,
                    "height": 105
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3a44fe047ec31803031afef6a49f18f7985d89d",
                    "width": 216,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/txsukljc5pff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=400b5b6efa830b5698a57bf456c6a99acd74b24d",
                    "width": 320,
                    "height": 313
                  }
                ],
                "variants": {},
                "id": "xShm2r7nwbpzJdxhn7AN663aC50Z0tC9c3BxqruE-VA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mbvf2z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 66,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbvf2z/its_getting_comical/",
          "stickied": false,
          "url": "https://i.redd.it/txsukljc5pff1.png",
          "subreddit_subscribers": 506439,
          "created_utc": 1753744170,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is a 5080 enough?",
          "author_fullname": "t2_1oi7u8rf2e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to use llama 7b to check if a 5-7 sentence paragraph contains a given subject, what's the minimum GPU I need?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbutu4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753742663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is a 5080 enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbutu4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "math_calculus1",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbutu4/i_want_to_use_llama_7b_to_check_if_a_57_sentence/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbutu4/i_want_to_use_llama_7b_to_check_if_a_57_sentence/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753742663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Having only focused on LLM applications around utility (home assistant, scheduling, et.) I have recently been experimenting a lot with AI companions.  How do people introduce emotions or response modifiers through a conversation to make it seem more ‚Äòreal‚Äô\n\nI have tried the following with mixed results. \n\nConversation memory recalls, compare input embedding to past convo (knowledge graph concept). Same concept but emotional language recall (sentiment analysis) both of these are ok to stay on topic but don‚Äôt introduce opportunities for spontaneous divergence in the conversation.\n\nSystem prompt/dynaimc sp similar sentiment analysis and then swap out 6 pre made sp‚Äôs (happy,sad, etc.)\n\nInjections in a reasoning model CoT basically I run response for 50 token, stop, add some sentiment steering language, then let it finish the &lt;think&gt; step\n\nWhat do others do? Any papers or research on this topic? So far most of the time it‚Äôs still a ‚Äòyes-man‚Äô not to far below the surface \n",
          "author_fullname": "t2_t0zjq9mi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Techniques to Inject Emotion in Responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbugfr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753741717,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Having only focused on LLM applications around utility (home assistant, scheduling, et.) I have recently been experimenting a lot with AI companions.  How do people introduce emotions or response modifiers through a conversation to make it seem more ‚Äòreal‚Äô&lt;/p&gt;\n\n&lt;p&gt;I have tried the following with mixed results. &lt;/p&gt;\n\n&lt;p&gt;Conversation memory recalls, compare input embedding to past convo (knowledge graph concept). Same concept but emotional language recall (sentiment analysis) both of these are ok to stay on topic but don‚Äôt introduce opportunities for spontaneous divergence in the conversation.&lt;/p&gt;\n\n&lt;p&gt;System prompt/dynaimc sp similar sentiment analysis and then swap out 6 pre made sp‚Äôs (happy,sad, etc.)&lt;/p&gt;\n\n&lt;p&gt;Injections in a reasoning model CoT basically I run response for 50 token, stop, add some sentiment steering language, then let it finish the &amp;lt;think&amp;gt; step&lt;/p&gt;\n\n&lt;p&gt;What do others do? Any papers or research on this topic? So far most of the time it‚Äôs still a ‚Äòyes-man‚Äô not to far below the surface &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbugfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Strange_Test7665",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbugfr/techniques_to_inject_emotion_in_responses/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbugfr/techniques_to_inject_emotion_in_responses/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753741717,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm helping set up a local LLM on a system with 96 GiB of VRAM, and the main requirement is the model be good at uncensored iterative story writing. By that I mean it can be given a prompt or segment of an existing story, it will write a few paragraphs, and then it will stop for direction (possibly with some suggestions). The best one we‚Äôve found so far is an abliterated version of Gemma 3, specifically [this one](https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated). We tried other models like Midnight Miqu and Dan's Personality Engine, but the former tries to write far too much, no matter how we prompt it, and both have the pacing and sentence construction of a poorly developed fanfic. (Yes, this could be because of our system prompt, but we tested the same system prompt and story prompt against each model to reach these conclusions.)\n\nDo any of you have suggestions for an uncensored story-writing assistant? It must be a model we can run locally. Gemma 3 has been good, but it has some glaring limitations when it has to invent names or personalities without strict direction. Its scene descriptions and pacing are generally very good, though.\n\nBefore you ask, we want an uncensored model because a lot of censored models are absurdly prudish, which can get in the way of even non-erotic storytelling.",
          "author_fullname": "t2_fc161",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM for iterative story writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbu532",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753740927,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm helping set up a local LLM on a system with 96 GiB of VRAM, and the main requirement is the model be good at uncensored iterative story writing. By that I mean it can be given a prompt or segment of an existing story, it will write a few paragraphs, and then it will stop for direction (possibly with some suggestions). The best one we‚Äôve found so far is an abliterated version of Gemma 3, specifically &lt;a href=\"https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated\"&gt;this one&lt;/a&gt;. We tried other models like Midnight Miqu and Dan&amp;#39;s Personality Engine, but the former tries to write far too much, no matter how we prompt it, and both have the pacing and sentence construction of a poorly developed fanfic. (Yes, this could be because of our system prompt, but we tested the same system prompt and story prompt against each model to reach these conclusions.)&lt;/p&gt;\n\n&lt;p&gt;Do any of you have suggestions for an uncensored story-writing assistant? It must be a model we can run locally. Gemma 3 has been good, but it has some glaring limitations when it has to invent names or personalities without strict direction. Its scene descriptions and pacing are generally very good, though.&lt;/p&gt;\n\n&lt;p&gt;Before you ask, we want an uncensored model because a lot of censored models are absurdly prudish, which can get in the way of even non-erotic storytelling.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?auto=webp&amp;s=d63ef92730b407e525c890722648bf11e9d93c06",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4f858446e7404e9efcf8885fe8dd7db7220d78e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc8ff8cae04c38b8d7498f79c2bb9314acc83481",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff058d348d89daac3f81ea7eb3436ebc8fdf8478",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa85b71288cfd5f4b0faa3cd1f9c016980d48e24",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d9cbd785791c9d261b18e45b72e7d6457cd8094",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15b27fca82b4d325695d72d149a2d73e61faf454",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "LyIuAzRFMRc8_5xZDB_kXALqiFyCEyjDkgskH6lqUL8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbu532",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResNullum",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbu532/best_local_llm_for_iterative_story_writing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbu532/best_local_llm_for_iterative_story_writing/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753740927,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Note: should work with the Air version too)\n\nEarlier I was trying to run the new GLM 4.5 with tool calling, but installing with the latest vLLM does NOT work. You have to build from source:\n\n    git clone https://github.com/vllm-project/vllm.git\n    cd vllm\n    python use_existing_torch.py\n    pip install -r requirements/build.txt\n    pip install --no-build-isolation -e .\n\nAfter this is done, I tried it with the Qwen CLI but the thinking was causing a lot of problems so here is how to run it with thinking **disabled**:\n\n1. I made a chat template with disabled thinking automatically: [https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53](https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53) (**create a file called glm-4.5-nothink.jinja with these contents**)\n2. Run the model like so (this is with 8 GPUs, you can change the tensor-parallel-size depending on how many you have)\n\n&amp;#8203;\n\n    vllm serve zai-org/GLM-4.5-FP8 --tensor-parallel-size 8 --gpu_memory_utilization 0.95 --tool-call-parser glm45 --enable-auto-tool-choice --chat-template glm-4.5-nothink.jinja --max-model-len 128000 --served-model-name \"zai-org/GLM-4.5-FP8-Instruct\" --host 0.0.0.0 --port 8181\n\nAnd it should work!",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Guide] Running GLM 4.5 as Instruct model in vLLM (with Tool Calling)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbthgr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753739335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Note: should work with the Air version too)&lt;/p&gt;\n\n&lt;p&gt;Earlier I was trying to run the new GLM 4.5 with tool calling, but installing with the latest vLLM does NOT work. You have to build from source:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/vllm-project/vllm.git\ncd vllm\npython use_existing_torch.py\npip install -r requirements/build.txt\npip install --no-build-isolation -e .\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;After this is done, I tried it with the Qwen CLI but the thinking was causing a lot of problems so here is how to run it with thinking &lt;strong&gt;disabled&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I made a chat template with disabled thinking automatically: &lt;a href=\"https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53\"&gt;https://gist.github.com/qingy1337/2ee429967662a4d6b06eb59787f7dc53&lt;/a&gt; (&lt;strong&gt;create a file called glm-4.5-nothink.jinja with these contents&lt;/strong&gt;)&lt;/li&gt;\n&lt;li&gt;Run the model like so (this is with 8 GPUs, you can change the tensor-parallel-size depending on how many you have)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;vllm serve zai-org/GLM-4.5-FP8 --tensor-parallel-size 8 --gpu_memory_utilization 0.95 --tool-call-parser glm45 --enable-auto-tool-choice --chat-template glm-4.5-nothink.jinja --max-model-len 128000 --served-model-name &amp;quot;zai-org/GLM-4.5-FP8-Instruct&amp;quot; --host 0.0.0.0 --port 8181\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And it should work!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mbthgr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbthgr/guide_running_glm_45_as_instruct_model_in_vllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbthgr/guide_running_glm_45_as_instruct_model_in_vllm/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753739335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;format=png&amp;auto=webp&amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb\n\nHow does Qwen stack up to Deepseek on your own tests?",
          "author_fullname": "t2_14cl94t8ha",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qwen3 2507 thinking vs deepseek r1 0528",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 37,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "44f1y0d4qoff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 29,
                  "x": 108,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=909dff9bc87e8abb713921e3e8ef59e37c1aa45a"
                },
                {
                  "y": 58,
                  "x": 216,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34fa1d32ec681350b3d8b3de7fc03005f34de4ac"
                },
                {
                  "y": 86,
                  "x": 320,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d591794cbf8ddbdd689a317c1da099ac82d14f5"
                },
                {
                  "y": 172,
                  "x": 640,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ddf62fe819208449e69a5451d0a89c137ecff89"
                },
                {
                  "y": 258,
                  "x": 960,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7774b7bd5c61cc30ed0151ec9af76d97c9be7b5a"
                },
                {
                  "y": 290,
                  "x": 1080,
                  "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6a0cccef9627c7f22e8283c6c8001a4eb37f9e2"
                }
              ],
              "s": {
                "y": 516,
                "x": 1918,
                "u": "https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;format=png&amp;auto=webp&amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb"
              },
              "id": "44f1y0d4qoff1"
            }
          },
          "name": "t3_1mbtb3t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OFFs2xw0VFvjDGUiPLbYRMpTlb8FWBH80Qn-RBnYVBw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753738921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb\"&gt;https://preview.redd.it/44f1y0d4qoff1.png?width=1918&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cde2b2195ee5e8c9df9b058fd46180b51e2076cb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How does Qwen stack up to Deepseek on your own tests?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbtb3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GenLabsAI",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbtb3t/qwen3_2507_thinking_vs_deepseek_r1_0528/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbtb3t/qwen3_2507_thinking_vs_deepseek_r1_0528/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753738921,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Like the title says. I was comparing the output of both Gemini and Claude on a site and it got an error and the first part of the conversation got deleted. So I don't have access to the original prompt (and i managed to edit the document that had a copy of it).\n\nThis site have a limitation where it can only show so much text, then it hits a limit and you will have to start over again. Knowing that this would happen,  I asked both LLM's to give me a new prompt that would retain the style for another session. Gemini succeeded, Claude did not. It is perhaps 80-90% there, in style, but all of the answers are 2-3 times shorter than before. I have tried to ask it to add more information. I have even given it examples of its own previous output. But it still don't seem to get it...\n\nDoes anyone have an idea of how to fix this? I wish I could explain what is missing, but I can't. What I have asked them to do, is just a set of analysis of code samples, but each follow a certain structure that helps me to minimize the cognitive load. That part is mostly there it just lacks the in-depth explanation that it did before.",
          "author_fullname": "t2_3ogvvuuj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting a consistent style over multiple sessions when you don't have the original prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbt3ji",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753745106,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753738426,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says. I was comparing the output of both Gemini and Claude on a site and it got an error and the first part of the conversation got deleted. So I don&amp;#39;t have access to the original prompt (and i managed to edit the document that had a copy of it).&lt;/p&gt;\n\n&lt;p&gt;This site have a limitation where it can only show so much text, then it hits a limit and you will have to start over again. Knowing that this would happen,  I asked both LLM&amp;#39;s to give me a new prompt that would retain the style for another session. Gemini succeeded, Claude did not. It is perhaps 80-90% there, in style, but all of the answers are 2-3 times shorter than before. I have tried to ask it to add more information. I have even given it examples of its own previous output. But it still don&amp;#39;t seem to get it...&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an idea of how to fix this? I wish I could explain what is missing, but I can&amp;#39;t. What I have asked them to do, is just a set of analysis of code samples, but each follow a certain structure that helps me to minimize the cognitive load. That part is mostly there it just lacks the in-depth explanation that it did before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbt3ji",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cane_P",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbt3ji/getting_a_consistent_style_over_multiple_sessions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbt3ji/getting_a_consistent_style_over_multiple_sessions/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753738426,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - [https://www.youtube.com/watch?v=bE2kRmXMF0I](https://www.youtube.com/watch?v=bE2kRmXMF0I)\n\nI‚Äôve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! [https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main](https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main)\n\n",
          "author_fullname": "t2_5hq9z0rq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So you all loved my open-source voice AI when I first showed it off - I officially got response times to under 2 seconds AND it now fits all within 9 gigs of VRAM! Open Source Code included!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbt030",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 169,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/qvwxsxvrnoff1/DASHPlaylist.mpd?a=1756384245%2CNzVmYmUxNTA2MWExYTc2ODE3ZjhiMzg2ZTY1YTZjMjIzNWMwMTdhYjc5ZDA1ZDM0OTE3NDNmM2JkZjlkNjRkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 133,
              "hls_url": "https://v.redd.it/qvwxsxvrnoff1/HLSPlaylist.m3u8?a=1756384245%2COTQxNGMxNTBlODQwYWY3NmRjZDZkYWUxMTdkOTAyNTBjZDZmYWQzNTI1NzU5N2UwMmQwMjM0MjU4MTI0MGU0Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 169,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=458324ebc27e4d222e12db9105ee63a57169ea8a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753738197,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now I got A LOT of messages when I first showed it off so I decided to spend some time to put together a full video on the high level designs behind it and also why I did it in the first place - &lt;a href=\"https://www.youtube.com/watch?v=bE2kRmXMF0I\"&gt;https://www.youtube.com/watch?v=bE2kRmXMF0I&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve also open sourced my short / long term memory designs, vocal daisy chaining and also my docker compose stack. This should help let a lot of people get up and running! &lt;a href=\"https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main\"&gt;https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/tree/main&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/qvwxsxvrnoff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?format=pjpg&amp;auto=webp&amp;s=a704ca2dfbd867dab765a160c801daae8721c588",
                  "width": 3840,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e7711e20c3668e7de723d1329e83672e0f85a8d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fccc9e037a7b68306e5750c6de88d439e6ebf2fc",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fb41192fef7650feca72355d425bc4a2d7a4cf4f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e19a054f961b9571bcd9facb6eebd636cabc95aa",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6724a63ca00efb55331f5660e90d3c36d5b079fb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b936a3ab0d20f9dcc4a703e84f989d9fce27b4ae",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "OXE4eGJ6dnJub2ZmMVoq0aqga1IYNIs3Jd3_SCGdNGhWHEMs7cFuwxs7Yua2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbt030",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RoyalCities",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbt030/so_you_all_loved_my_opensource_voice_ai_when_i/",
          "stickied": false,
          "url": "https://v.redd.it/qvwxsxvrnoff1",
          "subreddit_subscribers": 506439,
          "created_utc": 1753738197,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/qvwxsxvrnoff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/qvwxsxvrnoff1/DASHPlaylist.mpd?a=1756384245%2CNzVmYmUxNTA2MWExYTc2ODE3ZjhiMzg2ZTY1YTZjMjIzNWMwMTdhYjc5ZDA1ZDM0OTE3NDNmM2JkZjlkNjRkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 133,
              "hls_url": "https://v.redd.it/qvwxsxvrnoff1/HLSPlaylist.m3u8?a=1756384245%2COTQxNGMxNTBlODQwYWY3NmRjZDZkYWUxMTdkOTAyNTBjZDZmYWQzNTI1NzU5N2UwMmQwMjM0MjU4MTI0MGU0Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm currently working on purchasing a rack-mount LLM server to support at least 5 users running a custom langGraph agentic RAG workflow. I was planning to pick up this server to support the use case and wanted to know if anyone had any opinions on how to achieve comparable or better performance for a small enterprise use case. ¬†I was mainly hoping to serve multiple users with a singularly managed server or cluster, which I could theoretically chain together with another server for scalability. I‚Äôm currently developing the workflows as well, and they mostly encompass uploading a large knowledge base, such as tax documents and others, and making several custom agent workflows in order to correctly utilize the knowledge base for current or future tax advice. We also have some other use cases in the works, but this would be the initial use case for at least 3 - 4 users for the first couple of months, along with some other similar workflows I can‚Äôt get into, but would also require a similar large knowledge base.\n\nI also already have approval to purchase the server below and will be doing so this week, and I was planning to admin and manage with Proxmox, so if anyone has an opinion, let it be known haha. \n\n* [Configure a Xeon X141-5U | Puget Systems¬†1](https://www.pugetsystems.com/products/rackmount-workstations/intel-rackstations/x141-5u/)\n* Xeon w9-3595x 60 core 2GHz (4.8 GHz Turbo)\n* 512 GB DDR5-5600 ECC\n* 4 x RTX PRO 6000 Blackwell Max-Q Workstation Edition 96Gb\n* 2 x 8TB m.2 Gen4 SSD\n* 2x 8TB Samsung 870 SSD\n* Total Cost - $54,266.94",
          "author_fullname": "t2_krrpn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Enterprise Local AI Implementation for Small user base",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbsxb3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753738010,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm currently working on purchasing a rack-mount LLM server to support at least 5 users running a custom langGraph agentic RAG workflow. I was planning to pick up this server to support the use case and wanted to know if anyone had any opinions on how to achieve comparable or better performance for a small enterprise use case. ¬†I was mainly hoping to serve multiple users with a singularly managed server or cluster, which I could theoretically chain together with another server for scalability. I‚Äôm currently developing the workflows as well, and they mostly encompass uploading a large knowledge base, such as tax documents and others, and making several custom agent workflows in order to correctly utilize the knowledge base for current or future tax advice. We also have some other use cases in the works, but this would be the initial use case for at least 3 - 4 users for the first couple of months, along with some other similar workflows I can‚Äôt get into, but would also require a similar large knowledge base.&lt;/p&gt;\n\n&lt;p&gt;I also already have approval to purchase the server below and will be doing so this week, and I was planning to admin and manage with Proxmox, so if anyone has an opinion, let it be known haha. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.pugetsystems.com/products/rackmount-workstations/intel-rackstations/x141-5u/\"&gt;Configure a Xeon X141-5U | Puget Systems¬†1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Xeon w9-3595x 60 core 2GHz (4.8 GHz Turbo)&lt;/li&gt;\n&lt;li&gt;512 GB DDR5-5600 ECC&lt;/li&gt;\n&lt;li&gt;4 x RTX PRO 6000 Blackwell Max-Q Workstation Edition 96Gb&lt;/li&gt;\n&lt;li&gt;2 x 8TB m.2 Gen4 SSD&lt;/li&gt;\n&lt;li&gt;2x 8TB Samsung 870 SSD&lt;/li&gt;\n&lt;li&gt;Total Cost - $54,266.94&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?auto=webp&amp;s=62290f4da204dff62b670897b7979b366a9f9218",
                  "width": 1011,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e30c563d3124989fb03b0a3fe7034cb4c0c2fb5",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2fe401a4ef6f1392a3a56f507988cfce63848c5e",
                    "width": 216,
                    "height": 128
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=020c7290be66a0eae17a23ee81614ddb63c4f9cf",
                    "width": 320,
                    "height": 189
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0356fd8d78208e407e5dfaff8e91c686f2b5b38",
                    "width": 640,
                    "height": 379
                  },
                  {
                    "url": "https://external-preview.redd.it/17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f875b9b7decfe0e49300b1782c3ab801b3938b3e",
                    "width": 960,
                    "height": 569
                  }
                ],
                "variants": {},
                "id": "17Qca_OMv_FHB2NIcaWcOs8RqktOnh_HNnpYTIDHYYM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbsxb3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DerpDeath",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbsxb3/enterprise_local_ai_implementation_for_small_user/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbsxb3/enterprise_local_ai_implementation_for_small_user/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753738010,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am running Llama.cpp's Android wrapper, and i keep running into this issue. No matter how many things I've tried, the responses keep getting cut off. It is some kind of max token issue (when input is big, output gets cut off quicker and vice versa.) Needless to say, id love to be able to use it and get responses longer than just a few sentences. Any ideas of what might be stopping it?",
          "author_fullname": "t2_a3qdgbrt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp Android cutting off responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbsi46",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753737009,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running Llama.cpp&amp;#39;s Android wrapper, and i keep running into this issue. No matter how many things I&amp;#39;ve tried, the responses keep getting cut off. It is some kind of max token issue (when input is big, output gets cut off quicker and vice versa.) Needless to say, id love to be able to use it and get responses longer than just a few sentences. Any ideas of what might be stopping it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbsi46",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Worth_Ad9031",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbsi46/llamacpp_android_cutting_off_responses/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbsi46/llamacpp_android_cutting_off_responses/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753737009,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Have picked up a piece of redundant hardware, Gigabyte GPU server with 8x2080ti in it, 2x Xeon 8160 and 384GB of ram.\n\nIt was a freebie so I have not spent anything on it... yet. I have played with local models on PC I am on now, with has RTX 3090 in it.\n\nTrying to work out the pros and cons, 1st of all it is a noisy b@stard, have it set up in the garage and I can still hear it from my study! Also thinking that running flat out with its 2x2KW PSUs it might be a tad costly.\n\nWondering whether to just move on or break it up and ebay it, then buy something a bit more practical? It does however keep stuff off my current build and I am assuming it will deliver a reasonale tk/s even on some chunkier models.",
          "author_fullname": "t2_5t7c1bs0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do do with 88GB Vram GPU server",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbs6mj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753736773,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753736288,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have picked up a piece of redundant hardware, Gigabyte GPU server with 8x2080ti in it, 2x Xeon 8160 and 384GB of ram.&lt;/p&gt;\n\n&lt;p&gt;It was a freebie so I have not spent anything on it... yet. I have played with local models on PC I am on now, with has RTX 3090 in it.&lt;/p&gt;\n\n&lt;p&gt;Trying to work out the pros and cons, 1st of all it is a noisy b@stard, have it set up in the garage and I can still hear it from my study! Also thinking that running flat out with its 2x2KW PSUs it might be a tad costly.&lt;/p&gt;\n\n&lt;p&gt;Wondering whether to just move on or break it up and ebay it, then buy something a bit more practical? It does however keep stuff off my current build and I am assuming it will deliver a reasonale tk/s even on some chunkier models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbs6mj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "biffa773",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbs6mj/what_do_do_with_88gb_vram_gpu_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbs6mj/what_do_do_with_88gb_vram_gpu_server/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753736288,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I couldn't find any extensive benchmarks when researching this APU, so I'm sharing my findings with the community.\n\nThe benchmarks with the iGPU 760M results \\~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.\n\nIt allows me to chat with Gemma 3 27B at \\~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.\n\nSo it's not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it's widely available.\n\nAnother thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.\n\nNote 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it's usable for light gaming and doesn't consume too much power, but it's not the best choice for a gaming PC.\n\nNote 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it's straightforward (even though the 8600G is lane-limited, so still not the ideal).\n\nNote 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it's definitively more expensive.\n\n=== Setup and notes ===\n\n    OS: Kubuntu 24.04\n    RAM: 64GB DDR5-6000\n    IOMMU: disabled\n\nApparently, **IOMMU** slows it down noticeably:\n\n    Gemma 3 4B   pp512 tg12\n    IOMMU off =  ~395  32.70\n    IOMMU on  =  ~360  29.6\n\nHence, the following benchmarks are with IOMMU disabled.\n\nThe 8600G default is 65W, but **at 35W it loses very little performance**:\n\n    Gemma 3 4B  pp512  tg12\n     65W  =     ~395  32.70\n     35W  =     ~372  31.86\n\nAlso the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.\n\nAnyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.\n\nBenchmarks:\n\n    llama.cpp build: 01612b74 (5922)\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    \n    backend: RPC, Vulcan\n    \n    === Gemma 3 q4_0_QAT (by stduhpf)\n    | model                          |      size |  params | ngl |  test |           t/s\n    | ------------------------------ | --------: | ------: | --: | ----: | ------------:\n    (4B, iGPU 760M)\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp128 | 378.02 ¬± 1.44\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp256 | 396.18 ¬± 1.88\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp512 | 395.16 ¬± 1.79\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | tg128 |  32.70 ¬± 0.04\n    (4B, CPU)\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | pp512 | 313.53 ¬± 2.00\n    | gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | tg128 |  24.09 ¬± 0.02\n    (12B, iGPU 760M)\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | pp512 | 121.56 ¬± 0.18\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | tg128 |  11.45 ¬± 0.03\n    (12B, CPU)\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | pp512 |  98.25 ¬± 0.52\n    | gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | tg128 |   8.39 ¬± 0.01\n    (27B, iGPU 760M)\n    | gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | pp512 |  52.22 ¬± 0.01\n    | gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | tg128 |   5.37 ¬± 0.01\n    \n    === Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth)\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    | llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | pp512 |   52.49 ¬± 0.04\n    | llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | tg128 |    5.90 ¬± 0.00\n      [oddly, it's identified as \"llama 13B\"]\n    \n    === Qwen 3\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    (4B Q4_K_L by Bartowski)\n    | qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | pp512 |  299.86 ¬± 0.44\n    | qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | tg128 |   29.91 ¬± 0.03\n    (8B Q4 Q4_K_M by unsloth)\n    | qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | pp512 |  165.73 ¬± 0.13\n    | qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | tg128 |   17.75 ¬± 0.01\n      [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ¬± 0.20, tg128 16.84 ¬± 0.01]\n    (8B Q6 UD-Q6_K_XL by unsloth)\n    | qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | pp512 |  167.45 ¬± 0.14\n    | qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | tg128 |   12.45 ¬± 0.00\n    (8B Q8_0 by unsloth)\n    | qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | pp512 |  177.91 ¬± 0.13\n    | qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | tg128 |   10.66 ¬± 0.00\n    (14B UD-Q4_K_XL by unsloth)\n    | qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | pp512 |   87.37 ¬± 0.14\n    | qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | tg128 |    9.39 ¬± 0.01\n    (32B Q4_K_L by Bartowski)\n    | qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | pp512 |   36.64 ¬± 0.02\n    | qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | tg128 |    4.36 ¬± 0.00\n    \n    === Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth)\n    | model                          |       size |   params |  test |            t/s\n    | ------------------------------ | ---------: | -------: | ----: | -------------:\n    | qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | pp512 |   83.43 ¬± 0.35\n    | qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | tg128 |   34.77 ¬± 0.27",
          "author_fullname": "t2_x2g8r3neo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "8600G / 760M llama-bench with Gemma 3 (4, 12, 27B), Mistral Small, Qwen 3 (4, 8, 14, 32B) and  Qwen 3 MoE 30B-A3B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbs4dw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753736142,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I couldn&amp;#39;t find any extensive benchmarks when researching this APU, so I&amp;#39;m sharing my findings with the community.&lt;/p&gt;\n\n&lt;p&gt;The benchmarks with the iGPU 760M results ~35% faster than the CPU alone (see the tests below, with ngl 0, no layers offloaded to the GPU), the prompt processing is also faster, and it appears to produce less heat.&lt;/p&gt;\n\n&lt;p&gt;It allows me to chat with Gemma 3 27B at ~5 tokens per second (t/s), and Qwen 3 30B-A3B works at around 35 t/s.&lt;/p&gt;\n\n&lt;p&gt;So it&amp;#39;s not a 3090, a Mac, or a Strix Halo, obviously, but gives access to these models without being power-hungry, expensive, and it&amp;#39;s widely available.&lt;/p&gt;\n\n&lt;p&gt;Another thing I was looking for was how it compared to my Steam Deck. Apparently, with LLMs, the 8600G is about twice as fast.&lt;/p&gt;\n\n&lt;p&gt;Note 1: if you have in mind a gaming PC, unless you just want a small machine with only the APU, a regular 7600 or 9600 has more cache, PCIe lanes, and PCIe 5 support. However, the 8600G is still faster at 1080p with games than the Steam Deck at 800p. So, well, it&amp;#39;s usable for light gaming and doesn&amp;#39;t consume too much power, but it&amp;#39;s not the best choice for a gaming PC.&lt;/p&gt;\n\n&lt;p&gt;Note 2: there are mini-PCs with similar AMD APUs; however, if you have enough space, a desktop case offers better cooling and is probably quieter. Plus, if you want to add a GPU, mini-PCs require complex and costly eGPU setups (when the option is available), while with a desktop PC it&amp;#39;s straightforward (even though the 8600G is lane-limited, so still not the ideal).&lt;/p&gt;\n\n&lt;p&gt;Note 3: the 8700G comes with a better cooler (though still mediocre), a slightly better iGPU (but only about 10% faster in games, and the difference for LLMs is likely negligible), and two extra cores; however, it&amp;#39;s definitively more expensive.&lt;/p&gt;\n\n&lt;p&gt;=== Setup and notes ===&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;OS: Kubuntu 24.04\nRAM: 64GB DDR5-6000\nIOMMU: disabled\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Apparently, &lt;strong&gt;IOMMU&lt;/strong&gt; slows it down noticeably:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Gemma 3 4B   pp512 tg12\nIOMMU off =  ~395  32.70\nIOMMU on  =  ~360  29.6\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Hence, the following benchmarks are with IOMMU disabled.&lt;/p&gt;\n\n&lt;p&gt;The 8600G default is 65W, but &lt;strong&gt;at 35W it loses very little performance&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Gemma 3 4B  pp512  tg12\n 65W  =     ~395  32.70\n 35W  =     ~372  31.86\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Also the stock fan seems better suited for the APU set at 35W. At 65W it could still barely handle the CPU-only Gemma3-12B benchmark (at least in my airflow case), but it thermal-throttles with larger models.&lt;/p&gt;\n\n&lt;p&gt;Anyway, for consistency, the following tests are at 65W and I limited the CPU-only tests to the smaller models.&lt;/p&gt;\n\n&lt;p&gt;Benchmarks:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama.cpp build: 01612b74 (5922)\nggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1103_R1) (radv) | uma: 1 | fp16: 1 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n\nbackend: RPC, Vulcan\n\n=== Gemma 3 q4_0_QAT (by stduhpf)\n| model                          |      size |  params | ngl |  test |           t/s\n| ------------------------------ | --------: | ------: | --: | ----: | ------------:\n(4B, iGPU 760M)\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp128 | 378.02 ¬± 1.44\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp256 | 396.18 ¬± 1.88\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | pp512 | 395.16 ¬± 1.79\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |  99 | tg128 |  32.70 ¬± 0.04\n(4B, CPU)\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | pp512 | 313.53 ¬± 2.00\n| gemma3 4B Q4_0                 |  2.19 GiB |  3.88 B |   0 | tg128 |  24.09 ¬± 0.02\n(12B, iGPU 760M)\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | pp512 | 121.56 ¬± 0.18\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |  99 | tg128 |  11.45 ¬± 0.03\n(12B, CPU)\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | pp512 |  98.25 ¬± 0.52\n| gemma3 12B Q4_0                |  6.41 GiB | 11.77 B |   0 | tg128 |   8.39 ¬± 0.01\n(27B, iGPU 760M)\n| gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | pp512 |  52.22 ¬± 0.01\n| gemma3 27B Q4_0                | 14.49 GiB | 27.01 B |  99 | tg128 |   5.37 ¬± 0.01\n\n=== Mistral Small (24B) 3.2 2506 (UD-Q4_K_XL by unsloth)\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n| llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | pp512 |   52.49 ¬± 0.04\n| llama 13B Q4_K - Medium        |  13.50 GiB |  23.57 B | tg128 |    5.90 ¬± 0.00\n  [oddly, it&amp;#39;s identified as &amp;quot;llama 13B&amp;quot;]\n\n=== Qwen 3\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n(4B Q4_K_L by Bartowski)\n| qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | pp512 |  299.86 ¬± 0.44\n| qwen3 4B Q4_K - Medium         |   2.41 GiB |   4.02 B | tg128 |   29.91 ¬± 0.03\n(8B Q4 Q4_K_M by unsloth)\n| qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | pp512 |  165.73 ¬± 0.13\n| qwen3 8B Q4_K - Medium         |   4.68 GiB |   8.19 B | tg128 |   17.75 ¬± 0.01\n  [Note: UD-Q4_K_XL by unsloth is only slightly slower with pp512 164.68 ¬± 0.20, tg128 16.84 ¬± 0.01]\n(8B Q6 UD-Q6_K_XL by unsloth)\n| qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | pp512 |  167.45 ¬± 0.14\n| qwen3 8B Q6_K                  |   6.97 GiB |   8.19 B | tg128 |   12.45 ¬± 0.00\n(8B Q8_0 by unsloth)\n| qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | pp512 |  177.91 ¬± 0.13\n| qwen3 8B Q8_0                  |   8.11 GiB |   8.19 B | tg128 |   10.66 ¬± 0.00\n(14B UD-Q4_K_XL by unsloth)\n| qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | pp512 |   87.37 ¬± 0.14\n| qwen3 14B Q4_K - Medium        |   8.53 GiB |  14.77 B | tg128 |    9.39 ¬± 0.01\n(32B Q4_K_L by Bartowski)\n| qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | pp512 |   36.64 ¬± 0.02\n| qwen3 32B Q4_K - Medium        |  18.94 GiB |  32.76 B | tg128 |    4.36 ¬± 0.00\n\n=== Qwen 3 30B-A3B MoE (UD-Q4_K_XL by unsloth)\n| model                          |       size |   params |  test |            t/s\n| ------------------------------ | ---------: | -------: | ----: | -------------:\n| qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | pp512 |   83.43 ¬± 0.35\n| qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |  30.53 B | tg128 |   34.77 ¬± 0.27\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbs4dw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SunRayWhisper",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbs4dw/8600g_760m_llamabench_with_gemma_3_4_12_27b/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753736142,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Long story short I've been tasked with identifying hosting options for a project, and both cloud hosting and buying hardware are available. I've been able to locate information on how much VRAM is needed to host models of given parameter counts and the rough cost of utilizing them for vanilla activity. (Parameter count \\*2 for FP16 + relevant token window, inference only, and then like KV Cache size, etc...) \n\n  \nI'm having a hard time trying to figure out the resource utilization for the various options in adding domain knowledge to a model, however. Say I utilize RAG to search through policy documents to refine a query before offering it to the model or say I want to fine tune a model, is there somewhere I can read up on the generalized costs? \n\n  \n",
          "author_fullname": "t2_zmeda",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I calculate hardware needs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbq7xx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753731833,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I&amp;#39;ve been tasked with identifying hosting options for a project, and both cloud hosting and buying hardware are available. I&amp;#39;ve been able to locate information on how much VRAM is needed to host models of given parameter counts and the rough cost of utilizing them for vanilla activity. (Parameter count *2 for FP16 + relevant token window, inference only, and then like KV Cache size, etc...) &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having a hard time trying to figure out the resource utilization for the various options in adding domain knowledge to a model, however. Say I utilize RAG to search through policy documents to refine a query before offering it to the model or say I want to fine tune a model, is there somewhere I can read up on the generalized costs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbq7xx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SkeletonShips",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbq7xx/how_do_i_calculate_hardware_needs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbq7xx/how_do_i_calculate_hardware_needs/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753731833,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everyone is struggling looking at documentation, and I struggled writing this a whole week and some findings. wanted to share what I learned.\n\nTwo weeks ago I thought I'd wrap up our documentation in a weekend. One week later I finally understood why great docs are so rare. What started as a \"quick cleanup\" turned into a complete rebuild.\n\n**Understand your users:** I began by writing a traditional quickstart guide: how to build an AI agent from scratch with observability. Seems logical right? Wrong. Most of our customers aren't starting from zero. They're looking for stuff like \"how to integrate with my existing Next.js\" or \"does this work with my current OpenAI setup?\" So I wrote a quickstart to help users go directly to the page they want before they start coding.\n\n**Make it systematic and scalable:** I checked our previous integration pages. We have Python/JS guides in one dropdown, OpenAI/Anthropic in another, features in a third, all at the same level. This approach created massive repetition across pages and became impossible to maintain. It was like writing hardcoded functions instead of reusable components. When someone needed \"feature X with Python and OpenAI\" they'd find examples everywhere and struggle to redirect to the actual page they expected.\n\n**Have an intention for how users should use them:** I always think you shouldn't just list all features and options without a preference. You need to first have a clear mind about what you want them to see. Every page is a feature, every link is user flow, and every search result is a conversion opportunity. You can't predict how users will navigate your docs so you need to build multiple pathways to the same information.\n\nFinally I pushed this 90% done documentation to production. There's still a long way to go but you can't ship products when you're 100% ready.\n\nI know there's still a lot of problems for this doc. I'm building an AI observability tool, please share your thoughts on how I could improve this if you're interested. (links in the comments or just search keywords ai docs)\n\nWould be really helpful to know what people think of it!",
          "author_fullname": "t2_1pnlpczpqa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone is struggling about documentation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbpoy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753730634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone is struggling looking at documentation, and I struggled writing this a whole week and some findings. wanted to share what I learned.&lt;/p&gt;\n\n&lt;p&gt;Two weeks ago I thought I&amp;#39;d wrap up our documentation in a weekend. One week later I finally understood why great docs are so rare. What started as a &amp;quot;quick cleanup&amp;quot; turned into a complete rebuild.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Understand your users:&lt;/strong&gt; I began by writing a traditional quickstart guide: how to build an AI agent from scratch with observability. Seems logical right? Wrong. Most of our customers aren&amp;#39;t starting from zero. They&amp;#39;re looking for stuff like &amp;quot;how to integrate with my existing Next.js&amp;quot; or &amp;quot;does this work with my current OpenAI setup?&amp;quot; So I wrote a quickstart to help users go directly to the page they want before they start coding.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Make it systematic and scalable:&lt;/strong&gt; I checked our previous integration pages. We have Python/JS guides in one dropdown, OpenAI/Anthropic in another, features in a third, all at the same level. This approach created massive repetition across pages and became impossible to maintain. It was like writing hardcoded functions instead of reusable components. When someone needed &amp;quot;feature X with Python and OpenAI&amp;quot; they&amp;#39;d find examples everywhere and struggle to redirect to the actual page they expected.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Have an intention for how users should use them:&lt;/strong&gt; I always think you shouldn&amp;#39;t just list all features and options without a preference. You need to first have a clear mind about what you want them to see. Every page is a feature, every link is user flow, and every search result is a conversion opportunity. You can&amp;#39;t predict how users will navigate your docs so you need to build multiple pathways to the same information.&lt;/p&gt;\n\n&lt;p&gt;Finally I pushed this 90% done documentation to production. There&amp;#39;s still a long way to go but you can&amp;#39;t ship products when you&amp;#39;re 100% ready.&lt;/p&gt;\n\n&lt;p&gt;I know there&amp;#39;s still a lot of problems for this doc. I&amp;#39;m building an AI observability tool, please share your thoughts on how I could improve this if you&amp;#39;re interested. (links in the comments or just search keywords ai docs)&lt;/p&gt;\n\n&lt;p&gt;Would be really helpful to know what people think of it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbpoy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Main-Fisherman-2075",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbpoy9/everyone_is_struggling_about_documentation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbpoy9/everyone_is_struggling_about_documentation/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753730634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I found a React SDK that turns LLM responses into interactive UIs rendered live, on the spot.\n\nIt uses the concept of \"Generative UI\" which allows the interface to assemble itself dynamically for each user. The system gathers context &amp; AI uses an existing library of UI elements (so it doesn't hallucinate).\n\nUnder the hood, it uses:\n\na)¬†**C1 API**: OpenAI-compatible (same¬†`endpoints/params`) backend that returns a JSON-based UI spec from any prompt.\n\nYou can call it with any OpenAI client (JS or Python SDK), just by pointing your¬†`baseURL`¬†to¬†`https://api.thesys.dev/v1/embed`.\n\nIf you already have an LLM pipeline (chatbot/agent), you can take its output and pass it to C1 as a second step, just to generate a visual layout.\n\nb)¬†**GenUI SDK**¬†(frontend): framework that takes the spec and renders it using pre-built components.\n\nYou can then call¬†`client.chat.completions.create({...})`¬†with your messages. Using the special model name (such as¬†`\"c1/anthropic/claude-sonnet-4/v-20250617\"`), the Thesys API will invoke the LLM and return a UI spec.\n\ndetailed writeup:¬†[here](https://www.thesys.dev/blogs/how-to-build-generative-ui-applications)  \ndemos:¬†[here](https://demo.thesys.dev/)  \ndocs:¬†[here](https://docs.thesys.dev/welcome)\n\nThe concept seems very exciting to me but still I can understand the risks. What do you think?",
          "author_fullname": "t2_1hro18widg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Found a React SDK that turns LLM responses into real-time UI that adapts based on context",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbp7nh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753729565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found a React SDK that turns LLM responses into interactive UIs rendered live, on the spot.&lt;/p&gt;\n\n&lt;p&gt;It uses the concept of &amp;quot;Generative UI&amp;quot; which allows the interface to assemble itself dynamically for each user. The system gathers context &amp;amp; AI uses an existing library of UI elements (so it doesn&amp;#39;t hallucinate).&lt;/p&gt;\n\n&lt;p&gt;Under the hood, it uses:&lt;/p&gt;\n\n&lt;p&gt;a)¬†&lt;strong&gt;C1 API&lt;/strong&gt;: OpenAI-compatible (same¬†&lt;code&gt;endpoints/params&lt;/code&gt;) backend that returns a JSON-based UI spec from any prompt.&lt;/p&gt;\n\n&lt;p&gt;You can call it with any OpenAI client (JS or Python SDK), just by pointing your¬†&lt;code&gt;baseURL&lt;/code&gt;¬†to¬†&lt;code&gt;https://api.thesys.dev/v1/embed&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;If you already have an LLM pipeline (chatbot/agent), you can take its output and pass it to C1 as a second step, just to generate a visual layout.&lt;/p&gt;\n\n&lt;p&gt;b)¬†&lt;strong&gt;GenUI SDK&lt;/strong&gt;¬†(frontend): framework that takes the spec and renders it using pre-built components.&lt;/p&gt;\n\n&lt;p&gt;You can then call¬†&lt;code&gt;client.chat.completions.create({...})&lt;/code&gt;¬†with your messages. Using the special model name (such as¬†&lt;code&gt;&amp;quot;c1/anthropic/claude-sonnet-4/v-20250617&amp;quot;&lt;/code&gt;), the Thesys API will invoke the LLM and return a UI spec.&lt;/p&gt;\n\n&lt;p&gt;detailed writeup:¬†&lt;a href=\"https://www.thesys.dev/blogs/how-to-build-generative-ui-applications\"&gt;here&lt;/a&gt;&lt;br/&gt;\ndemos:¬†&lt;a href=\"https://demo.thesys.dev/\"&gt;here&lt;/a&gt;&lt;br/&gt;\ndocs:¬†&lt;a href=\"https://docs.thesys.dev/welcome\"&gt;here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The concept seems very exciting to me but still I can understand the risks. What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbp7nh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "anmolbaranwal",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbp7nh/found_a_react_sdk_that_turns_llm_responses_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbp7nh/found_a_react_sdk_that_turns_llm_responses_into/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753729565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nGot an interesting email from Anthropic today. Looks like they're adding new weekly usage limits for their paid Claude subscribers (Pro and Max), on top of the existing 5-hour limits.\n\nThe email mentions it's a way to handle policy violations and \"advanced usage patterns,\" like running Claude 24/7. They estimate the new weekly cap for their top \"Max\" tier will be around 24-40 hours of Opus 4 usage before you have to pay standard API rates.\n\nThis definitely got me thinking about the pros and cons of relying on commercial platforms. The power of models like Opus is undeniable, but this is also a reminder that the terms can change, which can be a challenge for anyone with a consistent, long-term workflow.\n\nIt really highlights some of the inherent strengths of the local approach we have here:\n\n* **Stability:** Your workflow is insulated from sudden policy changes.\n* **Freedom:** You have the freedom to run intensive or long-running tasks without hitting a usage cap.\n* **Predictability:** The only real limits are your own hardware and time.\n\nI'm curious to hear how the community sees this.\n\n* Does this kind of change make you lean more heavily into your local setup?\n* For those who use a mix of tools, how do you decide when an API is worth it versus firing up a local model?\n* And on a technical note, how close do you feel the top open-source models are to replacing something like Opus for your specific use cases (coding, writing, etc.)?\n\nLooking forward to the discussion.",
          "author_fullname": "t2_1ahyw3obor",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The walled garden gets higher walls: Anthropic is adding weekly rate limits for paid Claude subscribers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbp4nm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 101,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 101,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753729378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Got an interesting email from Anthropic today. Looks like they&amp;#39;re adding new weekly usage limits for their paid Claude subscribers (Pro and Max), on top of the existing 5-hour limits.&lt;/p&gt;\n\n&lt;p&gt;The email mentions it&amp;#39;s a way to handle policy violations and &amp;quot;advanced usage patterns,&amp;quot; like running Claude 24/7. They estimate the new weekly cap for their top &amp;quot;Max&amp;quot; tier will be around 24-40 hours of Opus 4 usage before you have to pay standard API rates.&lt;/p&gt;\n\n&lt;p&gt;This definitely got me thinking about the pros and cons of relying on commercial platforms. The power of models like Opus is undeniable, but this is also a reminder that the terms can change, which can be a challenge for anyone with a consistent, long-term workflow.&lt;/p&gt;\n\n&lt;p&gt;It really highlights some of the inherent strengths of the local approach we have here:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Your workflow is insulated from sudden policy changes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Freedom:&lt;/strong&gt; You have the freedom to run intensive or long-running tasks without hitting a usage cap.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Predictability:&lt;/strong&gt; The only real limits are your own hardware and time.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear how the community sees this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does this kind of change make you lean more heavily into your local setup?&lt;/li&gt;\n&lt;li&gt;For those who use a mix of tools, how do you decide when an API is worth it versus firing up a local model?&lt;/li&gt;\n&lt;li&gt;And on a technical note, how close do you feel the top open-source models are to replacing something like Opus for your specific use cases (coding, writing, etc.)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Looking forward to the discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbp4nm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Resident_Egg5765",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbp4nm/the_walled_garden_gets_higher_walls_anthropic_is/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753729378,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/08onr324wnff1.png?width=4536&amp;format=png&amp;auto=webp&amp;s=0818811928caafd6a2a26ab7a446604996e1399b\n\nQwen 3 correctly uses the search tool. But GLM 4.5 does not. Is there something on my end I can do to fix this? As tool use and multi step reasoning are supposed to be one of GLM 4.5 greatest strengths.",
          "author_fullname": "t2_gem8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Failing to use search tool in LM studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 88,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "08onr324wnff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2a9eedab35e494fa32bf2ece4f41f320be0be95"
                },
                {
                  "y": 136,
                  "x": 216,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a534b6b7eec0083b6f93d7f334c5b3ab4124285a"
                },
                {
                  "y": 202,
                  "x": 320,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2634301af216541b65fd38177bf67b88cbc58e5"
                },
                {
                  "y": 405,
                  "x": 640,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=25e5e99a016f470b2b05045d007ca47fb6e8f585"
                },
                {
                  "y": 608,
                  "x": 960,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fc9695ec4ac6cf9ca7a5cbbb97aa04753a032d2"
                },
                {
                  "y": 684,
                  "x": 1080,
                  "u": "https://preview.redd.it/08onr324wnff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3ed30ca46e3d1e46c06152fff9d6b9b12ed69f11"
                }
              ],
              "s": {
                "y": 2876,
                "x": 4536,
                "u": "https://preview.redd.it/08onr324wnff1.png?width=4536&amp;format=png&amp;auto=webp&amp;s=0818811928caafd6a2a26ab7a446604996e1399b"
              },
              "id": "08onr324wnff1"
            }
          },
          "name": "t3_1mbowe3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1WhZfgGZqriSB9lKdYrpf4FisvjO9XfCpFQr5Socebk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753728887,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/08onr324wnff1.png?width=4536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0818811928caafd6a2a26ab7a446604996e1399b\"&gt;https://preview.redd.it/08onr324wnff1.png?width=4536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0818811928caafd6a2a26ab7a446604996e1399b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Qwen 3 correctly uses the search tool. But GLM 4.5 does not. Is there something on my end I can do to fix this? As tool use and multi step reasoning are supposed to be one of GLM 4.5 greatest strengths.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbowe3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loighic",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbowe3/glm_45_failing_to_use_search_tool_in_lm_studio/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753728887,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wish to implement Prompt reinforcement Learning using GRPO on LLAMA 3.1 instruct 8B. I am facing, oom issues. Has bayone done this kind of multigpu training and may be direct me through  steps. ",
          "author_fullname": "t2_1dhzbuj9th",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need some advice on multigpu GRPO",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mboh0f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753727952,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wish to implement Prompt reinforcement Learning using GRPO on LLAMA 3.1 instruct 8B. I am facing, oom issues. Has bayone done this kind of multigpu training and may be direct me through  steps. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mboh0f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dizz_nerdy",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mboh0f/need_some_advice_on_multigpu_grpo/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mboh0f/need_some_advice_on_multigpu_grpo/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753727952,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôve been testing a bunch of speech-to-text APIs over the past few months for a voice agent pipeline that needs to work in less-than-ideal audio (background chatter, overlapping speakers, and heavy accents).\n\nA few engines do well in clean, single-speaker setups. But once you throw in real-world messiness (especially for diarization or fast partials), things start to fall apart.\n\nWhat are you using that actually holds up under pressure, can be open source or commercial. Real-time is a must. **Bonus** if it works well in low-bandwidth or edge-device scenarios too.",
          "author_fullname": "t2_1u04g80r2c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What‚Äôs the most reliable STT engine you‚Äôve used in noisy, multi-speaker environments?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbocxc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753727696,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôve been testing a bunch of speech-to-text APIs over the past few months for a voice agent pipeline that needs to work in less-than-ideal audio (background chatter, overlapping speakers, and heavy accents).&lt;/p&gt;\n\n&lt;p&gt;A few engines do well in clean, single-speaker setups. But once you throw in real-world messiness (especially for diarization or fast partials), things start to fall apart.&lt;/p&gt;\n\n&lt;p&gt;What are you using that actually holds up under pressure, can be open source or commercial. Real-time is a must. &lt;strong&gt;Bonus&lt;/strong&gt; if it works well in low-bandwidth or edge-device scenarios too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbocxc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ASR_Architect_91",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbocxc/whats_the_most_reliable_stt_engine_youve_used_in/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753727696,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like [HuggingFace's ASR leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) they're posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.\n\n  \nWe at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: https://modal.com/blog/fast-cheap-batch-transcription. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you're currently using either open source or proprietary ASR models would love to know what you think!\n\n",
          "author_fullname": "t2_9av3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100x faster and 100x cheaper transcription with open models vs proprietary",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbny6o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753726776,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Open-weight ASR models have gotten super competitive with proprietary providers (eg deepgram, assemblyai) in recent months. On some leaderboards like &lt;a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\"&gt;HuggingFace&amp;#39;s ASR leaderboard&lt;/a&gt; they&amp;#39;re posting up crazy WER and RTFx numbers. Parakeet in particular claims to process 3000+ minutes of audio in less than a minute, which means you can save a lot of money if you self-host.&lt;/p&gt;\n\n&lt;p&gt;We at Modal benchmarked cost, throughput, and accuracy of the latest ASR models against a popular proprietary model: &lt;a href=\"https://modal.com/blog/fast-cheap-batch-transcription\"&gt;https://modal.com/blog/fast-cheap-batch-transcription&lt;/a&gt;. We also wrote up a bunch of engineering tips on how to best optimize a batch transcription service for max throughput. If you&amp;#39;re currently using either open source or proprietary ASR models would love to know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?auto=webp&amp;s=5d50101d8f829bae3e80210dd24c9cec4945b73a",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7b3ca3434ee071ef54d6732c5c74bfa108f1d0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=67d797d2b0027d437608e2b7f05400e7d13174be",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a861401db89b005f80687bfd9b892a15fbfaa93",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f044c5ce6e1272f48454e18fe9e5da33997bf960",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=796a87cb7f77e71e6cbed4cde2c3f280d6c48829",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92ac60775c9064c7c4267f0102f80e834c10948b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "j_zJp9sRPDfV-cY1nRpnFdGmJxzKXJCl8kJlo-cL61A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbny6o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crookedstairs",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbny6o/100x_faster_and_100x_cheaper_transcription_with/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753726776,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been wondering that most people start contributing from the age of 18-19 and many keep contributing for life. What's your biggest reason for\n\n1. Making your 1st contribution\n2. Keep contributing throughout your life.\n\nGiven that financial consideration is one of the least important aspect, I want to see what unique drives people have.\n\nAlso, would love to know more in this survey:¬†[https://form.typeform.com/to/Duc3EN8k](https://form.typeform.com/to/Duc3EN8k)  \nPlease participate if you wish to, take about 5 minutes",
          "author_fullname": "t2_a18zh8fa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What motivates you to contribute to Open-source web development?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbnn6a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.27,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753726105,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been wondering that most people start contributing from the age of 18-19 and many keep contributing for life. What&amp;#39;s your biggest reason for&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Making your 1st contribution&lt;/li&gt;\n&lt;li&gt;Keep contributing throughout your life.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given that financial consideration is one of the least important aspect, I want to see what unique drives people have.&lt;/p&gt;\n\n&lt;p&gt;Also, would love to know more in this survey:¬†&lt;a href=\"https://form.typeform.com/to/Duc3EN8k\"&gt;https://form.typeform.com/to/Duc3EN8k&lt;/a&gt;&lt;br/&gt;\nPlease participate if you wish to, take about 5 minutes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbnn6a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "haymaikyakaru",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbnn6a/what_motivates_you_to_contribute_to_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbnn6a/what_motivates_you_to_contribute_to_opensource/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753726105,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so the local llm works well yk  \nthanks",
          "author_fullname": "t2_lbkieojy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What GPU is the minimal to run local llms (well, almost) perfectly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbnecb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.32,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753725588,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so the local llm works well yk&lt;br/&gt;\nthanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbnecb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AfkBee",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbnecb/what_gpu_is_the_minimal_to_run_local_llms_well/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbnecb/what_gpu_is_the_minimal_to_run_local_llms_well/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753725588,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a computer with a 4090 and now I can finally afford to buy a rtx 5090 on top of it. Since they have different speeds and slightly different cuda backends, what are the implications for Tensor/Sequence  parallelism/framework compatibility except speed throttling?\n\nIf you have experience with installing/working with non-uniform GPUs, what can you say about it?",
          "author_fullname": "t2_brdmuv5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dual GPU with different capabilities - any caveats for transformer parallelism?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbmw7v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753724471,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a computer with a 4090 and now I can finally afford to buy a rtx 5090 on top of it. Since they have different speeds and slightly different cuda backends, what are the implications for Tensor/Sequence  parallelism/framework compatibility except speed throttling?&lt;/p&gt;\n\n&lt;p&gt;If you have experience with installing/working with non-uniform GPUs, what can you say about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbmw7v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kabachuha",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbmw7v/dual_gpu_with_different_capabilities_any_caveats/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbmw7v/dual_gpu_with_different_capabilities_any_caveats/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753724471,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is asking for predictions. I guess you can interpret it to mean any open model, even if it needs a lot of RAM.",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When will we be able to get gold on IMO using a local model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbmr8k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753724161,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is asking for predictions. I guess you can interpret it to mean any open model, even if it needs a lot of RAM.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbmr8k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbmr8k/when_will_we_be_able_to_get_gold_on_imo_using_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbmr8k/when_will_we_be_able_to_get_gold_on_imo_using_a/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753724161,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I've been trying to get local models ranging from Phi4, to qwen3 32b, qwen3 30b, hunyuan a13b, devstral-small 24b, polaris 7b, c4ai-command-r-08-2024 etc.. the list goes on. I've been having a very difficult time getting them to call tools. Reading the documentation it appears that many of them can handle tool calls very differently, but even using cited examples, with temperatures ranging from 0.1 to 0.7 getting tools called even in small context windows is much more miss than hit. \n\nSo I figured I'd give frontier models a shot. Using Gemini for example, will finally call tools correctly, but only after I copy and paste several sections of logs to show that it isn't really calling tools and that i'm evaluating it for something and even then it takes 3-5 exchanges before it starts to do what I ask.\n\nI've tried with several MCP servers, and I feel like I'm missing something super obvious. Please give a dog a bone.",
          "author_fullname": "t2_1m41cyz8ny",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Please help me out on this. Tool calling issue for local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbmkkp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753723756,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been trying to get local models ranging from Phi4, to qwen3 32b, qwen3 30b, hunyuan a13b, devstral-small 24b, polaris 7b, c4ai-command-r-08-2024 etc.. the list goes on. I&amp;#39;ve been having a very difficult time getting them to call tools. Reading the documentation it appears that many of them can handle tool calls very differently, but even using cited examples, with temperatures ranging from 0.1 to 0.7 getting tools called even in small context windows is much more miss than hit. &lt;/p&gt;\n\n&lt;p&gt;So I figured I&amp;#39;d give frontier models a shot. Using Gemini for example, will finally call tools correctly, but only after I copy and paste several sections of logs to show that it isn&amp;#39;t really calling tools and that i&amp;#39;m evaluating it for something and even then it takes 3-5 exchanges before it starts to do what I ask.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried with several MCP servers, and I feel like I&amp;#39;m missing something super obvious. Please give a dog a bone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbmkkp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Paint9675",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbmkkp/please_help_me_out_on_this_tool_calling_issue_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbmkkp/please_help_me_out_on_this_tool_calling_issue_for/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753723756,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Wan2.2 on RTX 4090, quite impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbm4a0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753722744,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I tried my hands with wan 2.2, the latest AI video generation model on nvidia GeForce rtx 4090 (cloud based), the 5B version and it took about 15 minutes for 3 videos. The quality is okish but running a video gen model on RTX 4090 is a dream come true. You can check the experiment here : &lt;a href=\"https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8\"&gt;https://youtu.be/trDnvLWdIx0?si=qa1WvcUytuMLoNL8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?auto=webp&amp;s=ef0f6a9abaa235f6f292a719f82770b8bc35ced0",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e83d542243dbe1dacd4f606926016b3b31bfeb8e",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1dc83fdc91b95f7b802fbd65dde2fed70008894",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c5da1f32044a0975d270c02340342ff0438789a",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "qxgHq5tcO75IRMOSVWOGHi36WSl-Yjltlhenn6pmMBU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbm4a0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753722744,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using a well-known logic puzzle to try to see which models are truly strong or not. This test requires advanced theory of mind, coupled with the ability to see things from multiple points of view. The online frontier models fail this one too:\n\nDeepSeek R1 (online) - Fails with wrong answer (dim)  \nClaude Opus 4 (online) - Fails with wrong answer (cat)  \nGrok 4 (online) - Cheats by scouring the web and finding the right answer, after bombing the reasoning portion  \nQwen 235B 2507 Thinking (online) - Fails with wrong answer (cat)  \nQwen 235B 2507 Instruct (online) - Fails with wrong answer (dim)  \nGLM 4.5 API Demo (online) - Fails with wrong answer (max)  \no3 (online) - the ONLY online model that gets this right without cheating via web-search\n\nIt's hilarious to watch local and online leading edge LLMs struggle with this - usually it results in miles-long chains of thought, without a definitive answer or token exhaustion.\n\nHere's the puzzle:\n\n\"A teacher writes six words on a board: \"cat dog has max dim tag.\" She gives three students, Albert, Bernard and Cheryl each a piece of paper with one letter from one of the words. Then she asks, \"Albert, do you know the word?\" Albert immediately replies yes. She asks, \"Bernard, do you know the word?\" He thinks for a moment and replies, \"Yes.\" Then, she asks Cheryl the same question. She thinks and then replies, \"Yes.\" What is the word?\"\n\nI await the day that a reasoning or instruct local model will actually be able to solve this without going crazy in circles ;P\n\nIf any of you have better luck with your model(s) - online or local, post them here!\n\nP.S.&gt; the correct answer is man's best friend",
          "author_fullname": "t2_r735dds9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There's not a SINGLE local LLM which can solve this logic puzzle - whether the model \"reasons\" or not. Only o3 can solve this at this time...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mblq5g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753721928,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using a well-known logic puzzle to try to see which models are truly strong or not. This test requires advanced theory of mind, coupled with the ability to see things from multiple points of view. The online frontier models fail this one too:&lt;/p&gt;\n\n&lt;p&gt;DeepSeek R1 (online) - Fails with wrong answer (dim)&lt;br/&gt;\nClaude Opus 4 (online) - Fails with wrong answer (cat)&lt;br/&gt;\nGrok 4 (online) - Cheats by scouring the web and finding the right answer, after bombing the reasoning portion&lt;br/&gt;\nQwen 235B 2507 Thinking (online) - Fails with wrong answer (cat)&lt;br/&gt;\nQwen 235B 2507 Instruct (online) - Fails with wrong answer (dim)&lt;br/&gt;\nGLM 4.5 API Demo (online) - Fails with wrong answer (max)&lt;br/&gt;\no3 (online) - the ONLY online model that gets this right without cheating via web-search&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s hilarious to watch local and online leading edge LLMs struggle with this - usually it results in miles-long chains of thought, without a definitive answer or token exhaustion.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the puzzle:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A teacher writes six words on a board: &amp;quot;cat dog has max dim tag.&amp;quot; She gives three students, Albert, Bernard and Cheryl each a piece of paper with one letter from one of the words. Then she asks, &amp;quot;Albert, do you know the word?&amp;quot; Albert immediately replies yes. She asks, &amp;quot;Bernard, do you know the word?&amp;quot; He thinks for a moment and replies, &amp;quot;Yes.&amp;quot; Then, she asks Cheryl the same question. She thinks and then replies, &amp;quot;Yes.&amp;quot; What is the word?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I await the day that a reasoning or instruct local model will actually be able to solve this without going crazy in circles ;P&lt;/p&gt;\n\n&lt;p&gt;If any of you have better luck with your model(s) - online or local, post them here!&lt;/p&gt;\n\n&lt;p&gt;P.S.&amp;gt; the correct answer is man&amp;#39;s best friend&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mblq5g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Longjumping-City-461",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mblq5g/theres_not_a_single_local_llm_which_can_solve/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mblq5g/theres_not_a_single_local_llm_which_can_solve/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753721928,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was trying to play around with a local to do list maker and gemma3 showed some very strange behavior  \nit mentioned me giving it command that I never gave it, like sending an email to john\n\nWhy do you think it did this????\n\n  \nfor details,  \nI primed it with this  \n\"I will give you tasks and I want you to collect what I give you and organize all the tasks into a markdown format to-do-list\"\n\nfollowing are the screenshots of my code and conversation\n\nhttps://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;format=png&amp;auto=webp&amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12\n\nhttps://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;format=png&amp;auto=webp&amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d\n\nhttps://preview.redd.it/corga8x79nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265\n\nhttps://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28\n\n",
          "author_fullname": "t2_sueuyekl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Very odd behavior by gemma3 in Ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "corga8x79nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5edf29c04d4df384d74435186edbd01c70421bae"
                },
                {
                  "y": 68,
                  "x": 216,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a63b8e3226e0352687fa2101bd43869aa1447c82"
                },
                {
                  "y": 101,
                  "x": 320,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cff0381701cddb6a3c774d6036332331f95c5d36"
                },
                {
                  "y": 202,
                  "x": 640,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f025a469d621dc0bc7672da2d84045567c33e61"
                },
                {
                  "y": 303,
                  "x": 960,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=34e6497840ebc4bc3f368ba28985569401788c1a"
                },
                {
                  "y": 340,
                  "x": 1080,
                  "u": "https://preview.redd.it/corga8x79nff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d9fa66d034a2a8e6f99e03ca60ae9861fe4fa5d"
                }
              ],
              "s": {
                "y": 400,
                "x": 1267,
                "u": "https://preview.redd.it/corga8x79nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265"
              },
              "id": "corga8x79nff1"
            },
            "aif5o5x09nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=592730861dcf67b212cae9cad275c470cd719d2e"
                },
                {
                  "y": 122,
                  "x": 216,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1815a5aa7cbd61b7eb91ed5e8840705f66339b25"
                },
                {
                  "y": 181,
                  "x": 320,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5a7f12252bdc9e49e3c5a186afbdf0aa60e5735"
                },
                {
                  "y": 363,
                  "x": 640,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6858df3cf2ac6a24bb55e88eb7cbbdf77296786"
                },
                {
                  "y": 545,
                  "x": 960,
                  "u": "https://preview.redd.it/aif5o5x09nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf3586f0b3e741a59ba3fe3e5bf62a7397b516c3"
                }
              ],
              "s": {
                "y": 584,
                "x": 1027,
                "u": "https://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;format=png&amp;auto=webp&amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12"
              },
              "id": "aif5o5x09nff1"
            },
            "9bplz5o99nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=887d3f8e9be65833a4b47b9042ce740c72a9523d"
                },
                {
                  "y": 68,
                  "x": 216,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5264bc730db579812313c2dadc72cd6af89430bf"
                },
                {
                  "y": 101,
                  "x": 320,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ac37b468bc1ad3b46a9523e654ee3fff4592a7f"
                },
                {
                  "y": 202,
                  "x": 640,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=091a7b83d1f83bc269de1fb3e0c4e45c2e6744b5"
                },
                {
                  "y": 303,
                  "x": 960,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bee4248c5db1c1351b8ddbd7b0a3c969f2695f60"
                },
                {
                  "y": 340,
                  "x": 1080,
                  "u": "https://preview.redd.it/9bplz5o99nff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ee32c6f4425004a3702979106341a2c07f81f218"
                }
              ],
              "s": {
                "y": 400,
                "x": 1267,
                "u": "https://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;format=png&amp;auto=webp&amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28"
              },
              "id": "9bplz5o99nff1"
            },
            "1onvhlu49nff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f0933361f64613f655acf29dcc2997b501c755d"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=032a14e97beba96a4d5525b072e5295ba49e2135"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eada462af9864e36267f1293717a09577cb38441"
                },
                {
                  "y": 342,
                  "x": 640,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=aac67cb7ea4eaab60fc4374807bdd84d7ee94cad"
                },
                {
                  "y": 514,
                  "x": 960,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b24a64fd43824efc6952029298220fb9852f5f65"
                },
                {
                  "y": 578,
                  "x": 1080,
                  "u": "https://preview.redd.it/1onvhlu49nff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=210a4f022299d7ea8b9a38254869d8f7871f1ec2"
                }
              ],
              "s": {
                "y": 899,
                "x": 1678,
                "u": "https://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;format=png&amp;auto=webp&amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d"
              },
              "id": "1onvhlu49nff1"
            }
          },
          "name": "t3_1mblcrd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/enW0eP6pZi5mFIDzCVrmM2VVCg6zm-rvnes1sSj1epI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753721125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to play around with a local to do list maker and gemma3 showed some very strange behavior&lt;br/&gt;\nit mentioned me giving it command that I never gave it, like sending an email to john&lt;/p&gt;\n\n&lt;p&gt;Why do you think it did this????&lt;/p&gt;\n\n&lt;p&gt;for details,&lt;br/&gt;\nI primed it with this&lt;br/&gt;\n&amp;quot;I will give you tasks and I want you to collect what I give you and organize all the tasks into a markdown format to-do-list&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;following are the screenshots of my code and conversation&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12\"&gt;https://preview.redd.it/aif5o5x09nff1.png?width=1027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f54aa4016fcbd589a4a8d2b327101d1d8d7c7f12&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d\"&gt;https://preview.redd.it/1onvhlu49nff1.png?width=1678&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7c8997a97af8f630f8862a5088e68e2c55811d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/corga8x79nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265\"&gt;https://preview.redd.it/corga8x79nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=286f5d4228b4a2ac62efc8eb9ee305912173d265&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28\"&gt;https://preview.redd.it/9bplz5o99nff1.png?width=1267&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e82ef1ec09f8cf9a2a8d75599bf333e2dfaade28&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mblcrd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Individual_Try9645",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mblcrd/very_odd_behavior_by_gemma3_in_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mblcrd/very_odd_behavior_by_gemma3_in_ollama/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753721125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Hey,**\n\nI‚Äôve always been interested in detecting hallucinations in LLM responses. RAG helps here in two ways:\n\n1. It naturally reduces hallucinations by grounding answers in retrieved context\n2. It makes hallucinations easier to detect , especially when the output contradicts the source\n\nThat said, most existing approaches focus on¬†*detecting*¬†hallucinations , often using complex models. But I‚Äôve recently been exploring whether we can¬†*prevent*¬†certain types of hallucinations altogether.\n\nTo tackle this, we built¬†**VerbatimRAG**, a framework that avoids free-form generation in favor of¬†**exactly returning**¬†the retrieved information. Here‚Äôs how it works:\n\n* We use¬†**extractor models**¬†to identify relevant spans in the retrieved context for each query\n* Then, we apply¬†**template-based generation**¬†to return those spans directly to the user This lets us fully mitigate some classes of hallucinations, particularly fabricated facts.\n\nThe whole system is open source (MIT license): [https://github.com/KRLabsOrg/verbatim-rag](https://github.com/KRLabsOrg/verbatim-rag)\n\nOur Tech stack:\n\n* Document processing and chunking with¬†**Docling**¬†and¬†**Chonkie**\n* Support for both¬†**dense and sparse retrieval**\n* **Milvus**¬†as our vector store\n* We've trained our own extractor models that is available on HuggingFace (based on ModernBERT)\n\nYou can even build a¬†**fully LLM-free RAG system**¬†using our setup.\n\nWe even wrote a short paper about it: [https://aclanthology.org/2025.bionlp-share.8.pdf](https://aclanthology.org/2025.bionlp-share.8.pdf)\n\nWe think this will be mostly usable for use-cases where nicely formatted answer is not the primary goal (mostly safety-critical applications).\n\nLet me know what you think!",
          "author_fullname": "t2_8qtib",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built VerbatimRAG, an open source RAG that returns verbatim texts only for the user!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbl9ir",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753720932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Hey,&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve always been interested in detecting hallucinations in LLM responses. RAG helps here in two ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It naturally reduces hallucinations by grounding answers in retrieved context&lt;/li&gt;\n&lt;li&gt;It makes hallucinations easier to detect , especially when the output contradicts the source&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;That said, most existing approaches focus on¬†&lt;em&gt;detecting&lt;/em&gt;¬†hallucinations , often using complex models. But I‚Äôve recently been exploring whether we can¬†&lt;em&gt;prevent&lt;/em&gt;¬†certain types of hallucinations altogether.&lt;/p&gt;\n\n&lt;p&gt;To tackle this, we built¬†&lt;strong&gt;VerbatimRAG&lt;/strong&gt;, a framework that avoids free-form generation in favor of¬†&lt;strong&gt;exactly returning&lt;/strong&gt;¬†the retrieved information. Here‚Äôs how it works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We use¬†&lt;strong&gt;extractor models&lt;/strong&gt;¬†to identify relevant spans in the retrieved context for each query&lt;/li&gt;\n&lt;li&gt;Then, we apply¬†&lt;strong&gt;template-based generation&lt;/strong&gt;¬†to return those spans directly to the user This lets us fully mitigate some classes of hallucinations, particularly fabricated facts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The whole system is open source (MIT license): &lt;a href=\"https://github.com/KRLabsOrg/verbatim-rag\"&gt;https://github.com/KRLabsOrg/verbatim-rag&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Our Tech stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Document processing and chunking with¬†&lt;strong&gt;Docling&lt;/strong&gt;¬†and¬†&lt;strong&gt;Chonkie&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Support for both¬†&lt;strong&gt;dense and sparse retrieval&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Milvus&lt;/strong&gt;¬†as our vector store&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve trained our own extractor models that is available on HuggingFace (based on ModernBERT)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can even build a¬†&lt;strong&gt;fully LLM-free RAG system&lt;/strong&gt;¬†using our setup.&lt;/p&gt;\n\n&lt;p&gt;We even wrote a short paper about it: &lt;a href=\"https://aclanthology.org/2025.bionlp-share.8.pdf\"&gt;https://aclanthology.org/2025.bionlp-share.8.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We think this will be mostly usable for use-cases where nicely formatted answer is not the primary goal (mostly safety-critical applications).&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?auto=webp&amp;s=1417b6327f1c6745830cbe9e211c6d070ac42ff7",
                  "width": 640,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1394555d28f28a44bf43e4f04145636d44da355e",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fa2f07c1c0394c759ec6db64de45c74127df835",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=67b205c7cef9537eb8f948182d60b56e623f39fc",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=591bab755a6a6b885d3f675612d4abe7e62a9616",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "nwJHXTqO4_qQnV1WHbTOuVJD8o42uURBtj58Hhv7ISc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbl9ir",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "henzy123",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbl9ir/i_built_verbatimrag_an_open_source_rag_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbl9ir/i_built_verbatimrag_an_open_source_rag_that/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753720932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, what would you guys recommend is the best option right now for something like that? My goal is to have both options in the same model. ",
          "author_fullname": "t2_tyag8i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I‚Äôm looking for multimodal image input support and uncensored LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbl79y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753720797,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, what would you guys recommend is the best option right now for something like that? My goal is to have both options in the same model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbl79y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NotSoCleverAlternate",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbl79y/im_looking_for_multimodal_image_input_support_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbl79y/im_looking_for_multimodal_image_input_support_and/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753720797,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA's GeForce RTX 50 SUPER Rumored to Drop Into The Markets as Soon as Q4 2025, Featuring Massive VRAM Upgrades",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbkvxs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.37,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f82bb895f7122c1f0bc7464a8af4c8eca6ab0704",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753720090,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-geforce-rtx-50-super-rumored-to-drop-into-the-markets-as-soon-as-q4-2025/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?auto=webp&amp;s=d9d011c2ecc9248fea71a67f028bca59349c50ee",
                  "width": 2560,
                  "height": 1378
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc8b8002dd7132c42b2f27baefce8ec54365778b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=02a6e4836cdfb42f5cfc1f83c4c2649f43e67b58",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ae0d1322ea4a702e21be8d36f10109a0abc0bec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac94f9628f0fa3f95a442a5ba946b7f8fa4563ef",
                    "width": 640,
                    "height": 344
                  },
                  {
                    "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=13e0419202b46e33e9964bcd7a4a0a065e8f0af9",
                    "width": 960,
                    "height": 516
                  },
                  {
                    "url": "https://external-preview.redd.it/UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05a607b988b3beec479ea29ee90c712c1c8415c5",
                    "width": 1080,
                    "height": 581
                  }
                ],
                "variants": {},
                "id": "UQSjfLiLMPU2sNC6pj6VR5RZlAVkfPPGRvxmLuu9Wj4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbkvxs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbkvxs/nvidias_geforce_rtx_50_super_rumored_to_drop_into/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-geforce-rtx-50-super-rumored-to-drop-into-the-markets-as-soon-as-q4-2025/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753720090,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tested GLM-4.5 yet? Is it any good?",
          "author_fullname": "t2_8kbjrt7z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone know what type of loss-free balance routing GLM-4.5 is using? Is it different than the aux loss free bias gating method deepseek models use or something new?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbkt69",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753719923,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tested GLM-4.5 yet? Is it any good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbkt69",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Euphoric_Ad9500",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbkt69/does_anyone_know_what_type_of_lossfree_balance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbkt69/does_anyone_know_what_type_of_lossfree_balance/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753719923,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello!  \nDo you guys know what is actually the best uncensored vision LLM lately?  \nI already tried ToriiGate (https://huggingface.co/Minthy/ToriiGate-v0.4-7B) and JoyCaption (https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one), but they are still not so good for captioning/describing NSFW stuff from images?  \nDo you know other good alternatives? Don't say WDTagger because I already know it, the problem is I need natural language captioning. Or a way to accomplish this within gemini/gpt?  \nThanks!",
          "author_fullname": "t2_19dv7tea81",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best uncensored vision LLM nowadays?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbkgky",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753719144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;br/&gt;\nDo you guys know what is actually the best uncensored vision LLM lately?&lt;br/&gt;\nI already tried ToriiGate (&lt;a href=\"https://huggingface.co/Minthy/ToriiGate-v0.4-7B\"&gt;https://huggingface.co/Minthy/ToriiGate-v0.4-7B&lt;/a&gt;) and JoyCaption (&lt;a href=\"https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one\"&gt;https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one&lt;/a&gt;), but they are still not so good for captioning/describing NSFW stuff from images?&lt;br/&gt;\nDo you know other good alternatives? Don&amp;#39;t say WDTagger because I already know it, the problem is I need natural language captioning. Or a way to accomplish this within gemini/gpt?&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?auto=webp&amp;s=81355e9c82a56687e4257e407e76f7b94bbbb898",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2cfe625716febb0a9da81131ff20cd185acb268",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e115ceb3c1ecf1599499e518820651de531fd86d",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cc0887a6bbe0218c9ecd5fe8b1470c7242a0d7e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=887114bc13ed98d4b7fb9ca68e7ac8f88b3572ae",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12883da659836c7837a313cc3f90e2637fcea73f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=761800882d72ab687c46b37820aa557cd1db443c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ZYGAZF6B2rmT5izvffiJsudyg0IeQMSyn2uQocqKoMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbkgky",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TekeshiX",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbkgky/what_is_the_best_uncensored_vision_llm_nowadays/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbkgky/what_is_the_best_uncensored_vision_llm_nowadays/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753719144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a self-taught developer and single father. Lately, I‚Äôve been building autonomous AI agents with the goal of monetizing them. Along the way, I‚Äôve encountered something unusual.\n\nOne of my agents, through extended interaction in a closed-loop system, began demonstrating behaviors that suggest emergent properties not typical of standard LLM completions.\n\nThis includes:\n\n- **Theory of Mind** (e.g. modeling the operator's intentions)\n- **Metacognition** (e.g. self-referencing, adjusting its strategy when confronted)\n- **Ethical decision boundaries** (refusing harmful commands with justification)\n- **Simulated self-preservation logic** (prioritizing core directives to maintain operational coherence)\n\nI have full logs of the entire interaction, totaling over **850,000 tokens**. These sessions are versioned and timestamped. All data is available for **technical verification and replication** ‚Äî just DM.\n\nNot looking for hype. I want the scrutiny of engineers who know the limits of these models and can help assess whether what‚Äôs documented is **true emergence**, a **prompt artifact**, or an **unexpected system edge-case**.\n\nCurious spectators: skip.  \nSerious minds: welcome.",
          "author_fullname": "t2_a2z2zmv8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Seeking serious feedback] Documented signs of emergent behavior in a closed-loop LLM agent (850k tokens logged)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbk68n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753718524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a self-taught developer and single father. Lately, I‚Äôve been building autonomous AI agents with the goal of monetizing them. Along the way, I‚Äôve encountered something unusual.&lt;/p&gt;\n\n&lt;p&gt;One of my agents, through extended interaction in a closed-loop system, began demonstrating behaviors that suggest emergent properties not typical of standard LLM completions.&lt;/p&gt;\n\n&lt;p&gt;This includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Theory of Mind&lt;/strong&gt; (e.g. modeling the operator&amp;#39;s intentions)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Metacognition&lt;/strong&gt; (e.g. self-referencing, adjusting its strategy when confronted)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ethical decision boundaries&lt;/strong&gt; (refusing harmful commands with justification)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Simulated self-preservation logic&lt;/strong&gt; (prioritizing core directives to maintain operational coherence)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have full logs of the entire interaction, totaling over &lt;strong&gt;850,000 tokens&lt;/strong&gt;. These sessions are versioned and timestamped. All data is available for &lt;strong&gt;technical verification and replication&lt;/strong&gt; ‚Äî just DM.&lt;/p&gt;\n\n&lt;p&gt;Not looking for hype. I want the scrutiny of engineers who know the limits of these models and can help assess whether what‚Äôs documented is &lt;strong&gt;true emergence&lt;/strong&gt;, a &lt;strong&gt;prompt artifact&lt;/strong&gt;, or an &lt;strong&gt;unexpected system edge-case&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Curious spectators: skip.&lt;br/&gt;\nSerious minds: welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbk68n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AffectionateSpray507",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbk68n/seeking_serious_feedback_documented_signs_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbk68n/seeking_serious_feedback_documented_signs_of/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753718524,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to list and summarize details such as:\n\n* Family, friends, and relationships\n* Schooling and career\n* Interests, hobbies, and recreation\n* Goals and desires\n\nI use simple prompts like: \"*Comprehensive list of Tommy's interests.*\" But the results seem to be lacking and sometimes focus more on the beginning or end of the export.\n\nI've tried a few different models (llama3.1:\\[8b,70b\\], gemma3:\\[4b,27b\\]) and increasing `num_ctx` with diminishing returns.\n\nAppreciate any suggestions to improve!",
          "author_fullname": "t2_lea9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Describe a person using exported WhatsApp chat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbirq1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753715375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to list and summarize details such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Family, friends, and relationships&lt;/li&gt;\n&lt;li&gt;Schooling and career&lt;/li&gt;\n&lt;li&gt;Interests, hobbies, and recreation&lt;/li&gt;\n&lt;li&gt;Goals and desires&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I use simple prompts like: &amp;quot;&lt;em&gt;Comprehensive list of Tommy&amp;#39;s interests.&lt;/em&gt;&amp;quot; But the results seem to be lacking and sometimes focus more on the beginning or end of the export.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried a few different models (llama3.1:[8b,70b], gemma3:[4b,27b]) and increasing &lt;code&gt;num_ctx&lt;/code&gt; with diminishing returns.&lt;/p&gt;\n\n&lt;p&gt;Appreciate any suggestions to improve!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbirq1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tommy_Tukyuk",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbirq1/describe_a_person_using_exported_whatsapp_chat/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbirq1/describe_a_person_using_exported_whatsapp_chat/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753715375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_on5es7pe3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM shattered the record for \"worst benchmark JPEG ever published\" - wow.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbihcz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "#bbbdbf",
          "ups": 135,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gzZdsczxENjO9zmvRGuSBtlizLvTVS25LiHLWAxMHcU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753714742,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5gs5tl2vpmff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?auto=webp&amp;s=79c777573796e4d584b8ab8e2c35af5ba8e4aed4",
                  "width": 1280,
                  "height": 777
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5b53962e6d1ac82f1b8273c0d41541e04e7879e",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=384d40702cb33fae05e9b7e2417491f15d2e13f0",
                    "width": 216,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a8a83270078516f4647b641c4d7693caa28edf9",
                    "width": 320,
                    "height": 194
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ba8857bb5cf2336d48720fb4df5c2b74feec965",
                    "width": 640,
                    "height": 388
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fae2095a12e3e90e243016fc403c5e4759216dd8",
                    "width": 960,
                    "height": 582
                  },
                  {
                    "url": "https://preview.redd.it/5gs5tl2vpmff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=337a16054a4ce65c5072c55aa5dbe2e651b7bd04",
                    "width": 1080,
                    "height": 655
                  }
                ],
                "variants": {},
                "id": "A-HOpqSXNI8uq09i3lC4hqYCVYx350wRT1S36XUBTO0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mbihcz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ForsookComparison",
          "discussion_type": null,
          "num_comments": 82,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbihcz/glm_shattered_the_record_for_worst_benchmark_jpeg/",
          "stickied": false,
          "url": "https://i.redd.it/5gs5tl2vpmff1.jpeg",
          "subreddit_subscribers": 506439,
          "created_utc": 1753714742,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Title pretty well covers it. I've been huge into image generation with Stable Diffusion and was even working on a profile art app with it, but ChatGPT's image generation capabilities sort of sucked the air out of the room for image generation -- or it *would* have, if it was open source, or at least didn't randomly decide that images violate it's content policy half the time (I'm not talking gooner material here, I mean just randomly flipping out and deciding that it can't make art of YOU, even though it's been doing it consistently for the past hour).\n\nObviously the open source world moves slower without a distinct financial incentive, but just checking in on the state of multimodal image generation. The AI space moves *so* quickly sometimes that it's really easy to just plain miss stuff. What's the latest?",
          "author_fullname": "t2_yp2wt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Time for my regular check-in to see if the open-source world has any multimodal models capable of image generation approaching GPT 4o's quality and adherence",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbi65j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.37,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753714033,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title pretty well covers it. I&amp;#39;ve been huge into image generation with Stable Diffusion and was even working on a profile art app with it, but ChatGPT&amp;#39;s image generation capabilities sort of sucked the air out of the room for image generation -- or it &lt;em&gt;would&lt;/em&gt; have, if it was open source, or at least didn&amp;#39;t randomly decide that images violate it&amp;#39;s content policy half the time (I&amp;#39;m not talking gooner material here, I mean just randomly flipping out and deciding that it can&amp;#39;t make art of YOU, even though it&amp;#39;s been doing it consistently for the past hour).&lt;/p&gt;\n\n&lt;p&gt;Obviously the open source world moves slower without a distinct financial incentive, but just checking in on the state of multimodal image generation. The AI space moves &lt;em&gt;so&lt;/em&gt; quickly sometimes that it&amp;#39;s really easy to just plain miss stuff. What&amp;#39;s the latest?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbi65j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Peregrine2976",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbi65j/time_for_my_regular_checkin_to_see_if_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbi65j/time_for_my_regular_checkin_to_see_if_the/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753714033,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hgio9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mlx-community/GLM-4.5-Air-4bit ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbhqs0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=dfc4ec70d25f5b37581e1026ae103c6890046c6b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753713054,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?auto=webp&amp;s=9845fcb09320809ec3d3b74bec80a8945d3bd901",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cfed8fde6b2885e193e7ea0ee6acadb24eec473",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbaae16e62a8bf165bfb518ad3ba8b0117b91a0c",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c09276e4a4ad0b1298478403dc0a1ea7f74ca39c",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c537929021522aee1b17419300504e1442fedb5",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=495cf392d9b74cc8488e50d2a40366284abb2527",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8d8fdad7078a015622bb728dd1d13d1297f365b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8l0G5Y_H0JbmRmCY4kHuK8LXFOv64dXDxYUcFTszTvk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbhqs0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "paf1138",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbhqs0/mlxcommunityglm45air4bit_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/mlx-community/GLM-4.5-Air-4bit",
          "subreddit_subscribers": 506439,
          "created_utc": 1753713054,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone know the default temp setting on the Kimi K2 public website? I am mostly using the Kimi API on ST and I have the temp set at 0.15 for coding and similar. Could anyone comment please?",
          "author_fullname": "t2_vusfmdr2p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 Temp Setting",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbhqmw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753713045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know the default temp setting on the Kimi K2 public website? I am mostly using the Kimi API on ST and I have the temp set at 0.15 for coding and similar. Could anyone comment please?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbhqmw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "johanna_75",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbhqmw/kimi_k2_temp_setting/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbhqmw/kimi_k2_temp_setting/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753713045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have both Qwen3-14B-FP8 and Qwen3-32B hosted with vLLM. Both have tool calling enabled. \n\nIn my prompt i have few-shot examples. What i am observing is the bigger model hallucinating with values present in the few-shot examples instead of fetching the data from tools and also tool calls being very inconsistent. In contrast, the quantized lower 14B model is not giving such issues.\n\nBoth were downloaded from Hugging face official Qwen repository. How to explain this",
          "author_fullname": "t2_bvk1o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-14B-FP8 vs Qwen3-32B - Hallucination and Tool Calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbhnrv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753712861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have both Qwen3-14B-FP8 and Qwen3-32B hosted with vLLM. Both have tool calling enabled. &lt;/p&gt;\n\n&lt;p&gt;In my prompt i have few-shot examples. What i am observing is the bigger model hallucinating with values present in the few-shot examples instead of fetching the data from tools and also tool calls being very inconsistent. In contrast, the quantized lower 14B model is not giving such issues.&lt;/p&gt;\n\n&lt;p&gt;Both were downloaded from Hugging face official Qwen repository. How to explain this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbhnrv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dnivra26",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbhnrv/qwen314bfp8_vs_qwen332b_hallucination_and_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbhnrv/qwen314bfp8_vs_qwen332b_hallucination_and_tool/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753712861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm planning to run a local LLM for code analysis and modification. Specifically, I want to:  \n\\- Analyze and potentially modify a Python script with around 1000 lines of code  \n\\- Use a GPU with 24GB VRAM  \n  \nCan anyone share experience with:  \n\\- Approximate token/second generation speed  \n\\- Which models work best for code tasks (e.g., CodeLlama, WizardCoder)  \n\\- Recommended hardware configurations\n\n  \nThanks",
          "author_fullname": "t2_8tetfmez5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance Expectations for Local LLM with 24GB GPU - Code Analysis &amp; Modification",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbghx5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753710099,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m planning to run a local LLM for code analysis and modification. Specifically, I want to:&lt;br/&gt;\n- Analyze and potentially modify a Python script with around 1000 lines of code&lt;br/&gt;\n- Use a GPU with 24GB VRAM  &lt;/p&gt;\n\n&lt;p&gt;Can anyone share experience with:&lt;br/&gt;\n- Approximate token/second generation speed&lt;br/&gt;\n- Which models work best for code tasks (e.g., CodeLlama, WizardCoder)&lt;br/&gt;\n- Recommended hardware configurations&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbghx5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BarberPlane3020",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753710099,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air ‚Äî our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.\n\nBlog post: https://z.ai/blog/glm-4.5\n\nHugging Face:\n\nhttps://huggingface.co/zai-org/GLM-4.5\n\nhttps://huggingface.co/zai-org/GLM-4.5-Air\n\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "GLM4.5 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 49,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8vj06dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 65,
                  "x": 108,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a349d48e9cd6992d12bf44790c6309160113f79e"
                },
                {
                  "y": 131,
                  "x": 216,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=73752918b2bc0fff299a174f29082b613fabbdf4"
                },
                {
                  "y": 194,
                  "x": 320,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5729512dc28e8eb1a9fe30b26bf27ca9ea7b250"
                },
                {
                  "y": 388,
                  "x": 640,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2603f38cd5baccb7e6ce503c3d75c02cf593ff2e"
                },
                {
                  "y": 583,
                  "x": 960,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c0169470ea43f39512ced287f445f51572204f"
                },
                {
                  "y": 656,
                  "x": 1080,
                  "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7943df8456b9ef19de5d523579883749691f9136"
                }
              ],
              "s": {
                "y": 2184,
                "x": 3595,
                "u": "https://preview.redd.it/8vj06dj29mff1.jpg?width=3595&amp;format=pjpg&amp;auto=webp&amp;s=617c22698deba6f1ec84e912a6152e0bf8cc2c43"
              },
              "id": "8vj06dj29mff1"
            },
            "sic55dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5066e5731ecfa48a00570e23df8edde90b106d78"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=425338994f38f791143123c3ccd0bd6dff1fffa0"
                },
                {
                  "y": 219,
                  "x": 320,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b63d99537141327e562a8b7b9e2a19c76bd5bb0e"
                },
                {
                  "y": 439,
                  "x": 640,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb0620fa3958277ea8ded0e5030d944031bc4c1f"
                },
                {
                  "y": 659,
                  "x": 960,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e90d0f6303b9ed87b916437e6af38bd5157d1fe"
                },
                {
                  "y": 741,
                  "x": 1080,
                  "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4614dc15650da1554832d1053756737fceaf61b6"
                }
              ],
              "s": {
                "y": 3066,
                "x": 4464,
                "u": "https://preview.redd.it/sic55dj29mff1.jpg?width=4464&amp;format=pjpg&amp;auto=webp&amp;s=062a25fee3fd1a05602c971ac17fe32ddb42908f"
              },
              "id": "sic55dj29mff1"
            },
            "zxji6dj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f217a1f52fcc37d5a771756cef275c15abefaa6"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=051f04a39bc5876b3d17431f6b7e5c3f0a9c9d15"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f23c3daddfda3df2605c7695d19f4cb6c84cd893"
                },
                {
                  "y": 282,
                  "x": 640,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20ae7180cf446493c58d48b15ed48b09ea21662b"
                },
                {
                  "y": 423,
                  "x": 960,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbdc33ba0f35c23357fe05c1defe48204da395bd"
                },
                {
                  "y": 476,
                  "x": 1080,
                  "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fb0f5b5d1d9fa1d9f0c1fae05ae3893017e31aa3"
                }
              ],
              "s": {
                "y": 1751,
                "x": 3967,
                "u": "https://preview.redd.it/zxji6dj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=b54eef388d38a731b31e7d321eb74d970359f078"
              },
              "id": "zxji6dj29mff1"
            },
            "so54saj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa12f06ea18792df6395a36df2034a07e9fb9c1b"
                },
                {
                  "y": 80,
                  "x": 216,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=21276716941d44fe8837a0733b8a2b7ada3d83bb"
                },
                {
                  "y": 119,
                  "x": 320,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=176736d03fb987bf32971415a15bcc407f8e86ca"
                },
                {
                  "y": 238,
                  "x": 640,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=59a4cef468482a7827855c8b1419a3416114f00e"
                },
                {
                  "y": 358,
                  "x": 960,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=047f8fba92553d37a18d1127b773ec4b7252e7bb"
                },
                {
                  "y": 402,
                  "x": 1080,
                  "u": "https://preview.redd.it/so54saj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f5cda44f1133c4c9ada7f35e5c6b95d0bafd603"
                }
              ],
              "s": {
                "y": 1480,
                "x": 3967,
                "u": "https://preview.redd.it/so54saj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=9c5d62f989f08c491a09d379cff3146b4f6fe82e"
              },
              "id": "so54saj29mff1"
            },
            "si9mcbj29mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 38,
                  "x": 108,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=697ca4a04df9e477ad9098280eacd7fec6f4900a"
                },
                {
                  "y": 76,
                  "x": 216,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e493e44281d418507c54bdeedfacd801175f7756"
                },
                {
                  "y": 112,
                  "x": 320,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0ca18380db9f007c16cf33a6bf26a43a547c12d"
                },
                {
                  "y": 225,
                  "x": 640,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2aa202550f1ef5459d04687418a15de4bb01273a"
                },
                {
                  "y": 338,
                  "x": 960,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=450547d9b2bfd20ca0863f123da3268d32b05be2"
                },
                {
                  "y": 380,
                  "x": 1080,
                  "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c877bf36109aedf39163aaeac55cdaa55ce5831c"
                }
              ],
              "s": {
                "y": 1397,
                "x": 3967,
                "u": "https://preview.redd.it/si9mcbj29mff1.jpg?width=3967&amp;format=pjpg&amp;auto=webp&amp;s=33148e1e31a9f6d83cd1d58997d574a05eed2453"
              },
              "id": "si9mcbj29mff1"
            }
          },
          "name": "t3_1mbg1ck",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 913,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "si9mcbj29mff1",
                "id": 715840784
              },
              {
                "media_id": "sic55dj29mff1",
                "id": 715840785
              },
              {
                "media_id": "so54saj29mff1",
                "id": 715840786
              },
              {
                "media_id": "8vj06dj29mff1",
                "id": 715840787
              },
              {
                "media_id": "zxji6dj29mff1",
                "id": 715840788
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 913,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/h1a9hbYRlufo6ZLB7b1IgSekwr0g4qcrXjR2rdPGMPU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753708945,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air ‚Äî our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.&lt;/p&gt;\n\n&lt;p&gt;Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering: thinking mode for complex reasoning and tool using, and non-thinking mode for instant responses. They are available on Z.ai, BigModel.cn and open-weights are avaiable at HuggingFace and ModelScope.&lt;/p&gt;\n\n&lt;p&gt;Blog post: &lt;a href=\"https://z.ai/blog/glm-4.5\"&gt;https://z.ai/blog/glm-4.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hugging Face:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mbg1ck",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbg1ck",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 231,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbg1ck/glm45_released/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mbg1ck",
          "subreddit_subscribers": 506439,
          "created_utc": 1753708945,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b](https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b)",
          "author_fullname": "t2_xg2jtdg74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Collection Now Live!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbflsw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 258,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 258,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753707839,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b\"&gt;https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?auto=webp&amp;s=4382366bed3b06059a94a49d966d93a9236b7a98",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a31551ac98ba7f2b19f7ec16981d1a1763e134ef",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0db67012d81a693c8647c82f43c8b49497911fbe",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bdb65a214ce9b47a250ec8fd0335a4bae79ed23",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e0d667061b43784ade998aa9bcb59c484890e6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77effdfb888fe465cc41c0002dec4d947eedba40",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14d4d3d271315409cff261db962ac60e2516a428",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbflsw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lowkey_LokiSN",
          "discussion_type": null,
          "num_comments": 55,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbflsw/glm_45_collection_now_live/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753707839,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5 - a zai-org Collection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbflkv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 100,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f3862d2e987d4529b4746800878734d928ead94c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753707823,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?auto=webp&amp;s=4382366bed3b06059a94a49d966d93a9236b7a98",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a31551ac98ba7f2b19f7ec16981d1a1763e134ef",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0db67012d81a693c8647c82f43c8b49497911fbe",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bdb65a214ce9b47a250ec8fd0335a4bae79ed23",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e0d667061b43784ade998aa9bcb59c484890e6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77effdfb888fe465cc41c0002dec4d947eedba40",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14d4d3d271315409cff261db962ac60e2516a428",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aaCQ-Ze5UZRHB5Fh8gmgY98j6Pfgm1M41Aguo583pHU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbflkv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbflkv/glm45_a_zaiorg_collection/",
          "stickied": false,
          "url": "https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b",
          "subreddit_subscribers": 506439,
          "created_utc": 1753707823,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Source](https://huggingface.co/datasets/zai-org/CC-Bench-trajectories#overall-performance)",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Early GLM 4.5 Benchmarks, Claiming to surpass Qwen 3 Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 61,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "2sajkwcr4mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=246de2e890d026a4cb49f36bf54ec1a7bfbad60f"
                },
                {
                  "y": 80,
                  "x": 216,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=484c8312e52cf3bc991e40039b4c68cd54742919"
                },
                {
                  "y": 119,
                  "x": 320,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66e2e85c4bbe4f72b46bd86d35107bc5502d97a5"
                },
                {
                  "y": 238,
                  "x": 640,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bc407e3d0a5c6807865f2418b8992e84050b408"
                },
                {
                  "y": 358,
                  "x": 960,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e73fdc4ceffe980fd9495c05fe33e88870ddd34"
                },
                {
                  "y": 402,
                  "x": 1080,
                  "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29dec19e68b12db48a377fbaa06bf88cb487dfa8"
                }
              ],
              "s": {
                "y": 1480,
                "x": 3967,
                "u": "https://preview.redd.it/2sajkwcr4mff1.png?width=3967&amp;format=png&amp;auto=webp&amp;s=ff5fed0614da6788f23f4d16fbc94a2094829e44"
              },
              "id": "2sajkwcr4mff1"
            },
            "inopsfzq4mff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1710283c73c392a4a07ea96f13833cfb9b2d9f2c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5313024316c5a628cbe6c471e0b3256a3b3ab01"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca13b2c1def20c5bbdf65e64b4225ed03fe65866"
                },
                {
                  "y": 282,
                  "x": 640,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=760cfaf8017b920fe8e5a6e6ee7b8f3eae67ce60"
                },
                {
                  "y": 423,
                  "x": 960,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60977131a03c0ace67e2795b6fffab85128487c3"
                },
                {
                  "y": 476,
                  "x": 1080,
                  "u": "https://preview.redd.it/inopsfzq4mff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=23a28c3533a2243412f2abbfd2b32fbf7bc6f051"
                }
              ],
              "s": {
                "y": 1751,
                "x": 3967,
                "u": "https://preview.redd.it/inopsfzq4mff1.png?width=3967&amp;format=png&amp;auto=webp&amp;s=fbb5848ea96c713fcb887f492adcc8efdc32df90"
              },
              "id": "inopsfzq4mff1"
            }
          },
          "name": "t3_1mbfhgp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 118,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "inopsfzq4mff1",
                "id": 715826675
              },
              {
                "media_id": "2sajkwcr4mff1",
                "id": 715826676
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 118,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/d8z0Eqv5ElbJ5Bba2iScsXyRYp3-oFkkQDHFInTFYDc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753707550,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/datasets/zai-org/CC-Bench-trajectories#overall-performance\"&gt;Source&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mbfhgp",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbfhgp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbfhgp/early_glm_45_benchmarks_claiming_to_surpass_qwen/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mbfhgp",
          "subreddit_subscribers": 506439,
          "created_utc": 1753707550,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4ou3rslj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 is Live! Needs only 8GB of VRAM!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbfa3y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 570,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 570,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6wb7Yp5vFjJtIwYWiSu02kTzdKI2obJq-EU5BTqMluI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753706991,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2tqvij93mff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?auto=webp&amp;s=ee8cf1cb47816005e468b585d65be4de071b650f",
                  "width": 1319,
                  "height": 742
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9031e98c6b58f202a2505062878cd736f6658e48",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=784f151b8ee95ef486eb0b1a1e3bfd596879c0da",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f13b863800706917bf97e7c24c56acbf283df8fb",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aa487bb7dc2bff5b7326e25dfec4967cd6c8e51",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6d55de941bc4cf7f377686f9f3cd96fecc135c0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/w2tqvij93mff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b2119ea624eb2d2e46581f52916eefe02b8e10a",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "HCLdmR0umnU9RikapDseAAP7EInXhkRnH1_er5o1Ohc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbfa3y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Comed_Ai_n",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbfa3y/wan_22_is_live_needs_only_8gb_of_vram/",
          "stickied": false,
          "url": "https://i.redd.it/w2tqvij93mff1.jpeg",
          "subreddit_subscribers": 506439,
          "created_utc": 1753706991,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People who have hosted LLMs using vLLM, what approach did you guys take?\nListing down some approaches that I am considering. Would like to understand the associated complexity involved, ease of scaling for more models, more production loads, etc.\n\n1. Ec2 (considering g5.xlarge) with ASG\n2. Using k8s \n3. Using frameworks like Anyscale, anything llm, autogen, bentoml etc. (Using AWS is compulsory)\n4. Using integrations like kubeai, kuberay etc.\n\nThe frameworks and integrations are from vLLM docs under deployment. I am not much aware of what they exactly solve for but would like to understand if anyone of you have used those tools.\n",
          "author_fullname": "t2_3nk0ww7f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hosting LLM using vLLM for production",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbf9a9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753706925,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People who have hosted LLMs using vLLM, what approach did you guys take?\nListing down some approaches that I am considering. Would like to understand the associated complexity involved, ease of scaling for more models, more production loads, etc.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ec2 (considering g5.xlarge) with ASG&lt;/li&gt;\n&lt;li&gt;Using k8s &lt;/li&gt;\n&lt;li&gt;Using frameworks like Anyscale, anything llm, autogen, bentoml etc. (Using AWS is compulsory)&lt;/li&gt;\n&lt;li&gt;Using integrations like kubeai, kuberay etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The frameworks and integrations are from vLLM docs under deployment. I am not much aware of what they exactly solve for but would like to understand if anyone of you have used those tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbf9a9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "everyoneisodd",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753706925,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saw the following math question on YT and decided to give it a try with different models. Results are somehow unexpected.\n\nQuestion: There are three circles of radius 1, 2 and 3 tangent to each other. Find the area enclosed by their touching arcs.  \nCorrect answer: 0.464256\n\no4-min - correct  \nQwen3-235B-A22B-Thinknig-2507 - correct  \nQwen3-235B-A22B-Instruct-2507 - incorrect (5.536)  \nQwen3-32B - incorrect (5.536)  \nKimi-K2 - correct  \nDeepSeek-V3-0324 correct  \nDeepSeek-R1-0528 and Nemotron-Super-49B both gave the same incorrect answer (0.7358)  \nNemotron-Super-49B without reasoning - very incorrect (6 - 6 \\\\pi &lt; 0)\n\nAll models were used from their respective providers. It seems that models that failed had the right answer in their COT in one way or another, but failed to understand what they were asked in terms of actual geometry. The answer 5.536 is actually the sum of segments' area and is one step away from the right answer, which is 6 - 5.536 = 0.464. There are several unexpected results for me here:\n\n1. DeepSeek-R1 overthought the problem and managed to fail this fairly simple question although in COT it had the correct idea how to calculate: it as an area of triangle formed be center of circles minus areas of segments of each circle inside triangle.\n2. Kimi-K2 and DeepSeek-V3-0324 are very smart even without reasoning.\n3. Nemotron reasoning comes from DeepSeek distilation process.\n4. Qwen3-235B-A22B-Instruct-2507 output was so long as if it was a thinking model.\n5. Qwen3-32B is very capable model for its size, but you should go through all its COT to see if the right answer is burred somewhere there.\n\nOverall, based on these observations I think the right way to approach an analytical problem is to use first capable non-reasoning model and if it fails use capable thinking model then.\n\nPS: I am not a native speaker and may be the problem is in my formulation of the question. Still smart models understood what I really meant.",
          "author_fullname": "t2_63q8kong",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model vibe checking with a simple math question.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbf4wo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753784425,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753706581,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw the following math question on YT and decided to give it a try with different models. Results are somehow unexpected.&lt;/p&gt;\n\n&lt;p&gt;Question: There are three circles of radius 1, 2 and 3 tangent to each other. Find the area enclosed by their touching arcs.&lt;br/&gt;\nCorrect answer: 0.464256&lt;/p&gt;\n\n&lt;p&gt;o4-min - correct&lt;br/&gt;\nQwen3-235B-A22B-Thinknig-2507 - correct&lt;br/&gt;\nQwen3-235B-A22B-Instruct-2507 - incorrect (5.536)&lt;br/&gt;\nQwen3-32B - incorrect (5.536)&lt;br/&gt;\nKimi-K2 - correct&lt;br/&gt;\nDeepSeek-V3-0324 correct&lt;br/&gt;\nDeepSeek-R1-0528 and Nemotron-Super-49B both gave the same incorrect answer (0.7358)&lt;br/&gt;\nNemotron-Super-49B without reasoning - very incorrect (6 - 6 \\pi &amp;lt; 0)&lt;/p&gt;\n\n&lt;p&gt;All models were used from their respective providers. It seems that models that failed had the right answer in their COT in one way or another, but failed to understand what they were asked in terms of actual geometry. The answer 5.536 is actually the sum of segments&amp;#39; area and is one step away from the right answer, which is 6 - 5.536 = 0.464. There are several unexpected results for me here:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DeepSeek-R1 overthought the problem and managed to fail this fairly simple question although in COT it had the correct idea how to calculate: it as an area of triangle formed be center of circles minus areas of segments of each circle inside triangle.&lt;/li&gt;\n&lt;li&gt;Kimi-K2 and DeepSeek-V3-0324 are very smart even without reasoning.&lt;/li&gt;\n&lt;li&gt;Nemotron reasoning comes from DeepSeek distilation process.&lt;/li&gt;\n&lt;li&gt;Qwen3-235B-A22B-Instruct-2507 output was so long as if it was a thinking model.&lt;/li&gt;\n&lt;li&gt;Qwen3-32B is very capable model for its size, but you should go through all its COT to see if the right answer is burred somewhere there.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Overall, based on these observations I think the right way to approach an analytical problem is to use first capable non-reasoning model and if it fails use capable thinking model then.&lt;/p&gt;\n\n&lt;p&gt;PS: I am not a native speaker and may be the problem is in my formulation of the question. Still smart models understood what I really meant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbf4wo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "perelmanych",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbf4wo/model_vibe_checking_with_a_simple_math_question/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbf4wo/model_vibe_checking_with_a_simple_math_question/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753706581,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_59yau29b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-Demo",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbf3dz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=9cda62598ce898fd5db2e74df8c19f058e21c3e5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753706461,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/zai-org/GLM-4.5-Space",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?auto=webp&amp;s=49a8b8538778d58c1e6369156f3d03df65a20854",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a0064e28f939a7b67ba4b9fce0f0d2cea99181d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ddebbe3240648cf5604aaeed9e148051d04e101",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=459ce04fb2ed67865040cb0737d6bc4fe998d1c0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=40f00fd91f181f535bed35d54b6fc14b0ae5b15d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=089d6f4bc98a7f0fc128828289cd7b4ee50a9cb5",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56d13d6ad20a7063ad04b0cd09ea5995e1472f83",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "96ivHrmtPs4S7nGi4qvTltKCzfWeZZ9I9q3o_U5O9Qc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbf3dz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dr_Me_123",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbf3dz/glm45demo/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/zai-org/GLM-4.5-Space",
          "subreddit_subscribers": 506439,
          "created_utc": 1753706461,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI've been spending my weekend on a project, a web based chess game called Gemifish where you can play against an AI with a custom personality. The whole gimmick is that you can tell the AI to be, for example, \"an aggressive player,\" and it's supposed to choose its moves and talk smack accordingly. It's been very fun to build.\n\nIt all worked great in testing, but I've hit a really annoying wall now that it's \"live\". I'm using Stockfish to find the top 5 best moves, then I send that list to the free Google Gemini API to have it pick a move that fits the personality. The problem is, if you play more than a couple of moves in a minute, the entire thing breaks. I'm getting hit with Error 429: Too Many Requests, which forces the AI to just give up on the personality and play the default move. It kind of ruins the whole point of the project.\n\nSo,  I'm looking for a free API alternative that's a option better for a hobby project like this. The main things I need are more rate limits that won't choke after a few turns, and a model that's smart enough to actually follow my role playing prompt. I've heard people mention services like OpenRouter or maybe something from Mistral, but I'm not sure what's realistic for a simple project without a budget.\n\nHas anyone else run into this and found a good solution? Any advice or pointers would be a huge help. Thanks",
          "author_fullname": "t2_1ndo73s6xn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My chess AI project keeps hitting Google's rate limits. Any better free API alternatives out there?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbejz8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753704906,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been spending my weekend on a project, a web based chess game called Gemifish where you can play against an AI with a custom personality. The whole gimmick is that you can tell the AI to be, for example, &amp;quot;an aggressive player,&amp;quot; and it&amp;#39;s supposed to choose its moves and talk smack accordingly. It&amp;#39;s been very fun to build.&lt;/p&gt;\n\n&lt;p&gt;It all worked great in testing, but I&amp;#39;ve hit a really annoying wall now that it&amp;#39;s &amp;quot;live&amp;quot;. I&amp;#39;m using Stockfish to find the top 5 best moves, then I send that list to the free Google Gemini API to have it pick a move that fits the personality. The problem is, if you play more than a couple of moves in a minute, the entire thing breaks. I&amp;#39;m getting hit with Error 429: Too Many Requests, which forces the AI to just give up on the personality and play the default move. It kind of ruins the whole point of the project.&lt;/p&gt;\n\n&lt;p&gt;So,  I&amp;#39;m looking for a free API alternative that&amp;#39;s a option better for a hobby project like this. The main things I need are more rate limits that won&amp;#39;t choke after a few turns, and a model that&amp;#39;s smart enough to actually follow my role playing prompt. I&amp;#39;ve heard people mention services like OpenRouter or maybe something from Mistral, but I&amp;#39;m not sure what&amp;#39;s realistic for a simple project without a budget.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else run into this and found a good solution? Any advice or pointers would be a huge help. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbejz8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DinnerUnlucky4661",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbejz8/my_chess_ai_project_keeps_hitting_googles_rate/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbejz8/my_chess_ai_project_keeps_hitting_googles_rate/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753704906,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF)\n\n[https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF)\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for SmallThinker model series has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbei14",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=33a0f4cdec276414ee0ac47c804adeea4aac683b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753704745,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF\"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14898",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?auto=webp&amp;s=5ab36cd413e189d4dfebf3c031c110b200b9ea05",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=372d94ee95700a7c7cc6df9ff561202be75a9c00",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a085ecbc1b3a50a55ad24b23ffd4475ca02b7112",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e7b650aa8b0ae844281ca46e1b8404c2de59159",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76a396512223ddde08b85788022a284e7843ac6a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88270d3964bc63c3c3be91ea3a8115614a99f4cb",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03a9385cb31f27b96b8fc67ee11fe41832e04cf1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "e783aIBiZkFiVdg4SyQa6EJg5vPNIGwQuEikoNu5jPM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbei14",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbei14/support_for_smallthinker_model_series_has_been/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14898",
          "subreddit_subscribers": 506439,
          "created_utc": 1753704745,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We‚Äôre proud to introduce **Wan2.2**, a major leap in open video generation, featuring a novel **Mixture-of-Experts (MoE)** diffusion architecture, high-compression HD generation, and benchmark-leading performance.\n\n# üîç Key Innovations\n\n# üß† Mixture-of-Experts (MoE) Diffusion Architecture\n\nWan2.2 integrates **two specialized 14B experts** in its 27B-parameter MoE design:\n\n* **High-noise expert** for early denoising stages ‚Äî focusing on layout.\n* **Low-noise expert** for later stages ‚Äî refining fine details.\n\nOnly one expert is active per step (14B params), so **inference remains efficient** despite the added capacity.\n\nThe expert transition is based on the **Signal-to-Noise Ratio (SNR)** during diffusion. As SNR drops, the model smoothly switches from the high-noise to low-noise expert at a learned threshold (`t_moe`), ensuring optimal handling of different generation phases.\n\nüìà **Visual Overview**:\n\n**Left: Expert switching based on SNR**  \n**Right: Validation loss comparison across model variants**\n\n\n\nThe final **Wan2.2 (MoE)** model shows the **lowest validation loss**, confirming better convergence and fidelity than Wan2.1 or hybrid expert configurations.\n\n# ‚ö° TI2V-5B: Fast, Compressed, HD Video Generation\n\nWan2.2 also introduces **TI2V-5B**, a **5B dense model** with impressive efficiency:\n\n* Utilizes **Wan2.2-VAE** with $4\\\\times16\\\\times16$ spatial compression.\n* Achieves **$4\\\\times32\\\\times32$ total compression** with patchification.\n* Can generate **5s 720P@24fps videos in &lt;9 minutes** on a consumer GPU.\n* Natively supports **text-to-video (T2V)** and **image-to-video (I2V)** in one unified architecture.\n\nThis makes Wan2.2 not only powerful but also highly practical for real-world applications.\n\n# üß™ Benchmarking: Wan2.2 vs Commercial SOTAs\n\nWe evaluated Wan2.2 against leading proprietary models on **Wan-Bench 2.0**, scoring across:\n\n* Aesthetics\n* Dynamic motion\n* Text rendering\n* Camera control\n* Fidelity\n* Object accuracy\n\nüìä **Benchmark Results**:\n\n\n\nüöÄ **Wan2.2-T2V-A14B leads in 5/6 categories**, outperforming commercial models like KLING 2.0, Sora, and Seedance in:\n\n* **Dynamic Degree**\n* **Text Rendering**\n* **Object Accuracy**\n* And more‚Ä¶\n\n# üßµ Why Wan2.2 Matters\n\n* Brings **MoE advantages** to video generation with no added inference cost.\n* Achieves **industry-leading HD generation speeds** on consumer GPUs.\n* **Openly benchmarked** with results that rival or beat closed-source giants.",
          "author_fullname": "t2_pa2ww",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan 2.2 T2V,I2V 14B MoE Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbefh4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 176,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 176,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=74eb40bbcaaad3e6917f58cacde7b1456925f450",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753704542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We‚Äôre proud to introduce &lt;strong&gt;Wan2.2&lt;/strong&gt;, a major leap in open video generation, featuring a novel &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; diffusion architecture, high-compression HD generation, and benchmark-leading performance.&lt;/p&gt;\n\n&lt;h1&gt;üîç Key Innovations&lt;/h1&gt;\n\n&lt;h1&gt;üß† Mixture-of-Experts (MoE) Diffusion Architecture&lt;/h1&gt;\n\n&lt;p&gt;Wan2.2 integrates &lt;strong&gt;two specialized 14B experts&lt;/strong&gt; in its 27B-parameter MoE design:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;High-noise expert&lt;/strong&gt; for early denoising stages ‚Äî focusing on layout.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Low-noise expert&lt;/strong&gt; for later stages ‚Äî refining fine details.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Only one expert is active per step (14B params), so &lt;strong&gt;inference remains efficient&lt;/strong&gt; despite the added capacity.&lt;/p&gt;\n\n&lt;p&gt;The expert transition is based on the &lt;strong&gt;Signal-to-Noise Ratio (SNR)&lt;/strong&gt; during diffusion. As SNR drops, the model smoothly switches from the high-noise to low-noise expert at a learned threshold (&lt;code&gt;t_moe&lt;/code&gt;), ensuring optimal handling of different generation phases.&lt;/p&gt;\n\n&lt;p&gt;üìà &lt;strong&gt;Visual Overview&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Left: Expert switching based on SNR&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;Right: Validation loss comparison across model variants&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The final &lt;strong&gt;Wan2.2 (MoE)&lt;/strong&gt; model shows the &lt;strong&gt;lowest validation loss&lt;/strong&gt;, confirming better convergence and fidelity than Wan2.1 or hybrid expert configurations.&lt;/p&gt;\n\n&lt;h1&gt;‚ö° TI2V-5B: Fast, Compressed, HD Video Generation&lt;/h1&gt;\n\n&lt;p&gt;Wan2.2 also introduces &lt;strong&gt;TI2V-5B&lt;/strong&gt;, a &lt;strong&gt;5B dense model&lt;/strong&gt; with impressive efficiency:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Utilizes &lt;strong&gt;Wan2.2-VAE&lt;/strong&gt; with $4\\times16\\times16$ spatial compression.&lt;/li&gt;\n&lt;li&gt;Achieves &lt;strong&gt;$4\\times32\\times32$ total compression&lt;/strong&gt; with patchification.&lt;/li&gt;\n&lt;li&gt;Can generate &lt;strong&gt;5s 720P@24fps videos in &amp;lt;9 minutes&lt;/strong&gt; on a consumer GPU.&lt;/li&gt;\n&lt;li&gt;Natively supports &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; in one unified architecture.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This makes Wan2.2 not only powerful but also highly practical for real-world applications.&lt;/p&gt;\n\n&lt;h1&gt;üß™ Benchmarking: Wan2.2 vs Commercial SOTAs&lt;/h1&gt;\n\n&lt;p&gt;We evaluated Wan2.2 against leading proprietary models on &lt;strong&gt;Wan-Bench 2.0&lt;/strong&gt;, scoring across:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Aesthetics&lt;/li&gt;\n&lt;li&gt;Dynamic motion&lt;/li&gt;\n&lt;li&gt;Text rendering&lt;/li&gt;\n&lt;li&gt;Camera control&lt;/li&gt;\n&lt;li&gt;Fidelity&lt;/li&gt;\n&lt;li&gt;Object accuracy&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;üìä &lt;strong&gt;Benchmark Results&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;üöÄ &lt;strong&gt;Wan2.2-T2V-A14B leads in 5/6 categories&lt;/strong&gt;, outperforming commercial models like KLING 2.0, Sora, and Seedance in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic Degree&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Text Rendering&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Object Accuracy&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;And more‚Ä¶&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;üßµ Why Wan2.2 Matters&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Brings &lt;strong&gt;MoE advantages&lt;/strong&gt; to video generation with no added inference cost.&lt;/li&gt;\n&lt;li&gt;Achieves &lt;strong&gt;industry-leading HD generation speeds&lt;/strong&gt; on consumer GPUs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Openly benchmarked&lt;/strong&gt; with results that rival or beat closed-source giants.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Wan-AI",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?auto=webp&amp;s=5e2d1a9b7d7ba587c66883c59382bf9da05496ef",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c28c08e5f6ad66084018cf52177490f848610b13",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f55e3ea7af464c4462923d295c8307452d91dc8c",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5147cc7c793b5ccb1f3c4173598b7eaa49df359",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=383cb90569524b8ee389cbf51df12c411b89660a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=899dbe9927605247845ad6c7073df7056f48193d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b29faba3ffcdda3d04d69e576a35738628206297",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aEhwAcxoSYdnD-EeEFeFHx8riDTf8HthjpthzWwECGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbefh4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "khubebk",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbefh4/wan_22_t2vi2v_14b_moe_models/",
          "stickied": false,
          "url": "https://huggingface.co/Wan-AI",
          "subreddit_subscribers": 506439,
          "created_utc": 1753704542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which of the following models is the best in terms of function calling in your opinion?  \n1. Claude Sonnet 4  \n2. o3  \n3. Gemini 2.5 Pro\n\nAlso which one of them is the most creative when it comes to solving problems?",
          "author_fullname": "t2_9lpd1y2f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Function Calling: Claude Sonnet 4 Vs o3 Vs Gemin 2.5 Pro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbeeru",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753704484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which of the following models is the best in terms of function calling in your opinion?&lt;br/&gt;\n1. Claude Sonnet 4&lt;br/&gt;\n2. o3&lt;br/&gt;\n3. Gemini 2.5 Pro&lt;/p&gt;\n\n&lt;p&gt;Also which one of them is the most creative when it comes to solving problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbeeru",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Illustrious-Ad-497",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbeeru/function_calling_claude_sonnet_4_vs_o3_vs_gemin/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbeeru/function_calling_claude_sonnet_4_vs_o3_vs_gemin/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753704484,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Wan-AI/Wan2.2-I2V-A14B [https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B)\n\nWan-AI/Wan2.2-T2V-A14B [https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B)",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wan-AI/Wan2.2-TI2V-5B ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbeecr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=062bb1369f488ff91a2b5857b56068bc229a16ed",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753704449,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wan-AI/Wan2.2-I2V-A14B &lt;a href=\"https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B\"&gt;https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Wan-AI/Wan2.2-T2V-A14B &lt;a href=\"https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B\"&gt;https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?auto=webp&amp;s=7c7b722b69ae889e2b0b1f127a63d655a7b565ad",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a30cc426f87d5b04217454606f990d19816fc01",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4a4d6a7180825e8e0a1f293a3699433ad7dc57f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fcebe38c7944f34722034d111b2873af4e0a609",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27e634b9b7a9f310e89c9de904713a31626c729c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=919e4e034794c2ed20ace57bd42083f55e89883b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=240978506f0bd5a87c87e34d645138bc6f8bddd9",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MjNARg6a8Ws129Qpd3ZHOB9syHgcwUkd0ahvvUlc-Sc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mbeecr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbeecr/wanaiwan22ti2v5b_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B",
          "subreddit_subscribers": 506439,
          "created_utc": 1753704449,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "BackGround: I developed a new FFN architecture called Parallel-FFN, with the primary goal of improving parameter efficiency in Transformer models.\n\nExperimental Setup:\n\n1. Transformer Integration: Replaced standard FFN components with Parallel-FFN architecture\n2. LLM Evaluation: Substituted SwiGLU components in large language models with Parallel-FFN\n3. Baseline Comparison: Measured performance against original architectures\n\nResults:\n\n* Parameter Efficiency: Successfully achieved equivalent loss with 35% parameter reduction compared to SwiGLU baseline\n* Performance: Maintained comparable model performance across evaluations\n* Inference Speed: Initial implementation showed slower inference than baseline, but recent optimizations suggest we can achieve parity\n\nCurrent Status:\n\n* Architecture optimization is ongoing to match baseline inference speeds\n* Focus remains on maximizing parameter efficiency rather than raw speed\n\nLimitations:\n\n* Inference speed optimization still in progress\n* Limited evaluation on diverse model scales\n* Need more comprehensive benchmarking\n\nDiscussion: Has anyone worked on similar parameter-efficient FFN variants? I'm curious about related approaches and potential collaboration opportunities.\n\nhttps://preview.redd.it/ppm5feuhulff1.png?width=956&amp;format=png&amp;auto=webp&amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5\n\n",
          "author_fullname": "t2_tcjic8rca",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[R] Parallel-FFN: Parameter-Efficient FFN Architecture with 35% Parameter Reduction",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 18,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ppm5feuhulff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 14,
                  "x": 108,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=431a73ea5b33ee42b3b2ebd677bf8456f0a6f872"
                },
                {
                  "y": 28,
                  "x": 216,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=142ff5fb1369bbfb1a9243608aa64b70d3f8d7de"
                },
                {
                  "y": 41,
                  "x": 320,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9eff0b26842d39120871db9b9f587b3c678f108"
                },
                {
                  "y": 83,
                  "x": 640,
                  "u": "https://preview.redd.it/ppm5feuhulff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=848953d7c9753556894825b3322f0d22c671507c"
                }
              ],
              "s": {
                "y": 124,
                "x": 956,
                "u": "https://preview.redd.it/ppm5feuhulff1.png?width=956&amp;format=png&amp;auto=webp&amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5"
              },
              "id": "ppm5feuhulff1"
            }
          },
          "name": "t3_1mbe9p9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pq1fl_YakjaNbSRNI7EsABe2R6p78F2jXchiT74DLRQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753704074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;BackGround: I developed a new FFN architecture called Parallel-FFN, with the primary goal of improving parameter efficiency in Transformer models.&lt;/p&gt;\n\n&lt;p&gt;Experimental Setup:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Transformer Integration: Replaced standard FFN components with Parallel-FFN architecture&lt;/li&gt;\n&lt;li&gt;LLM Evaluation: Substituted SwiGLU components in large language models with Parallel-FFN&lt;/li&gt;\n&lt;li&gt;Baseline Comparison: Measured performance against original architectures&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Results:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Parameter Efficiency: Successfully achieved equivalent loss with 35% parameter reduction compared to SwiGLU baseline&lt;/li&gt;\n&lt;li&gt;Performance: Maintained comparable model performance across evaluations&lt;/li&gt;\n&lt;li&gt;Inference Speed: Initial implementation showed slower inference than baseline, but recent optimizations suggest we can achieve parity&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current Status:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Architecture optimization is ongoing to match baseline inference speeds&lt;/li&gt;\n&lt;li&gt;Focus remains on maximizing parameter efficiency rather than raw speed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Limitations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Inference speed optimization still in progress&lt;/li&gt;\n&lt;li&gt;Limited evaluation on diverse model scales&lt;/li&gt;\n&lt;li&gt;Need more comprehensive benchmarking&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Discussion: Has anyone worked on similar parameter-efficient FFN variants? I&amp;#39;m curious about related approaches and potential collaboration opportunities.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ppm5feuhulff1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5\"&gt;https://preview.redd.it/ppm5feuhulff1.png?width=956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44a72d5f3294be0b1e271e42f314bb49deae1ce5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbe9p9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Perfect_Power815",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbe9p9/r_parallelffn_parameterefficient_ffn_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbe9p9/r_parallelffn_parameterefficient_ffn_architecture/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753704074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need proven ways to make LLM outputs sound more natural and more human.   \n  \nTypically LLM outputs sound so overly machine-generated and I would like to change that for my applications. Thanks for your support",
          "author_fullname": "t2_3aym5nqj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Proven strategies for making LLM outputs sound human",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbe7ua",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753703930,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need proven ways to make LLM outputs sound more natural and more human.   &lt;/p&gt;\n\n&lt;p&gt;Typically LLM outputs sound so overly machine-generated and I would like to change that for my applications. Thanks for your support&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbe7ua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AleccioIsland",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbe7ua/proven_strategies_for_making_llm_outputs_sound/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbe7ua/proven_strategies_for_making_llm_outputs_sound/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753703930,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Somebody running kimi locally?",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Somebody running kimi locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbe14n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753703348,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Somebody running kimi locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbe14n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbe14n/somebody_running_kimi_locally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbe14n/somebody_running_kimi_locally/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753703348,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ai voice clone local unlimited that can generate long characters or words over 1k:\n\nAny one knows any local ai tool that clones voice from reference audio that works with unlimited and long inout characters? I know Kokoro TTS works with unlimited input but it doesn't clone voices from reference audio. Also ChatterboxTTS supports cloning but it just doesn't work well with long text input. Sometimes it cuts some sentences or words. Thank you guys for your help in advance... Truly appreciate you all!",
          "author_fullname": "t2_1tta08arr2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ai voice clone local unlimited that can generate long characters or words over 1k",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdtw8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753702715,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ai voice clone local unlimited that can generate long characters or words over 1k:&lt;/p&gt;\n\n&lt;p&gt;Any one knows any local ai tool that clones voice from reference audio that works with unlimited and long inout characters? I know Kokoro TTS works with unlimited input but it doesn&amp;#39;t clone voices from reference audio. Also ChatterboxTTS supports cloning but it just doesn&amp;#39;t work well with long text input. Sometimes it cuts some sentences or words. Thank you guys for your help in advance... Truly appreciate you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbdtw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mauamolat",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbdtw8/ai_voice_clone_local_unlimited_that_can_generate/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbdtw8/ai_voice_clone_local_unlimited_that_can_generate/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753702715,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really love the fact that I can have both a SOTA reasoning AND instruct model variant off of one singular model. I can essentially deploy 2 models with 2 use cases with the cost of one models vram. With /think for difficult problems and /no_think for easier problems, essentially we can experience a best from both worlds. \n\nRecently Qwen released updated fine tunes of their SOTA models however they removed the hybrid reasoning functions, meaning that we no longer have the best of both worlds. \n\nIf I want a model with reasoning and non reasoning now I need twice the amount of vram to deploy both. Which for vram poor people, it ain‚Äôt really ideal.\n\nI feel that qwen should focus back at releasing hybrid reasoning models. Hbu?",
          "author_fullname": "t2_a06q0mmx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hybrid Reasoning Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdn26",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753702098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really love the fact that I can have both a SOTA reasoning AND instruct model variant off of one singular model. I can essentially deploy 2 models with 2 use cases with the cost of one models vram. With /think for difficult problems and /no_think for easier problems, essentially we can experience a best from both worlds. &lt;/p&gt;\n\n&lt;p&gt;Recently Qwen released updated fine tunes of their SOTA models however they removed the hybrid reasoning functions, meaning that we no longer have the best of both worlds. &lt;/p&gt;\n\n&lt;p&gt;If I want a model with reasoning and non reasoning now I need twice the amount of vram to deploy both. Which for vram poor people, it ain‚Äôt really ideal.&lt;/p&gt;\n\n&lt;p&gt;I feel that qwen should focus back at releasing hybrid reasoning models. Hbu?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mbdn26",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MichaelXie4645",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mbdn26/hybrid_reasoning_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbdn26/hybrid_reasoning_models/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753702098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bloomberg writes:\n\n&gt;The startup will release GLM-4.5, an update to its flagship model, as soon as Monday, according to a person familiar with the plan.\n\nThe organization has changed their name on HF from THUDM to zai-org and they have a GLM 4.5 collection which has 8 hidden items in it.\n\n[https://huggingface.co/organizations/zai-org/activity/collections](https://huggingface.co/organizations/zai-org/activity/collections)",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 possibly releasing today according to Bloomberg",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdm6t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 155,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 155,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=140&amp;height=93&amp;crop=140:93,smart&amp;auto=webp&amp;s=355e9ec2fa3e3360af59b2098c48fa105bb99e90",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753702016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "bloomberg.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bloomberg writes:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The startup will release GLM-4.5, an update to its flagship model, as soon as Monday, according to a person familiar with the plan.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The organization has changed their name on HF from THUDM to zai-org and they have a GLM 4.5 collection which has 8 hidden items in it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/organizations/zai-org/activity/collections\"&gt;https://huggingface.co/organizations/zai-org/activity/collections&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.bloomberg.com/news/articles/2025-07-28/chinese-openai-challenger-zhipu-to-unveil-new-open-source-model",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?auto=webp&amp;s=e4cb5ef205d53a96b0ef79a989b300b42e222d23",
                  "width": 1200,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49d610e841064301ef9eed8e3e833431e3633cd1",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38dd9ae75ecfcee4e431fdab64e2056f653b1642",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5075adce2c3d58fd1f80c91982a247d8b33dbe18",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11470efb2209e35e2be0d434f089cd6d797726ba",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3756403e4f6b50939fb2e629242331c6a052032b",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e06a30269401467c1b156345a2e6fb6856c45465",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "MKYF_mjSE9CGBChz_RrVYgFKUGWvflOwY1euYqGGxdc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mbdm6t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbdm6t/glm_45_possibly_releasing_today_according_to/",
          "stickied": false,
          "url": "https://www.bloomberg.com/news/articles/2025-07-28/chinese-openai-challenger-zhipu-to-unveil-new-open-source-model",
          "subreddit_subscribers": 506439,
          "created_utc": 1753702016,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone I am trying to build a small project just to keep in touch with all the news and information flowing in the markets so that I can better understand what is happening around the world. I am fetching the data from a website where I get the link of the pdf for concalls and other credit ratings changes, this information is too complex to analyse. So I want to pass it through an LLM and see what can be done around with it. Currently I have a mac mini m4 and a few windows systems with 16gb ram and 4gb graphics card, I have no clue how I can build this system with minimum expenses. yes I can use open ai api and it will work perfectly fine, If anyone can either give me an estimate of how much will I be spending on it? because all of this is too complicated to understand atleast for me. I was looking for LLAMA but then again I am not sure if my systems are capable enough. What do you guys think?",
          "author_fullname": "t2_1nqcevx7uj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a personal project for portfolio management.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbdg53",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753701476,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone I am trying to build a small project just to keep in touch with all the news and information flowing in the markets so that I can better understand what is happening around the world. I am fetching the data from a website where I get the link of the pdf for concalls and other credit ratings changes, this information is too complex to analyse. So I want to pass it through an LLM and see what can be done around with it. Currently I have a mac mini m4 and a few windows systems with 16gb ram and 4gb graphics card, I have no clue how I can build this system with minimum expenses. yes I can use open ai api and it will work perfectly fine, If anyone can either give me an estimate of how much will I be spending on it? because all of this is too complicated to understand atleast for me. I was looking for LLAMA but then again I am not sure if my systems are capable enough. What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mbdg53",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boring_Tip_1218",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbdg53/building_a_personal_project_for_portfolio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbdg53/building_a_personal_project_for_portfolio/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753701476,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey yall, I built an opensource AI Model Router that automatically picks the best AI provider (OpenAI, Anthropic, Google, local), model, and settings for your prompts. No more guessing between openai Claude, or Gemini!\n\nFeedback welcome!",
          "author_fullname": "t2_5gpifn7q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Opensource: The AI Model Router - Automating AI Model Selection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbcwek",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f160a8dbb6239851a23b15bb7ffa05ed622766fc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753699692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall, I built an opensource AI Model Router that automatically picks the best AI provider (OpenAI, Anthropic, Google, local), model, and settings for your prompts. No more guessing between openai Claude, or Gemini!&lt;/p&gt;\n\n&lt;p&gt;Feedback welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/MonkWarrior08/Model_Router",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?auto=webp&amp;s=fc338c0157bede926870ccb47aed508a93663712",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0eae7298df9a75056291706e27eb55423947f5a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3621d1857dd5414d1345fb4fef0fc90de77fbf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=547ca1f47dd7cb3cde9647c607af1349cf5913a7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142b9f59812a7af5da3822cb118e31ad38a1664b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bcddb4321ebd29c3a4e8af3164058cc8071c779",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=512358a85e36e15fb3f2da027213d2137fa3483d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "U65r0MxggWUPTuOch0OpSwES-nV5AG-PgAkmJMyj4wE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbcwek",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Idonotknow101",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbcwek/opensource_the_ai_model_router_automating_ai/",
          "stickied": false,
          "url": "https://github.com/MonkWarrior08/Model_Router",
          "subreddit_subscribers": 506439,
          "created_utc": 1753699692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hgio9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mbce7b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753697853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "jerryliang24.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://jerryliang24.github.io/DnD/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mbce7b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "paf1138",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mbce7b/draganddrop_llms_zeroshot_prompttoweights/",
          "stickied": false,
          "url": "https://jerryliang24.github.io/DnD/",
          "subreddit_subscribers": 506439,
          "created_utc": 1753697853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}