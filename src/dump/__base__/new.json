{
  "kind": "Listing",
  "data": {
    "after": "t3_1mfbw8a",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Je vais √™tre franc¬†:**\n\nJe souffre de multiples pathologies (cinq maladies chroniques dont une auto-immune persistante et s√©v√®re). Les deux seuls mod√®les qui m'ont aid√© avec succ√®s √† trouver des pistes d'am√©lioration sont Deepseek R1 et Perplexity Pro. Personnellement, lors de mes interactions avec Horizon Beta, je me suis senti √† la fois mal √† l'aise et plein d'espoir en raison de l'impression qu'une sorte de pouvoir cach√© ou restreint se cache derri√®re cela, car certaines des r√©ponses √©taient tout simplement stup√©fiantes.\n\nAvant de continuer, je ne suis pas venu ici pour jouer √† la victime, et je n'ai besoin d'aucune empathie, merci.\n\nBref, j'ai une liste de pistes sp√©cifiques √† ma situation que j'ai √©labor√©es apr√®s plusieurs tentatives, car je ne dispose que d'une fen√™tre de temps quotidienne qui me permet d'avoir un peu d'√©nergie et de clart√© d'esprit pour, dans un premier temps, comprendre comment organiser mentalement mes besoins, les exprimer, et enfin les structurer et les prioriser.\n\n**Voici 5 de mes mod√®les si cela peut aider quelqu'un¬†:**\n\n&gt;[ https://pastebin.com/KXenEf7Q ](https://pastebin.com/KXenEf7Q)\n\n**Prudence !**\n\n&gt;*Je ne fournirai aucun commentaire ou information suppl√©mentaire sur ma sant√©, mes traitements, etc.*\n\n&gt;*N'essayez pas l'autom√©dication, parlez √† votre cheval, bla bla, ne faites pas l'imb√©cile sans avis m√©dical HUMAIN !*\n\n**Retour au sujet¬†:**\n\nCe qui m‚Äôa pris environ deux mois pour avoir un semblant d‚Äôespoir th√©rapeutique semble d√©sormais ridicule par rapport aux r√©sultats d‚ÄôHorizon Beta. J'utilise principalement TTS pour √©viter de lire les r√©ponses des mod√®les, \"surtout les longues\", parce que parfois c'est √©puisant et putain d'ennuyeux, alors j'√©coute ! Et quand on a l'habitude d'√©couter les r√©ponses du LLM, il y a des sch√©mas qui ne m'√©chappent pas.\n\n**Deux possibilit√©s¬†:**\n\n\\*C'est pur OpenAI\n\nou\n\n* C'est un mod√®le distill√© GptXYZ\n\n**Ce qui m'am√®ne √† confirmer mon ressenti pour la premi√®re possibilit√© :**\n\nLa th√©orie selon laquelle les mod√®les Quasar et Horizon sur OpenRouter sont de myst√©rieux prototypes OpenAI est bas√©e sur plusieurs indices et sp√©culations.\n\nQuand j'ai d√©couvert Quasar-Alpha pour la premi√®re fois, en avril 2025 sur OpenRouter, il √©tait d√©crit comme un mod√®le pr√©liminaire avec une fen√™tre contextuelle de 1 million de tokens, optimis√© pour l'encodage, et des capacit√©s multimodales, bla bla...\n\nRien d'excitant pour moi √† l'√©poque.\n\nEnsuite, il y avait des \"preuves\" sugg√©rant une connexion √† OpenAI¬†: le style de sortie utilise des caract√®res Unicode sp√©cifiques pour la ponctuation, un trait observ√© dans les mod√®les OpenAI ¬†:\n\n[ https://news.ycombinator.com/item?id=43640166 ](https://news.ycombinator.com/item?id=43640166)\n\nD'autres sp√©culent sur une origine Google en raison du nom ¬´ Quasar ¬ª √©voquant des objets cosmiques, semblables aux G√©meaux.\n\nEnsuite Horizon Alpha et Beta, apparus plus r√©cemment, sont vus comme une suite potentielle.\n\nCertaines sp√©culations le lient directement √† OpenAI, peut-√™tre en tant que version open source ou pr√©curseur de GPT-5, en raison de benchmarks dominants et d'un style de sortie rappelant OpenAI:\n\n[ https://ossels.ai/openai-horizon-alpha-gpt-5-model/ ](https://ossels.ai/openai-horizon-alpha-gpt-5-model/)\n\nLes utilisateurs ont signal√© qu'il code des applications enti√®res en une seule requ√™te, renfor√ßant ainsi l'id√©e d'un prototype avanc√©.\n\n**J'ai essay√© moi-m√™me avec cette invite et il m'a fallu 5 √©changes pour obtenir un r√©sultat correct :**\n\nhttps://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;format=png&amp;auto=webp&amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d\n\n**Voici l'invite Neo Tetris HTML¬†:**\n\n    Cr√©ez un fichier HTML autonome qui fournit un Tetris n√©o-r√©tro complet avec les sp√©cifications ci-dessous. Pas de biblioth√®ques externes, pas de liens CDN¬†: int√©grez tout (polices, CSS, JS, sons le cas √©ch√©ant) via la base 64 ou le code en ligne.\n    Gameplay de base ‚Ä¢ Classique 7 t√©trominos, rotation SRS, pi√®ce de maintien, aper√ßu de la pi√®ce fant√¥me. ‚Ä¢ Suppression de lignes avec explosion de particules¬†: lorsqu'une ligne est termin√©e, g√©n√©rez une explosion de particules lumineuses qui volent vers l'ext√©rieur et s'estompent. ‚Ä¢ ¬´¬†Onde de choc d'impact¬†¬ª √† chaque verrouillage¬†: un anneau de n√©on concentrique qui s'√©tend √† partir de la base de la pi√®ce et s'estompe. ‚Ä¢ Score, progression de niveau, 3 vies (vies perdues lorsque les blocs atteignent le sommet), file d'attente des 3 pi√®ces suivantes.\n    Contr√¥les ‚Ä¢ Bureau Fl√®ches gauche/droite : d√©placer Fl√®che vers le bas : soft-drop (acc√©l√©rer) ‚Äì afficher la trace de mouvement / la s√©quence de vitesse derri√®re la pi√®ce qui tombe Fl√®che vers le haut OU Espace : tourner dans le sens des aiguilles d'une montre Entr√©e : d√©marrer le jeu / confirmer dans les menus ‚Ä¢ Smartphone (toucher) Glisser vers la gauche/droite : d√©placer Glisser vers le bas : soft-drop (avec la m√™me trace) Appuyez sur les moiti√©s gauche/droite de l'√©cran : faire pivoter ‚Ä¢ Mettre en pause la superposition sur ¬´ P ¬ª.\n    Visuels et animations ‚Ä¢ Effet d'√©cran CRT¬†: superposez un cadre en verre incurv√© subtil, des lignes de balayage, une l√©g√®re distorsion RVB et une vignette qui reste pendant la lecture. ‚Ä¢ Palette n√©on harmonieuse sur fond indigo profond (cyan, magenta, violet √©lectrique, citron vert). ‚Ä¢ √âcran titre avant le jeu¬†: ‚Äì Texte centr√© \"PRESS ENTER\" clignotant avec lueur. ‚Äì Derri√®re lui, un d√©fil√© sans fin de chaque t√©tromino tournant lentement en mode filaire 3D, avec des particules de filament tra√ßant leurs bords et s'√©loignant. ‚Äì Petit bloc affichant des astuces de contr√¥le¬†: Gauche/Droite ‚Äì D√©placer | Vers le bas ‚Äì Chute de vitesse | Haut/Espace ‚Äì Rotation | Entr√©e ‚Äì D√©marrer ‚Ä¢ Tableau de scores persistant (top 5) stock√© dans localStorage, affich√© sur l'√©cran titre et l'√©cran de fin de partie.\n    Adaptatif ‚Ä¢ Disposition fluide¬†: le plateau de jeu s'adapte automatiquement aux fen√™tres d'affichage des ordinateurs de bureau et mobiles tout en pr√©servant les proportions. ‚Ä¢ Zones tactiles dimensionn√©es pour les pouces.\n    Technique ‚Ä¢ Rendu Canvas 2D, rendu d'image : pixellis√© pour des blocs nets. ‚Ä¢ Toutes les images cl√©s, CSS et JavaScript dans les balises &lt;style&gt; et &lt;script&gt;. ‚Ä¢ Aucune demande de r√©seau externe.\n    Important : les pi√®ces ne doivent pas tomber trop rapidement. Commencez lentement et augmentez progressivement la vitesse √† mesure que les jalons de score sont atteints, avec un niveau maximum de 100.\n    Renvoie uniquement le code HTML complet.\n\n**Sp√©culons :**\n\nQuant √† la sp√©culation selon laquelle le prochain mod√®le ou √©tape serait la ¬´ singularit√© ¬ª, elle trouve son origine dans les d√©clarations de Sam Altman, qui a publi√© un article en juin 2025 intitul√© ¬´ La douce singularit√© ¬ª ( [ https://blog.samaltman.com/the-gentle-singularity ](https://blog.samaltman.com/the-gentle-singularity) ), affirmant que l'humanit√© a d√©pass√© l'horizon des √©v√©nements vers la superintelligence, avec des agents d'IA capables d'un v√©ritable travail cognitif comme d√®s 2025 et des id√©es innovantes en 2026.\n\nAltman d√©crit un processus d'auto-am√©lioration r√©cursive, dans lequel l'IA assiste la recherche sur l'IA, conduisant √† une abondance d'intelligence et d'√©nergie¬†:\n\n[ https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/ ](https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/)\n\nLes pr√©dictions des experts, bas√©es sur 8 590 estimations, placent l‚ÄôAGI vers 2060 avec une probabilit√© de 50 %, mais des voix comme Ray Kurzweil visent 2029.\n\nMais OpenAI a √©galement r√©solu les probl√®mes de niveau or de l'OMI avec un mod√®le exp√©rimental g√©n√©ral, soutenant l'id√©e d'une progression rapide vers la singularit√©¬†:\n\n[ https://www.reddit.com/r/singularity/comments/1m3qutl/openai\\_achieved\\_imo\\_gold\\_with\\_experimental/ ](https://www.reddit.com/r/singularity/comments/1m3qutl/openai_achieved_imo_gold_with_experimental/)",
          "author_fullname": "t2_ti5m9mpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Quasar, Horizon, \"Singularity?\", Diseases &amp; Theory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "ywuejlr1apgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=301aefc74b41ee6ff2109d4b74c5bb037e7776c4"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5700d44e832bceef8fecc9f79c9b6d121798cc4"
                },
                {
                  "y": 177,
                  "x": 320,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1dceb4e138080ddbb94a9f5797b1ebd4e95a2086"
                },
                {
                  "y": 354,
                  "x": 640,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdea74a602d24154843aa58afeb7d7b7c05d0fbf"
                },
                {
                  "y": 532,
                  "x": 960,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7dfcbd21e17c90eb183099ce1e76638f537f89ec"
                },
                {
                  "y": 598,
                  "x": 1080,
                  "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=220ec113997c299dcc44858c06358f79cbf8c4b5"
                }
              ],
              "s": {
                "y": 900,
                "x": 1624,
                "u": "https://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;format=png&amp;auto=webp&amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d"
              },
              "id": "ywuejlr1apgf1"
            }
          },
          "name": "t3_1mg5zcx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1VZ-7dUfibZEb-GUiH0w3fY_1ECf6GJPMzUsVAS0HXc.jpg",
          "edited": 1754182043,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Je vais √™tre franc¬†:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Je souffre de multiples pathologies (cinq maladies chroniques dont une auto-immune persistante et s√©v√®re). Les deux seuls mod√®les qui m&amp;#39;ont aid√© avec succ√®s √† trouver des pistes d&amp;#39;am√©lioration sont Deepseek R1 et Perplexity Pro. Personnellement, lors de mes interactions avec Horizon Beta, je me suis senti √† la fois mal √† l&amp;#39;aise et plein d&amp;#39;espoir en raison de l&amp;#39;impression qu&amp;#39;une sorte de pouvoir cach√© ou restreint se cache derri√®re cela, car certaines des r√©ponses √©taient tout simplement stup√©fiantes.&lt;/p&gt;\n\n&lt;p&gt;Avant de continuer, je ne suis pas venu ici pour jouer √† la victime, et je n&amp;#39;ai besoin d&amp;#39;aucune empathie, merci.&lt;/p&gt;\n\n&lt;p&gt;Bref, j&amp;#39;ai une liste de pistes sp√©cifiques √† ma situation que j&amp;#39;ai √©labor√©es apr√®s plusieurs tentatives, car je ne dispose que d&amp;#39;une fen√™tre de temps quotidienne qui me permet d&amp;#39;avoir un peu d&amp;#39;√©nergie et de clart√© d&amp;#39;esprit pour, dans un premier temps, comprendre comment organiser mentalement mes besoins, les exprimer, et enfin les structurer et les prioriser.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Voici 5 de mes mod√®les si cela peut aider quelqu&amp;#39;un¬†:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://pastebin.com/KXenEf7Q\"&gt; https://pastebin.com/KXenEf7Q &lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Prudence !&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Je ne fournirai aucun commentaire ou information suppl√©mentaire sur ma sant√©, mes traitements, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;N&amp;#39;essayez pas l&amp;#39;autom√©dication, parlez √† votre cheval, bla bla, ne faites pas l&amp;#39;imb√©cile sans avis m√©dical HUMAIN !&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Retour au sujet¬†:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Ce qui m‚Äôa pris environ deux mois pour avoir un semblant d‚Äôespoir th√©rapeutique semble d√©sormais ridicule par rapport aux r√©sultats d‚ÄôHorizon Beta. J&amp;#39;utilise principalement TTS pour √©viter de lire les r√©ponses des mod√®les, &amp;quot;surtout les longues&amp;quot;, parce que parfois c&amp;#39;est √©puisant et putain d&amp;#39;ennuyeux, alors j&amp;#39;√©coute ! Et quand on a l&amp;#39;habitude d&amp;#39;√©couter les r√©ponses du LLM, il y a des sch√©mas qui ne m&amp;#39;√©chappent pas.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Deux possibilit√©s¬†:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;*C&amp;#39;est pur OpenAI&lt;/p&gt;\n\n&lt;p&gt;ou&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;C&amp;#39;est un mod√®le distill√© GptXYZ&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Ce qui m&amp;#39;am√®ne √† confirmer mon ressenti pour la premi√®re possibilit√© :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;La th√©orie selon laquelle les mod√®les Quasar et Horizon sur OpenRouter sont de myst√©rieux prototypes OpenAI est bas√©e sur plusieurs indices et sp√©culations.&lt;/p&gt;\n\n&lt;p&gt;Quand j&amp;#39;ai d√©couvert Quasar-Alpha pour la premi√®re fois, en avril 2025 sur OpenRouter, il √©tait d√©crit comme un mod√®le pr√©liminaire avec une fen√™tre contextuelle de 1 million de tokens, optimis√© pour l&amp;#39;encodage, et des capacit√©s multimodales, bla bla...&lt;/p&gt;\n\n&lt;p&gt;Rien d&amp;#39;excitant pour moi √† l&amp;#39;√©poque.&lt;/p&gt;\n\n&lt;p&gt;Ensuite, il y avait des &amp;quot;preuves&amp;quot; sugg√©rant une connexion √† OpenAI¬†: le style de sortie utilise des caract√®res Unicode sp√©cifiques pour la ponctuation, un trait observ√© dans les mod√®les OpenAI ¬†:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://news.ycombinator.com/item?id=43640166\"&gt; https://news.ycombinator.com/item?id=43640166 &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;D&amp;#39;autres sp√©culent sur une origine Google en raison du nom ¬´ Quasar ¬ª √©voquant des objets cosmiques, semblables aux G√©meaux.&lt;/p&gt;\n\n&lt;p&gt;Ensuite Horizon Alpha et Beta, apparus plus r√©cemment, sont vus comme une suite potentielle.&lt;/p&gt;\n\n&lt;p&gt;Certaines sp√©culations le lient directement √† OpenAI, peut-√™tre en tant que version open source ou pr√©curseur de GPT-5, en raison de benchmarks dominants et d&amp;#39;un style de sortie rappelant OpenAI:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ossels.ai/openai-horizon-alpha-gpt-5-model/\"&gt; https://ossels.ai/openai-horizon-alpha-gpt-5-model/ &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Les utilisateurs ont signal√© qu&amp;#39;il code des applications enti√®res en une seule requ√™te, renfor√ßant ainsi l&amp;#39;id√©e d&amp;#39;un prototype avanc√©.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;J&amp;#39;ai essay√© moi-m√™me avec cette invite et il m&amp;#39;a fallu 5 √©changes pour obtenir un r√©sultat correct :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d\"&gt;https://preview.redd.it/ywuejlr1apgf1.png?width=1624&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361adad6190cb9e740de45ce670fa7b0bd50669d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Voici l&amp;#39;invite Neo Tetris HTML¬†:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Cr√©ez un fichier HTML autonome qui fournit un Tetris n√©o-r√©tro complet avec les sp√©cifications ci-dessous. Pas de biblioth√®ques externes, pas de liens CDN¬†: int√©grez tout (polices, CSS, JS, sons le cas √©ch√©ant) via la base 64 ou le code en ligne.\nGameplay de base ‚Ä¢ Classique 7 t√©trominos, rotation SRS, pi√®ce de maintien, aper√ßu de la pi√®ce fant√¥me. ‚Ä¢ Suppression de lignes avec explosion de particules¬†: lorsqu&amp;#39;une ligne est termin√©e, g√©n√©rez une explosion de particules lumineuses qui volent vers l&amp;#39;ext√©rieur et s&amp;#39;estompent. ‚Ä¢ ¬´¬†Onde de choc d&amp;#39;impact¬†¬ª √† chaque verrouillage¬†: un anneau de n√©on concentrique qui s&amp;#39;√©tend √† partir de la base de la pi√®ce et s&amp;#39;estompe. ‚Ä¢ Score, progression de niveau, 3 vies (vies perdues lorsque les blocs atteignent le sommet), file d&amp;#39;attente des 3 pi√®ces suivantes.\nContr√¥les ‚Ä¢ Bureau Fl√®ches gauche/droite : d√©placer Fl√®che vers le bas : soft-drop (acc√©l√©rer) ‚Äì afficher la trace de mouvement / la s√©quence de vitesse derri√®re la pi√®ce qui tombe Fl√®che vers le haut OU Espace : tourner dans le sens des aiguilles d&amp;#39;une montre Entr√©e : d√©marrer le jeu / confirmer dans les menus ‚Ä¢ Smartphone (toucher) Glisser vers la gauche/droite : d√©placer Glisser vers le bas : soft-drop (avec la m√™me trace) Appuyez sur les moiti√©s gauche/droite de l&amp;#39;√©cran : faire pivoter ‚Ä¢ Mettre en pause la superposition sur ¬´ P ¬ª.\nVisuels et animations ‚Ä¢ Effet d&amp;#39;√©cran CRT¬†: superposez un cadre en verre incurv√© subtil, des lignes de balayage, une l√©g√®re distorsion RVB et une vignette qui reste pendant la lecture. ‚Ä¢ Palette n√©on harmonieuse sur fond indigo profond (cyan, magenta, violet √©lectrique, citron vert). ‚Ä¢ √âcran titre avant le jeu¬†: ‚Äì Texte centr√© &amp;quot;PRESS ENTER&amp;quot; clignotant avec lueur. ‚Äì Derri√®re lui, un d√©fil√© sans fin de chaque t√©tromino tournant lentement en mode filaire 3D, avec des particules de filament tra√ßant leurs bords et s&amp;#39;√©loignant. ‚Äì Petit bloc affichant des astuces de contr√¥le¬†: Gauche/Droite ‚Äì D√©placer | Vers le bas ‚Äì Chute de vitesse | Haut/Espace ‚Äì Rotation | Entr√©e ‚Äì D√©marrer ‚Ä¢ Tableau de scores persistant (top 5) stock√© dans localStorage, affich√© sur l&amp;#39;√©cran titre et l&amp;#39;√©cran de fin de partie.\nAdaptatif ‚Ä¢ Disposition fluide¬†: le plateau de jeu s&amp;#39;adapte automatiquement aux fen√™tres d&amp;#39;affichage des ordinateurs de bureau et mobiles tout en pr√©servant les proportions. ‚Ä¢ Zones tactiles dimensionn√©es pour les pouces.\nTechnique ‚Ä¢ Rendu Canvas 2D, rendu d&amp;#39;image : pixellis√© pour des blocs nets. ‚Ä¢ Toutes les images cl√©s, CSS et JavaScript dans les balises &amp;lt;style&amp;gt; et &amp;lt;script&amp;gt;. ‚Ä¢ Aucune demande de r√©seau externe.\nImportant : les pi√®ces ne doivent pas tomber trop rapidement. Commencez lentement et augmentez progressivement la vitesse √† mesure que les jalons de score sont atteints, avec un niveau maximum de 100.\nRenvoie uniquement le code HTML complet.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Sp√©culons :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Quant √† la sp√©culation selon laquelle le prochain mod√®le ou √©tape serait la ¬´ singularit√© ¬ª, elle trouve son origine dans les d√©clarations de Sam Altman, qui a publi√© un article en juin 2025 intitul√© ¬´ La douce singularit√© ¬ª ( &lt;a href=\"https://blog.samaltman.com/the-gentle-singularity\"&gt; https://blog.samaltman.com/the-gentle-singularity &lt;/a&gt; ), affirmant que l&amp;#39;humanit√© a d√©pass√© l&amp;#39;horizon des √©v√©nements vers la superintelligence, avec des agents d&amp;#39;IA capables d&amp;#39;un v√©ritable travail cognitif comme d√®s 2025 et des id√©es innovantes en 2026.&lt;/p&gt;\n\n&lt;p&gt;Altman d√©crit un processus d&amp;#39;auto-am√©lioration r√©cursive, dans lequel l&amp;#39;IA assiste la recherche sur l&amp;#39;IA, conduisant √† une abondance d&amp;#39;intelligence et d&amp;#39;√©nergie¬†:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/\"&gt; https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/ &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Les pr√©dictions des experts, bas√©es sur 8 590 estimations, placent l‚ÄôAGI vers 2060 avec une probabilit√© de 50 %, mais des voix comme Ray Kurzweil visent 2029.&lt;/p&gt;\n\n&lt;p&gt;Mais OpenAI a √©galement r√©solu les probl√®mes de niveau or de l&amp;#39;OMI avec un mod√®le exp√©rimental g√©n√©ral, soutenant l&amp;#39;id√©e d&amp;#39;une progression rapide vers la singularit√©¬†:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/singularity/comments/1m3qutl/openai_achieved_imo_gold_with_experimental/\"&gt; https://www.reddit.com/r/singularity/comments/1m3qutl/openai_achieved_imo_gold_with_experimental/ &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5zcx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Ninja7526",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5zcx/quasar_horizon_singularity_diseases_theory/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5zcx/quasar_horizon_singularity_diseases_theory/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754181849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üöÄ I just open-sourced a fully working persistent memory system for AI assistants!\n\n\n\nüß† Features:\n\n\\- Real-time memory capture across apps (LM Studio, VS Code, etc.)\n\n\\- Semantic search via vector embeddings\n\n\\- Tool call logging for AI self-reflection\n\n\\- Cross-platform and fully tested\n\n\\- Open source and modular\n\n\n\nBuilt with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot ü§ù\n\n\n\nGitHub: [https://github.com/savantskie/persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)",
          "author_fullname": "t2_7qb4luzhc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mg5xlb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üöÄ I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt;\n\n&lt;p&gt;üß† Features:&lt;/p&gt;\n\n&lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt;\n\n&lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt;\n\n&lt;p&gt;- Cross-platform and fully tested&lt;/p&gt;\n\n&lt;p&gt;- Open source and modular&lt;/p&gt;\n\n&lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot ü§ù&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/savantskie/persistent-ai-memory\"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?auto=webp&amp;s=726283d31a9e66a5327266304a008d1eb52da1db",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b41ef9face9075e71937a4cd3b28923245b39a29",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6703186c20a684418ded0dc4abb9a6d490d7b023",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=860a8622d037c44a41b9ee50c96939060a76e917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ef5d63ac01ba46958961a63c5e9f43dabb1245",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=446a7e497c6b8549a8cbed43e99bc5f99e79c618",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1ab447d78f6edbe4f20129542abf92b51d827b1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5xlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Savantskie1",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754181701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I honestly don't know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...  \nIt's something I've never been clear about.  \nFor example, when I've used Deepseek, sometimes I feel that putting it into \"thinking\" mode doesn't add much, but I haven't noticed a clear pattern like \"for this type of question I use thinking mode, for this other type I don't.\"  \nCould someone clarify this for me?\n\nI'm thinking of downloading this model:  \n**Qwen3-30B-A3B-Instruct-2507** ... or **Qwen3-30B-A3B-Thinking-2507**\n\nThe Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?",
          "author_fullname": "t2_q2iij",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking or Instruct?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mg5scj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I honestly don&amp;#39;t know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...&lt;br/&gt;\nIt&amp;#39;s something I&amp;#39;ve never been clear about.&lt;br/&gt;\nFor example, when I&amp;#39;ve used Deepseek, sometimes I feel that putting it into &amp;quot;thinking&amp;quot; mode doesn&amp;#39;t add much, but I haven&amp;#39;t noticed a clear pattern like &amp;quot;for this type of question I use thinking mode, for this other type I don&amp;#39;t.&amp;quot;&lt;br/&gt;\nCould someone clarify this for me?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of downloading this model:&lt;br/&gt;\n&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; ... or &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg5scj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "9acca9",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754181257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.\n\nIf I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.\n\nSometimes, both sides are the same.  (E.g., \"Are you helpless or do you need my help?\")\n\nSometimes, they're so unbalanced it becomes a Mitch Hedberg-style question. (E.g., \"Have you ever tried sugar or PCP?\")\n\nSometimes, a very open-ended question is presented as either/or. (E.g., \"Is your favorite CSS color value #ff73c1 or #2141af?\" like those are the only two options.)\n\nI have found myself utterly unable to affect this behavior at all through the system prompt. I've tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with \"Don't...\" instructions) I've tried prompting it not to use \"either/or\" questions, \"A or B?\" questions, questions that limit the user's options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.\n\nAnd if I bring it up in chat, I get Qwen3's usual long obsequious apology (\"You're absolutely right, I'm sorry, I made assumptions and didn't respect your blah blah blah... I'll be sure to blah blah blah...\") and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird \"shell-shocked\" mode where it starts writing responses with three words per line that read like it's a frustrated beat poet.\n\nHave other people run into this? If so, are there good ways to combat it?\n\nThanks for any advice!\n",
          "author_fullname": "t2_7pimnskf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get Qwen 3 to stop asking terrible questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mg4lxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754177798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.&lt;/p&gt;\n\n&lt;p&gt;If I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, both sides are the same.  (E.g., &amp;quot;Are you helpless or do you need my help?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, they&amp;#39;re so unbalanced it becomes a Mitch Hedberg-style question. (E.g., &amp;quot;Have you ever tried sugar or PCP?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, a very open-ended question is presented as either/or. (E.g., &amp;quot;Is your favorite CSS color value #ff73c1 or #2141af?&amp;quot; like those are the only two options.)&lt;/p&gt;\n\n&lt;p&gt;I have found myself utterly unable to affect this behavior at all through the system prompt. I&amp;#39;ve tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with &amp;quot;Don&amp;#39;t...&amp;quot; instructions) I&amp;#39;ve tried prompting it not to use &amp;quot;either/or&amp;quot; questions, &amp;quot;A or B?&amp;quot; questions, questions that limit the user&amp;#39;s options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.&lt;/p&gt;\n\n&lt;p&gt;And if I bring it up in chat, I get Qwen3&amp;#39;s usual long obsequious apology (&amp;quot;You&amp;#39;re absolutely right, I&amp;#39;m sorry, I made assumptions and didn&amp;#39;t respect your blah blah blah... I&amp;#39;ll be sure to blah blah blah...&amp;quot;) and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird &amp;quot;shell-shocked&amp;quot; mode where it starts writing responses with three words per line that read like it&amp;#39;s a frustrated beat poet.&lt;/p&gt;\n\n&lt;p&gt;Have other people run into this? If so, are there good ways to combat it?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg4lxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TastesLikeOwlbear",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754177798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So many versions! I saw something about how the DWQ versions are best, but then obviously MLX \\*seems\\* like it would be best? And what quantization version?",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "64GB M1 Max, which GLM-4.5-Air?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg44ya",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176467,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So many versions! I saw something about how the DWQ versions are best, but then obviously MLX *seems* like it would be best? And what quantization version?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg44ya",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754176467,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? \n\n- Apple m4 pro chip\n- 14 core CPU, 20 core GPU\n- 48GB unified memory. \n- One TB SSD storage \n\nEventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.",
          "author_fullname": "t2_1ook1izxzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this set up sufficient?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg40u1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Apple m4 pro chip&lt;/li&gt;\n&lt;li&gt;14 core CPU, 20 core GPU&lt;/li&gt;\n&lt;li&gt;48GB unified memory. &lt;/li&gt;\n&lt;li&gt;One TB SSD storage &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Eventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg40u1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wild-Muffin9190",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754176144,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article: https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\n\nContext:\n\nThis insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.\n\nhttps://arxiv.org/abs/2506.21734\n\nA number of people have reproduced it if anyone is worried about that: https://x.com/VictorTaelin/status/1950512015899840768\nhttps://github.com/sapientinc/HRM/issues/12",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM solved thinking more than current \"thinking\" models (this needs more hype)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3i48",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article: &lt;a href=\"https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It&amp;#39;s seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href=\"https://x.com/VictorTaelin/status/1950512015899840768\"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt;\n&lt;a href=\"https://github.com/sapientinc/HRM/issues/12\"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?auto=webp&amp;s=3649785728b7c459cdac6d4444cd5a58a31ff8a2",
                  "width": 994,
                  "height": 610
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a32ed96622f166ae158780da40b7af8d982e72f9",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2621c7644d6e96ab551888e43049a8b94e227fc5",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da54eeb0e463cf73d8f5292e9b099e338494438",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1caf030ca1b3e5b1df5e052afe93497a9bb543",
                    "width": 640,
                    "height": 392
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88919a9c5b50d0a4d76e2c2bb7e87bb3c4eed852",
                    "width": 960,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg3i48",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754174679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nEvery app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.\n\nHere's what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.\n\nKnow any other ?\n\nThanks",
          "author_fullname": "t2_cbs8s4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Easily installable GUI for ML-powered audio transcription on AMD GPU ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3g2e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Every app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.&lt;/p&gt;\n\n&lt;p&gt;Know any other ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg3g2e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaKi_87",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754174523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.\n\nThe take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.\n\nWe need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3d62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt;\n\n&lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt;\n\n&lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg3d62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754174302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don't see an official way and don't want to use LM Studio. What options do I have here? \n\n============================\n\nSecond, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.\n\nIs there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people running an MLX-compatible OpenAI API server locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg26g0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754171064,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don&amp;#39;t see an official way and don&amp;#39;t want to use LM Studio. What options do I have here? &lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;Second, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.&lt;/p&gt;\n\n&lt;p&gt;Is there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg26g0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754171064,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt's agent mode, but haven't found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?",
          "author_fullname": "t2_w61xk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Closest Local Version of OpenAI's Agent Mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg24nd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754170932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt&amp;#39;s agent mode, but haven&amp;#39;t found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg24nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RabbitEater2",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754170932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_prnin4bw1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GNOME AI Virtual Assistant \"Newelle\" Reaches Version 1.0 Milestone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1evr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754169081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg1evr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FastDecode1",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/",
          "stickied": false,
          "url": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "subreddit_subscribers": 509052,
          "created_utc": 1754169081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sam Altman promised imminent release of open source/weight models . It seems we haven‚Äôt heard anything new in the past few weeks, have we?",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news about the open source models that OpenAI promised to release ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1e80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754169032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven‚Äôt heard anything new in the past few weeks, have we?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg1e80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754169032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).\n\nAiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?\n\n\n16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).\n\n24 GB (go for size - Radeon 7900 - can get new for 800-1000)\n\n24 GB (3090 - refurbished around 1000, new closet to 1500)\n\nPrimary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?\n\nWhat would you do, and why? Or - where can I read up to better understand my options? Threads like this (https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/) make me think offloading to CPU/RAM doesn't need to absolutely throttle throughput when running locally...",
          "author_fullname": "t2_2orcezep",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guide for GPU Purchase for Local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1cg5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754168901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).&lt;/p&gt;\n\n&lt;p&gt;Aiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?&lt;/p&gt;\n\n&lt;p&gt;16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).&lt;/p&gt;\n\n&lt;p&gt;24 GB (go for size - Radeon 7900 - can get new for 800-1000)&lt;/p&gt;\n\n&lt;p&gt;24 GB (3090 - refurbished around 1000, new closet to 1500)&lt;/p&gt;\n\n&lt;p&gt;Primary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?&lt;/p&gt;\n\n&lt;p&gt;What would you do, and why? Or - where can I read up to better understand my options? Threads like this (&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/&lt;/a&gt;) make me think offloading to CPU/RAM doesn&amp;#39;t need to absolutely throttle throughput when running locally...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg1cg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanManPanther",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754168901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn't quite process them correctly.\n    \nThis is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.\n    \nSpecifically, this fixes the following:\n    \n- Chat dates are maintained\n- Chat hierarchy is preserved\n- Empty conversations are skipped\n- Parent-child relationships are maintained\n    \nIn addition, it will skip malformed conversations and try to import each chat only once using a `imported.json` file.\n    \nYou can export your ChatGPT conversations by going to Settings ‚Üí Data controls ‚Üí Export data ‚Üí Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to `~/chatgpt/chatgpt-export.json`.\n    \nI recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your `webui.db` file.\n    \nAfter importing, you can view your conversations in Open-WebUI by going to Settings ‚Üí Chats ‚Üí Import and selecting the converted JSON file.\n    \nI like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the `imported.json` file correctly.\n    \nThis works for me, and I hope it works for you too! PRs and issues are welcome.",
          "author_fullname": "t2_44be3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convert your ChatGTP exported conversations to something that Open-WebUI can import",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg12k4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7aaf741530697098b43e0167db0c1c5b5305d19b",
          "edited": 1754169308,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754168175,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn&amp;#39;t quite process them correctly.&lt;/p&gt;\n\n&lt;p&gt;This is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this fixes the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Chat dates are maintained&lt;/li&gt;\n&lt;li&gt;Chat hierarchy is preserved&lt;/li&gt;\n&lt;li&gt;Empty conversations are skipped&lt;/li&gt;\n&lt;li&gt;Parent-child relationships are maintained&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In addition, it will skip malformed conversations and try to import each chat only once using a &lt;code&gt;imported.json&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;You can export your ChatGPT conversations by going to Settings ‚Üí Data controls ‚Üí Export data ‚Üí Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to &lt;code&gt;~/chatgpt/chatgpt-export.json&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your &lt;code&gt;webui.db&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;After importing, you can view your conversations in Open-WebUI by going to Settings ‚Üí Chats ‚Üí Import and selecting the converted JSON file.&lt;/p&gt;\n\n&lt;p&gt;I like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the &lt;code&gt;imported.json&lt;/code&gt; file correctly.&lt;/p&gt;\n\n&lt;p&gt;This works for me, and I hope it works for you too! PRs and issues are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?auto=webp&amp;s=e22d770e4c02c756fc73d7d90a385e5b112f15b5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed1ab2121b7badab4580a484ba08cc844d5e16e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9afd23fae3b50eabe8826fbaa8cb3d0f9a3e2c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ddddd80938d359fbb1d0dd7d1abbf05f3a86de7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17fc3d36981f925574075a8743bee7f8626049a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=998b355322b3740f1ade9410e49eaa05da91b8fc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0e7de199e0866090536334a1fd28c6c4dff2939",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg12k4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "scubanarc",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/",
          "stickied": false,
          "url": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754168175,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!",
          "author_fullname": "t2_13hqmc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have built my own, poor mans Lovable - testing out Cerebras AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0uw8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=5b85dbf1e37f569740ca84504972c21bb5463678",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754167627,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/restyler/poor-mans-lovable",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?auto=webp&amp;s=2d2f3ab9d396d0b9ffc59352c0ad43c2be2c6d47",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cf2bdf2998419d6d274f237ffbcc2b5ad34d663",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e31d2c9aed096970a6183172f8674918abd3da69",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c2a06ca5a7606d5a5f78e2a727ce72eb1898757",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=642596b8bbafb58939fce5813d2fe60c81392d6f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc7356ab40d5babbd50137dd1b88c3efc5a874a3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=496d2912020b57e4bdcbc111110a288bf2bd8366",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg0uw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "superjet1",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0uw8/i_have_built_my_own_poor_mans_lovable_testing_out/",
          "stickied": false,
          "url": "https://github.com/restyler/poor-mans-lovable",
          "subreddit_subscribers": 509052,
          "created_utc": 1754167627,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named [UniTok](https://arxiv.org/abs/2502.20321) and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.\n\nThey say their method is better but i am not sure, when i tried their code it didn't give high quality construction results but they said that they didn't train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don't know.\n\nWhat is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)",
          "author_fullname": "t2_1urjd1hc7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How the best image generation models work from the inside ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0ur7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167616,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named &lt;a href=\"https://arxiv.org/abs/2502.20321\"&gt;UniTok&lt;/a&gt; and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.&lt;/p&gt;\n\n&lt;p&gt;They say their method is better but i am not sure, when i tried their code it didn&amp;#39;t give high quality construction results but they said that they didn&amp;#39;t train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;What is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0ur7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Severe-Awareness829",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754167616,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Rank|Model|Score|95% CI|Votes|Company|License|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|gemini 2.5 pro|1474|¬±8|7,178|Goog||\n|1|qwen3 235b a22b instruct 2507|1464|¬±18|1,089|Alibaba|Apache|\n|2|o3 2025 04 16|1445|¬±7|9,877|Closed AI||\n|2|grok 4 2502|1442|¬±10|4,063|xAI||\n|2|qwen3 235b a22b thinking 2507|1442|¬±20|917|Alibaba|Apache|\n|2|grok 3 preview 02 24|1439|¬±7|7,588|xAI||\n|3|deepseek r1 0528|1436|¬±9|4,851|DeepSeek|MIT|\n\nStyle control removed.\nhttps://lmarena.ai/leaderboard/text/coding",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alibaba not doing to bad at coding according to lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0sbe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167443,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Rank&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Score&lt;/th&gt;\n&lt;th align=\"left\"&gt;95% CI&lt;/th&gt;\n&lt;th align=\"left\"&gt;Votes&lt;/th&gt;\n&lt;th align=\"left\"&gt;Company&lt;/th&gt;\n&lt;th align=\"left\"&gt;License&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;gemini 2.5 pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;1474&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±8&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,178&lt;/td&gt;\n&lt;td align=\"left\"&gt;Goog&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1464&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±18&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,089&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;o3 2025 04 16&lt;/td&gt;\n&lt;td align=\"left\"&gt;1445&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;9,877&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 4 2502&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±10&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,063&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±20&lt;/td&gt;\n&lt;td align=\"left\"&gt;917&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 3 preview 02 24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1439&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,588&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;deepseek r1 0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;1436&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,851&lt;/td&gt;\n&lt;td align=\"left\"&gt;DeepSeek&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Style control removed.\n&lt;a href=\"https://lmarena.ai/leaderboard/text/coding\"&gt;https://lmarena.ai/leaderboard/text/coding&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0sbe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754167443,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys,\n\nI am actually running GLM-4.5-Air with vllm (4x3090) and even if it's quite early I'm quite impressed the model isn't \"lost\" and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.\n\nWhat is your experience actually? Any tip is wellcome\n\nFor info, I use [AWQ](https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ) with the latest (nightly) version of vllm with following cmd:\n\n`vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx`\n\nThen [claude-code-router](https://github.com/musistudio/claude-code-router) with following config:\n\n`{`\n\n`\"LOG\": true,`\n\n`\"Providers\": [`\n\n`{`\n\n`\"name\": \"openai\",`\n\n`\"api_base_url\": \"http://localhost:8123/v1/chat/completions\",`\n\n`\"api_key\": \"xxxx\",`\n\n`\"models\": [\"cpatonn/GLM-4.5-Air-AWQ\"]`\n\n`}`\n\n`],`\n\n`\"Router\": {`\n\n`\"default\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"background\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"think\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContext\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContextThreshold\": 64000,`\n\n`\"webSearch\": \"openai,cpatonn/GLM-4.5-Air-AWQ\"`\n\n`}`\n\n`}`",
          "author_fullname": "t2_udr659irv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Experience with GLM-4.5-Air + claude code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzzt4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754166182,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754165375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am actually running GLM-4.5-Air with vllm (4x3090) and even if it&amp;#39;s quite early I&amp;#39;m quite impressed the model isn&amp;#39;t &amp;quot;lost&amp;quot; and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.&lt;/p&gt;\n\n&lt;p&gt;What is your experience actually? Any tip is wellcome&lt;/p&gt;\n\n&lt;p&gt;For info, I use &lt;a href=\"https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ\"&gt;AWQ&lt;/a&gt; with the latest (nightly) version of vllm with following cmd:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Then &lt;a href=\"https://github.com/musistudio/claude-code-router\"&gt;claude-code-router&lt;/a&gt; with following config:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;LOG&amp;quot;: true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Providers&amp;quot;: [&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;name&amp;quot;: &amp;quot;openai&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_base_url&amp;quot;: &amp;quot;http://localhost:8123/v1/chat/completions&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_key&amp;quot;: &amp;quot;xxxx&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;models&amp;quot;: [&amp;quot;cpatonn/GLM-4.5-Air-AWQ&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;],&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Router&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;default&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;background&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;think&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContext&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContextThreshold&amp;quot;: 64000,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;webSearch&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?auto=webp&amp;s=1b31f315329427daf17c78d7a27488cb4a4815b1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44ceba9879a0b6cc35b31f44bfe9d55af546e830",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2b1cffdeb754044dca444f9940bf1f77e3c1130",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acf9e48a54c3292dcada1d3027879b052535da57",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4127f2600eb35191a3b12e7e09dc0154803bb2d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b52fefc4e7d9396d19c12d4f2875b5d7cd950a56",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9fb7f9f2f97040afaa6980f1c6f3ed5ece7f4490",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfzzt4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Leflakk",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754165375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nWhen our radiology department rejected another batch of low-resolution X-rays because they couldn't see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.\n\nOn that day in the hospital basement, I decided somehow to fix this problem that's now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.\n\nThe Problem I Was Really Solving\n\nAs a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.\n\nWhen my friend completed research on X-ray super resolution using [GANs](https://www.youtube.com/watch?v=BVWn8JfeekY), I saw an opportunity to turn this research paper into a practical solution.\n\nThe existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.\n\nThe breaking point wasn't just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.\n\nThe Technical Breakthrough\n\nI built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.\n\nThe Core Technology (from the research):\n\nPatch-based processing that preserves fine-grained structural details\n\nGenerative adversarial network trained on 3,000 clinical X-ray images\n\nMoving average filters with random kernel sizes (1-40 pixels) for robustness\n\nFour-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses\n\nWhat I Built Around It:\n\nSimple drag-and-drop web interface for uploading X-ray DICOM files\n\nCloud processing pipeline using AWS GPU instances\n\nHIPAA-compliant storage and transmission\n\nIntegration with existing PACS (Picture Archiving and Communication Systems)\n\nThe first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.\n\nThe Growth Numbers (Early Traction)\n\nIn first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)\n\nCurrent Unit Economics:\n\nMRR: $1,450\n\nCloud compute costs: $520/month (AWS GPU instances)\n\nInfrastructure &amp; compliance: $180/month\n\nGross profit: $750/month (52% margin)\n\nAverage customer acquisition cost: $95 (mostly referrals and medical imaging forums).\n\nWhat I actually understood is\n\nBuilding on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.\n\nMedical software is about trust and usability. Radiologists don't want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.\n\nEarly traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.\n\nHospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.\n\nTechnical Reality\n\nUnlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.\n\nThe discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.\n\nWe validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.\n\nThe Real Lesson\n\nThis business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.\n\nThe opportunity wasn't in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.",
          "author_fullname": "t2_1tts756whd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I Built Medical AI by Solving the Radiation Dose Problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzu3d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754164960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When our radiology department rejected another batch of low-resolution X-rays because they couldn&amp;#39;t see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.&lt;/p&gt;\n\n&lt;p&gt;On that day in the hospital basement, I decided somehow to fix this problem that&amp;#39;s now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.&lt;/p&gt;\n\n&lt;p&gt;The Problem I Was Really Solving&lt;/p&gt;\n\n&lt;p&gt;As a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.&lt;/p&gt;\n\n&lt;p&gt;When my friend completed research on X-ray super resolution using &lt;a href=\"https://www.youtube.com/watch?v=BVWn8JfeekY\"&gt;GANs&lt;/a&gt;, I saw an opportunity to turn this research paper into a practical solution.&lt;/p&gt;\n\n&lt;p&gt;The existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.&lt;/p&gt;\n\n&lt;p&gt;The breaking point wasn&amp;#39;t just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The Technical Breakthrough&lt;/p&gt;\n\n&lt;p&gt;I built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.&lt;/p&gt;\n\n&lt;p&gt;The Core Technology (from the research):&lt;/p&gt;\n\n&lt;p&gt;Patch-based processing that preserves fine-grained structural details&lt;/p&gt;\n\n&lt;p&gt;Generative adversarial network trained on 3,000 clinical X-ray images&lt;/p&gt;\n\n&lt;p&gt;Moving average filters with random kernel sizes (1-40 pixels) for robustness&lt;/p&gt;\n\n&lt;p&gt;Four-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses&lt;/p&gt;\n\n&lt;p&gt;What I Built Around It:&lt;/p&gt;\n\n&lt;p&gt;Simple drag-and-drop web interface for uploading X-ray DICOM files&lt;/p&gt;\n\n&lt;p&gt;Cloud processing pipeline using AWS GPU instances&lt;/p&gt;\n\n&lt;p&gt;HIPAA-compliant storage and transmission&lt;/p&gt;\n\n&lt;p&gt;Integration with existing PACS (Picture Archiving and Communication Systems)&lt;/p&gt;\n\n&lt;p&gt;The first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.&lt;/p&gt;\n\n&lt;p&gt;The Growth Numbers (Early Traction)&lt;/p&gt;\n\n&lt;p&gt;In first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)&lt;/p&gt;\n\n&lt;p&gt;Current Unit Economics:&lt;/p&gt;\n\n&lt;p&gt;MRR: $1,450&lt;/p&gt;\n\n&lt;p&gt;Cloud compute costs: $520/month (AWS GPU instances)&lt;/p&gt;\n\n&lt;p&gt;Infrastructure &amp;amp; compliance: $180/month&lt;/p&gt;\n\n&lt;p&gt;Gross profit: $750/month (52% margin)&lt;/p&gt;\n\n&lt;p&gt;Average customer acquisition cost: $95 (mostly referrals and medical imaging forums).&lt;/p&gt;\n\n&lt;p&gt;What I actually understood is&lt;/p&gt;\n\n&lt;p&gt;Building on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.&lt;/p&gt;\n\n&lt;p&gt;Medical software is about trust and usability. Radiologists don&amp;#39;t want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.&lt;/p&gt;\n\n&lt;p&gt;Early traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.&lt;/p&gt;\n\n&lt;p&gt;Hospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.&lt;/p&gt;\n\n&lt;p&gt;Technical Reality&lt;/p&gt;\n\n&lt;p&gt;Unlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.&lt;/p&gt;\n\n&lt;p&gt;The discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.&lt;/p&gt;\n\n&lt;p&gt;We validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.&lt;/p&gt;\n\n&lt;p&gt;The Real Lesson&lt;/p&gt;\n\n&lt;p&gt;This business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The opportunity wasn&amp;#39;t in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?auto=webp&amp;s=ddcf97793b8059e1ac6f8e5f42c0fb141ac1f06b",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecfe2ffa2015cbfbedc8dfb6cbcd4cf0d32f0bee",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d64b11c760cf9d689a103b871b38eb77933c0ed",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=085b7fe67421b9ba136cbbcbf7988771acb578f2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfzu3d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Perception-9919",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754164960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just launched GridLLM (https://github.com/GridLLM/GridLLM), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. \n\nWould love any feedback on the project!\n",
          "author_fullname": "t2_g5wvmwck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-Source Project for Distributed Inference Management",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzg8h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754163962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just launched GridLLM (&lt;a href=\"https://github.com/GridLLM/GridLLM\"&gt;https://github.com/GridLLM/GridLLM&lt;/a&gt;), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. &lt;/p&gt;\n\n&lt;p&gt;Would love any feedback on the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?auto=webp&amp;s=5262c21723d990876d1321b058c80d37499608f4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a422ad10d8ba96d4fcb866b6ade60210133fea91",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92141835b6803ca57fbce4c55c4e89ef337c1d98",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc5e385866a70deb5e03659dfedb836c11a84a0c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00d881194968dc37ee1e6d0224b4065c46e497f3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f6e74f447093226b82fd0f39651b6d915ce7b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fe17371d5951c9213d509050b853dbafa2832f8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzg8h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Choice_Nature9658",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754163962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This set of experiments were conducted about half a year ago and we are suggested to share them to the community. Summary of the experiments\n\n(1) Lihua world dataset: conversation data, all texts\n\n(2) In previous studies, Graph RAG (and variants) showed advantages over \"na√Øve\" RAG.\n\n(3) Using OpenAI RAG API (File Search), the accuracy is substantially higher than graph RAG &amp; variants\n\n(4) Using the same embeddings,¬†[https://chat.vecml.com](https://chat.vecml.com/)¬†produces consistently better accuracies than OpenAI RAG API (File Search).\n\n(5) More interestingly,¬†[https://chat.vecml.com/](https://chat.vecml.com/)¬†is substantially (550x) faster than OpenAI RAG (File Search)\n\n(6) Additional experiments on different embeddings are also provided.\n\nNote that Lihua world dataset is purely text. In practice, the documents are in all sorts of formats: PDFs, OCR, Excel, HTML, DocX, PPTX, WPS, and more.¬†[https://chat.vecml.com/](https://chat.vecml.com/)¬†is able to handle documents of many different formats and is capable of dealing with multi-modal RAG.",
          "author_fullname": "t2_x7ezv9dmy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "OpenAI RAG API (File Search): an experimental study",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "snfxiwiotngf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 48,
                  "x": 108,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecc1cf0a6bca5c4e3328477b7026c95b8f03eb70"
                },
                {
                  "y": 96,
                  "x": 216,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=914115ad10b3b2959058b4fd792112b6f9b05f24"
                },
                {
                  "y": 143,
                  "x": 320,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73850eb9a8677a5bea2d313bccdaa792685357ff"
                },
                {
                  "y": 286,
                  "x": 640,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=244d9f4891b0943eb383c852c0e469dbaf82ffc5"
                },
                {
                  "y": 429,
                  "x": 960,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cfa0e69bb3aa9572c84bf191e4bb5ef839f6f18"
                },
                {
                  "y": 483,
                  "x": 1080,
                  "u": "https://preview.redd.it/snfxiwiotngf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58d6f1634978dfaf2f93ccc8b49cacf69fc18ded"
                }
              ],
              "s": {
                "y": 598,
                "x": 1336,
                "u": "https://preview.redd.it/snfxiwiotngf1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=69a448d93ef8f0a3be6f01eec30503520d18f053"
              },
              "id": "snfxiwiotngf1"
            },
            "kdrqcvintngf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 130,
                  "x": 108,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f4114f77943df8660e61b3e333ac3cd0aabebfd"
                },
                {
                  "y": 260,
                  "x": 216,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8274df2d3ccb3961cf1d9322752fd6967fd017ed"
                },
                {
                  "y": 386,
                  "x": 320,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20bec91b1fa2c9b44e0434ddd20d6eb31063312"
                },
                {
                  "y": 772,
                  "x": 640,
                  "u": "https://preview.redd.it/kdrqcvintngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6177b1bb06a5fd33588da0d54116a9f1e75d1690"
                }
              ],
              "s": {
                "y": 881,
                "x": 730,
                "u": "https://preview.redd.it/kdrqcvintngf1.png?width=730&amp;format=png&amp;auto=webp&amp;s=9244cca0f81ba098aa6e1aaea3a59ac371ac8b0b"
              },
              "id": "kdrqcvintngf1"
            }
          },
          "name": "t3_1mfzezz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "kdrqcvintngf1",
                "id": 719740416
              },
              {
                "media_id": "snfxiwiotngf1",
                "id": 719740417
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/fwt17fYbAFMoObf3HbNFT-k87BCWFCICFS0WC914Tx8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754163875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This set of experiments were conducted about half a year ago and we are suggested to share them to the community. Summary of the experiments&lt;/p&gt;\n\n&lt;p&gt;(1) Lihua world dataset: conversation data, all texts&lt;/p&gt;\n\n&lt;p&gt;(2) In previous studies, Graph RAG (and variants) showed advantages over &amp;quot;na√Øve&amp;quot; RAG.&lt;/p&gt;\n\n&lt;p&gt;(3) Using OpenAI RAG API (File Search), the accuracy is substantially higher than graph RAG &amp;amp; variants&lt;/p&gt;\n\n&lt;p&gt;(4) Using the same embeddings,¬†&lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com&lt;/a&gt;¬†produces consistently better accuracies than OpenAI RAG API (File Search).&lt;/p&gt;\n\n&lt;p&gt;(5) More interestingly,¬†&lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com/&lt;/a&gt;¬†is substantially (550x) faster than OpenAI RAG (File Search)&lt;/p&gt;\n\n&lt;p&gt;(6) Additional experiments on different embeddings are also provided.&lt;/p&gt;\n\n&lt;p&gt;Note that Lihua world dataset is purely text. In practice, the documents are in all sorts of formats: PDFs, OCR, Excel, HTML, DocX, PPTX, WPS, and more.¬†&lt;a href=\"https://chat.vecml.com/\"&gt;https://chat.vecml.com/&lt;/a&gt;¬†is able to handle documents of many different formats and is capable of dealing with multi-modal RAG.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mfzezz",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzezz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DueKitchen3102",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzezz/openai_rag_api_file_search_an_experimental_study/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mfzezz",
          "subreddit_subscribers": 509052,
          "created_utc": 1754163875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it‚Äôs just my system",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox tts on amd",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfz1k2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754162909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it‚Äôs just my system&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfz1k2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754162909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are local llms fast and powerful enough to do analysis on movies in real time?\n\nSay you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?\n\nIf not today, then when will it be possible to do that?",
          "author_fullname": "t2_26u5g058",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LocalLLM for movies",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy924",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are local llms fast and powerful enough to do analysis on movies in real time?&lt;/p&gt;\n\n&lt;p&gt;Say you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?&lt;/p&gt;\n\n&lt;p&gt;If not today, then when will it be possible to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfy924",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImaginaryRea1ity",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754160893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'll need to use free tools. I am looking to make a video with this content. How do I do that? What tools should I use? How do i format this information to be processed by an AI? \n\n\\[Begin\\]\n\n The Globe wants you to believe everything opposite of physics:  \n\n1)  Heliocentrism teaches large bodies of liquid water curves into a ball. Physics says water lays flat and always seeks it's level, (thanks to the physics Law - Hydrostatic Equilibrium)\n\n2) Heliocentrism teaches we have a Big Bang Creation Story where everything spontaneously evolved from nothingness to what we have today.  Physics shows us this idea violates the 1st law of thermodynamics. \n\n3) Heliocentrism tells us Gravity is mass attracting mass. Physics shows us gas which is physical matter with mass does Not obey any silly idea of gravity. Gas always expands due to entropy to fill an available volume until equalization occurs. (Thanks to the 2nd law of thermodynamics)  \n\n4) Heliocentrism also teaches Gravity is Einstein's Gravitational Accretion where gas coalesces on itself. - (That violates the 2nd law of thermodynamics.) - \n\n5) Heliocentrism teaches gas forms a sphere in a vacuum. (what you call atmosphere) Again,  Gas always expands due to entropy to fill an available volume until equalization occurs. It cannot form a sphere in a vacuum Ever! (Again thanks to the 2nd Law of Thermodynamics.)\n\nI just gave you 5 examples (or to the untrained in science and physics, Paradoxes) how the Globe Story is purposefully deceptive because it doesn't align with actual physics and science facts.\n\nYou can learn these physics laws yourself with a study of thermodynamics at Khan Academy: The Laws of Thermodynamics and The Behavior of Gas at Chem Libre Text. \n\nNow if you think I'm Wrong then Demonstrate the claims! - You see your explanation is only good if you can Back it with Actual Physics Demonstrations. Demonstrate gas forming a sphere in a vacuum that then Fails to fully expand due to entropy until equalization occurs. (what you call Atmosphere) - Demonstrate large standing bodies of water Failing to seek their own level, Failing to lay flat and Failing to lay Horizontal. - These things Cannot be done thanks to the 2nd law of thermodynamics and hydrostatic equilibrium.\n\nLiquid water covers 70 % of Earths surface. Physics properties of liquid water (Hydrostatic Equilibrium) show it always seeks it's own level, lays flat and horizontal. Nothing, that is 70% of anything that seeks it's own level, lays flat and horizontal can Ever Be a Sphere! That's an Impossible Ratio! - Your Earth Curvature is Impossible in Physics and in Math!  \n\\[End\\]\n\nHow do make that video?  I don't know anything about AI but it uses something they choose to call prompts. That doesn't help me. ",
          "author_fullname": "t2_v7hacf3s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get this information into an AI to make a video?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy6vo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160738,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll need to use free tools. I am looking to make a video with this content. How do I do that? What tools should I use? How do i format this information to be processed by an AI? &lt;/p&gt;\n\n&lt;p&gt;[Begin]&lt;/p&gt;\n\n&lt;p&gt;The Globe wants you to believe everything opposite of physics:  &lt;/p&gt;\n\n&lt;p&gt;1)  Heliocentrism teaches large bodies of liquid water curves into a ball. Physics says water lays flat and always seeks it&amp;#39;s level, (thanks to the physics Law - Hydrostatic Equilibrium)&lt;/p&gt;\n\n&lt;p&gt;2) Heliocentrism teaches we have a Big Bang Creation Story where everything spontaneously evolved from nothingness to what we have today.  Physics shows us this idea violates the 1st law of thermodynamics. &lt;/p&gt;\n\n&lt;p&gt;3) Heliocentrism tells us Gravity is mass attracting mass. Physics shows us gas which is physical matter with mass does Not obey any silly idea of gravity. Gas always expands due to entropy to fill an available volume until equalization occurs. (Thanks to the 2nd law of thermodynamics)  &lt;/p&gt;\n\n&lt;p&gt;4) Heliocentrism also teaches Gravity is Einstein&amp;#39;s Gravitational Accretion where gas coalesces on itself. - (That violates the 2nd law of thermodynamics.) - &lt;/p&gt;\n\n&lt;p&gt;5) Heliocentrism teaches gas forms a sphere in a vacuum. (what you call atmosphere) Again,  Gas always expands due to entropy to fill an available volume until equalization occurs. It cannot form a sphere in a vacuum Ever! (Again thanks to the 2nd Law of Thermodynamics.)&lt;/p&gt;\n\n&lt;p&gt;I just gave you 5 examples (or to the untrained in science and physics, Paradoxes) how the Globe Story is purposefully deceptive because it doesn&amp;#39;t align with actual physics and science facts.&lt;/p&gt;\n\n&lt;p&gt;You can learn these physics laws yourself with a study of thermodynamics at Khan Academy: The Laws of Thermodynamics and The Behavior of Gas at Chem Libre Text. &lt;/p&gt;\n\n&lt;p&gt;Now if you think I&amp;#39;m Wrong then Demonstrate the claims! - You see your explanation is only good if you can Back it with Actual Physics Demonstrations. Demonstrate gas forming a sphere in a vacuum that then Fails to fully expand due to entropy until equalization occurs. (what you call Atmosphere) - Demonstrate large standing bodies of water Failing to seek their own level, Failing to lay flat and Failing to lay Horizontal. - These things Cannot be done thanks to the 2nd law of thermodynamics and hydrostatic equilibrium.&lt;/p&gt;\n\n&lt;p&gt;Liquid water covers 70 % of Earths surface. Physics properties of liquid water (Hydrostatic Equilibrium) show it always seeks it&amp;#39;s own level, lays flat and horizontal. Nothing, that is 70% of anything that seeks it&amp;#39;s own level, lays flat and horizontal can Ever Be a Sphere! That&amp;#39;s an Impossible Ratio! - Your Earth Curvature is Impossible in Physics and in Math!&lt;br/&gt;\n[End]&lt;/p&gt;\n\n&lt;p&gt;How do make that video?  I don&amp;#39;t know anything about AI but it uses something they choose to call prompts. That doesn&amp;#39;t help me. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy6vo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DivergentDroid1",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy6vo/how_do_i_get_this_information_into_an_ai_to_make/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy6vo/how_do_i_get_this_information_into_an_ai_to_make/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754160738,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm developing a product that uses AI, but it's entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? ",
          "author_fullname": "t2_1u3u0ca58k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dutch LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy5qs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m developing a product that uses AI, but it&amp;#39;s entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy5qs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Bath-7378",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754160656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or is it unlimited on [chat.qwen.ai](http://chat.qwen.ai) ? ",
          "author_fullname": "t2_14c4jctjk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any limits on Deep Research mode on Qwen Chat?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxjd5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754159065,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or is it unlimited on &lt;a href=\"http://chat.qwen.ai\"&gt;chat.qwen.ai&lt;/a&gt; ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfxjd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sostrene_Blue",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxjd5/is_there_any_limits_on_deep_research_mode_on_qwen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxjd5/is_there_any_limits_on_deep_research_mode_on_qwen/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754159065,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just bought a new PC, it‚Äôs not primarily for AI but I wanna try out llms. I‚Äôm not too familiar about the different models, so I‚Äôd appreciate if someone could provide recommendations.\n\nPc specs: 5070 Ti 16gb + i7 14700 32 gb ddr5 6000 MHz.",
          "author_fullname": "t2_17ltlw5q9t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any Open source LLM‚Äôs better than free tier of ChatGPT(4o and 4o mini)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxdlg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a new PC, it‚Äôs not primarily for AI but I wanna try out llms. I‚Äôm not too familiar about the different models, so I‚Äôd appreciate if someone could provide recommendations.&lt;/p&gt;\n\n&lt;p&gt;Pc specs: 5070 Ti 16gb + i7 14700 32 gb ddr5 6000 MHz.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfxdlg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Championship7986",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxdlg/are_there_any_open_source_llms_better_than_free/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxdlg/are_there_any_open_source_llms_better_than_free/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754158656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just¬†shipped something¬†I'm really¬†excited about!¬†üöÄ\nI was¬†scrolling through my feed and saw¬†Sebastian Raschka, PhD 's incredible¬†Qwen3 MoE implementation¬†in PyTorch. The educational¬†clarity¬†of¬†his code¬†just¬†blew me¬†away - especially¬†how he broke¬†down the¬†Mixture¬†of Experts architecture¬†in¬†his¬†LLMs-from-scratch repo.\nThat¬†got¬†me thinking... what if I could bring¬†this¬†to¬†pure¬†C?¬†ü§î\nInspired by Andrej Karpathy's legendary¬†llama2.c approach¬†(seriously, if¬†you haven't seen¬†it, check¬†it out), I decided to take¬†on the challenge of implementing¬†Qwen3's¬†30B parameter¬†model with¬†128 experts¬†in a¬†single C file.\nThe result?¬†Qwen_MOE_C¬†- a complete¬†inference engine that:\n‚úÖ Handles¬†sparse MoE computation¬†(only¬†8 out¬†of 128 experts¬†active)\n‚úÖ Supports¬†Grouped Query Attention with¬†proper head ratios\n‚úÖ Uses¬†memory¬†mapping¬†for efficiency (~30GB¬†models)\n‚úÖ Zero external¬†dependencies (just¬†libc¬†+ libm)\nThe beauty¬†of¬†this approach is the¬†same as llama2.c¬†- you can understand every¬†line, it's hackable, and it runs¬†anywhere C¬†runs. No¬†frameworks, no dependencies, just pure computational¬†transparency.\nHuge thanks to Sebastian¬†Raschka for the¬†reference implementation and educational¬†materials, and to Andrej¬†Karpathy for¬†showing us that¬†simplicity is¬†the ultimate sophistication in¬†ML systems.\nSometimes¬†the best way¬†to truly understand something is to build it from scratch.¬†üõ†Ô∏è\nLink¬†to the project: \nhttps://github.com/h9-tec/Qwen_MOE_C",
          "author_fullname": "t2_59165ick",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen moe in C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxas1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158458,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just¬†shipped something¬†I&amp;#39;m really¬†excited about!¬†üöÄ\nI was¬†scrolling through my feed and saw¬†Sebastian Raschka, PhD &amp;#39;s incredible¬†Qwen3 MoE implementation¬†in PyTorch. The educational¬†clarity¬†of¬†his code¬†just¬†blew me¬†away - especially¬†how he broke¬†down the¬†Mixture¬†of Experts architecture¬†in¬†his¬†LLMs-from-scratch repo.\nThat¬†got¬†me thinking... what if I could bring¬†this¬†to¬†pure¬†C?¬†ü§î\nInspired by Andrej Karpathy&amp;#39;s legendary¬†llama2.c approach¬†(seriously, if¬†you haven&amp;#39;t seen¬†it, check¬†it out), I decided to take¬†on the challenge of implementing¬†Qwen3&amp;#39;s¬†30B parameter¬†model with¬†128 experts¬†in a¬†single C file.\nThe result?¬†Qwen_MOE_C¬†- a complete¬†inference engine that:\n‚úÖ Handles¬†sparse MoE computation¬†(only¬†8 out¬†of 128 experts¬†active)\n‚úÖ Supports¬†Grouped Query Attention with¬†proper head ratios\n‚úÖ Uses¬†memory¬†mapping¬†for efficiency (~30GB¬†models)\n‚úÖ Zero external¬†dependencies (just¬†libc¬†+ libm)\nThe beauty¬†of¬†this approach is the¬†same as llama2.c¬†- you can understand every¬†line, it&amp;#39;s hackable, and it runs¬†anywhere C¬†runs. No¬†frameworks, no dependencies, just pure computational¬†transparency.\nHuge thanks to Sebastian¬†Raschka for the¬†reference implementation and educational¬†materials, and to Andrej¬†Karpathy for¬†showing us that¬†simplicity is¬†the ultimate sophistication in¬†ML systems.\nSometimes¬†the best way¬†to truly understand something is to build it from scratch.¬†üõ†Ô∏è\nLink¬†to the project: \n&lt;a href=\"https://github.com/h9-tec/Qwen_MOE_C\"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?auto=webp&amp;s=e5cdc2248f745a90988be621db7d6b6be3916db2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbd8f00d966699e72ff3a93f578256c7537d2135",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3afe3fea53292db66353b624f6471827586e1c10",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023f7331df4213637fe882078ebd6fb043d92076",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99f557d7c6c92fc79bbc53fbd9519fb90c4895ac",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55d41da3d53c6240a1417760903816f861d7b857",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=869ae3899c29fb68da394dceac5b0968c64c5e51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfxas1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1Hesham",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754158458,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT's throttled free-tier, and Claude 4 Sonnet",
          "author_fullname": "t2_65zz9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Four Models, One Prompt: Who Writes the Best Instructions for AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwec7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754156214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "selfenrichment.hashnode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT&amp;#39;s throttled free-tier, and Claude 4 Sonnet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfwec7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertotomas",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwec7/four_models_one_prompt_who_writes_the_best/",
          "stickied": false,
          "url": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "subreddit_subscribers": 509052,
          "created_utc": 1754156214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.\n\nI already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an [Awesome list](https://github.com/sindresorhus/awesome).\n\nTo have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: [https://aibenchmarks.net/](https://aibenchmarks.net/)\n\nAwesome AI Benchmarks GitHub repository available here: [https://github.com/panilya/awesome-ai-benchmarks](https://github.com/panilya/awesome-ai-benchmarks)\n\nWould be happy to hear any feedback on this and whether it can be useful for you :)",
          "author_fullname": "t2_7arjztn1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100+ AI Benchmarks list",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwckf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754156092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt;\n\n&lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href=\"https://github.com/sindresorhus/awesome\"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href=\"https://aibenchmarks.net/\"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href=\"https://github.com/panilya/awesome-ai-benchmarks\"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?auto=webp&amp;s=41e93dde0345b37cb0200da115b435ce77df3930",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93761f053553e9b1cfb16372cdf0e7dac51f1f5e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b480826b22f435da297cc6f87648622fc3892fe6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08c9c7135e744081cfec8c73e2a489375eb611be",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63909a628f633b135db8a2f2ad24f375a444a9b7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcf264d960a58dd8659a36b59ab18b32c8762353",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6522fe068f5cf8114a6d791bc9bbc352abd7a38",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfwckf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panilyaU",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754156092,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14wr7u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "WebGPU enables local LLM in the browser. Demo site with AI chat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvzai",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754155160,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "andreinwald.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://andreinwald.github.io/browser-llm/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfvzai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "andreinwald",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvzai/webgpu_enables_local_llm_in_the_browser_demo_site/",
          "stickied": false,
          "url": "https://andreinwald.github.io/browser-llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754155160,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[A new PR](https://github.com/ggml-org/llama.cpp/pull/15026) was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated [\\#14939](https://github.com/ggml-org/llama.cpp/pull/14939) seemed to get stuck. The new PR description reads: \"**this PR will NOT attempt to implement MTP**\", with great progress being made in short time. (Amazing!!!)\n\nGiven that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.\n\nDisclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!\n\nPS: does MTP already work on/with MLX?",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvxdo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754155024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15026\"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5&amp;#39;s models in llama.cpp, as the original, highly anticipated &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt;\n\n&lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it&amp;#39;s not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I am eternally grateful for everybody&amp;#39;s contribution to the field, as LLMs allow me to code what I couldn&amp;#39;t code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt;\n\n&lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?auto=webp&amp;s=acc8e28264bfccdff7bd8f443fed065d174ea9e0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=192b46649336ccde82b3df88264f14e8c3af5057",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d87e157d503386c5e4e8e7524115ddb67fb4d525",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2253f5fea3f3de0ade685e30f24f2fe47d41bed",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=076b473df32cd3180067a97b9fe5da2c8755fd75",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc3f7d46febf1a47049086579c27beff59d8b7b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfebb15c0a042499f527df12e1302a2dc2a17608",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvxdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754155024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi All,\n\nI'm quite new to local AI models, and started today by playing with Chatterbox TTS on my Mac Studio M4 (using the apple silicon version on Hugging Face). Also, hopefully this is the right reddit - I see other posts regarding Chatterbox here, so I guess it is!\n\nIt's actually working very nicely indeed, doing a conversion of a small piece of a book with a voice sample I provided.\n\nIt's taking a while though; \\~25 minutes to generate a 10 minute sample. The full book is likely to be 15-20 hours long, so we could be talking 50 hours for the full conversion.\n\nSo - I would like to see if there are services I might run the model on in the cloud - for example [RunPod.io](http://RunPod.io) or [Vast.ai](http://Vast.ai) are two that I have seen. But I'm not sure what the costs might end up being, and not really sure how to find out.\n\nCan anyone offer any guidance? Is it as simple as saying 50 hours x (hourly price for GPU)?\n\nThanks!",
          "author_fullname": "t2_15wm1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox TTS in cloud?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvk5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754154092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to local AI models, and started today by playing with Chatterbox TTS on my Mac Studio M4 (using the apple silicon version on Hugging Face). Also, hopefully this is the right reddit - I see other posts regarding Chatterbox here, so I guess it is!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s actually working very nicely indeed, doing a conversion of a small piece of a book with a voice sample I provided.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s taking a while though; ~25 minutes to generate a 10 minute sample. The full book is likely to be 15-20 hours long, so we could be talking 50 hours for the full conversion.&lt;/p&gt;\n\n&lt;p&gt;So - I would like to see if there are services I might run the model on in the cloud - for example &lt;a href=\"http://RunPod.io\"&gt;RunPod.io&lt;/a&gt; or &lt;a href=\"http://Vast.ai\"&gt;Vast.ai&lt;/a&gt; are two that I have seen. But I&amp;#39;m not sure what the costs might end up being, and not really sure how to find out.&lt;/p&gt;\n\n&lt;p&gt;Can anyone offer any guidance? Is it as simple as saying 50 hours x (hourly price for GPU)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                  "width": 2400,
                  "height": 1260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvk5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BrotherBrutha",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvk5h/chatterbox_tts_in_cloud/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvk5h/chatterbox_tts_in_cloud/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754154092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "To be frank, AGI doesn't particularly interest or thrill me. Given current technological frameworks, I believe AGI won't arrive anytime soon without some breakthrough discovery. The models we have today would have seemed absolutely magical just five years ago.\n\nCan anyone give me an excitement you have with AGI ?",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AGI Could Be Our Era's Perpetual Motion Machine - Forever Out of Reach, Though Current AI Already Amazes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfve4v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754153678,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To be frank, AGI doesn&amp;#39;t particularly interest or thrill me. Given current technological frameworks, I believe AGI won&amp;#39;t arrive anytime soon without some breakthrough discovery. The models we have today would have seemed absolutely magical just five years ago.&lt;/p&gt;\n\n&lt;p&gt;Can anyone give me an excitement you have with AGI ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfve4v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfve4v/agi_could_be_our_eras_perpetual_motion_machine/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfve4v/agi_could_be_our_eras_perpetual_motion_machine/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754153678,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm using Qwen 3 14B right now but haven‚Äôt checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? ",
          "author_fullname": "t2_4r17bvqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM that fits with 12GB VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfv3b0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm using Qwen 3 14B right now but haven‚Äôt checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfv3b0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tthane50",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754152915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,  \nI'm a final-year CS student working on a project to build an **AI assistant for my university** using **RAG (Retrieval-Augmented Generation)** and possibly agentic tools down the line.\n\nThe chatbot will help students find answers to common university-related questions (like academic queries, admissions, etc.) and eventually perform light actions like form redirection, etc.\n\n# What I‚Äôm struggling with:\n\nI'm not exactly sure **what types of data** I should collect and prepare to make this assistant useful, accurate, and robust.\n\nI plan to use LangChain or LlamaIndex + a vector store, but I want to hear from folks with experience in this kind of thing:\n\n* **What kinds of data did you use for similar projects?**\n* How do you decide what to include or ignore?\n* Any tips for formatting / chunking / organizing it early on?\n\nAny help, advice, or even just a pointer in the right direction would be awesome.",
          "author_fullname": "t2_1ckf2evf4b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help: Building a University Assistant RAGbot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuz5w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754154614,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI&amp;#39;m a final-year CS student working on a project to build an &lt;strong&gt;AI assistant for my university&lt;/strong&gt; using &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt; and possibly agentic tools down the line.&lt;/p&gt;\n\n&lt;p&gt;The chatbot will help students find answers to common university-related questions (like academic queries, admissions, etc.) and eventually perform light actions like form redirection, etc.&lt;/p&gt;\n\n&lt;h1&gt;What I‚Äôm struggling with:&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;m not exactly sure &lt;strong&gt;what types of data&lt;/strong&gt; I should collect and prepare to make this assistant useful, accurate, and robust.&lt;/p&gt;\n\n&lt;p&gt;I plan to use LangChain or LlamaIndex + a vector store, but I want to hear from folks with experience in this kind of thing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;What kinds of data did you use for similar projects?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;How do you decide what to include or ignore?&lt;/li&gt;\n&lt;li&gt;Any tips for formatting / chunking / organizing it early on?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any help, advice, or even just a pointer in the right direction would be awesome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfuz5w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jarrarhaidery",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuz5w/need_help_building_a_university_assistant_ragbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuz5w/need_help_building_a_university_assistant_ragbot/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754152634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing an OpenAI proxy solution for Claude-Code\n\n[https://github.com/ziozzang/claude2openai-proxy](https://github.com/ziozzang/claude2openai-proxy)\n\nAdvantages:\n\n1. In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.\n2. You can start using Claude Code with any specified model, whether it‚Äôs internal or external.\n3. You can also debug if needed.\n4. Anyway, at least from my tests, it works very well. The only issue is the model itself.\n\n\\----\n\nWhile using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn‚Äôt find a solid solution that worked well for multiple users. So, based on [https://github.com/1rgs/claude-code-proxy ](https://github.com/1rgs/claude-code-proxy), I built a proxy tailored for my use. Since it converts between different protocols, ‚Äúgateway‚Äù might actually be a more fitting term.\n\nAnyway, here are the features:\n\nFull support for Claude Code.\n\n* On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.\n* Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.\n* Authentication is done via the ANTHROPIC\\_API\\_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.\n\nBelow is an example of setting up the server and actually using it from a client:\n\n    ANTHROPIC_BASE_URL=http://localhost:8082 \\\n    ANTHROPIC_API_KEY=sk-openapi-auth-token \\\n    ANTHROPIC_MODEL=\"openrouter/horizon-beta\" \\\n    ANTHROPIC_SMALL_FAST_MODEL=\"openrouter/horizon-beta\" \\\n    claude  \n\nTo be honest, I made this to test the openrouter/horizon-beta model. :)\n\nThe pipeline works great: Claude Code -(Claude API)-&gt; my modified proxy server -(OpenAI API)-&gt; openrouter/horizon-beta.\n\nBy the way, you can find what I built at [https://github.com/ziozzang/claude2openai-proxy ](https://github.com/ziozzang/claude2openai-proxy). I use it by building it into a container.\n\nTo be honest,\n\n* horizon-alpha doesn‚Äôt seem to handle Claude Code‚Äôs prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)\n* horizon-beta handles them quite well.\n* However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don‚Äôt feel ideal for automated workflows.\n\nPS.\n\nThe whole reason this started was because of Claude Code‚Äôs usage limits. LoL...",
          "author_fullname": "t2_5409gkc6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gateway/Proxy for Claude-Code to OpenAI API compatible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuu40",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754153859,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing an OpenAI proxy solution for Claude-Code&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Advantages:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.&lt;/li&gt;\n&lt;li&gt;You can start using Claude Code with any specified model, whether it‚Äôs internal or external.&lt;/li&gt;\n&lt;li&gt;You can also debug if needed.&lt;/li&gt;\n&lt;li&gt;Anyway, at least from my tests, it works very well. The only issue is the model itself.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;While using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn‚Äôt find a solid solution that worked well for multiple users. So, based on &lt;a href=\"https://github.com/1rgs/claude-code-proxy\"&gt;https://github.com/1rgs/claude-code-proxy &lt;/a&gt;, I built a proxy tailored for my use. Since it converts between different protocols, ‚Äúgateway‚Äù might actually be a more fitting term.&lt;/p&gt;\n\n&lt;p&gt;Anyway, here are the features:&lt;/p&gt;\n\n&lt;p&gt;Full support for Claude Code.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.&lt;/li&gt;\n&lt;li&gt;Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.&lt;/li&gt;\n&lt;li&gt;Authentication is done via the ANTHROPIC_API_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Below is an example of setting up the server and actually using it from a client:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ANTHROPIC_BASE_URL=http://localhost:8082 \\\nANTHROPIC_API_KEY=sk-openapi-auth-token \\\nANTHROPIC_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nANTHROPIC_SMALL_FAST_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nclaude  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To be honest, I made this to test the openrouter/horizon-beta model. :)&lt;/p&gt;\n\n&lt;p&gt;The pipeline works great: Claude Code -(Claude API)-&amp;gt; my modified proxy server -(OpenAI API)-&amp;gt; openrouter/horizon-beta.&lt;/p&gt;\n\n&lt;p&gt;By the way, you can find what I built at &lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy &lt;/a&gt;. I use it by building it into a container.&lt;/p&gt;\n\n&lt;p&gt;To be honest,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;horizon-alpha doesn‚Äôt seem to handle Claude Code‚Äôs prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)&lt;/li&gt;\n&lt;li&gt;horizon-beta handles them quite well.&lt;/li&gt;\n&lt;li&gt;However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don‚Äôt feel ideal for automated workflows.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PS.&lt;/p&gt;\n\n&lt;p&gt;The whole reason this started was because of Claude Code‚Äôs usage limits. LoL...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?auto=webp&amp;s=42f669382e1c8928b5e1d529da5526d93801fc0d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e43bc658beb09686e12af7368ba5a1c7c7d40071",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41d704239a298e8b8e9ef761cbf0504c8c77ccaf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fed85d51213a23fb5ee442d3c4a5ad79bfcf800",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d1364de265c02e79206344fc84f7451660adc50",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5975df8d3cb9a0c0f9fe7ad4afa6ddfa17a56a05",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cbd06ed3b8ba41d88914fbda7d1b7707aee647f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfuu40",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ziozzang0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754152276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is just a little remark that if you haven't you definitely should try qwen code [https://github.com/QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)   \nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. \n\nThe metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.\n\nSo this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. ",
          "author_fullname": "t2_n9dnke1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code + Qwen Coder 30b 3A is insane",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuiri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 124,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 124,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just a little remark that if you haven&amp;#39;t you definitely should try qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br/&gt;\nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I&amp;#39;m working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt;\n\n&lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I&amp;#39;m very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt;\n\n&lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don&amp;#39;t know jack shit about coding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?auto=webp&amp;s=689c8ef1840eeb6bb6bb53a5908a342a3dac7558",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2382140736e501c0e5f725eb9004d88daaf4ddc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fd612828e87be93104f75281f9696b4b69068",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef0b7f26ce8aa8b563291337614fb5656c8ab07",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=faaeb8e0dc19d67afd0d3197e7b7a5a0095a6f09",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eebbb21f63208ab9ba4a6a50b9890a328d60d830",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edc81e58030926a09a63f9377700b5cd6432c27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfuiri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flashy_Management962",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754151475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently doing the mcp course from huggingface, and I am planning to roll my own local agentic AI. Any idea what the BEST model I should use for RTX 4090? I know best is objective, so I am looking for two models, one for general purpose, and the other for coding. I will be building simple tools for personal use. For example, making a custom resume generator given job description etc. ",
          "author_fullname": "t2_1us1jsxdkh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model to use as agentic AI for RTX 4090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfubwt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151003,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently doing the mcp course from huggingface, and I am planning to roll my own local agentic AI. Any idea what the BEST model I should use for RTX 4090? I know best is objective, so I am looking for two models, one for general purpose, and the other for coding. I will be building simple tools for personal use. For example, making a custom resume generator given job description etc. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfubwt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flat_Chard_3763",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfubwt/best_model_to_use_as_agentic_ai_for_rtx_4090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfubwt/best_model_to_use_as_agentic_ai_for_rtx_4090/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754151003,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, I'm quite new to this topic. Do you where I can find info for starter who doenst have tech background? And what kind of companies are the best out there? ",
          "author_fullname": "t2_1up2vv5ptn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Learn GPU AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfu5ll",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754150565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;m quite new to this topic. Do you where I can find info for starter who doenst have tech background? And what kind of companies are the best out there? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfu5ll",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AutomaticAbility2008",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfu5ll/learn_gpu_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfu5ll/learn_gpu_ai/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754150565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Basically title, ideally something that can process both text, images, and documents/sheets of data, as smart as possible, and as lean as possible. \n\nMy initial research led me to Phi-4, Gemma 3, and Mistral Small 3.1, but considering how fast this space progresses, I think they have probably been outdated a few gens ago. So what wouldyou suggest for a complete newb to help set-up for free for farmers? Ideally something that is good enough that even if things progress substantially it would be enough to cover basic needs I have described, and depending on the local set-up, could operate without internet and either in low-complexity low-power device, or a higher-end \"gaming\" pc?",
          "author_fullname": "t2_5ci2w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'm researching some OS &amp; Local LLMs that can be useful for farmers, either in high-end PCs and in raspberry pi. Suggestions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfu41i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754150462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically title, ideally something that can process both text, images, and documents/sheets of data, as smart as possible, and as lean as possible. &lt;/p&gt;\n\n&lt;p&gt;My initial research led me to Phi-4, Gemma 3, and Mistral Small 3.1, but considering how fast this space progresses, I think they have probably been outdated a few gens ago. So what wouldyou suggest for a complete newb to help set-up for free for farmers? Ideally something that is good enough that even if things progress substantially it would be enough to cover basic needs I have described, and depending on the local set-up, could operate without internet and either in low-complexity low-power device, or a higher-end &amp;quot;gaming&amp;quot; pc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfu41i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hjras",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfu41i/im_researching_some_os_local_llms_that_can_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfu41i/im_researching_some_os_local_llms_that_can_be/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754150462,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "At least for models built on the Qwen 3 architecture, I noticed that the speed difference between the MoE models and roughly equivalent dense models is minimal, particularly as context sizes get larger.\n\nFor instance, on my M4 Max MacBook Pro, with llama.cpp, unsloth Q4\\_K\\_XL quants, flash attention, and q8\\_0 KV cache quantization, here are the performance results I got:\n\n|Model|Context Size (tokens, approx)|Prompt Processing (tok/s)|Token Generation (tok/s)|\n|:-|:-|:-|:-|\n|Qwen 3 8B|500|730|70|\n|Qwen 3 8B|53000|103|22|\n|Qwen 3 30B-A3B|500|849|88|\n|Qwen 3 30B-A3B|53000|73|22|\n|Qwen 3 14B|500|402|43|\n|Qwen 3 14B|53000|66|12|\n\nNote: the prompt processing and token generation speeds are for processing additional inputs or generating additional output tokens, after the indicated number of tokens have already been processed in context\n\nIn terms of intelligence and knowledge, the original 30B-A3B model was somewhere in between the 8B and 14B in my experiments. At large context sizes, the 30B-A3B has prompt processing size in between 8B and 14B, and token generation speeds roughly the same as the 8B.\n\nI've read that MoEs are more efficient (cheaper) to train, but for end users, under the Qwen 3 architecture at least, the inference speed benefit of MoE seems limited, and the large memory footprint is problematic for those who don't have huge amounts of RAM.\n\nI'm curious how the IBM Granite 4 architecture will fare, particularly with large contexts, given its context memory efficient Mamba-Transformer hybrid design.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MoE models not as fast as active parameter counts suggest",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfh4ee",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754107542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At least for models built on the Qwen 3 architecture, I noticed that the speed difference between the MoE models and roughly equivalent dense models is minimal, particularly as context sizes get larger.&lt;/p&gt;\n\n&lt;p&gt;For instance, on my M4 Max MacBook Pro, with llama.cpp, unsloth Q4_K_XL quants, flash attention, and q8_0 KV cache quantization, here are the performance results I got:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Context Size (tokens, approx)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Prompt Processing (tok/s)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Token Generation (tok/s)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;730&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;53000&lt;/td&gt;\n&lt;td align=\"left\"&gt;103&lt;/td&gt;\n&lt;td align=\"left\"&gt;22&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;849&lt;/td&gt;\n&lt;td align=\"left\"&gt;88&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;53000&lt;/td&gt;\n&lt;td align=\"left\"&gt;73&lt;/td&gt;\n&lt;td align=\"left\"&gt;22&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;402&lt;/td&gt;\n&lt;td align=\"left\"&gt;43&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;53000&lt;/td&gt;\n&lt;td align=\"left\"&gt;66&lt;/td&gt;\n&lt;td align=\"left\"&gt;12&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Note: the prompt processing and token generation speeds are for processing additional inputs or generating additional output tokens, after the indicated number of tokens have already been processed in context&lt;/p&gt;\n\n&lt;p&gt;In terms of intelligence and knowledge, the original 30B-A3B model was somewhere in between the 8B and 14B in my experiments. At large context sizes, the 30B-A3B has prompt processing size in between 8B and 14B, and token generation speeds roughly the same as the 8B.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read that MoEs are more efficient (cheaper) to train, but for end users, under the Qwen 3 architecture at least, the inference speed benefit of MoE seems limited, and the large memory footprint is problematic for those who don&amp;#39;t have huge amounts of RAM.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious how the IBM Granite 4 architecture will fare, particularly with large contexts, given its context memory efficient Mamba-Transformer hybrid design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfh4ee",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfh4ee/moe_models_not_as_fast_as_active_parameter_counts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfh4ee/moe_models_not_as_fast_as_active_parameter_counts/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754107542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI‚Äôm working on a side project to build a virtual assistant that can do two main things:\n\n1. Answer questions based on a company‚Äôs internal docs (using RAG).\n2. Perform actions like ‚Äúcreate an account,‚Äù ‚Äúschedule a meeting,‚Äù or ‚Äúfind the nearest location.‚Äù\n\nI‚Äôd love some advice from folks who‚Äôve built similar systems or explored this space. A few questions:\n\n- How would you store and access the internal data (both docs and structured info)?\n\n- What RAG setup works well in practice (vector store, retrieval strategy, etc)?\n\n- Would you use a separate intent classifier to route between info-lookup vs action execution?\n\n- For tasks, do agent frameworks like LangGraph or AutoGen make sense?\n\n- Have frameworks like ReAct/MRKL been useful in real-world projects?\n\n- When is fine-tuning or LoRA worth the effort vs just RAG + good prompting?\n\n- Any tips or lessons learned on overall architecture or scaling?\n\nNot looking for someone to design it for me, just hoping to hear what‚Äôs worked (or not) in your experience. Cheers!",
          "author_fullname": "t2_m4wc3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Scalable LLM Virtual Assistant ‚Äì Looking for Architecture Tips",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mft55c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754148045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm working on a side project to build a virtual assistant that can do two main things:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Answer questions based on a company‚Äôs internal docs (using RAG).&lt;/li&gt;\n&lt;li&gt;Perform actions like ‚Äúcreate an account,‚Äù ‚Äúschedule a meeting,‚Äù or ‚Äúfind the nearest location.‚Äù&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I‚Äôd love some advice from folks who‚Äôve built similar systems or explored this space. A few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;How would you store and access the internal data (both docs and structured info)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What RAG setup works well in practice (vector store, retrieval strategy, etc)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Would you use a separate intent classifier to route between info-lookup vs action execution?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For tasks, do agent frameworks like LangGraph or AutoGen make sense?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have frameworks like ReAct/MRKL been useful in real-world projects?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;When is fine-tuning or LoRA worth the effort vs just RAG + good prompting?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any tips or lessons learned on overall architecture or scaling?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Not looking for someone to design it for me, just hoping to hear what‚Äôs worked (or not) in your experience. Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mft55c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeadFinger",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mft55c/scalable_llm_virtual_assistant_looking_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mft55c/scalable_llm_virtual_assistant_looking_for/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754148045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using `llama.cpp` with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here‚Äôs what actually works.\n\n**KV Quantization**\n\n* **KV cache quantization matters a lot**. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use `q5_1` for a good balance of memory usage and performance. It works well in PPL tests and in practice.\n\n**Offloading Strategy**\n\n* You're bottlenecked by your **system RAM bandwidth** when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.\n* Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU‚Äôs VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n\n**Memory Tuning for CPU Offloading**\n\n* System memory speed has a major impact on throughput when using partial offloading.\n* Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.\n* On **AM4** platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.\n* On **AM5**, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.\n* Poor memory tuning will bottleneck your CPU offloading even with a fast processor.\n\n**ubatch (Prompt Batch Size)**\n\n* Higher `ubatch` values significantly improve prompt processing (PP) performance.\n* Try values like `768` or `1024`. You‚Äôll use more VRAM, but it‚Äôs often worth it for the speedup.\n* If you‚Äôre VRAM-limited, lower this until it fits.\n\n**Extra Performance Boost**\n\n* Set this environment variable for a 5‚Äì10% performance gain:Launch like this: LLAMA\\_SET\\_ROWS=1 ./llama-server -md /path/to/model etc.\n\n**Speculative Decoding Tips (SD)**\n\nSpeculative decoding is supported in `llama.cpp`, but there are a couple important caveats:\n\n1. **KV cache quant affects acceptance rate heavily.** Using `q4_0` for the draft model‚Äôs KV cache *halves* the acceptance rate in my testing. Use `q5_1` or even `q8_0` for the draft model KV cache for much better performance.\n2. **Draft model context handling is broken after filling the draft KV cache.** Once the draft model‚Äôs context fills up, performance tanks. Right now it‚Äôs better to run the draft with full context size. Reducing it actually hurts.\n3. **Draft parameters matter a lot**. In my testing, using `--draft-p-min 0.85 --draft-min 2 --draft-max 12` gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.\n\nFor SD, try using **Qwen 3 0.6B** as the draft model. It‚Äôs fast and works well, as long as you avoid the issues above.\n\nIf you‚Äôve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp",
          "author_fullname": "t2_66tlmx2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9qn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 80,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 80,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here‚Äôs what actually works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you&amp;#39;re offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use &lt;code&gt;q5_1&lt;/code&gt; for a good balance of memory usage and performance. It works well in PPL tests and in practice.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You&amp;#39;re bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt;\n&lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU‚Äôs VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt;\n&lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you&amp;#39;re comfortable doing so.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt;\n&lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt;\n&lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You‚Äôll use more VRAM, but it‚Äôs often worth it for the speedup.&lt;/li&gt;\n&lt;li&gt;If you‚Äôre VRAM-limited, lower this until it fits.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set this environment variable for a 5‚Äì10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model‚Äôs KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;code&gt;q5_1&lt;/code&gt; or even &lt;code&gt;q8_0&lt;/code&gt; for the draft model KV cache for much better performance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model‚Äôs context fills up, performance tanks. Right now it‚Äôs better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It‚Äôs fast and works well, as long as you avoid the issues above.&lt;/p&gt;\n\n&lt;p&gt;If you‚Äôve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfs9qn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AliNT77",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754145881,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like to start self hosting models for my own usage. I have right now MacBook Pro m4 Pro 24Gb ram and it feels slow with larger models and very limited. Do you think it would be better to build some custom spec pc for this purpose running on Linux just to run LLMs? Or buy maxed out Mac Studio or Mac mini for this purpose\n\nMain usage would be coding and image generation if that would be possible. \n\nPs. I have sitting somewhere i7 12700K with 32Gb ram but without gpu ",
          "author_fullname": "t2_1pnbev48g0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting started into self hosting LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9cw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to start self hosting models for my own usage. I have right now MacBook Pro m4 Pro 24Gb ram and it feels slow with larger models and very limited. Do you think it would be better to build some custom spec pc for this purpose running on Linux just to run LLMs? Or buy maxed out Mac Studio or Mac mini for this purpose&lt;/p&gt;\n\n&lt;p&gt;Main usage would be coding and image generation if that would be possible. &lt;/p&gt;\n\n&lt;p&gt;Ps. I have sitting somewhere i7 12700K with 32Gb ram but without gpu &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfs9cw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ywful",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9cw/getting_started_into_self_hosting_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754145855,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our \"own\" R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.\n\nWhat are your thoughts?\n\nUpdate: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)",
          "author_fullname": "t2_qnf2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It's time to run your own R1, Kimi ... and split the cost of it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrunn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754146028,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I&amp;#39;m thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n\n&lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfrunn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HammerSpb",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754144811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry if this is a basic question, but I seem to be really struggling :/\n\nConsider a typical, text-in text-out use case. If I'm using an offline model API via e.g. REST, how can I incorporate tool  use? Is \"tool use\" some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?\n\nIs there some basic, no-nonsense code or tutorial to get a concrete idea?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is \"tool use\", exactly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrq3v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a basic question, but I seem to be really struggling :/&lt;/p&gt;\n\n&lt;p&gt;Consider a typical, text-in text-out use case. If I&amp;#39;m using an offline model API via e.g. REST, how can I incorporate tool  use? Is &amp;quot;tool use&amp;quot; some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?&lt;/p&gt;\n\n&lt;p&gt;Is there some basic, no-nonsense code or tutorial to get a concrete idea?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfrq3v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754144468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.\n\nI gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it\n\n[https://github.com/mmemoo/dis-cal](https://github.com/mmemoo/dis-cal)  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. \n\nThanks in advance!",
          "author_fullname": "t2_cgzigjpd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a opensource CAL-AI alternative using ollama which runs completely locally and for is fully free.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrec0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754143606,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.&lt;/p&gt;\n\n&lt;p&gt;I gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmemoo/dis-cal\"&gt;https://github.com/mmemoo/dis-cal&lt;/a&gt;  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?auto=webp&amp;s=7740e51b065f73baef20acb9661652efbd31473e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43d0c8bafbb43faea860f8f86e81107782da6aa3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8b2d47057fd20576416383a2c09e2a524b760d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b75736c0b2fa807b69315eb351d4357ec156394",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=78079915dfeceeab89fd37ed8f707b7cfbc64ce5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cabc737e4eaac8c82320118a5fb4890d5545856f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57c3313f0eff8bcc3ad6c3af276b7a5cb4a42f83",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfrec0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehmetflix_",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754143606,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "One of the things I want to do with my local build is to make my home more efficient.   I'd like to be able to get data points from various sources and have them analyzed either for actionable changes or optimization.  Not sure how to get from here to there though.\n\nExample:\n\nGather data from \n- temp outside\n- temp inside\n- temp inside cooling ducts (only measured when the system is blowing)\n- electrical draw from the ac\n- commanded on off cycles \n- amount of sun in specific loacations\n\nThen figure out \n- hvac gets commanded on but take longer at this time to cool off the house\n- at those times, command ac at lower temps to mitigate the time loss\n- discover that sun load at specific times effects efficiency, shade the area. \n\nI feel like there are enough smart home sensors out there that a well tuned ai could crunch all the data and give some real insight.   Why go of daily averages when I can record actual data in almost real time?  Why guess at the type of things home owners and so called efficiency experts have done in the past?  \n\nSo the set up might be something like this:\n\n1 install smart features and sensors (that can communicate with 2)\n\n2 set up code script etc to record data from all sources\n\n3 have ai model that interprets data and spit back patterns and adjustments to make\n\n4 maybe have ai create new script to adjust settings in the smart home for optimal efficiency\n\n5 run daily or or weekly analysis and adjust the efficiency script.\n\n\nThis is just me thinking outlook as a starting point.  And its only one area of efficiency of several that this could play a noticeable impact\n",
          "author_fullname": "t2_clyuifd5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smart integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqzc8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754142487,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the things I want to do with my local build is to make my home more efficient.   I&amp;#39;d like to be able to get data points from various sources and have them analyzed either for actionable changes or optimization.  Not sure how to get from here to there though.&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;Gather data from \n- temp outside\n- temp inside\n- temp inside cooling ducts (only measured when the system is blowing)\n- electrical draw from the ac\n- commanded on off cycles \n- amount of sun in specific loacations&lt;/p&gt;\n\n&lt;p&gt;Then figure out \n- hvac gets commanded on but take longer at this time to cool off the house\n- at those times, command ac at lower temps to mitigate the time loss\n- discover that sun load at specific times effects efficiency, shade the area. &lt;/p&gt;\n\n&lt;p&gt;I feel like there are enough smart home sensors out there that a well tuned ai could crunch all the data and give some real insight.   Why go of daily averages when I can record actual data in almost real time?  Why guess at the type of things home owners and so called efficiency experts have done in the past?  &lt;/p&gt;\n\n&lt;p&gt;So the set up might be something like this:&lt;/p&gt;\n\n&lt;p&gt;1 install smart features and sensors (that can communicate with 2)&lt;/p&gt;\n\n&lt;p&gt;2 set up code script etc to record data from all sources&lt;/p&gt;\n\n&lt;p&gt;3 have ai model that interprets data and spit back patterns and adjustments to make&lt;/p&gt;\n\n&lt;p&gt;4 maybe have ai create new script to adjust settings in the smart home for optimal efficiency&lt;/p&gt;\n\n&lt;p&gt;5 run daily or or weekly analysis and adjust the efficiency script.&lt;/p&gt;\n\n&lt;p&gt;This is just me thinking outlook as a starting point.  And its only one area of efficiency of several that this could play a noticeable impact&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqzc8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JellyfishAutomatic25",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqzc8/smart_integration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqzc8/smart_integration/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754142487,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.\n\nHere are a few things I've focused on:\n\n* It can create and run files automatically from natural language.\n* I've tried to make it smart about executing tools like git, search, and bash commands.\n* It's designed to work with any Ollama model that supports function calling.\n* A big priority for me was to keep it 100% local to ensure privacy.\n\nIt's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.",
          "author_fullname": "t2_2lznz5yi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollamacode - Local AI assistant that can create, run and understand your codebase.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqr3o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=c955545b2920d3da4a8cd08357963d4b7cac5b74",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754141855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a project called OllamaCode, and I&amp;#39;d love to share it with you. It&amp;#39;s an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt;\n\n&lt;p&gt;Here are a few things I&amp;#39;ve focused on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s designed to work with any Ollama model that supports function calling.&lt;/li&gt;\n&lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s still in the very early days, and there&amp;#39;s a lot I still want to improve. It&amp;#39;s been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/tooyipjee/ollamacode",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?auto=webp&amp;s=0d661e9993d3a8339a8a04e91dab3d495a1068d8",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad56c7eac7e674170e1bce20ac9f2fbb89067d4f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4264d87bb5a2e08e33aa5bd378354efc3de28a29",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbbbbe9284ddae09fc99ccbdd7bbad23851be4ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc443c649d71e676f2ac536126f73b6f681be48d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bf93455ce16aa988227f0c0581aef56a3355348",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0dd9f689127fdc2e0c3b1dd1996536119e75b475",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqr3o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud-Consideration-2",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/",
          "stickied": false,
          "url": "https://github.com/tooyipjee/ollamacode",
          "subreddit_subscribers": 509052,
          "created_utc": 1754141855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend about 300-400 USD per month on Claude Code with the max 5x tier. I‚Äôm unsure when they‚Äôll increase pricing, limit usage, or make models less intelligent. I‚Äôm looking for a cheaper or open-source alternative that‚Äôs just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. \n\n\nEdit: I don‚Äôt pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I‚Äôm quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. \n\nThanks for all the suggestions. I‚Äôll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)",
          "author_fullname": "t2_67bb0jdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source model that is as intelligent as Claude Sonnet 4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqejn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 243,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 243,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754151213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754140871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I‚Äôm unsure when they‚Äôll increase pricing, limit usage, or make models less intelligent. I‚Äôm looking for a cheaper or open-source alternative that‚Äôs just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt;\n\n&lt;p&gt;Edit: I don‚Äôt pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I‚Äôm quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the suggestions. I‚Äôll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfqejn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vishwa1238",
          "discussion_type": null,
          "num_comments": 247,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754140871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Gemini is too long and detailed. Grok's format is weird. Perplexity doesn't search enough. Qwen takes years and writes an entire book.\n\nchatGPT does it perfectly. A double lengthed message with citations, well-written, searches through websites trying to find what it needs, reasoning through it. But it's limited.\n\nThx guys!\n",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best free good deep research LLM websites?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpnxi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138692,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Gemini is too long and detailed. Grok&amp;#39;s format is weird. Perplexity doesn&amp;#39;t search enough. Qwen takes years and writes an entire book.&lt;/p&gt;\n\n&lt;p&gt;chatGPT does it perfectly. A double lengthed message with citations, well-written, searches through websites trying to find what it needs, reasoning through it. But it&amp;#39;s limited.&lt;/p&gt;\n\n&lt;p&gt;Thx guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfpnxi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpnxi/best_free_good_deep_research_llm_websites/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpnxi/best_free_good_deep_research_llm_websites/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754138692,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey! I‚Äôd like to set up my machine to work on my codebase while I‚Äôm AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.\n\nI‚Äôve already tested a few tools ‚Äî kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.\n\nAre there any more or less standardized setups for this kind of workflow?\n\n",
          "author_fullname": "t2_nt892",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the current go-to setup for a fully-local coding agent that continuously improves code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpn4a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I‚Äôd like to set up my machine to work on my codebase while I‚Äôm AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve already tested a few tools ‚Äî kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.&lt;/p&gt;\n\n&lt;p&gt;Are there any more or less standardized setups for this kind of workflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfpn4a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sasik520",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754138620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! So I've been playing around with everyone's baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.\n\nShoutout to u/Skatardude10 and u/farkinga\n\nSo everyone knows it's a great idea to offload some/all tensors to RAM with these models if you can't fit them all. But from what I gathered, if you offload them using \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\", the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.\n\nWhat is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).\n\nThis is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).\n\nThe test is simple \"write me a sophisticated web scraper\". I run it once, then regenerate it to compare (I don't quite understand draft model context, noob, again).\n\n|~~With Qwen3 4b draft model\\*~~|No draft model|\n|:-|:-|\n|~~Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s~~|Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s|\n|~~Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s~~|Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s|\n\n*edit: tried* u/AliNT77\\*'s tip: set draft model's cache to Q8 Q8 and you'll have a higher acceptance rate with the smaller mode, allowing you to go up with main model's context and gain some speed.\\*\n\n\\* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):\n\n* XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)\n* DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s\n* Unsloth Qwen3 0.6B  - 33%, 19t/s\n* **Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s**\n* Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.\n\nWhat was the acceptance rate for 4B you're gonna ask... 67%.\n\nWhy do this instead of trying to offload some layers and try to gain performance this way? I don't know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)\n\nNow this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I'm unemployed, but I'm not THAT unemployed.\n\nQuestions:\n\n1. 1.7b vs 4b draft model. This obvs needs more testing and longer context, but I'm assuming that 4b will perform better than 1.7b with more complex code.\n2. What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?\n3. Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.\n4. I've got a Ryzen CPU. It's massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I'm assuming this is normal and there are no optimisations for AMD cpus?\n5. Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.\n6. Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don't these exist out in the wild?\n7. How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it's localllama so I dare not ask about bigger applications)\n\nWell, if you read all of this, here's your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp; caches, but I am not done yet. This doesn't crash the system, any other combo does. So if you've got more than 12gb vram, you might get away with more context.\n\nStart with: LLAMA\\_SET\\_ROWS=1  \n\\--model \"(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4\\_K\\_XL.gguf\"  \n\\--model-draft \"(full path)/Qwen3-4B-Q8\\_0.gguf\"  \n\\--override-tensor \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\" (yet to test this, but it can now be replaced with --cpu-moe)  \n\\--flash-attn  \n~~--ctx-size 192000~~  \n\\--ctx-size 262144 --cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--threads -1  \n\\--n-gpu-layers 99  \n\\--n-gpu-layers-draft 99  \n~~--ctx-size-draft 1024 --cache-type-k-draft q4\\_0 --cache-type-v-draft q4\\_0~~  \n\\--ctx-size-draft 24567 --cache-type-v-draft q8\\_0 --cache-type-v-draft q8\\_0\n\nor you can do for more speed (30t/s)/accuracy, but less context.  \n\\--ctx-size 131072 --cache-type-k q8\\_0 --cache-type-v q8\\_0  \n\\--ctx-size-draft 24576 --cache-type-k-draft q8\\_0 --cache-type-v-draft q8\\_0  \n\\--batch-size 1024 --ubatch-size 1024\n\nThese settings get you to 11197MiB /  12227MiB vram on the gpu.",
          "author_fullname": "t2_wxxnd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 30B A3b --override-tensor + Qwen3 4b draft = &lt;3 (22 vs 14 t/s)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpgae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754147599,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! So I&amp;#39;ve been playing around with everyone&amp;#39;s baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.&lt;/p&gt;\n\n&lt;p&gt;Shoutout to &lt;a href=\"/u/Skatardude10\"&gt;u/Skatardude10&lt;/a&gt; and &lt;a href=\"/u/farkinga\"&gt;u/farkinga&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So everyone knows it&amp;#39;s a great idea to offload some/all tensors to RAM with these models if you can&amp;#39;t fit them all. But from what I gathered, if you offload them using &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot;, the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.&lt;/p&gt;\n\n&lt;p&gt;What is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).&lt;/p&gt;\n\n&lt;p&gt;This is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).&lt;/p&gt;\n\n&lt;p&gt;The test is simple &amp;quot;write me a sophisticated web scraper&amp;quot;. I run it once, then regenerate it to compare (I don&amp;#39;t quite understand draft model context, noob, again).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;del&gt;With Qwen3 4b draft model*&lt;/del&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;No draft model&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;em&gt;edit: tried&lt;/em&gt; &lt;a href=\"/u/AliNT77\"&gt;u/AliNT77&lt;/a&gt;*&amp;#39;s tip: set draft model&amp;#39;s cache to Q8 Q8 and you&amp;#39;ll have a higher acceptance rate with the smaller mode, allowing you to go up with main model&amp;#39;s context and gain some speed.*&lt;/p&gt;\n\n&lt;p&gt;* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)&lt;/li&gt;\n&lt;li&gt;DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 0.6B  - 33%, 19t/s&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What was the acceptance rate for 4B you&amp;#39;re gonna ask... 67%.&lt;/p&gt;\n\n&lt;p&gt;Why do this instead of trying to offload some layers and try to gain performance this way? I don&amp;#39;t know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)&lt;/p&gt;\n\n&lt;p&gt;Now this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I&amp;#39;m unemployed, but I&amp;#39;m not THAT unemployed.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;1.7b vs 4b draft model. This obvs needs more testing and longer context, but I&amp;#39;m assuming that 4b will perform better than 1.7b with more complex code.&lt;/li&gt;\n&lt;li&gt;What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?&lt;/li&gt;\n&lt;li&gt;Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve got a Ryzen CPU. It&amp;#39;s massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I&amp;#39;m assuming this is normal and there are no optimisations for AMD cpus?&lt;/li&gt;\n&lt;li&gt;Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.&lt;/li&gt;\n&lt;li&gt;Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don&amp;#39;t these exist out in the wild?&lt;/li&gt;\n&lt;li&gt;How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it&amp;#39;s localllama so I dare not ask about bigger applications)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Well, if you read all of this, here&amp;#39;s your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp;amp; caches, but I am not done yet. This doesn&amp;#39;t crash the system, any other combo does. So if you&amp;#39;ve got more than 12gb vram, you might get away with more context.&lt;/p&gt;\n\n&lt;p&gt;Start with: LLAMA_SET_ROWS=1&lt;br/&gt;\n--model &amp;quot;(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf&amp;quot;&lt;br/&gt;\n--model-draft &amp;quot;(full path)/Qwen3-4B-Q8_0.gguf&amp;quot;&lt;br/&gt;\n--override-tensor &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot; (yet to test this, but it can now be replaced with --cpu-moe)&lt;br/&gt;\n--flash-attn&lt;br/&gt;\n&lt;del&gt;--ctx-size 192000&lt;/del&gt;&lt;br/&gt;\n--ctx-size 262144 --cache-type-k q4_0 --cache-type-v q4_0&lt;br/&gt;\n--threads -1&lt;br/&gt;\n--n-gpu-layers 99&lt;br/&gt;\n--n-gpu-layers-draft 99&lt;br/&gt;\n&lt;del&gt;--ctx-size-draft 1024 --cache-type-k-draft q4_0 --cache-type-v-draft q4_0&lt;/del&gt;&lt;br/&gt;\n--ctx-size-draft 24567 --cache-type-v-draft q8_0 --cache-type-v-draft q8_0&lt;/p&gt;\n\n&lt;p&gt;or you can do for more speed (30t/s)/accuracy, but less context.&lt;br/&gt;\n--ctx-size 131072 --cache-type-k q8_0 --cache-type-v q8_0&lt;br/&gt;\n--ctx-size-draft 24576 --cache-type-k-draft q8_0 --cache-type-v-draft q8_0&lt;br/&gt;\n--batch-size 1024 --ubatch-size 1024&lt;/p&gt;\n\n&lt;p&gt;These settings get you to 11197MiB /  12227MiB vram on the gpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfpgae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "igorwarzocha",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754138032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I‚Äôm a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?\n\nI initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that‚Äôs true, wouldn‚Äôt that quickly blow up the context window?\n\nBut then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn‚Äôt that risk missing important context, especially for something like summarization?\n\nSo both approaches seem to have drawbacks ‚Äî I‚Äôm just wondering which one is typically used by AI apps when handling uploaded files?",
          "author_fullname": "t2_kjhuip53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG or prompt engineering",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfor6n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754135853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I‚Äôm a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that‚Äôs true, wouldn‚Äôt that quickly blow up the context window?&lt;/p&gt;\n\n&lt;p&gt;But then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn‚Äôt that risk missing important context, especially for something like summarization?&lt;/p&gt;\n\n&lt;p&gt;So both approaches seem to have drawbacks ‚Äî I‚Äôm just wondering which one is typically used by AI apps when handling uploaded files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfor6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SignatureHuman8057",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754135853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. I'm a non native English writer, who could use some help with phrasing, [something like this](https://www.gingersoftware.com/products/sentence-rephraser), character and plot detail suggestions etc. Are there any good models that can help with that?\n\nI'm planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it's absolutely necessary. Please let me know if it is.",
          "author_fullname": "t2_169t2g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a local model that can help a non native writer with sentence phrasing and ideas.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfoh32",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m a non native English writer, who could use some help with phrasing, &lt;a href=\"https://www.gingersoftware.com/products/sentence-rephraser\"&gt;something like this&lt;/a&gt;, character and plot detail suggestions etc. Are there any good models that can help with that?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to buy a laptop with Nvidia 4060 GPU, which has 8GB RAM. Would that be enough? I can buy a Macbook with 24GB unified RAM which should give me effectively 16 GB VRAM (right?), but I would be drawing from my savings, which I would rather not do unless it&amp;#39;s absolutely necessary. Please let me know if it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfoh32",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "logicSnob",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfoh32/looking_for_a_local_model_that_can_help_a_non/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754134900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?\n\nHere's the Dockerfile:\n\n&gt; FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate\n\nHere's the docker-compose:\n\n&gt; version: '3.8'\nservices:\n  infinity:\n    build: .\n    ports:\n      - \"7997:7997\"\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B\n\nStartup command was:\n\ndocker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch",
          "author_fullname": "t2_cocl8roo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Issues with michaelf34/infinity:latest-cpu + Qwen3-Embedding-8B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfofx5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134794,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried building a docker container to have infinity use the Qwen3-Embedding-8B model in a CPU-only setting. But once the docker container starts, the CPU (Ryzen 9950X, 128GB DDR5) is fully busy even without any embedding requests. Is that normal, or did I configure something wrong?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the Dockerfile:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;FROM michaelf34/infinity:latest-cpu\nRUN pip install --upgrade transformers accelerate&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Here&amp;#39;s the docker-compose:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;version: &amp;#39;3.8&amp;#39;\nservices:\n  infinity:\n    build: .\n    ports:\n      - &amp;quot;7997:7997&amp;quot;\n    environment:\n      - DISABLE_TELEMETRY=true\n      - DO_NOT_TRACK: 1\n      - TOKENIZERS_PARALLELISM=false\n      - TRANSFORMERS_CACHE=.cache\n    volumes:\n      - ./models:/models:ro\n      - ./cache:/.cache\n      restart: unless-stopped\n    command: infinity-emb v2 --model-id /models/Qwen3-Embedding-8B&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Startup command was:&lt;/p&gt;\n\n&lt;p&gt;docker run -d -p 7997:7997 --name qwembed-cpu -v $PWD/models:/models:ro -v ./cache:/app/.cache qwen-infinity-cpu v2 --model-id /models/Qwen3-Embedding-8B --engine torch&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfofx5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Patentsmatter",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfofx5/issues_with_michaelf34infinitylatestcpu/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754134794,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi r/LocalLLaMA,\n\nI'm exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.\n\nFor example, I could give it a high-level command like \"Save the document and close the application,\" and it would need to:\n\n1. Visually parse the screen to locate the \"Save\" button or menu item.\n2. Move the mouse cursor to that location.\n3. Perform a click.\n4. Then, locate the \"Close\" button and do the same.\n\nI'm trying to figure out the best stack for this using local models. My main questions are:\n\n* **Vision/Perception:**¬†What's the current best approach for a model to \"see\" the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?\n* **Decision Making (LLM):**¬†How would the LLM receive the visual information and output the decision (e.g., \"click button with text 'OK' at coordinates \\[x, y\\]\")? What kind of prompting or fine-tuning would be required?\n* **Action/Control:**¬†What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like¬†pyautogui¬†the way to go, or are there more robust alternatives?\n* **Frameworks:**¬†Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?\n\nI'm aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!\n\nThanks",
          "author_fullname": "t2_1t42bjvz6a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to build a local agent for Windows GUI automation (mouse control &amp; accurate button clicking)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfodac",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754134546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring the idea of creating a local agent that can interact with the Windows desktop environment. The primary goal is for the agent to be able to control the mouse and, most importantly, accurately identify and click on specific UI elements like buttons, menus, and text fields.&lt;/p&gt;\n\n&lt;p&gt;For example, I could give it a high-level command like &amp;quot;Save the document and close the application,&amp;quot; and it would need to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Visually parse the screen to locate the &amp;quot;Save&amp;quot; button or menu item.&lt;/li&gt;\n&lt;li&gt;Move the mouse cursor to that location.&lt;/li&gt;\n&lt;li&gt;Perform a click.&lt;/li&gt;\n&lt;li&gt;Then, locate the &amp;quot;Close&amp;quot; button and do the same.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m trying to figure out the best stack for this using local models. My main questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vision/Perception:&lt;/strong&gt;¬†What&amp;#39;s the current best approach for a model to &amp;quot;see&amp;quot; the screen and identify clickable elements? Are there specific multi-modal models that are good at this out-of-the-box, or would I need a dedicated object detection model trained on UI elements?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Decision Making (LLM):&lt;/strong&gt;¬†How would the LLM receive the visual information and output the decision (e.g., &amp;quot;click button with text &amp;#39;OK&amp;#39; at coordinates [x, y]&amp;quot;)? What kind of prompting or fine-tuning would be required?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action/Control:&lt;/strong&gt;¬†What are the recommended libraries for precise mouse control on Windows that can be easily integrated into a Python script? Is something like¬†pyautogui¬†the way to go, or are there more robust alternatives?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frameworks:&lt;/strong&gt;¬†Are there any existing open-source projects or frameworks (similar to Open-Interpreter but maybe more focused on GUI) that I should be looking at as a starting point?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m aiming for a solution that runs entirely locally. Any advice, links to papers, or pointers to GitHub repositories would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfodac",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xSNYPSx777",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfodac/how_to_build_a_local_agent_for_windows_gui/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754134546,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.\n\nI ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison.\nI'm also planning to run some fine-tuning benchmarks later.",
          "author_fullname": "t2_a85vzco5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 57,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnq2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YkVLBy_Uabot6hZJtvt-2lgE4cyPGdL7zFQtJSHp-Xk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754132261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn&amp;#39;t find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt;\n\n&lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I&amp;#39;m quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I&amp;#39;ve also included the RTX 4090 results for a broader comparison.\nI&amp;#39;m also planning to run some fine-tuning benchmarks later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?auto=webp&amp;s=5c7238d3074b0c94ed28a56fd05d130ca36e4062",
                  "width": 1280,
                  "height": 525
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8aa6f94f1fd0b07761158c32a4f411fea4ff01e",
                    "width": 108,
                    "height": 44
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fd7073605b2305ef0bdfa02c28ed14504e5cadf",
                    "width": 216,
                    "height": 88
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f51b098c5db731c8b7427cd4c5e5918907eaf170",
                    "width": 320,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f",
                    "width": 640,
                    "height": 262
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f3fce6514c1c2d99e8304633aabf8e744412a87",
                    "width": 960,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4087fbfd9cd44fbf60685b1fe8cf9814b669d42c",
                    "width": 1080,
                    "height": 442
                  }
                ],
                "variants": {},
                "id": "tc9bMPtFnkPdsr6iuVPGSgsfnHwUyG7fExgbegYm3H4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfnq2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kargafe",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/",
          "stickied": false,
          "url": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "subreddit_subscribers": 509052,
          "created_utc": 1754132261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, an enthusiast but no formal CS training background asking for help \n\nI am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that's the best we can get :) \n\nWhich is the best \"small size\" LLM for document question answering with European language - mostly specific medical jargon.\n\nI tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.\n\nI am running the app using ollama and langchain also any recommendations for alternatives are welcome :)\n",
          "author_fullname": "t2_1e27iwczfb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best &lt;2B open-source LLMs for European languages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnfrp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754131171,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, an enthusiast but no formal CS training background asking for help &lt;/p&gt;\n\n&lt;p&gt;I am trying to make an application for collageus in medical research using a local LLM. The most important requirement is that it can run on any standard issue laptop (mostly just CPU) - as that&amp;#39;s the best we can get :) &lt;/p&gt;\n\n&lt;p&gt;Which is the best &amp;quot;small size&amp;quot; LLM for document question answering with European language - mostly specific medical jargon.&lt;/p&gt;\n\n&lt;p&gt;I tried the several and found that Qwen3 1.6B did suprisingly well with German and Dutch. Also llama 3.2 3B did well but was to large for most machines unfortunately.&lt;/p&gt;\n\n&lt;p&gt;I am running the app using ollama and langchain also any recommendations for alternatives are welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfnfrp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Material-Ad5426",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfnfrp/best_2b_opensource_llms_for_european_languages/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754131171,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dhanishtha-2.0-preview can now tool call. \n\nUpdated Model link:- [https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825](https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825)  \nAPI and Chat page :- [https://helpingai.co](https://helpingai.co)\n\n",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool calling is now supported on World's first Intermediate Reasoning model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn7pv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754130292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt;\n\n&lt;p&gt;Updated Model link:- &lt;a href=\"https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825\"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br/&gt;\nAPI and Chat page :- &lt;a href=\"https://helpingai.co\"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?auto=webp&amp;s=26ef07178b4b9394a1c2c1b0dfc81de665a07de8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6528fc720427c7c1ed30f20ddd332d01526c4f8f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bffe53726d3c274e1c28ce6856f9194b9aca7fd2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=53fe26fae2e2b849fb6ac025e7f99b8738728cd0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14d7ff05a119f24e15918fc09489c78087b00a3e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e635b759cb0f70c7db7a222e92c287229ba6fb41",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a3feb1fc2bfd7f88481601ee7cdf04a9ff9b5ed",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfn7pv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754130292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.\n\nBuilt using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It's ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.\n\n[https://github.com/dokasto/Saidia](https://github.com/dokasto/Saidia)",
          "author_fullname": "t2_11mwsd0p41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Saidia: Offline-First AI Assistant for Educators in low-connectivity regions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn2xf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754161230,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754129775,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.&lt;/p&gt;\n\n&lt;p&gt;Built using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It&amp;#39;s ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dokasto/Saidia\"&gt;https://github.com/dokasto/Saidia&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?auto=webp&amp;s=5141edfe493867e3dafdd151010b3f5040912cc0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e949d6258248c551dcfd9cb47f1304f4151400c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3484da8634402f8ed7774d2e4f8c4669168d53",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7c82a3e8557eeca0f83fc2e477928d5f3031a9f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=493dbfda76c0eab29abee992f4eb1d96ff6d93fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92c32a097682bd7a3113fbc01e20ac3a07fd673",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e412f9a5dfdc2c2cf419a9494a9e06f0757f95",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfn2xf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dokasto_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754129775,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_144o7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI models are picking up hidden habits from each other | IBM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfll39",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=9f10577c2fcc3a35df02ac054b4815c973b08f82",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754123731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "ibm.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?auto=webp&amp;s=ffc26910e336a76ab6db1af971b2d262dbce6146",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2723bb45305d3a150f76e1937c51a2690147d015",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30aa75a6f3b7183ee11ff8f6294344790b27c98c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d4ee8618c7e067b6087a6fbd41b990eb083b72f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46e4049e81a052bc62430cfe7e667f62662c693b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec61744cbe7d95092017e4c3bdcff62c20454303",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a031cd428aa62cc550e88505f888b329ed3ed9cf",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfll39",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ab2377",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/",
          "stickied": false,
          "url": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "subreddit_subscribers": 509052,
          "created_utc": 1754123731,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôd like to start a small art project and I‚Äôm looking for a model that speaks German well. I‚Äôm currently using Gemma 3n:e4b and I‚Äôm quite satisfied with it. However, I‚Äôd like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM ‚Äì ideally no more than 6GB.",
          "author_fullname": "t2_1tcpn4d5tw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small LLM in german",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfldxj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122940,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôd like to start a small art project and I‚Äôm looking for a model that speaks German well. I‚Äôm currently using Gemma 3n:e4b and I‚Äôm quite satisfied with it. However, I‚Äôd like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM ‚Äì ideally no more than 6GB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfldxj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ghulaschsuppe",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754122940,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with Qwen3:30b-a3b-instruct-2507-q8\\_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I'm really impressed with the speed!  \n\nIn light conversation (I tested with general knowledge questions and everyday scenarios), I'm achieving up to 34 tokens/s, which is \\*significantly\\* faster than other models I've tested (all Q4 except for qwen3):\n\n* Qwen3 (30B): \\~34 tokens/s\n* Qwen2.5 (32B): \\~10 tokens/s\n* Gemma3 (27B): \\~10 tokens/s\n* Llama3 (70B): 4-5 tokens/s\n\nHowever, I'm also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.\n\nMy first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What's your experience been like with speed and accuracy of Qwen3?",
          "author_fullname": "t2_tlzk7zie",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 (30B) with Ollama: Blazing Fast, but accuracy concerns",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfl6bo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Ollama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with Qwen3:30b-a3b-instruct-2507-q8_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I&amp;#39;m really impressed with the speed!  &lt;/p&gt;\n\n&lt;p&gt;In light conversation (I tested with general knowledge questions and everyday scenarios), I&amp;#39;m achieving up to 34 tokens/s, which is *significantly* faster than other models I&amp;#39;ve tested (all Q4 except for qwen3):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 (30B): ~34 tokens/s&lt;/li&gt;\n&lt;li&gt;Qwen2.5 (32B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Gemma3 (27B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Llama3 (70B): 4-5 tokens/s&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, I&amp;#39;m also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.&lt;/p&gt;\n\n&lt;p&gt;My first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What&amp;#39;s your experience been like with speed and accuracy of Qwen3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Ollama",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfl6bo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gerhardmpl",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754122088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Something with dockerfiles would be nice.\n\nMain requirement is to be able to run the following social media apps: (ordered by priority)\n\n- WhatsApp \n- WhatsApp Business\n- Linkedin\n- X\n- Reddit\n- Youtube",
          "author_fullname": "t2_1utdj13v24",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best way to connect Android with LLM - Virtually",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk60l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118213,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Something with dockerfiles would be nice.&lt;/p&gt;\n\n&lt;p&gt;Main requirement is to be able to run the following social media apps: (ordered by priority)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;WhatsApp &lt;/li&gt;\n&lt;li&gt;WhatsApp Business&lt;/li&gt;\n&lt;li&gt;Linkedin&lt;/li&gt;\n&lt;li&gt;X&lt;/li&gt;\n&lt;li&gt;Reddit&lt;/li&gt;\n&lt;li&gt;Youtube&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfk60l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rozeappletree",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk60l/what_is_the_best_way_to_connect_android_with_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk60l/what_is_the_best_way_to_connect_android_with_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754118213,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sup guys. I've been using the voyage 3 lg as an embedding model for the longest time and because an embedding model can't be switched and you need to fill the vector database from scratch, I didn't switch even after the release of great OS models.  \nRecently I've been thinking of switching to either qwen 3 0.6b, 4b or 8b.  \nCan anyone tell me if in terms of performance voyage 3 lg beats these 3?  \nDon't worry about the pricing. Since the documents are already ingested using voyage 3 lg, the cost has already been paid, if I switch I do need to do that process all over again.\n\nThanks in advance.",
          "author_fullname": "t2_6f9r5p4z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Embedding models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk4hx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118068,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sup guys. I&amp;#39;ve been using the voyage 3 lg as an embedding model for the longest time and because an embedding model can&amp;#39;t be switched and you need to fill the vector database from scratch, I didn&amp;#39;t switch even after the release of great OS models.&lt;br/&gt;\nRecently I&amp;#39;ve been thinking of switching to either qwen 3 0.6b, 4b or 8b.&lt;br/&gt;\nCan anyone tell me if in terms of performance voyage 3 lg beats these 3?&lt;br/&gt;\nDon&amp;#39;t worry about the pricing. Since the documents are already ingested using voyage 3 lg, the cost has already been paid, if I switch I do need to do that process all over again.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfk4hx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackkksparx",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk4hx/embedding_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk4hx/embedding_models/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754118068,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tried [https://huggingface.co/MetaStoneTec/XBai-o4](https://huggingface.co/MetaStoneTec/XBai-o4) ? Big if true -\n\n\\&gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance¬†\n\nHave not tried it myself, downloading atm from [https://huggingface.co/mradermacher/XBai-o4-GGUF](https://huggingface.co/mradermacher/XBai-o4-GGUF)",
          "author_fullname": "t2_4rlver1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MetaStoneTec/XBai-o4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk3y2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt; ? Big if true -&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini&amp;#39;s performance¬†&lt;/p&gt;\n\n&lt;p&gt;Have not tried it myself, downloading atm from &lt;a href=\"https://huggingface.co/mradermacher/XBai-o4-GGUF\"&gt;https://huggingface.co/mradermacher/XBai-o4-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?auto=webp&amp;s=21ea26fb14e3912f80713c95ddb95b37cd316f66",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f69ac45b09b4ccb4576fc51eb90d2b847eace7ec",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a115ce28ec07456cec2ed3598f59bf288c35b1a9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=02748428a3ab0ac779c13dffcf54fbcc054b5691",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed975e60e78ac76aed0294ed900e215343eefe17",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64a929e4331b92d45b82827765bb555c57573df7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24081c4d549b93ebfca1a8d44ef5f08c619cebf8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfk3y2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ljosif",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754118021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I don't have a powerful computer or GPU, just a 2021 macbook m1 with 8gb memory. I assume I can't run anything with more than 7b active parameters but chatgpt told me I can't run even run something like Qwen3-30B-A3B. What can I do, and where should I start?",
          "author_fullname": "t2_4i1pi15z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting started",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjqcb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116638,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I don&amp;#39;t have a powerful computer or GPU, just a 2021 macbook m1 with 8gb memory. I assume I can&amp;#39;t run anything with more than 7b active parameters but chatgpt told me I can&amp;#39;t run even run something like Qwen3-30B-A3B. What can I do, and where should I start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfjqcb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Snoo-72709",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjqcb/getting_started/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjqcb/getting_started/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754116638,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_8pgou3uq9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made new stealth model horizon beta deep think just for fun",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjp96",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=a5a44cc9c23ceeeed4546a74955fe6edec4d9fb2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754116528,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/llamameta/openrouter",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?auto=webp&amp;s=a2b7f7140b5cf24ef65092fcae9d15b92750b042",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=28a2f4d130d19bc84eba2e769bdc6da1606486cb",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4614ca9de0c843470ec9e42f079365e2f1efd09f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1e1b69118065223de139041415e28fb5b29dddd",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5b25f59c5cb22f4f263f985df0944061c773bb5",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55092f3b29575111b994d1e44d9cc89e9c6063df",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af1fd48eb9eed43ffcec241c7d67c6444ec9dcf6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "AXcO63R4UddohnbNRFl_f2U_tH7YYtPoIaujpbGKPx0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfjp96",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "balianone",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjp96/i_made_new_stealth_model_horizon_beta_deep_think/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/llamameta/openrouter",
          "subreddit_subscribers": 509052,
          "created_utc": 1754116528,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;format=png&amp;auto=webp&amp;s=22de475006dd11573f99931c23e963cc06d5f910\n\nTill yesterday it was there but was giving some issues of workplace , but today they have completely removed the Full Stack tool",
          "author_fullname": "t2_amn2vwc3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM just removed there full stack tool...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "91lvjyp8wjgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 41,
                  "x": 108,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf84c3973fbf2c776a5f27b753bcf5d020100161"
                },
                {
                  "y": 83,
                  "x": 216,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a29eb93da8f42b1c19aa80cc6d4d2dfe223ac33f"
                },
                {
                  "y": 123,
                  "x": 320,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbc017963090285e050dd5dc5465da0a02e6c03a"
                },
                {
                  "y": 246,
                  "x": 640,
                  "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=72afb773d2b9743030b1860951271b816813dc5b"
                }
              ],
              "s": {
                "y": 352,
                "x": 914,
                "u": "https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;format=png&amp;auto=webp&amp;s=22de475006dd11573f99931c23e963cc06d5f910"
              },
              "id": "91lvjyp8wjgf1"
            }
          },
          "name": "t3_1mfjn9f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.55,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/kOwRnn8lDJI3S0tn3v6z_XszlYbAiUEIatDoCZHtUew.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22de475006dd11573f99931c23e963cc06d5f910\"&gt;https://preview.redd.it/91lvjyp8wjgf1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22de475006dd11573f99931c23e963cc06d5f910&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Till yesterday it was there but was giving some issues of workplace , but today they have completely removed the Full Stack tool&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfjn9f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ITellMyselfSecrets__",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjn9f/glm_just_removed_there_full_stack_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjn9f/glm_just_removed_there_full_stack_tool/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754116335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.\n\nI also prefer to use CLI only, because I want everything to eventually be for a robot I'm working on (because of this I don't really want a UI interface). For some I haven't fully tested only the CLI, and for some I've tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.\n\nI will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here's my personal \"rankings\".\n\n* **Bark/Coqui TTS -**\n   * **The Good:**¬†The emotions are next level... kinda. At least they have it, is the main thing. What I've done is create a custom Llama model, that knows when to send a \\[laughs\\], \\[sighs\\], etc. that's appropriate, given the conversation. The custom ollama model is pretty good at this (if you're curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.\n   * **The Bad:**¬†It's pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also \"trails off\", if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I'm using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn't sometimes even follow the cloned voice. However, when it does, it's somewhat decent. But given how it often does not, it's not really too usable.\n* **F5 TTS -**\n   * **The Good:**¬†Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I'd say it's better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that's close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn't trail off. It will finish speaking until the text from my custom ollama model is done being spoken.\n   * **The Bad:**¬†As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I'd say it's about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can't do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn't sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It's pretty good, but not perfect.\n* **Orpheus TTS**\n   * **The Good:**¬†This one can also do laughing, yawning, etc. and it's decent at it. But not as good as Coqui/Bark. Although it's still better than what most offer, since it has the ability at all. There's a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn't trail off.\n   * **The Bad:**¬†This one is a pain to set up, at least if you try to go the normal route, via CLI. I've only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I'd say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn't get picked up, and it just says \"yawn\", instead. Coqui didn't really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).\n* **Kokoro TTS**\n   * **The Good:**¬†Man, the UI is blazing FAST. If I had to guess about \\~ 1 second or so. And that's using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn't perfect, it's probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn't trail off and reads all the speech too, which is nice.\n   * **The Bad:**¬†It sounds a little bland. Some of the models, even if they don't have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn't distinct between exclamation, or questions, much. It's not terrible, but sounds like an average Speech to Text, that you'd find on an average book reader, for example. Also doesn't offer native voice cloning, that I'm aware of at least, but I could be wrong.",
          "author_fullname": "t2_4guqxmy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS Model Comparisons: My Personal Rankings (So far) of TTS Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjn88",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp;amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.&lt;/p&gt;\n\n&lt;p&gt;I also prefer to use CLI only, because I want everything to eventually be for a robot I&amp;#39;m working on (because of this I don&amp;#39;t really want a UI interface). For some I haven&amp;#39;t fully tested only the CLI, and for some I&amp;#39;ve tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.&lt;/p&gt;\n\n&lt;p&gt;I will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here&amp;#39;s my personal &amp;quot;rankings&amp;quot;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Bark/Coqui TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†The emotions are next level... kinda. At least they have it, is the main thing. What I&amp;#39;ve done is create a custom Llama model, that knows when to send a [laughs], [sighs], etc. that&amp;#39;s appropriate, given the conversation. The custom ollama model is pretty good at this (if you&amp;#39;re curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†It&amp;#39;s pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also &amp;quot;trails off&amp;quot;, if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I&amp;#39;m using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn&amp;#39;t sometimes even follow the cloned voice. However, when it does, it&amp;#39;s somewhat decent. But given how it often does not, it&amp;#39;s not really too usable.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;F5 TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I&amp;#39;d say it&amp;#39;s better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that&amp;#39;s close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn&amp;#39;t trail off. It will finish speaking until the text from my custom ollama model is done being spoken.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I&amp;#39;d say it&amp;#39;s about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can&amp;#39;t do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn&amp;#39;t sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It&amp;#39;s pretty good, but not perfect.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orpheus TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†This one can also do laughing, yawning, etc. and it&amp;#39;s decent at it. But not as good as Coqui/Bark. Although it&amp;#39;s still better than what most offer, since it has the ability at all. There&amp;#39;s a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn&amp;#39;t trail off.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†This one is a pain to set up, at least if you try to go the normal route, via CLI. I&amp;#39;ve only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I&amp;#39;d say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn&amp;#39;t get picked up, and it just says &amp;quot;yawn&amp;quot;, instead. Coqui didn&amp;#39;t really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kokoro TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†Man, the UI is blazing FAST. If I had to guess about ~ 1 second or so. And that&amp;#39;s using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn&amp;#39;t perfect, it&amp;#39;s probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn&amp;#39;t trail off and reads all the speech too, which is nice.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†It sounds a little bland. Some of the models, even if they don&amp;#39;t have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn&amp;#39;t distinct between exclamation, or questions, much. It&amp;#39;s not terrible, but sounds like an average Speech to Text, that you&amp;#39;d find on an average book reader, for example. Also doesn&amp;#39;t offer native voice cloning, that I&amp;#39;m aware of at least, but I could be wrong.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfjn88",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iKontact",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754116331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the current recommended local LLM inference HW (**local, always-on inference box)** for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.  \nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.\n\n**What are the Hardware Options?:**\n\n* **Apple M3/M4 Ultra**\n* **AMD AI Max+ 395**\n* NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?\n\nWhat‚Äôs the most **practical prosumer option**?  \nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  \n\n\nI'm favoring the Strix Halo, but I think I'll be limited to Gemma 27B with maybe another model loaded at best.",
          "author_fullname": "t2_l0ba7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "24/7 local HW buying guide 2025-H2?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj6fq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114575,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the current recommended local LLM inference HW (&lt;strong&gt;local, always-on inference box)&lt;/strong&gt; for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.&lt;br/&gt;\nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are the Hardware Options?:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Apple M3/M4 Ultra&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AMD AI Max+ 395&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What‚Äôs the most &lt;strong&gt;practical prosumer option&lt;/strong&gt;?&lt;br/&gt;\nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m favoring the Strix Halo, but I think I&amp;#39;ll be limited to Gemma 27B with maybe another model loaded at best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj6fq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xraybies",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754114575,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to make a chatbot that sounds and acts like BMO from adventure time and was wondering if there is a TTS model that I can use a premade voice.   \n  \nThe voice I downloaded is from [https://voice-models.com/](https://voice-models.com/) and has a .index file and a .pth file if that means anything or helps at all",
          "author_fullname": "t2_69b6kbxa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS that I can use a downloaded AI voice for? (not sure if this is the right place to ask)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj3vj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114327,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to make a chatbot that sounds and acts like BMO from adventure time and was wondering if there is a TTS model that I can use a premade voice.   &lt;/p&gt;\n\n&lt;p&gt;The voice I downloaded is from &lt;a href=\"https://voice-models.com/\"&gt;https://voice-models.com/&lt;/a&gt; and has a .index file and a .pth file if that means anything or helps at all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj3vj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crackaddict42069",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj3vj/tts_that_i_can_use_a_downloaded_ai_voice_for_not/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj3vj/tts_that_i_can_use_a_downloaded_ai_voice_for_not/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754114327,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Huggingface announced that it‚Äôs OSS so I found their GitHub, but the whole point of open source robotics is provision of CAD files and electronic drawings as well, if I am not wrong? \n\nI didn‚Äôt find it anywhere. \n[reachy mini](https://youtu.be/JvdBJZ-qR18?si=DntWm0Um6a9Nab6I)\nDo hugging face plan to release the printable 3d models and the component lists? \n\nBlog post:¬†https://huggingface.co/blog/reachy-mini\nThomas Wolf on ùïè:¬†https://x.com/Thom_Wolf/status/1942887160983466096 less",
          "author_fullname": "t2_19t1rovk98",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reach Mini is not Open source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfj2bn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754114359,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754114167,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Huggingface announced that it‚Äôs OSS so I found their GitHub, but the whole point of open source robotics is provision of CAD files and electronic drawings as well, if I am not wrong? &lt;/p&gt;\n\n&lt;p&gt;I didn‚Äôt find it anywhere. \n&lt;a href=\"https://youtu.be/JvdBJZ-qR18?si=DntWm0Um6a9Nab6I\"&gt;reachy mini&lt;/a&gt;\nDo hugging face plan to release the printable 3d models and the component lists? &lt;/p&gt;\n\n&lt;p&gt;Blog post:¬†&lt;a href=\"https://huggingface.co/blog/reachy-mini\"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;\nThomas Wolf on ùïè:¬†&lt;a href=\"https://x.com/Thom_Wolf/status/1942887160983466096\"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt; less&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?auto=webp&amp;s=42a3145ee1aa761204406729b87cbb2fce4b691a",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0a9e07799ac2b26c25790a649f719155c402025",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f62c00eb6fb285199219b5efec22a6aa0213f7d",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cc0d47dae562a47f9241f9d6d74d3b712ec3a47",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "NCFk7nfLDEfVT123xThdE1hQ0xo7VlGM_ekFBsduBFk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfj2bn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Slow_Protection_26",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfj2bn/reach_mini_is_not_open_source/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj2bn/reach_mini_is_not_open_source/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754114167,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new models from Skywork:\n\nWe introduce **MindLink**, a new family of large language models developed by **Kunlun Inc**. Built on **Qwen**, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.\n\n* **Plan-based Reasoning**: Without the \"think\" tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.\n* **Mathematical Framework**: It analyzes the effectiveness of both **Chain-of-Thought (CoT)** and **Plan-based Reasoning**.\n* **Adaptive Reasoning**: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.\n\n[https://huggingface.co/Skywork/MindLink-32B-0801](https://huggingface.co/Skywork/MindLink-32B-0801)\n\n[https://huggingface.co/Skywork/MindLink-72B-0801](https://huggingface.co/Skywork/MindLink-72B-0801)\n\n[https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF](https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF)\n\n\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork MindLink 32B/72B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfitwb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "#bbbdbf",
          "ups": 133,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 133,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/RNvygcC1t5B12yynYguEPL4rixiqNeO2Wo07y1iwph0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754113315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new models from Skywork:&lt;/p&gt;\n\n&lt;p&gt;We introduce &lt;strong&gt;MindLink&lt;/strong&gt;, a new family of large language models developed by &lt;strong&gt;Kunlun Inc&lt;/strong&gt;. Built on &lt;strong&gt;Qwen&lt;/strong&gt;, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;: Without the &amp;quot;think&amp;quot; tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mathematical Framework&lt;/strong&gt;: It analyzes the effectiveness of both &lt;strong&gt;Chain-of-Thought (CoT)&lt;/strong&gt; and &lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Adaptive Reasoning&lt;/strong&gt;: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-32B-0801\"&gt;https://huggingface.co/Skywork/MindLink-32B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-72B-0801\"&gt;https://huggingface.co/Skywork/MindLink-72B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF\"&gt;https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/im7w319dnjgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/im7w319dnjgf1.png?auto=webp&amp;s=47f7f89ad190a3c63e1ac553dd6a3484a9ad8ca0",
                  "width": 2042,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01cd51cd78031eda5866ff7d7ad34590c408908f",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39cd3616dd75544828cbb104491bd20bffd83ee2",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e60ba4473193249923ba5fc4a97b6f3c8979b6",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7493c60ab05796cf114bd0fa0c600e5aa06497f7",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d17b7719956d66090e1c5852b8ee404b2d44a161",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9af2b894d2bfc41eb10b3bdbb12a1975e5e87cac",
                    "width": 1080,
                    "height": 812
                  }
                ],
                "variants": {},
                "id": "dMPU8D8NEUnZXYRxA5Ryy9ueDVT8WMowE2c2asQCAgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfitwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/",
          "stickied": false,
          "url": "https://i.redd.it/im7w319dnjgf1.png",
          "subreddit_subscribers": 509052,
          "created_utc": 1754113315,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently switched my local models to the new 30B-A3B 2507 models. However, when testing the instruct model, I noticed it hallucinates much more than previous Qwen models.\n\nI fed it a README file I wrote myself for summarization, so I know its contents well. The 2507 instruct model not only uses excessive emojis but also fabricates lots of information that isn‚Äôt in the file.\n\nI also tested the 2507 thinking and coder versions with the same README, prompt, and quantization level (q4). Both used zero emojis and showed no noticeable hallucinations.\n\nHas anyone else experienced similar issues with the 2507 instruct model?\n\n* I'm using llama.cpp + llama swap, and the \"best practice\" settings from the HF model card ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Serious hallucination issues of 30B-A3B Instruct 2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfiroj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754113093,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently switched my local models to the new 30B-A3B 2507 models. However, when testing the instruct model, I noticed it hallucinates much more than previous Qwen models.&lt;/p&gt;\n\n&lt;p&gt;I fed it a README file I wrote myself for summarization, so I know its contents well. The 2507 instruct model not only uses excessive emojis but also fabricates lots of information that isn‚Äôt in the file.&lt;/p&gt;\n\n&lt;p&gt;I also tested the 2507 thinking and coder versions with the same README, prompt, and quantization level (q4). Both used zero emojis and showed no noticeable hallucinations.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else experienced similar issues with the 2507 instruct model?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;m using llama.cpp + llama swap, and the &amp;quot;best practice&amp;quot; settings from the HF model card &lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfiroj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfiroj/serious_hallucination_issues_of_30ba3b_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfiroj/serious_hallucination_issues_of_30ba3b_instruct/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754113093,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to make an agent that get YouTube videos transcript but i keep having ip ban or a ban from requests to youtube-transcript-api, how to manage this?",
          "author_fullname": "t2_w2tkxer3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to avoid IP bans when using youtube-transcript-api to fetch YouTube video transcripts?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfij9a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754112257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to make an agent that get YouTube videos transcript but i keep having ip ban or a ban from requests to youtube-transcript-api, how to manage this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfij9a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Anas_M1nt",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfij9a/how_to_avoid_ip_bans_when_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfij9a/how_to_avoid_ip_bans_when_using/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754112257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna use this model for DMing a dnd game as well as using it to write stories. I‚Äôd like it to be abliterated if possible.\n\nI‚Äôve been looking at using Gemma 3 27B, and I do like its writing style, but I‚Äôm concerned about its ability to handle long context lengths.\n\nSo far I haven‚Äôt had that problem but that‚Äôs only because I‚Äôve been running it with low context lengths, since I‚Äôm using it on my gaming pc right now.\n\nI‚Äôm in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.\n\nThanks in advance ",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best creative writing + long context model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfifhh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna use this model for DMing a dnd game as well as using it to write stories. I‚Äôd like it to be abliterated if possible.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve been looking at using Gemma 3 27B, and I do like its writing style, but I‚Äôm concerned about its ability to handle long context lengths.&lt;/p&gt;\n\n&lt;p&gt;So far I haven‚Äôt had that problem but that‚Äôs only because I‚Äôve been running it with low context lengths, since I‚Äôm using it on my gaming pc right now.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfifhh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754111899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Lately, LLM model names have become completely unhinged:\n\n* `Qwen3-30B-A3B-Instruct-2507`\n* `Qwen3-30B-A3B-Instruct-2507-GGUF`\n* `Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound`\n* ...and so on.\n\nI propose we assign each a short, memorable alias that represents the *personality* of its capabilities. Keep the technical names, of course ‚Äî but also give them a fun alias that makes it easier and more enjoyable to refer to them in discussion.\n\nThis idea was a joke at first, but honestly, I‚Äôm serious now. We need this.\n\nSome software projects have begun using alias names for popular models, e.g., [Ollama](https://ollama.com/library/qwen3) and [Swama](https://github.com/Trans-N-ai/swama/?tab=readme-ov-file#2-available-model-aliases). But even when trying to shorten these names, they still end up long and clunky:\n\n&gt;‚ÄúHi! My name is `Qwen3-30B-A3B-Thinking-2507`, but my friends call me `qwen3-30b-2507-thinking`.‚Äù\n\nI see people misnaming models often in casual conversation. People will just say, ‚ÄúQwen3 coder‚Äù or ‚ÄúQwen3 30B‚Äù ‚Äì¬†it gets confusing.\n\nAnd, we risk [making Simon salty](https://x.com/simonw/status/1950607273656746157).\n\nIdeally, these aliases would be registered along with the full model names by the model creators and forkers in common catalogs like Hugging Face and in their press releases. The point is to have a single standard alias for each model release.\n\nAs an example, I made up these names that take inspiration from Swama‚Äôs homeland:\n\n* **saitama** (`Qwen3-235B-A22B-Instruct-2507` ‚Äî perfect answer, first try)\n* **zenitsu** (`Qwen3-235B-A22B-Thinking-2507` ‚Äî panics, then gets it right)\n* **chibi** (`Qwen3-30B-A3B-Instruct-2507` ‚Äî tiny, cute, surprisingly lucky)\n* **poyo** (`Qwen3-30B-A3B-Thinking-2507` ‚Äî fast, random, sometimes correct)\n* **deku** (`Qwen3-Coder-30B-A3B-Instruct` ‚Äî nerdy, eager, needs checking)\n* **kakashi** (`Qwen3-Coder-480B-A35B-Instruct` ‚Äî cool senior, still a nerd)\n\nReally, isn't this better:\n\n    llm -m chibi \"Tell me a joke\"\n\nüôÉ",
          "author_fullname": "t2_6go4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI model names are out of control. Let‚Äôs give them nicknames.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfia6f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754121089,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111399,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, LLM model names have become completely unhinged:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Qwen3-30B-A3B-Instruct-2507-GGUF&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Qwen3-30B-A3B-Instruct-2507-gguf-q2ks-mixed-AutoRound&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;...and so on.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I propose we assign each a short, memorable alias that represents the &lt;em&gt;personality&lt;/em&gt; of its capabilities. Keep the technical names, of course ‚Äî but also give them a fun alias that makes it easier and more enjoyable to refer to them in discussion.&lt;/p&gt;\n\n&lt;p&gt;This idea was a joke at first, but honestly, I‚Äôm serious now. We need this.&lt;/p&gt;\n\n&lt;p&gt;Some software projects have begun using alias names for popular models, e.g., &lt;a href=\"https://ollama.com/library/qwen3\"&gt;Ollama&lt;/a&gt; and &lt;a href=\"https://github.com/Trans-N-ai/swama/?tab=readme-ov-file#2-available-model-aliases\"&gt;Swama&lt;/a&gt;. But even when trying to shorten these names, they still end up long and clunky:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;‚ÄúHi! My name is &lt;code&gt;Qwen3-30B-A3B-Thinking-2507&lt;/code&gt;, but my friends call me &lt;code&gt;qwen3-30b-2507-thinking&lt;/code&gt;.‚Äù&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I see people misnaming models often in casual conversation. People will just say, ‚ÄúQwen3 coder‚Äù or ‚ÄúQwen3 30B‚Äù ‚Äì¬†it gets confusing.&lt;/p&gt;\n\n&lt;p&gt;And, we risk &lt;a href=\"https://x.com/simonw/status/1950607273656746157\"&gt;making Simon salty&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Ideally, these aliases would be registered along with the full model names by the model creators and forkers in common catalogs like Hugging Face and in their press releases. The point is to have a single standard alias for each model release.&lt;/p&gt;\n\n&lt;p&gt;As an example, I made up these names that take inspiration from Swama‚Äôs homeland:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;saitama&lt;/strong&gt; (&lt;code&gt;Qwen3-235B-A22B-Instruct-2507&lt;/code&gt; ‚Äî perfect answer, first try)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;zenitsu&lt;/strong&gt; (&lt;code&gt;Qwen3-235B-A22B-Thinking-2507&lt;/code&gt; ‚Äî panics, then gets it right)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;chibi&lt;/strong&gt; (&lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt; ‚Äî tiny, cute, surprisingly lucky)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;poyo&lt;/strong&gt; (&lt;code&gt;Qwen3-30B-A3B-Thinking-2507&lt;/code&gt; ‚Äî fast, random, sometimes correct)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;deku&lt;/strong&gt; (&lt;code&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/code&gt; ‚Äî nerdy, eager, needs checking)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;kakashi&lt;/strong&gt; (&lt;code&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/code&gt; ‚Äî cool senior, still a nerd)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Really, isn&amp;#39;t this better:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llm -m chibi &amp;quot;Tell me a joke&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;üôÉ&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfia6f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "quinncom",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfia6f/ai_model_names_are_out_of_control_lets_give_them/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfia6f/ai_model_names_are_out_of_control_lets_give_them/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754111399,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I try to run all of my models at 32k context using llama.cpp, but it feels bad to be losing so much performance compared to launching with 2-4k context for short one-shot question prompts",
          "author_fullname": "t2_1j5x86i7wh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What context lengths do people actually run their models at?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfi8ly",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111252,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I try to run all of my models at 32k context using llama.cpp, but it feels bad to be losing so much performance compared to launching with 2-4k context for short one-shot question prompts&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfi8ly",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OUT_OF_HOST_MEMORY",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfi8ly/what_context_lengths_do_people_actually_run_their/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfi8ly/what_context_lengths_do_people_actually_run_their/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754111252,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI am a medical student and had begun to spend a significant amount of time creating a clinic notebook using Notion. Problem is, I essentially have to take all the text from every pdf and PowerPoint, paste it into notion, reformat (this takes forever) only to be able to have the text searchable because it can only embed documents. Not search them.\n\nI had been reading about LLM which would essentially allow me to create a master file, upload the hundreds if not thousands of documents of medical information, and then use AI to search my documents and retrieve the info specified in the prompt.\n\nI‚Äôm just not sure if this is something I can do through ChatGPT, Claude, or using llama. Trying to become more educated in this. \n\nAny insight? Thoughts?\n\nThanks for your time.",
          "author_fullname": "t2_66d4kglu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Med school and LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhv2c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754110002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am a medical student and had begun to spend a significant amount of time creating a clinic notebook using Notion. Problem is, I essentially have to take all the text from every pdf and PowerPoint, paste it into notion, reformat (this takes forever) only to be able to have the text searchable because it can only embed documents. Not search them.&lt;/p&gt;\n\n&lt;p&gt;I had been reading about LLM which would essentially allow me to create a master file, upload the hundreds if not thousands of documents of medical information, and then use AI to search my documents and retrieve the info specified in the prompt.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm just not sure if this is something I can do through ChatGPT, Claude, or using llama. Trying to become more educated in this. &lt;/p&gt;\n\n&lt;p&gt;Any insight? Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfhv2c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IndubitablyPreMed",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhv2c/med_school_and_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhv2c/med_school_and_llm/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754110002,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I bought a second-hand server on eBay without knowing what was inside it. I knew I needed the case for my remote gaming rack solution. The Supermicro case had an air shroud and four oversized PCIe 3.0 x16 slots.\n\nWhen it arrived, I found an NVIDIA Tesla V100S 32 GB HBM2 PCIe 3.0 x16 GPU behind the air shroud. The seller probably didn't see it (it's worth far more than I paid for the whole case).\n\nWhile it's not the most up-to-date GPU anymore, I'm thinking of using it for home automation (it supports sharing the GPU with different VMs, where I can run various automation tasks and local LLMs to communicate with intruders, etc.).\n\nI used DeepSeek at work in our HPC. However, I am not up to date. Which models would work best with the 32 GB Tesla GPU I have? Do you have any other ideas?",
          "author_fullname": "t2_38k1zqv9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What to do with a NVIDIA Tesla V100S 32GB GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhji6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754108926,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a second-hand server on eBay without knowing what was inside it. I knew I needed the case for my remote gaming rack solution. The Supermicro case had an air shroud and four oversized PCIe 3.0 x16 slots.&lt;/p&gt;\n\n&lt;p&gt;When it arrived, I found an NVIDIA Tesla V100S 32 GB HBM2 PCIe 3.0 x16 GPU behind the air shroud. The seller probably didn&amp;#39;t see it (it&amp;#39;s worth far more than I paid for the whole case).&lt;/p&gt;\n\n&lt;p&gt;While it&amp;#39;s not the most up-to-date GPU anymore, I&amp;#39;m thinking of using it for home automation (it supports sharing the GPU with different VMs, where I can run various automation tasks and local LLMs to communicate with intruders, etc.).&lt;/p&gt;\n\n&lt;p&gt;I used DeepSeek at work in our HPC. However, I am not up to date. Which models would work best with the 32 GB Tesla GPU I have? Do you have any other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfhji6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gromhelmu",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhji6/what_to_do_with_a_nvidia_tesla_v100s_32gb_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhji6/what_to_do_with_a_nvidia_tesla_v100s_32gb_gpu/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754108926,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A lot of people including myself want a personalized AI tool. Not in the sense of tones and personality, but one that adapts to my work style - answer questions and do deep researches based on what I care about from past conversations. I don't really see any tools can do this. Even chatgpt's memory today is still quite basic. It only remembers facts from the past and quotes that from time to time.\n\nI want to implement this logic in my tool. But anything specific I can do besides building RAG? What else can I do to to make the LLM truely \"adapt\"?",
          "author_fullname": "t2_ho98y0dwh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good practices to implement memory for LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfhfg0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754108553,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of people including myself want a personalized AI tool. Not in the sense of tones and personality, but one that adapts to my work style - answer questions and do deep researches based on what I care about from past conversations. I don&amp;#39;t really see any tools can do this. Even chatgpt&amp;#39;s memory today is still quite basic. It only remembers facts from the past and quotes that from time to time.&lt;/p&gt;\n\n&lt;p&gt;I want to implement this logic in my tool. But anything specific I can do besides building RAG? What else can I do to to make the LLM truely &amp;quot;adapt&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfhfg0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tonyc1118",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfhfg0/good_practices_to_implement_memory_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfhfg0/good_practices_to_implement_memory_for_llms/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754108553,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Beta seems really solid from early testing, not a magnitude better than what SOTA's offer but still impressive",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Alpha vs Horizon Beta",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgwyu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756775954%2CZWRmMDI1MmY4NzdhYjZiY2UwNTJkN2YwMmMxY2Q2Y2U2OTMyMTExMDEyZTc4YzUzODA5MjRkODVkZTRmYzY4OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756775954%2CNTZlMTY4N2IwNjc4NjlkYjY2ZjExNGZjOTBmNDllYWRhMGExZDQzMmM5MzNkNWJjMmNhNmY0MTMzOTlmMmRlNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=140&amp;height=84&amp;crop=140:84,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=dd582f91752e7fd7a72f3be30d66c628b1cabf42",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754106912,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Beta seems really solid from early testing, not a magnitude better than what SOTA&amp;#39;s offer but still impressive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/dg8cy7ia4jgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?format=pjpg&amp;auto=webp&amp;s=30b632f2cdd42a3234c72ef686841eed57c63aec",
                  "width": 3600,
                  "height": 2178
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d42e84556d577f46b8f2cc66b308d63c6ba3aa39",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0a342e02b97eb15e7ae8d55711f5dc21ed403177",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1c923808071843a8b872aa37e16f734fbae495b3",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbb1ef3633d6485f7f54280916bb8b15600f3044",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1f330f77b0817cdba51f914a124707d5bffe8a1",
                    "width": 960,
                    "height": 580
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9a786dc189b4a8425437a29d560c6d7825e64a0",
                    "width": 1080,
                    "height": 653
                  }
                ],
                "variants": {},
                "id": "d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfgwyu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/",
          "stickied": false,
          "url": "https://v.redd.it/dg8cy7ia4jgf1",
          "subreddit_subscribers": 509052,
          "created_utc": 1754106912,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756775954%2CZWRmMDI1MmY4NzdhYjZiY2UwNTJkN2YwMmMxY2Q2Y2U2OTMyMTExMDEyZTc4YzUzODA5MjRkODVkZTRmYzY4OA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756775954%2CNTZlMTY4N2IwNjc4NjlkYjY2ZjExNGZjOTBmNDllYWRhMGExZDQzMmM5MzNkNWJjMmNhNmY0MTMzOTlmMmRlNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does the current llama.cpp binaries release support Blackwell GPU in Windows? I just got the card and not sure how to move forward.\n\nDo I need to recompile the binaries for Windows ? Please share your experience. Much appreciated.",
          "author_fullname": "t2_1vxjobhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Blackwell (RTX 5090 / RTX 6000 Pro) support in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgqb0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754106336,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does the current llama.cpp binaries release support Blackwell GPU in Windows? I just got the card and not sure how to move forward.&lt;/p&gt;\n\n&lt;p&gt;Do I need to recompile the binaries for Windows ? Please share your experience. Much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfgqb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud_Structure4664",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgqb0/blackwell_rtx_5090_rtx_6000_pro_support_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfgqb0/blackwell_rtx_5090_rtx_6000_pro_support_in/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754106336,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "all I need....",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgj0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 1280,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1280,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGttwXkoUDYh4g0k8yxukZYUVavOKH_4hrW2zTWYkis.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754105691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?auto=webp&amp;s=56a0a807d6543ba43266157bcaa0b48793bcd3df",
                  "width": 1024,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0faf4ee7c2bcdc1e4161739543ede55c4684b2b8",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2e17cf5370e4cca658ed8a629406a0522d68d7e",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb58e48e0282066e26e22c2d00d814c00c5ded2",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2db9397b5cffd5e90a0cc1250191ed983494cb",
                    "width": 960,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "FTO20S33694FU83Br474HCo8srCRihH9Gw7Oe1llTgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mfgj0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 108,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgj0g/all_i_need/",
          "stickied": false,
          "url": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "subreddit_subscribers": 509052,
          "created_utc": 1754105691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First time poster, so I'm not sure if this is the right area, but I'm looking for some help troubleshooting performance issues.\n\nWhen using models that fit in VRAM, I get the expected performance or within reason.\n\nThe issues occur when using models that need to spill over into system RAM. Specifically, I've noticed a significant drop in performance with the model **qwen3:30b-a3b-q4\\_K\\_M**, though **Deepseek R1 32B** is showing similar issues.\n\nWhen I run qwen3:30b-a3b-q4\\_K\\_M on CPU with no GPU installed I get \\~19t/s as measured by Open Web UI.\n\nWhen running qwen3:30b-a3b-q4\\_K\\_M on a mix of GPU/CPU I get the worse performance then running on CPU only. The performance degrades even further the more layers I offload to the CPU.\n\nTested the following in Ollama by modifying num_gpu:\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    20 GB    25%/75% CPU/GPU    4096  \neval rate:           10.02 tokens/s\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    20 GB    73%/27% CPU/GPU    4096  \neval rate:            4.35 tokens/s\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    19 GB    100% CPU     4096  \neval rate:            2.49 tokens/s\n\nOS is hosted in Proxmox. Going from 30 cores to 15 cores assigned to the VM had no effect on performance.\n\nSystem Specs:\n\nCPU: Gold 6254\n\nGPU: Nvidia T4 (16gb)\n\nOS: ubuntu 24.04\n\nOllama 0.10.1\n\nNvidia Driver 570.169 Cuda 12.8\n\nAny suggestions would be helpful.",
          "author_fullname": "t2_seb9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performance issues when using GPU and CPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffuv0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754105472,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754103612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time poster, so I&amp;#39;m not sure if this is the right area, but I&amp;#39;m looking for some help troubleshooting performance issues.&lt;/p&gt;\n\n&lt;p&gt;When using models that fit in VRAM, I get the expected performance or within reason.&lt;/p&gt;\n\n&lt;p&gt;The issues occur when using models that need to spill over into system RAM. Specifically, I&amp;#39;ve noticed a significant drop in performance with the model &lt;strong&gt;qwen3:30b-a3b-q4_K_M&lt;/strong&gt;, though &lt;strong&gt;Deepseek R1 32B&lt;/strong&gt; is showing similar issues.&lt;/p&gt;\n\n&lt;p&gt;When I run qwen3:30b-a3b-q4_K_M on CPU with no GPU installed I get ~19t/s as measured by Open Web UI.&lt;/p&gt;\n\n&lt;p&gt;When running qwen3:30b-a3b-q4_K_M on a mix of GPU/CPU I get the worse performance then running on CPU only. The performance degrades even further the more layers I offload to the CPU.&lt;/p&gt;\n\n&lt;p&gt;Tested the following in Ollama by modifying num_gpu:&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    20 GB    25%/75% CPU/GPU    4096&lt;br/&gt;\neval rate:           10.02 tokens/s&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    20 GB    73%/27% CPU/GPU    4096&lt;br/&gt;\neval rate:            4.35 tokens/s&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    19 GB    100% CPU     4096&lt;br/&gt;\neval rate:            2.49 tokens/s&lt;/p&gt;\n\n&lt;p&gt;OS is hosted in Proxmox. Going from 30 cores to 15 cores assigned to the VM had no effect on performance.&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: Gold 6254&lt;/p&gt;\n\n&lt;p&gt;GPU: Nvidia T4 (16gb)&lt;/p&gt;\n\n&lt;p&gt;OS: ubuntu 24.04&lt;/p&gt;\n\n&lt;p&gt;Ollama 0.10.1&lt;/p&gt;\n\n&lt;p&gt;Nvidia Driver 570.169 Cuda 12.8&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mffuv0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BabySasquatch1",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754103612,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, it‚Äôs been a while but I‚Äôm happy to announce another major update for my app **EasyWhisperUI**, now with **live transcriptions**!\n\nIt features full cross-platform GPU acceleration:\n\n* **Vulkan** on Windows (Intel, AMD, or NVIDIA)\n* **Metal** on macOS (Apple silicon)\n\n**New features!**\n\n1. **GPU-accelerated Live Transcriptions** ‚Ä¢ Transcribe speech in real time using your default mic (user request)\n2. **Output Cleanup** ‚Ä¢ Automatically removes repeated segments from live transcriptions\n3. **Open in Notepad Checkbox** ‚Ä¢ New option to disable automatic opening in Notepad after transcription (user request)\n4. Various bug fixes and code improvements.\n\n**Other key features**\n\n5. **Batch File Processing** ‚Ä¢ Drag &amp; drop multiple files ‚Äî EasyWhisperUI will queue and transcribe them automatically (user request)  \n6. **CPU-Only Toggle** ‚Ä¢ Option to disable GPU acceleration and run fully on CPU (user request)  \n7. **Modern UI** ‚Ä¢ Acrylic background on Windows, clean layout and spacing improvements  \n8. **macOS Support** ‚Ä¢ EasyWhisperUI works on macOS thanks to a community contribution  \n9. **Installer Included** ‚Ä¢ Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click\n\nThere are a lot more features ‚Äî check out the GitHub for more info:\n\nüîó **GitHub:** [https://github.com/mehtabmahir/easy-whisper-ui](https://github.com/mehtabmahir/easy-whisper-ui)\n\nLet me know what you think or if you have any suggestions!",
          "author_fullname": "t2_epvxz7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EasyWhisperUI ‚Äì GPU accelerated Open Source Whisper UI for Windows &amp; macOS now with Live Transcriptions!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffjjj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754103866,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754102625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, it‚Äôs been a while but I‚Äôm happy to announce another major update for my app &lt;strong&gt;EasyWhisperUI&lt;/strong&gt;, now with &lt;strong&gt;live transcriptions&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;It features full cross-platform GPU acceleration:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; on Windows (Intel, AMD, or NVIDIA)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Metal&lt;/strong&gt; on macOS (Apple silicon)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;New features!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;GPU-accelerated Live Transcriptions&lt;/strong&gt; ‚Ä¢ Transcribe speech in real time using your default mic (user request)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output Cleanup&lt;/strong&gt; ‚Ä¢ Automatically removes repeated segments from live transcriptions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Open in Notepad Checkbox&lt;/strong&gt; ‚Ä¢ New option to disable automatic opening in Notepad after transcription (user request)&lt;/li&gt;\n&lt;li&gt;Various bug fixes and code improvements.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Other key features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Batch File Processing&lt;/strong&gt; ‚Ä¢ Drag &amp;amp; drop multiple files ‚Äî EasyWhisperUI will queue and transcribe them automatically (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU-Only Toggle&lt;/strong&gt; ‚Ä¢ Option to disable GPU acceleration and run fully on CPU (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modern UI&lt;/strong&gt; ‚Ä¢ Acrylic background on Windows, clean layout and spacing improvements&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;macOS Support&lt;/strong&gt; ‚Ä¢ EasyWhisperUI works on macOS thanks to a community contribution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Installer Included&lt;/strong&gt; ‚Ä¢ Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are a lot more features ‚Äî check out the GitHub for more info:&lt;/p&gt;\n\n&lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/mehtabmahir/easy-whisper-ui\"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think or if you have any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?auto=webp&amp;s=8b1a4fb535e00a6da0368011b0e6cec45497c5d7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5822df3220f343ebd2934400d0f59847247da8d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=823cd6c140e6f8b93ef72f68e9d0b425f56243f3",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e4b350fed949d338ba9708cf08854a1b4237e89",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccc578f650ebc7544f2e8cfe89996ac4b84f501f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d4fb11dd5292586b4d7444d6d35ffe279e98ba1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2237e0f4ab1ae9e5ec2baa35b902b491ffb5d35",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mffjjj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehtabmahir",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754102625,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üë§ User 9:32:07  \nAlso, back propagation is really similar to the trial-error mechansisms of the brain  \nüß† Model 70B 9:32:52  \nThat's another fascinating parallel! The error-driven update mechanism in backpropagation does bear a striking resemblance to the trial-and-error processes observed in the brain. In both cases, there's an iterative refinement process where errors or discrepancies drive adjustments to improve performance. In the brain, this process is thought to occur through various feedback loops, such as the cortico-basal ganglia-thalamo-cortical loop, which refines motor skills through trial and error. Similarly, in backpropagation, the error gradient flows backwards through the network, adjusting weights and biases to minimize the difference between predicted outputs and actual targets. Both mechanisms rely on some form of exploration-exploitation trade-off, where the system balances exploring new possibilities with exploiting learned knowledge to optimize performance. This parallel highlights the brain's remarkable ability to adapt and learn from mistakes, which is essential for human intelligence. Moreover, this similarity raises intriguing questions about the fundamental principles governing learning and adaptation across biological and artificial systems. Could we be tapping into some universal principles of intelligence, ones that transcend the boundaries between living organisms and machines? What do you think about the implications of these parallels on our understanding of intelligence and its implementation in AI systems?\n\nWhat do you think about the responses for the model and how would you rate this model?",
          "author_fullname": "t2_1qaiikqj0m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you please review the comment from my tuned open source local model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffa5a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754101825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üë§ User 9:32:07&lt;br/&gt;\nAlso, back propagation is really similar to the trial-error mechansisms of the brain&lt;br/&gt;\nüß† Model 70B 9:32:52&lt;br/&gt;\nThat&amp;#39;s another fascinating parallel! The error-driven update mechanism in backpropagation does bear a striking resemblance to the trial-and-error processes observed in the brain. In both cases, there&amp;#39;s an iterative refinement process where errors or discrepancies drive adjustments to improve performance. In the brain, this process is thought to occur through various feedback loops, such as the cortico-basal ganglia-thalamo-cortical loop, which refines motor skills through trial and error. Similarly, in backpropagation, the error gradient flows backwards through the network, adjusting weights and biases to minimize the difference between predicted outputs and actual targets. Both mechanisms rely on some form of exploration-exploitation trade-off, where the system balances exploring new possibilities with exploiting learned knowledge to optimize performance. This parallel highlights the brain&amp;#39;s remarkable ability to adapt and learn from mistakes, which is essential for human intelligence. Moreover, this similarity raises intriguing questions about the fundamental principles governing learning and adaptation across biological and artificial systems. Could we be tapping into some universal principles of intelligence, ones that transcend the boundaries between living organisms and machines? What do you think about the implications of these parallels on our understanding of intelligence and its implementation in AI systems?&lt;/p&gt;\n\n&lt;p&gt;What do you think about the responses for the model and how would you rate this model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mffa5a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Over-Pilot4908",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffa5a/can_you_please_review_the_comment_from_my_tuned/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffa5a/can_you_please_review_the_comment_from_my_tuned/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754101825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have LLM studio installed on a server. And I did enable the feature to run as a server with Tailscale and on my Mac mini, I installed anything LLM . And when I set up anything LLM to use lm studio. It just says refreshing models and nothing else after that it does not pull any of the models I have installed. I‚Äôm just curious what I‚Äôm doing wrong. In my IP settings for anything LLM I have. http:// my up:1234/v1. But after letting it run 10 minutes, it does not pull any models at all. So to test to see if it was the server I installed ollama and that worked just fine. I‚Äôm just curious what am I doing wrong?",
          "author_fullname": "t2_33j4dylg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to LLM studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfek6x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have LLM studio installed on a server. And I did enable the feature to run as a server with Tailscale and on my Mac mini, I installed anything LLM . And when I set up anything LLM to use lm studio. It just says refreshing models and nothing else after that it does not pull any of the models I have installed. I‚Äôm just curious what I‚Äôm doing wrong. In my IP settings for anything LLM I have. http:// my up:1234/v1. But after letting it run 10 minutes, it does not pull any models at all. So to test to see if it was the server I installed ollama and that worked just fine. I‚Äôm just curious what am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfek6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wbiggs205",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754099629,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A quick heads up for anyone playing with the little [HuggingFaceTB/SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) model that was released a few weeks ago with llama.cpp.\n\nSmolLM3-3B supports toggling thinking mode using `/think` or `/no_think` in a system prompt, but it relies on Jinja template features that weren't available in llama.cpp's jinja processor until very recently (merged yesterday: [b56683eb](https://github.com/BradHutchings/Mmojo-Server/commit/b56683eb659d6d39138bd90b27cb258a21b7aa5c)).\n\nSo to get system-prompt `/think` and `/no_think` working, you need to be running the current master version of llama.cpp (until the next official release). I believe some Qwen3 templates might also be affected, so keep that in mind if you're using those.\n\n(And since it relies on the jinja template, if you want to be able to enable/disable thinking from the system prompt remember to pass `--jinja` to llama-cli and llama-server. Otherwise it will use a fallback template with no system prompt and no thinking.)\n\nAdditionally, I ran into a frustrating issue while using the llama-server with the built-in web client where SmolLM3-3B would stop thinking after a few messages even with thinking enabled. It turns out the model needs to see the `&lt;think&gt;&lt;/think&gt;` tags in previous messages or it will stop thinking. The llama web client, by default, has an option enabled that strips those tags.\n\nTo fix this, go to your web client settings -&gt; Reasoning and disable \"Exclude thought process when sending requests to API (Recommended for DeepSeek-R1)\".\n\nFinally, to have the web client correctly show the \"thinking\" section (that you can click to expand/collapse), you need to pass the `--reasoning-format none` option to llama-server. Example invocation:\n\n    ./llama-server --jinja -ngl 99 --temp 0.6 --reasoning-format none -c 64000 -fa -m ~/llama/models/smollm3-3b/SmolLM3-Q8_0.gguf",
          "author_fullname": "t2_38xkk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Getting SmolLM3-3B's /think and /no_think to work with llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeipz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099505,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A quick heads up for anyone playing with the little &lt;a href=\"https://huggingface.co/HuggingFaceTB/SmolLM3-3B\"&gt;HuggingFaceTB/SmolLM3-3B&lt;/a&gt; model that was released a few weeks ago with llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;SmolLM3-3B supports toggling thinking mode using &lt;code&gt;/think&lt;/code&gt; or &lt;code&gt;/no_think&lt;/code&gt; in a system prompt, but it relies on Jinja template features that weren&amp;#39;t available in llama.cpp&amp;#39;s jinja processor until very recently (merged yesterday: &lt;a href=\"https://github.com/BradHutchings/Mmojo-Server/commit/b56683eb659d6d39138bd90b27cb258a21b7aa5c\"&gt;b56683eb&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;So to get system-prompt &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; working, you need to be running the current master version of llama.cpp (until the next official release). I believe some Qwen3 templates might also be affected, so keep that in mind if you&amp;#39;re using those.&lt;/p&gt;\n\n&lt;p&gt;(And since it relies on the jinja template, if you want to be able to enable/disable thinking from the system prompt remember to pass &lt;code&gt;--jinja&lt;/code&gt; to llama-cli and llama-server. Otherwise it will use a fallback template with no system prompt and no thinking.)&lt;/p&gt;\n\n&lt;p&gt;Additionally, I ran into a frustrating issue while using the llama-server with the built-in web client where SmolLM3-3B would stop thinking after a few messages even with thinking enabled. It turns out the model needs to see the &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags in previous messages or it will stop thinking. The llama web client, by default, has an option enabled that strips those tags.&lt;/p&gt;\n\n&lt;p&gt;To fix this, go to your web client settings -&amp;gt; Reasoning and disable &amp;quot;Exclude thought process when sending requests to API (Recommended for DeepSeek-R1)&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Finally, to have the web client correctly show the &amp;quot;thinking&amp;quot; section (that you can click to expand/collapse), you need to pass the &lt;code&gt;--reasoning-format none&lt;/code&gt; option to llama-server. Example invocation:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server --jinja -ngl 99 --temp 0.6 --reasoning-format none -c 64000 -fa -m ~/llama/models/smollm3-3b/SmolLM3-Q8_0.gguf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?auto=webp&amp;s=049d11d6074271bc2869bc098ea0e6349fad17b5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=210d7e5aa5feb8c1ba4995490dffc2c6390e22d2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72ddf9689e0962d05c6c024b4b695cb9794bb6ca",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=981e7a5684252e695178248adea004772a6db062",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5512ea26289819cd2f0dfd1f239935cece32dfcf",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=434567689943a3b73e96f753c8a3258d47c9a766",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ff25afc08fd9bbaf1816eb558f97ca00295655f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "tcN4L99bPskq12aPK3uI_je7rwGqtj2gvQXTVizAZ6M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfeipz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cristoper",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeipz/getting_smollm33bs_think_and_no_think_to_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeipz/getting_smollm33bs_think_and_no_think_to_work/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754099505,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Right now I am using Qwen and Gemma (32B and 27B) on my old pc from 2011 where the architecture isn‚Äôt compatible and doesn‚Äôt even detect my graphics card. \n\nI want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it‚Äôs either 30 minutes or 1 hour I get a response .\n\nIs there a logical reason for this? Is there some possible way I can figure this out and keep using the higher version models ? \n\n (I realize i need to get a new pc but now isn‚Äôt the best time for that)",
          "author_fullname": "t2_l4qac",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question about my dinosaur computer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfedua",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754099099,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I am using Qwen and Gemma (32B and 27B) on my old pc from 2011 where the architecture isn‚Äôt compatible and doesn‚Äôt even detect my graphics card. &lt;/p&gt;\n\n&lt;p&gt;I want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it‚Äôs either 30 minutes or 1 hour I get a response .&lt;/p&gt;\n\n&lt;p&gt;Is there a logical reason for this? Is there some possible way I can figure this out and keep using the higher version models ? &lt;/p&gt;\n\n&lt;p&gt;(I realize i need to get a new pc but now isn‚Äôt the best time for that)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfedua",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XiRw",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754099099,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nHeads up to anyone considering Cerebras. This is my conclusion of today's top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.\n\nThe marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn't mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.",
          "author_fullname": "t2_czmwiot",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cerebras Pro Coder Deceptive Limits",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeazc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 109,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 109,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heads up to anyone considering Cerebras. This is my conclusion of today&amp;#39;s top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.&lt;/p&gt;\n\n&lt;p&gt;The marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn&amp;#39;t mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfeazc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snipsthekittycat",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754098861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I knew quotas were getting adjusted but never thought they would concern me, I code a few hours a day and that's about it. Today I have noticed I reach my limits within an hour-1.5h of coding, and that's with me being super careful with the context size, I try not to burn tokens for now reason. Frankly, it's unreal. Anyone else is experiencing the same shenanigans? I'm on pro btw.",
          "author_fullname": "t2_5aux6jy3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude Code - limit reached super quickly",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfe77f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098552,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I knew quotas were getting adjusted but never thought they would concern me, I code a few hours a day and that&amp;#39;s about it. Today I have noticed I reach my limits within an hour-1.5h of coding, and that&amp;#39;s with me being super careful with the context size, I try not to burn tokens for now reason. Frankly, it&amp;#39;s unreal. Anyone else is experiencing the same shenanigans? I&amp;#39;m on pro btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfe77f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ys2020",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754098552,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I get it up and running, first pass and its response.. What the heck is this???\n\n    I'm sorry, but I cannot provide development services directly or review documents. However, if you have specific questions or concerns about the strategy or implementation details, please ask away! I can guide you on the platform and its programming environment, but additional development work would require a fee or contract with a licensed developer.",
          "author_fullname": "t2_kwkhg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wizard Coder... or not coder?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfe6jm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098495,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I get it up and running, first pass and its response.. What the heck is this???&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;I&amp;#39;m sorry, but I cannot provide development services directly or review documents. However, if you have specific questions or concerns about the strategy or implementation details, please ask away! I can guide you on the platform and its programming environment, but additional development work would require a fee or contract with a licensed developer.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfe6jm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "modernDayKing",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfe6jm/wizard_coder_or_not_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfe6jm/wizard_coder_or_not_coder/",
          "subreddit_subscribers": 509052,
          "created_utc": 1754098495,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_xq83l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta - new openai open source model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfda7s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=e6bc83d22983565c851331294c37a95f480af7fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754095791,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/openrouter/horizon-beta",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?auto=webp&amp;s=76311bc0d854d91946fad4dd34c15d2aabd68203",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f17bb8ad3532cb9e5aee2735555aab1785143fb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fde130f24de4941de6382c0b47920888676cb02",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e2b3b64a4ebeaaef6a2c94effc7c44b3d4bf9e5",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d1e79479fdaa990ea889c0b392a6ab4a884ffc4",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5298d732c100973951f754378b36e657d827055b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7a4a4538b90aad59b31dde3f164c87c0a08175",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfda7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "popsumbong",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/",
          "stickied": false,
          "url": "https://openrouter.ai/openrouter/horizon-beta",
          "subreddit_subscribers": 509052,
          "created_utc": 1754095791,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm using a 7900 GRE and training models via Applio. I‚Äôm getting about 1.88 seconds per iteration (see image). I've tried different setups and drivers with help from others, but the speed doesn't improve.\n\nJust wondering ‚Äî anyone else using a 7900 GRE? What kind of speeds are you getting? Would love to compare.",
          "author_fullname": "t2_utmosfnh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RX 7900 GRE users: What training speeds do you get on Applio? (I'm seeing ~1.88s/it)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 13,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfc1oj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/w8yTEmbw2r5SmwMUy9NzFfJLvnoRmaAsSryV5GLTnt4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754092203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a 7900 GRE and training models via Applio. I‚Äôm getting about 1.88 seconds per iteration (see image). I&amp;#39;ve tried different setups and drivers with help from others, but the speed doesn&amp;#39;t improve.&lt;/p&gt;\n\n&lt;p&gt;Just wondering ‚Äî anyone else using a 7900 GRE? What kind of speeds are you getting? Would love to compare.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hrd001ynwhgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hrd001ynwhgf1.png?auto=webp&amp;s=5c1b626d538a90bbc701de5ee36ef1f3b8364dc6",
                  "width": 1013,
                  "height": 99
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e50eaa420fd6a63e598023e3cde5be0294f7495e",
                    "width": 108,
                    "height": 10
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c4495a93af7c650adf677f85ec950907e730217",
                    "width": 216,
                    "height": 21
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6a87faa6041565843e93d320e9bb6e8f80d1bb7",
                    "width": 320,
                    "height": 31
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39770003cb041d457dd97ac2ebca7137dc3fe625",
                    "width": 640,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/hrd001ynwhgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7ca18a2e6ce08f8d5b0b583a2449fd7aaf42644",
                    "width": 960,
                    "height": 93
                  }
                ],
                "variants": {},
                "id": "NedzWxTqeORWwluACmoHPoFt8g062kGYd_D3VPX8sNo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfc1oj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lumpy-Quiet-7691",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfc1oj/rx_7900_gre_users_what_training_speeds_do_you_get/",
          "stickied": false,
          "url": "https://i.redd.it/hrd001ynwhgf1.png",
          "subreddit_subscribers": 509052,
          "created_utc": 1754092203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!",
          "author_fullname": "t2_kfjfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfbw8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 96,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 96,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=140&amp;height=76&amp;crop=140:76,smart&amp;auto=webp&amp;s=178741032ad68bb72212a1f0482ccf59165855d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754091780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "medium.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just because you are hosting locally, doesn&amp;#39;t mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?auto=webp&amp;s=db6f154eef502137479106eb0ee5e3497a2c8a5d",
                  "width": 1024,
                  "height": 559
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fc30b818f499ebfea16a1a44bc05f5b89c31100",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8f36d4f360255f41bf1da0fda787ab734a46fbd",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad1e44aa12420eb39ba559c3fdbad4ad0407e0b",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24b8b3213823bb044c73076e1852d1957545a17f",
                    "width": 640,
                    "height": 349
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cb31249007f2a139fccd496d749e3a58bdc7c0d",
                    "width": 960,
                    "height": 524
                  }
                ],
                "variants": {},
                "id": "1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfbw8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JAlbrethsen",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/",
          "stickied": false,
          "url": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "subreddit_subscribers": 509052,
          "created_utc": 1754091780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}