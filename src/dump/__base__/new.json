{
  "kind": "Listing",
  "data": {
    "after": "t3_1lyozcn",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I feel like people really limit themselves and they need to understand the concept of scale and instead of settling on something small you can go for something big.\n\nA lot of programmers are getting laid off and the tech space is in an interesting spot with the rise of AI. So I thought with all the tech layoffs why not make something positive out of it? I want to hire 100k programmers and create a tech giant to rival Microsoft, google, Amazon etc. this would be the first tech giant startup. Instead of being overly humble and self deprecating and thinking startups can only be small I will instead create the biggest and best startup ever.\n\nI would love to hire 100k+ programmers and get this thing going. What does everyone think?",
          "author_fullname": "t2_ohcs8ba5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to hire 100k programmers and create the first tech giant startup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzvuu7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752521034,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like people really limit themselves and they need to understand the concept of scale and instead of settling on something small you can go for something big.&lt;/p&gt;\n\n&lt;p&gt;A lot of programmers are getting laid off and the tech space is in an interesting spot with the rise of AI. So I thought with all the tech layoffs why not make something positive out of it? I want to hire 100k programmers and create a tech giant to rival Microsoft, google, Amazon etc. this would be the first tech giant startup. Instead of being overly humble and self deprecating and thinking startups can only be small I will instead create the biggest and best startup ever.&lt;/p&gt;\n\n&lt;p&gt;I would love to hire 100k+ programmers and get this thing going. What does everyone think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzvuu7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zeeza48",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzvuu7/i_want_to_hire_100k_programmers_and_create_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzvuu7/i_want_to_hire_100k_programmers_and_create_the/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752521034,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hdcx5ggfg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta’s New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzv16g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=684dded7ebf0cced3ec460c9dda8f551b9ecbd73",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752519240,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "nytimes.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?auto=webp&amp;s=211ff5c9d8860c633734a0f69515f881de8905e4",
                  "width": 1050,
                  "height": 550
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f431ed3ef795de81f0d9be2452ed2466f4727f88",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c728fd0b47256c06b7e53063606348710b74999",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1dc415541c74d1ee2dd3620b8e6997e56ad7f2",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=361e53f2aa166522931efd9533bd8c76685cfc5a",
                    "width": 960,
                    "height": 502
                  }
                ],
                "variants": {},
                "id": "62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzv16g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "showmeufos",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "subreddit_subscribers": 498850,
          "created_utc": 1752519240,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Obviously this is a silly question. 4k context is limiting to the point where even dumber models are \"better\" for almost any pipeline and use case.\n\nBut for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG's)? What model sizes now beat Llama2-70B in:\n\n- instruction following\n\n- depth of knowledge \n\n- writing skill\n\n- coding \n\n- logic",
          "author_fullname": "t2_w2gxqd6i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If you limit context to 4k tokens, which models today beat Llama2-70B from 2 years ago?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzuaa3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752517599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously this is a silly question. 4k context is limiting to the point where even dumber models are &amp;quot;better&amp;quot; for almost any pipeline and use case.&lt;/p&gt;\n\n&lt;p&gt;But for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG&amp;#39;s)? What model sizes now beat Llama2-70B in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;instruction following&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;depth of knowledge &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;writing skill&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;coding &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;logic&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzuaa3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EmPips",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752517599,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.\n\nMore broadly, I'm very interested in the potential of training a single model to work with different inferencing resources (Google's MatFormer work with Gemma 3n is the obvious other approach).\n\nI'd love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.\n\nYes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?\n\n**Clarifying a point from the thread:**\n\nThe end goal here isn't to distill a crappy small dense model from an MOE, it's to get a sense of how far the expert is from a small dense LLM. If it's not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.",
          "author_fullname": "t2_2roqrw5l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is the output of only the shared expert(s) in a MOE model coherent?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzu9e8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752520423,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752517544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.&lt;/p&gt;\n\n&lt;p&gt;More broadly, I&amp;#39;m very interested in the potential of training a single model to work with different inferencing resources (Google&amp;#39;s MatFormer work with Gemma 3n is the obvious other approach).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.&lt;/p&gt;\n\n&lt;p&gt;Yes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Clarifying a point from the thread:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The end goal here isn&amp;#39;t to distill a crappy small dense model from an MOE, it&amp;#39;s to get a sense of how far the expert is from a small dense LLM. If it&amp;#39;s not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzu9e8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gofiend",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752517544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?  \nI'd like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.  \nIt would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it's not a very \"popular\" language to train on. lol\n\n  \n",
          "author_fullname": "t2_dlu9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is real-time voice-to-voice still science fiction?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzts1z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752516472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?&lt;br/&gt;\nI&amp;#39;d like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.&lt;br/&gt;\nIt would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it&amp;#39;s not a very &amp;quot;popular&amp;quot; language to train on. lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzts1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "junior600",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752516472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Congrats to the Moonshot team on Kimi-K2!\n\n* Original Post: [Justin Wong (Kimi-K2) Blog Post](https://macro.com/app/md/29ef9c8b-b403-47d8-91c9-ff0520bb43c7/md/629ed15b-bb01-431d-a446-b4aa36436780)\n* Context: [https://moonshotai.github.io/Kimi-K2/](https://moonshotai.github.io/Kimi-K2/)\n\nhttps://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;format=pjpg&amp;auto=webp&amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64\n\nDisclaimer: Net positive outcome for open-source AI research and development, and geopolitical risk—not a partnership ",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi-K2 🤝 Anthropic | Blog Post by Justin Wong",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 52,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "ncv5le0hpvcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a60ceb065566b732aba10ca1716426b644ebecf4"
                },
                {
                  "y": 81,
                  "x": 216,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6feeeba9e3940a46d797821e6c33fbf473d2931"
                },
                {
                  "y": 120,
                  "x": 320,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b20204102a6df2cfbe4525ced39088787f59118"
                },
                {
                  "y": 241,
                  "x": 640,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20c8644f9a280ec7bd3444cbc619a3e6247a0165"
                },
                {
                  "y": 361,
                  "x": 960,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b786e3941166050874280f3ef029c9dc7f3f544"
                },
                {
                  "y": 406,
                  "x": 1080,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ae8aa8027033cb6951b3e442c28331bfde0dc2de"
                }
              ],
              "s": {
                "y": 560,
                "x": 1486,
                "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;format=pjpg&amp;auto=webp&amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64"
              },
              "id": "ncv5le0hpvcf1"
            }
          },
          "name": "t3_1lztjtc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MJFkGnjtgqNc1SeSwU5cxC7pWsvWWd9fDcod0-2vC8Y.jpg",
          "edited": 1752519267,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752515991,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Congrats to the Moonshot team on Kimi-K2!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Original Post: &lt;a href=\"https://macro.com/app/md/29ef9c8b-b403-47d8-91c9-ff0520bb43c7/md/629ed15b-bb01-431d-a446-b4aa36436780\"&gt;Justin Wong (Kimi-K2) Blog Post&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Context: &lt;a href=\"https://moonshotai.github.io/Kimi-K2/\"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64\"&gt;https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: Net positive outcome for open-source AI research and development, and geopolitical risk—not a partnership &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lztjtc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lztjtc/kimik2_anthropic_blog_post_by_justin_wong/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lztjtc/kimik2_anthropic_blog_post_by_justin_wong/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752515991,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recorded a userflow for my vibecoding pet project - character selection, model setup, inline replies, and image generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzsoqc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1440,
              "scrubber_media_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/bx3hl3q5kvcf1/DASHPlaylist.mpd?a=1755113126%2CMDBmODg0NTE0ZjVkNDc2OWZjM2I0NmY0OTdlM2JmMTA0OWIwNjdhMWY4M2I5OWVlNjY3YzU3MTk5NDM5YTU2Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 38,
              "hls_url": "https://v.redd.it/bx3hl3q5kvcf1/HLSPlaylist.m3u8?a=1755113126%2CMDMxMGQyMWZkMWU3Y2IwYzZkNjAxOWIxMzI3MmM3NmM2YTM2MmUzMDM3NDM2YTE0NjEwMTE5YmY5ZDFkN2M0Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=140&amp;height=105&amp;crop=140:105,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0a77c427ad0f08f0061b348e704987804dc1321b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752514110,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/bx3hl3q5kvcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?format=pjpg&amp;auto=webp&amp;s=6c9ab8578bf929eca5120be597d6df7b0ba43974",
                  "width": 1440,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d229202bb26b8decf4001cbe9fd8bb4d45adfea3",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b0fba2d38497e9a79d0dd3c6845939189cd1e736",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9be16d367724d3c86a3e7b5f317cd6d9e18331ae",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1432a2385579783d25465bd811ca11a3da2d989e",
                    "width": 640,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c9a248e58486fa1c9b75dec8df19d61240ef98fc",
                    "width": 960,
                    "height": 720
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d2f5de10741475c90aa05dd0031d6965662c06af",
                    "width": 1080,
                    "height": 810
                  }
                ],
                "variants": {},
                "id": "cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzsoqc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzsoqc/recorded_a_userflow_for_my_vibecoding_pet_project/",
          "stickied": false,
          "url": "https://v.redd.it/bx3hl3q5kvcf1",
          "subreddit_subscribers": 498850,
          "created_utc": 1752514110,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1440,
              "scrubber_media_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/bx3hl3q5kvcf1/DASHPlaylist.mpd?a=1755113126%2CMDBmODg0NTE0ZjVkNDc2OWZjM2I0NmY0OTdlM2JmMTA0OWIwNjdhMWY4M2I5OWVlNjY3YzU3MTk5NDM5YTU2Ng%3D%3D&amp;v=1&amp;f=sd",
              "duration": 38,
              "hls_url": "https://v.redd.it/bx3hl3q5kvcf1/HLSPlaylist.m3u8?a=1755113126%2CMDMxMGQyMWZkMWU3Y2IwYzZkNjAxOWIxMzI3MmM3NmM2YTM2MmUzMDM3NDM2YTE0NjEwMTE5YmY5ZDFkN2M0Mg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.\n\nStill, it seems pretty odd that they're missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.",
          "author_fullname": "t2_e9jh97s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama, Why No Reka Flash, SmolLM3, GLM-4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lzsnna",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752514045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.&lt;/p&gt;\n\n&lt;p&gt;Still, it seems pretty odd that they&amp;#39;re missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzsnna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chibop1",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752514045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I’ve been experimenting with Ollama locally and ended up creating a little game called Holy Arcana: From Profane to Divine. \n\nIt uses Llama-3.2 to generate poetry and responses as you make your way through Tarot-inspired challenges and Kabbalistic paths. \n\nIt’s just something I made for fun, mixing AI with esoteric themes and interactive storytelling. \n\nIf you’re curious about seeing Ollama put to creative use, feel free to check it out or play around with the \n\nrepo:  \n👉 [github.com/cyberAlchem1st/holy-arcana](https://github.com/cyberAlchem1st/holy-arcana)\n\n ",
          "author_fullname": "t2_x7pnnazak",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Esoteric Game with Llama3.2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzrqoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752512056,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I’ve been experimenting with Ollama locally and ended up creating a little game called Holy Arcana: From Profane to Divine. &lt;/p&gt;\n\n&lt;p&gt;It uses Llama-3.2 to generate poetry and responses as you make your way through Tarot-inspired challenges and Kabbalistic paths. &lt;/p&gt;\n\n&lt;p&gt;It’s just something I made for fun, mixing AI with esoteric themes and interactive storytelling. &lt;/p&gt;\n\n&lt;p&gt;If you’re curious about seeing Ollama put to creative use, feel free to check it out or play around with the &lt;/p&gt;\n\n&lt;p&gt;repo:&lt;br/&gt;\n👉 &lt;a href=\"https://github.com/cyberAlchem1st/holy-arcana\"&gt;github.com/cyberAlchem1st/holy-arcana&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?auto=webp&amp;s=a936e93d40d705213d3b629b6ef02606722b212b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab5b82fc39c178d66c6d577d1e97474b18e24c69",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecc83be8a24e065d7733bd95ddf69023f77d1f42",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29bb1ed6fa236a25f0b568b18647d6df229c150e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6bdc379ead682e593b23db0dea625df9dd1f0a4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=378e7312050f9adef7543d7d04e699498bc7903e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1eb296c462cf06737624be941504592293963b6a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lzrqoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ambitious_Ad497",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzrqoi/esoteric_game_with_llama32/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzrqoi/esoteric_game_with_llama32/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752512056,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering)",
          "author_fullname": "t2_1jhxe6m6wu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A practical handbook on Context Engineering with the latest research from IBM Zurich, ICML, Princeton, and more.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzql0b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752509454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/davidkimai/Context-Engineering\"&gt;https://github.com/davidkimai/Context-Engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?auto=webp&amp;s=f8680ad6bdadc6cdd4914fae9065ec6b47805ad6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f42f9a68c36f1e2ccc27fbc08f8a4e8edb8f1a70",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ba461a7633a2a5b765faf3ba8b9b9e94db25221",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6ab5733b0e448457e4ddfdd280b0486f09e8619",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3117ff6ef79e94d1a37e04e2704182174aaf1001",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0123ba313a39132b0d408b10e6733c20a1e07ec6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af3ec97c9e4656162e6fd49925a896f6596e3856",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lzql0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "recursiveauto",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752509454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am running Gemma 3 12B on my local computer. My prompt is about 1000 tokens of text + 3-4 images. My computer is just a regular AMD CPU (no GPU) + 64GB of DDR5 RAM, so understandably the response is slow. Particularly I have noticed that it takes more time to just process my input.\n\nMy question is what hardware would help improve this:  \n1. Obviously a GPU would help - but what should I look for in a GPU to get better response times?  \n2. Would the newer AMD Ryzen™ AI 9 HX 370 APU help or would I need to go for an AMD Ryzen AI Max+ 395 APU's?  \n3. If I got for the AMD Ryzen™ AI 9 HX 370 APU, some PCs come with upgradeable RAM i.e. DDR5 (going up to 96GB), while others come with faster LPDDR5 RAM - but with the caveat that the max RAM is capped at 64 GB. I want to be able to run slightly larger models on it (e.g. Gemma 3 27B), but not sure if I need to go for the LPDDR5x versions.",
          "author_fullname": "t2_1kusyf1nll",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to improve response times for multimodal requests?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzqh66",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752509218,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running Gemma 3 12B on my local computer. My prompt is about 1000 tokens of text + 3-4 images. My computer is just a regular AMD CPU (no GPU) + 64GB of DDR5 RAM, so understandably the response is slow. Particularly I have noticed that it takes more time to just process my input.&lt;/p&gt;\n\n&lt;p&gt;My question is what hardware would help improve this:&lt;br/&gt;\n1. Obviously a GPU would help - but what should I look for in a GPU to get better response times?&lt;br/&gt;\n2. Would the newer AMD Ryzen™ AI 9 HX 370 APU help or would I need to go for an AMD Ryzen AI Max+ 395 APU&amp;#39;s?&lt;br/&gt;\n3. If I got for the AMD Ryzen™ AI 9 HX 370 APU, some PCs come with upgradeable RAM i.e. DDR5 (going up to 96GB), while others come with faster LPDDR5 RAM - but with the caveat that the max RAM is capped at 64 GB. I want to be able to run slightly larger models on it (e.g. Gemma 3 27B), but not sure if I need to go for the LPDDR5x versions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzqh66",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "coolahavoc",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzqh66/how_to_improve_response_times_for_multimodal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzqh66/how_to_improve_response_times_for_multimodal/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752509218,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone - there are some **245GB quants (80% size reduction)** for Kimi K2 at https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF. The Unsloth dynamic Q2\\_K\\_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.\n\nPlease use `-ot \".ffn_.*_exps.=CPU\"` to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.\n\nYou need to use either [https://github.com/ggml-org/llama.cpp/pull/14654](https://github.com/ggml-org/llama.cpp/pull/14654) or our fork [https://github.com/unslothai/llama.cpp](https://github.com/unslothai/llama.cpp) to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!\n\nThe suggested parameters are:\n\n    temperature = 0.6\n    min_p = 0.01 (set it to a small number)\n\nDocs has more details: [https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally](https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 1.8bit Unsloth Dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzps3b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 173,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 173,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752507676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone - there are some &lt;strong&gt;245GB quants (80% size reduction)&lt;/strong&gt; for Kimi K2 at &lt;a href=\"https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF&lt;/a&gt;. The Unsloth dynamic Q2_K_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.&lt;/p&gt;\n\n&lt;p&gt;Please use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.&lt;/p&gt;\n\n&lt;p&gt;You need to use either &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14654\"&gt;https://github.com/ggml-org/llama.cpp/pull/14654&lt;/a&gt; or our fork &lt;a href=\"https://github.com/unslothai/llama.cpp\"&gt;https://github.com/unslothai/llama.cpp&lt;/a&gt; to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!&lt;/p&gt;\n\n&lt;p&gt;The suggested parameters are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;temperature = 0.6\nmin_p = 0.01 (set it to a small number)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docs has more details: &lt;a href=\"https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally\"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?auto=webp&amp;s=bc3027fa5da20b74e927173e28d8aca06d1918f9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=539e7c53ad8fe4d04c6029c11344ff605d38589a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6263e8d5e0d9129138827f26082b6f9517086361",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=247bab7783d0737b150b5f5183ba9db8a0966436",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3e7dfec653d356a4ccb836ae65b4546e9a5ad00",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a4a1e7fdeac330903838e1542ed085ea53ec142",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff85dc8c4e34c5580c7c7bf51d308d118a9e322f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzps3b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752507676,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as title says, lm studio always uses my cpu, I want to make lm uses the GPU tried several changes\n\nLaptop specs\n\n24gb ram  \n3070 8gb ram  \ni9-11 gen\n\ni cant seem to use gpu as main resource for llama in lmstudio\n\n[settings - hardware](https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1)\n\n[settings - runtime](https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;format=png&amp;auto=webp&amp;s=e59c270eef7d301d1bda499756496939c67e4099)\n\n  \n  \nthings I did  \nupdated GPU  \nruntime engine is set to  cuda 12 llama cpp  \ntried several changes in GPU offload, from maximum  to half,  \ntried changes CPU thread pool  \ntried changes in context length\n\nupon testing cpu usage spikes, while my GPU sits idle, only works if my cpu reach 70% above then gpu starts working\n\nthe models I used are :  \nopen hermes  2.5 mistral 7b  \nOpenOrca Platypus2 13B q 4 k s  \nMythomax L2 13b q4 k s",
          "author_fullname": "t2_1is18ejy13",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio cant use my gpu as main",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "v0fsjfwkuucf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=28304eefb1585ed0b0b966bf3bb044f06d87ac3e"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5103ec1903662ebf541200809894b7f0050373e3"
                },
                {
                  "y": 203,
                  "x": 320,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bce56794d9d7b1d50bb3dbf07cad90b792451e52"
                },
                {
                  "y": 407,
                  "x": 640,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=160bb93aa199368b865ae1a29783e2bb3c049631"
                },
                {
                  "y": 611,
                  "x": 960,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bd36193de71a6a4172c9362db5b1efe55d5237a"
                },
                {
                  "y": 687,
                  "x": 1080,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba101eaf58a755a88b279ec499ef4ed1665d311f"
                }
              ],
              "s": {
                "y": 781,
                "x": 1226,
                "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1"
              },
              "id": "v0fsjfwkuucf1"
            },
            "75o3tq0z5vcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=29cc677c8b11701bbd55ae946c097f48daf97e30"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb3f3c30a765ae1b8b0e2269137b6da98723141d"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=826ee8bb5ea9d079adee3ee3db96ac8ceca3e83b"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ef3e82b55da1f94203265ee9541b4251393f647"
                },
                {
                  "y": 631,
                  "x": 960,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=65c28555b2d9a4466aaf66240eb018987ac511f7"
                },
                {
                  "y": 710,
                  "x": 1080,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce63d924b98ef3795f1bb676a62cb3df1738eddf"
                }
              ],
              "s": {
                "y": 810,
                "x": 1232,
                "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;format=png&amp;auto=webp&amp;s=e59c270eef7d301d1bda499756496939c67e4099"
              },
              "id": "75o3tq0z5vcf1"
            }
          },
          "name": "t3_1lzoxbl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aO_NeeS70DPNibAf4xpd1tv7HSDOB7yzatP7amRYFhA.jpg",
          "edited": 1752509331,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752505748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as title says, lm studio always uses my cpu, I want to make lm uses the GPU tried several changes&lt;/p&gt;\n\n&lt;p&gt;Laptop specs&lt;/p&gt;\n\n&lt;p&gt;24gb ram&lt;br/&gt;\n3070 8gb ram&lt;br/&gt;\ni9-11 gen&lt;/p&gt;\n\n&lt;p&gt;i cant seem to use gpu as main resource for llama in lmstudio&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1\"&gt;settings - hardware&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e59c270eef7d301d1bda499756496939c67e4099\"&gt;settings - runtime&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;things I did&lt;br/&gt;\nupdated GPU&lt;br/&gt;\nruntime engine is set to  cuda 12 llama cpp&lt;br/&gt;\ntried several changes in GPU offload, from maximum  to half,&lt;br/&gt;\ntried changes CPU thread pool&lt;br/&gt;\ntried changes in context length&lt;/p&gt;\n\n&lt;p&gt;upon testing cpu usage spikes, while my GPU sits idle, only works if my cpu reach 70% above then gpu starts working&lt;/p&gt;\n\n&lt;p&gt;the models I used are :&lt;br/&gt;\nopen hermes  2.5 mistral 7b&lt;br/&gt;\nOpenOrca Platypus2 13B q 4 k s&lt;br/&gt;\nMythomax L2 13b q4 k s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzoxbl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zinxdia",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzoxbl/lm_studio_cant_use_my_gpu_as_main/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzoxbl/lm_studio_cant_use_my_gpu_as_main/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752505748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any advice ? :) ",
          "author_fullname": "t2_pmwcfxhz4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM for Educators ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzooed",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752505188,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any advice ? :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzooed",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creative_Structure22",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzooed/best_llm_for_educators/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzooed/best_llm_for_educators/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752505188,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I've also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they're \"exhausting.\"\n\nI feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people's examples just to implement some simple control flow.\n\nWith just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it's so lean and easy, and you get full visibility on how each API call went.",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I ditch all LLM framework and use only OpenAI SDK for everything, I start loving building AI application this way.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzocuk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752504444,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I&amp;#39;ve also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they&amp;#39;re &amp;quot;exhausting.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people&amp;#39;s examples just to implement some simple control flow.&lt;/p&gt;\n\n&lt;p&gt;With just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it&amp;#39;s so lean and easy, and you get full visibility on how each API call went.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzocuk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752504444,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Context of my project idea:**\n\nI have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay \"per request\" is pretty much unfeasible without factoring in excessive cold startup times.\n\nSo it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.\n\n**An alternative?**\n\nAll this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a **community-owned live inference server** where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?\n\nWe, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.\n\nI imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.",
          "author_fullname": "t2_19mrnrt357",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Project Idea: A REAL Community-driven LLM Stack",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lznxy5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752503693,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752503479,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context of my project idea:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay &amp;quot;per request&amp;quot; is pretty much unfeasible without factoring in excessive cold startup times.&lt;/p&gt;\n\n&lt;p&gt;So it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An alternative?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;All this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a &lt;strong&gt;community-owned live inference server&lt;/strong&gt; where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?&lt;/p&gt;\n\n&lt;p&gt;We, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.&lt;/p&gt;\n\n&lt;p&gt;I imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lznxy5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Budget_Map_3333",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752503479,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Recently decided to try out openwebui and something i noticed is that it does no batching for embedding multiple files, and in the scale of 5000 files it feels like it will take the better part of 5 hours, i can write a tiny python script to embed all of these files (and view them in qdrant) in an amount of time that is light years ahead of whatever openwebui is doing, except openwebui can’t use those for some reason.\n\nAny alternatives?\n\nI run everything locally through vllm, with qwen 4b embedding, qwen 0.6b reranker, and devstral",
          "author_fullname": "t2_9so78ol2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a better frontend than OpenWebui for RAG?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzna91",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently decided to try out openwebui and something i noticed is that it does no batching for embedding multiple files, and in the scale of 5000 files it feels like it will take the better part of 5 hours, i can write a tiny python script to embed all of these files (and view them in qdrant) in an amount of time that is light years ahead of whatever openwebui is doing, except openwebui can’t use those for some reason.&lt;/p&gt;\n\n&lt;p&gt;Any alternatives?&lt;/p&gt;\n\n&lt;p&gt;I run everything locally through vllm, with qwen 4b embedding, qwen 0.6b reranker, and devstral&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzna91",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Capable-Ad-7494",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzna91/is_there_a_better_frontend_than_openwebui_for_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzna91/is_there_a_better_frontend_than_openwebui_for_rag/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752501919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to create a new AI agent for my MySQL database. The database tables are complex and require extensive documentation to make them understandable for the AI to query effectively. \n\nI need guidance on selecting the right model and framework for this project.",
          "author_fullname": "t2_13usrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions for ai agent framework and ai model for Text-to-SQL ai agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzn9th",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to create a new AI agent for my MySQL database. The database tables are complex and require extensive documentation to make them understandable for the AI to query effectively. &lt;/p&gt;\n\n&lt;p&gt;I need guidance on selecting the right model and framework for this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzn9th",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "M7mDSa3eD_",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzn9th/suggestions_for_ai_agent_framework_and_ai_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzn9th/suggestions_for_ai_agent_framework_and_ai_model/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752501889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! \nI'm currently investigating and planning a very fun project, my ultimate personal assistant.\n\nThe idea is to have a multi-agent system, with one main point of contact; \"The Secretary\". Then I have task-specific agents with expertise in different areas, like my different work projects, or notion updating etc. I want to be able to configure system prompts, integrations (MCP probably aswell?) and memory. The agents should be able to communicate and get help from each other.\n\nThe actual architecture is not set in stone yet, maybe I will use a existing system if it managed to accomplish the UX features I want, and that's why I'm asking you. \n\nI wanted to check with you guys if anyone has a recommendation for a framework, tool or existing open source project that would be nice to look into.\n\nThese are some things I'm currently looking in to:\n\n- AGiXT (Agentic assistant framework)\n- SuperAGI (Agentic assistant framework)\n- CrewAI (multi-agent building framework)\n- Librechat (ChatGPT alternative)\n- Graphiti (Dynamic graph memory)\n- n8n (Visual flow process builder)\n\nI do know is that I want to work in Python. I will be locally hosting the system.\n\nAny recommendations for building something like this?",
          "author_fullname": "t2_uf4p8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Agentic Secretary System - Tips and Recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzn4ae",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501530,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! \nI&amp;#39;m currently investigating and planning a very fun project, my ultimate personal assistant.&lt;/p&gt;\n\n&lt;p&gt;The idea is to have a multi-agent system, with one main point of contact; &amp;quot;The Secretary&amp;quot;. Then I have task-specific agents with expertise in different areas, like my different work projects, or notion updating etc. I want to be able to configure system prompts, integrations (MCP probably aswell?) and memory. The agents should be able to communicate and get help from each other.&lt;/p&gt;\n\n&lt;p&gt;The actual architecture is not set in stone yet, maybe I will use a existing system if it managed to accomplish the UX features I want, and that&amp;#39;s why I&amp;#39;m asking you. &lt;/p&gt;\n\n&lt;p&gt;I wanted to check with you guys if anyone has a recommendation for a framework, tool or existing open source project that would be nice to look into.&lt;/p&gt;\n\n&lt;p&gt;These are some things I&amp;#39;m currently looking in to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AGiXT (Agentic assistant framework)&lt;/li&gt;\n&lt;li&gt;SuperAGI (Agentic assistant framework)&lt;/li&gt;\n&lt;li&gt;CrewAI (multi-agent building framework)&lt;/li&gt;\n&lt;li&gt;Librechat (ChatGPT alternative)&lt;/li&gt;\n&lt;li&gt;Graphiti (Dynamic graph memory)&lt;/li&gt;\n&lt;li&gt;n8n (Visual flow process builder)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I do know is that I want to work in Python. I will be locally hosting the system.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for building something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzn4ae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boltyx",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzn4ae/agentic_secretary_system_tips_and_recommendations/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzn4ae/agentic_secretary_system_tips_and_recommendations/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752501530,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author’s insights genuinely thought-provoking. The original Chinese version is [here](https://bigeagle.me/2025/07/kimi-k2/)—feel free to read it in full (and of course you can use Kimi K2 as your translator). Here’s my own distilled summary of the main points:\n\n\n\n• Beyond chatbots: Kimi K2 experiments with an “artifact-first” interaction model that has the AI immediately build interactive front-end deliverables—PPT-like pages, diagrams, even mini-games—rather than simply returning markdown text.\n\n• Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.\n\n• What makes an agentic model: A minimal loop—think, choose tools, observe results, iterate—can be learned from synthetic trajectories. Today’s agent abilities are early-stage; the next pre-training wave still holds plenty of upside.\n\n• Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit “hacky” hidden pipelines, forcing genuinely strong, general models—exactly what an AGI-oriented startup needs.\n\n• Marketing controversies &amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1’s viral rise proved that raw model quality markets itself and validates the “foundation-model-first” path.\n\n• Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.\n\n\n\nFrom the entire blog, this is the paragraph I loved the most:\n\n&gt;A while ago, ‘Agent’ products were all the rage. I kept hearing people say that Kimi shouldn’t compete on large models and should focus on Agents instead. Let me be clear: **the vast majority of Agent products are nothing without Claude behind them.** Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don’t keep pushing that ceiling higher, I won’t stay here a single extra day.\n\n&gt;Chasing AGI is an extremely narrow, perilous bridge—there’s no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‘As an investor, I care about the ROI of AI applications.’ In that moment I knew the company he founded wouldn’t last long.",
          "author_fullname": "t2_sqi8xxun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "After Kimi K2 Is Released: No Longer Just a ChatBot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzm645",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 186,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 186,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752499086,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author’s insights genuinely thought-provoking. The original Chinese version is &lt;a href=\"https://bigeagle.me/2025/07/kimi-k2/\"&gt;here&lt;/a&gt;—feel free to read it in full (and of course you can use Kimi K2 as your translator). Here’s my own distilled summary of the main points:&lt;/p&gt;\n\n&lt;p&gt;• Beyond chatbots: Kimi K2 experiments with an “artifact-first” interaction model that has the AI immediately build interactive front-end deliverables—PPT-like pages, diagrams, even mini-games—rather than simply returning markdown text.&lt;/p&gt;\n\n&lt;p&gt;• Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.&lt;/p&gt;\n\n&lt;p&gt;• What makes an agentic model: A minimal loop—think, choose tools, observe results, iterate—can be learned from synthetic trajectories. Today’s agent abilities are early-stage; the next pre-training wave still holds plenty of upside.&lt;/p&gt;\n\n&lt;p&gt;• Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit “hacky” hidden pipelines, forcing genuinely strong, general models—exactly what an AGI-oriented startup needs.&lt;/p&gt;\n\n&lt;p&gt;• Marketing controversies &amp;amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1’s viral rise proved that raw model quality markets itself and validates the “foundation-model-first” path.&lt;/p&gt;\n\n&lt;p&gt;• Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.&lt;/p&gt;\n\n&lt;p&gt;From the entire blog, this is the paragraph I loved the most:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A while ago, ‘Agent’ products were all the rage. I kept hearing people say that Kimi shouldn’t compete on large models and should focus on Agents instead. Let me be clear: &lt;strong&gt;the vast majority of Agent products are nothing without Claude behind them.&lt;/strong&gt; Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don’t keep pushing that ceiling higher, I won’t stay here a single extra day.&lt;/p&gt;\n\n&lt;p&gt;Chasing AGI is an extremely narrow, perilous bridge—there’s no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‘As an investor, I care about the ROI of AI applications.’ In that moment I knew the company he founded wouldn’t last long.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzm645",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nekofneko",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752499086,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,  \nlooking for a place to start to read and check a bit, but wanted to ask to just select good starting point.\n\nCurrently I have rtx 3070 8gb. What model can i run locally to get started with code assistant (means, asking about 'algoritm' snippets or checking code.  \nAlso, what I need to learn to setup Ai if I would like to give 'assistant' API docs (local or web hosted) and ask him about solutions using these methods?\n\nOn which budget starting point (3090?) is worth getting into code AI helper? Also, which model is worth checking in web(paid way) to get grasph what code ai can 'develop'. (not speaking about agents, just assistants). Is there any general good with code capabilities + vision or they always separate?",
          "author_fullname": "t2_lsxw9qy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "getting started with code assistant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzlm2t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752497608,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nlooking for a place to start to read and check a bit, but wanted to ask to just select good starting point.&lt;/p&gt;\n\n&lt;p&gt;Currently I have rtx 3070 8gb. What model can i run locally to get started with code assistant (means, asking about &amp;#39;algoritm&amp;#39; snippets or checking code.&lt;br/&gt;\nAlso, what I need to learn to setup Ai if I would like to give &amp;#39;assistant&amp;#39; API docs (local or web hosted) and ask him about solutions using these methods?&lt;/p&gt;\n\n&lt;p&gt;On which budget starting point (3090?) is worth getting into code AI helper? Also, which model is worth checking in web(paid way) to get grasph what code ai can &amp;#39;develop&amp;#39;. (not speaking about agents, just assistants). Is there any general good with code capabilities + vision or they always separate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzlm2t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "machond",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzlm2t/getting_started_with_code_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzlm2t/getting_started_with_code_assistant/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752497608,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UTCP: A safer, scalable tool-calling alternative to MCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzl5zk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 448,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 448,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NwQpLihWBkRwry7btAEv1kXjEd2jOrNWPfUJ4oMDWTQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752496381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wv84vx7h3ucf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wv84vx7h3ucf1.png?auto=webp&amp;s=9fc98e1863475ae1707dcf8e031f0c40856c1282",
                  "width": 1874,
                  "height": 1642
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=af7471a080008d1b2a11663755e2386042538cdb",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cbf92935540699f9a5b793b10b599033bdd378c",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5bc00f40fff44e8d417d1c4e7dcd1866bece5c55",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aa90f35e8734ab6dd4bb03e9d5344881c7be6c2",
                    "width": 960,
                    "height": 841
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cafe58b5e161c4d2b61d8cb324738c3b1b2459e3",
                    "width": 1080,
                    "height": 946
                  }
                ],
                "variants": {},
                "id": "kbRMMR47HDIi7lZVVAy5mGTwVKuCZQBEJufsqMy9_24"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzl5zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 88,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/",
          "stickied": false,
          "url": "https://i.redd.it/wv84vx7h3ucf1.png",
          "subreddit_subscribers": 498850,
          "created_utc": 1752496381,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   \n  \nI was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. \n\nQwen 2.5 VL 3B  \nCaption generation - average time taken for max pixel 768\\*768 - 1.62s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.02s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 2.79s\n\nQwen 2.5 VL 7B  \nCaption generation - average time taken for max pixel 768\\*768 - 2.21s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.73s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 3.64s  \n  \nQwen 2.5 VL 7B AWQ  \nCaption generation - average time taken for max pixel 768\\*768 - 2.84s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.94s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 3.85s  \n\n\n1. Why 7B AWQ is slower than 7B?  \n2. What other better Image caption/VQA model exists that runs in less or similar resource requirments?\n\n",
          "author_fullname": "t2_7doe6lck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions/Alternatives for Image captions with efficient system requirements",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzkrwg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752495232,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   &lt;/p&gt;\n\n&lt;p&gt;I was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. &lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 3B&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 1.62s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.02s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 2.79s&lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 7B&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 2.21s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.73s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 3.64s  &lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 7B AWQ&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 2.84s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.94s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 3.85s  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why 7B AWQ is slower than 7B?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzkrwg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "palaniappan_05",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752495232,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:\n\nWould it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.\n\nThe card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?\n\nBecause utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don't know how much that matters...",
          "author_fullname": "t2_3ogjqne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multiple 5060 Ti's",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzkcg3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752493925,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:&lt;/p&gt;\n\n&lt;p&gt;Would it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.&lt;/p&gt;\n\n&lt;p&gt;The card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?&lt;/p&gt;\n\n&lt;p&gt;Because utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don&amp;#39;t know how much that matters...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzkcg3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snorixx",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752493925,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to build a mid range Desktop to fine-tuning and host Small Language Models and LLMs.\nI am thinking about using 2 AMD Radeon 9060XT 16GB to reach 32 GB VRAM on budget.\nWill it help? Since 32GB Cards like Nvidia RTX5090 are absurdly expensive. What are your suggestions about the Motherboard and CPU for my build? Should I go for a Mac Mini M4 cluster, or other Single Board Chip cluster to achieve high VRAM?\nI am in India, btw.",
          "author_fullname": "t2_4v5d5bzj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A mid range PC build for Dual GPU Local LLMs and SLMs.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzk041",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752492834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to build a mid range Desktop to fine-tuning and host Small Language Models and LLMs.\nI am thinking about using 2 AMD Radeon 9060XT 16GB to reach 32 GB VRAM on budget.\nWill it help? Since 32GB Cards like Nvidia RTX5090 are absurdly expensive. What are your suggestions about the Motherboard and CPU for my build? Should I go for a Mac Mini M4 cluster, or other Single Board Chip cluster to achieve high VRAM?\nI am in India, btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzk041",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iammhk",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752492834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have tried Cline/Roo Code/OpenHands\nUsed Devstral, GLM4 32b, codellama etc\nWhen trying to navigate website using MCP server, the LLM gets stuck and cannot press on actual buttons and escape the captcha page / allow cookies pop up.\n\nIs there a better model to try? Or its only API claude model ",
          "author_fullname": "t2_cav43",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any local LLMS that support Browser use MCP?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjsu3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752492186,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried Cline/Roo Code/OpenHands\nUsed Devstral, GLM4 32b, codellama etc\nWhen trying to navigate website using MCP server, the LLM gets stuck and cannot press on actual buttons and escape the captcha page / allow cookies pop up.&lt;/p&gt;\n\n&lt;p&gt;Is there a better model to try? Or its only API claude model &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzjsu3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iChrist",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752492186,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so ive hosted ollama locally on my system on [http://localhost:11434/api/generate](http://localhost:11434/api/generate) and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory.\n\ni don't understand why this would happen because as much as i have seen modern llms, they don't change their weights during inference.\n\nScenario:\n\n1. makes a query to ollama for topic 1 with a very specific keyword that i have created\n2. makes another query to ollama for a topic that is similar to topic 1 but has a new keyword.\n\nTurns out that the first keyword shows up in the second response aswell. Not always, but this shouldn't happen at all as much as i know\n\nIs there something that i am missing?  \nI checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &lt;model\\_name&gt;",
          "author_fullname": "t2_c40awigh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama retaining history?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjlvi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752491545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so ive hosted ollama locally on my system on &lt;a href=\"http://localhost:11434/api/generate\"&gt;http://localhost:11434/api/generate&lt;/a&gt; and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory.&lt;/p&gt;\n\n&lt;p&gt;i don&amp;#39;t understand why this would happen because as much as i have seen modern llms, they don&amp;#39;t change their weights during inference.&lt;/p&gt;\n\n&lt;p&gt;Scenario:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;makes a query to ollama for topic 1 with a very specific keyword that i have created&lt;/li&gt;\n&lt;li&gt;makes another query to ollama for a topic that is similar to topic 1 but has a new keyword.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Turns out that the first keyword shows up in the second response aswell. Not always, but this shouldn&amp;#39;t happen at all as much as i know&lt;/p&gt;\n\n&lt;p&gt;Is there something that i am missing?&lt;br/&gt;\nI checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &amp;lt;model\\_name&amp;gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzjlvi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DimensionEnergy",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjlvi/ollama_retaining_history/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjlvi/ollama_retaining_history/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752491545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[**Foundations of Large Language Models (LLMs)**](https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458)\n\n* Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)\n* Original Source: [https://arxiv.org/abs/2501.09223](https://arxiv.org/abs/2501.09223)\n* Model: Claude 4.0 Sonnet\n\nNote: The research paper is v2, originally submitted on Jan 16, 2025 and revised on Jun 15, 2025",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Foundations of Large Language Models (LLMs) | NLP Lab Research",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjaf5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752496050,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752490469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458\"&gt;&lt;strong&gt;Foundations of Large Language Models (LLMs)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)&lt;/li&gt;\n&lt;li&gt;Original Source: &lt;a href=\"https://arxiv.org/abs/2501.09223\"&gt;https://arxiv.org/abs/2501.09223&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model: Claude 4.0 Sonnet&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Note: The research paper is v2, originally submitted on Jan 16, 2025 and revised on Jun 15, 2025&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lzjaf5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752490469,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks!\n\nI’m curious — has anyone thought about building or using an AI assistant that works entirely in the browser, without sending any data to external servers or APIs?\n\nWe’re experimenting with something like that. It runs local models via Ollama, and all tasks — including web search, summarization, translation, and tab-to-tab conversations — happen entirely in the local browser environment. The goal is to keep things completely private and offline.\n\nOur broader goal is to create a privacy-first, fully local AI assistant that can become a core part of users’ workflows — potentially replacing cloud-based apps like ChatGPT for many common tasks.\n\nWe’ve open-sourced our early work here: [https://github.com/NativeMindBrowser/NativeMindExtension](https://github.com/NativeMindBrowser/NativeMindExtension)\n\nWe’re not sure yet whether this direction is something people truly need, so I’d love your honest input:\n\n* Would you use something like this?\n* What kinds of browser-based AI features would actually be useful to you?\n* Anything you wish a browser assistant could do that doesn’t exist today?\n\nAppreciate any thoughts or ideas!",
          "author_fullname": "t2_1nibexa2mk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else interested in a 100% on-device browser AI assistant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzimcq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.48,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752488062,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;I’m curious — has anyone thought about building or using an AI assistant that works entirely in the browser, without sending any data to external servers or APIs?&lt;/p&gt;\n\n&lt;p&gt;We’re experimenting with something like that. It runs local models via Ollama, and all tasks — including web search, summarization, translation, and tab-to-tab conversations — happen entirely in the local browser environment. The goal is to keep things completely private and offline.&lt;/p&gt;\n\n&lt;p&gt;Our broader goal is to create a privacy-first, fully local AI assistant that can become a core part of users’ workflows — potentially replacing cloud-based apps like ChatGPT for many common tasks.&lt;/p&gt;\n\n&lt;p&gt;We’ve open-sourced our early work here: &lt;a href=\"https://github.com/NativeMindBrowser/NativeMindExtension\"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We’re not sure yet whether this direction is something people truly need, so I’d love your honest input:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Would you use something like this?&lt;/li&gt;\n&lt;li&gt;What kinds of browser-based AI features would actually be useful to you?&lt;/li&gt;\n&lt;li&gt;Anything you wish a browser assistant could do that doesn’t exist today?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Appreciate any thoughts or ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?auto=webp&amp;s=23e950bb35f3cf1ae03dce726407bd1fe554ed07",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a5f2844eac5a78cf9da1435b7a1fbab9c240448",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cc7933b9062d59a368a9d129ac64227bdd70859",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce59a0722f86b59c9f1b86bd53c5cae548087abb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b75011b06f3ac92f68b9ad4ef5d957187696636",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77a92d5dbd908a5439bb2e6d4c409ff6a744f7b1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b792955b4c1821334f88c2e0f7c36564d4dbd9c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "19hojlgJuBRLdMgJyi-b0TdNy1nDLtN6DNXkZuzqFTQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzimcq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InfiniteJX",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752488062,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let's me say ... dont know .. clumsy?\n\nSo I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....\n\nI went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything looked promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward.\n\nIntegration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.\n\nThrough. Done. Annoyed. Frustrated. Nuts. Rant over.\n\nBack to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?\n\nEDIT: Didnt thought that there are some many reasonable UIs out there. That's huge.",
          "author_fullname": "t2_185bgnisld",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Annoyed with LibreChat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzikqt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752507102,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752487893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let&amp;#39;s me say ... dont know .. clumsy?&lt;/p&gt;\n\n&lt;p&gt;So I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....&lt;/p&gt;\n\n&lt;p&gt;I went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything looked promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward.&lt;/p&gt;\n\n&lt;p&gt;Integration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.&lt;/p&gt;\n\n&lt;p&gt;Through. Done. Annoyed. Frustrated. Nuts. Rant over.&lt;/p&gt;\n\n&lt;p&gt;Back to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Didnt thought that there are some many reasonable UIs out there. That&amp;#39;s huge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzikqt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charming_Support726",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752487893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been considering an MI50 32gb for a budget AI desktop for a while. \n\nAn issue I found was that it does not natively support display output.\n\nBut I found a version with the Radeon pro vii’s bios flashed onto it that should allow it to output to a display.\n\nAnother issue was that it doesn’t actually have a fan, relying on other fans to blow air through it.\n\nBut the frankenversion I found also has a fan installed on it\n\nAnd all of that I found for roughly 170 USD\n\nI’ll probably install it into a ryzen 5 3600 system with another 32gb of cheap ddr4 ram that costs about another 150.\n\nAny major issues with this build? \n\nI’ll probably install Ubuntu just to try it out, I’ve always been a windows user but shouldn’t hurt to try. Always wanted to try out Linux anyways but never had a spare pc to work with.",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 32GB with bios flash",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzijk2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752487768,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been considering an MI50 32gb for a budget AI desktop for a while. &lt;/p&gt;\n\n&lt;p&gt;An issue I found was that it does not natively support display output.&lt;/p&gt;\n\n&lt;p&gt;But I found a version with the Radeon pro vii’s bios flashed onto it that should allow it to output to a display.&lt;/p&gt;\n\n&lt;p&gt;Another issue was that it doesn’t actually have a fan, relying on other fans to blow air through it.&lt;/p&gt;\n\n&lt;p&gt;But the frankenversion I found also has a fan installed on it&lt;/p&gt;\n\n&lt;p&gt;And all of that I found for roughly 170 USD&lt;/p&gt;\n\n&lt;p&gt;I’ll probably install it into a ryzen 5 3600 system with another 32gb of cheap ddr4 ram that costs about another 150.&lt;/p&gt;\n\n&lt;p&gt;Any major issues with this build? &lt;/p&gt;\n\n&lt;p&gt;I’ll probably install Ubuntu just to try it out, I’ve always been a windows user but shouldn’t hurt to try. Always wanted to try out Linux anyways but never had a spare pc to work with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzijk2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzijk2/mi50_32gb_with_bios_flash/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzijk2/mi50_32gb_with_bios_flash/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752487768,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. \n\nThe screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. \n\nI've tried prompting hard (\"Use ONLY full complete traditional sentences and grammar, write like Hemingway\" and variations of the same), and I've tried bringing the Temperature right down, but nothing seems to help. \n\nI've had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek's R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.\n\nAny advice, or just some bitter commiseration, gratefully accepted.",
          "author_fullname": "t2_i5ettea7e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Responses keep dissolving into word salad - how to stop it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 97,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhqz8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/KT3zIRhU3VM0HArg0M_K5OvqTcxlyaFFQUhqYMvfTZU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752484685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. &lt;/p&gt;\n\n&lt;p&gt;The screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried prompting hard (&amp;quot;Use ONLY full complete traditional sentences and grammar, write like Hemingway&amp;quot; and variations of the same), and I&amp;#39;ve tried bringing the Temperature right down, but nothing seems to help. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek&amp;#39;s R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.&lt;/p&gt;\n\n&lt;p&gt;Any advice, or just some bitter commiseration, gratefully accepted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lr7kq1452tcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lr7kq1452tcf1.png?auto=webp&amp;s=4b3ae39d0f36d78e751373129a21148da7beecfe",
                  "width": 2106,
                  "height": 1468
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc920b45a1223c1528565fd604812590fd688bc1",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40aefee037971cf3d6b6efb316c7b3e51c7517f4",
                    "width": 216,
                    "height": 150
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dd6d6bf1658fd7fe587e5003dba9ba42a7ad36a",
                    "width": 320,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7c9ff380a5c67f510c3b2b1cf4849772e667cf4",
                    "width": 640,
                    "height": 446
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce4eccc69c232a328a2c24e817b01c27f77cdfd0",
                    "width": 960,
                    "height": 669
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af4b94b12d7167b70c16f924fc869a65fe847a07",
                    "width": 1080,
                    "height": 752
                  }
                ],
                "variants": {},
                "id": "LwTV7b9YJU3kHEoNxw0COXZ7bbZXrcGjWQcefFCOVzU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzhqz8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gilgameshcomputing",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/",
          "stickied": false,
          "url": "https://i.redd.it/lr7kq1452tcf1.png",
          "subreddit_subscribers": 498850,
          "created_utc": 1752484685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Testing method** \n\n* For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.\n* If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.\n* Only one question couldn't be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.\n* Note that quantizations are not same. It's just me, trying to find the best reasoning &amp; coding model for my setup. \n\n**Coloring strategy:**\n\n* Mark the solution green if it's accepted.\n* Use red if it fails in the pre-test cases.\n* Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.\n* Use orange if it fails in the test cases but still manages to pass over 90%.\n\n**A few observations:**\n\n* Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn’t treat them as failures, since they were limited to single character issues that clearly qualify as typos.\n* Hunyuan fell short of my expectations.\n* Qwen-32B and OpenCodeReasoning model both performed better than expected.\n* The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.\n\n**Hardware: 2x H100**\n\n**Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)**\n\nFeel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.\n\n**Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills**, since everyday programming tasks faced by typical users are usually far less complex.\n\nAll questions are recent, with no data leakage involved. So don’t come back saying “LeetCode problems are easy for models, this test isn’t meaningful”. It's just your test questions have been seen by the model before.\n\n",
          "author_fullname": "t2_slwqrxz3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhns3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 110,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 110,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WppDZCrZ0xycGtKVlAsBndunRlo8Km7IfOuHDADfvik.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752484340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Testing method&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/li&gt;\n&lt;li&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/li&gt;\n&lt;li&gt;Only one question couldn&amp;#39;t be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.&lt;/li&gt;\n&lt;li&gt;Note that quantizations are not same. It&amp;#39;s just me, trying to find the best reasoning &amp;amp; coding model for my setup. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Coloring strategy:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Mark the solution green if it&amp;#39;s accepted.&lt;/li&gt;\n&lt;li&gt;Use red if it fails in the pre-test cases.&lt;/li&gt;\n&lt;li&gt;Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.&lt;/li&gt;\n&lt;li&gt;Use orange if it fails in the test cases but still manages to pass over 90%.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;A few observations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn’t treat them as failures, since they were limited to single character issues that clearly qualify as typos.&lt;/li&gt;\n&lt;li&gt;Hunyuan fell short of my expectations.&lt;/li&gt;\n&lt;li&gt;Qwen-32B and OpenCodeReasoning model both performed better than expected.&lt;/li&gt;\n&lt;li&gt;The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware: 2x H100&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Keep in mind that strong performance on LeetCode doesn&amp;#39;t automatically reflect real world coding skills&lt;/strong&gt;, since everyday programming tasks faced by typical users are usually far less complex.&lt;/p&gt;\n\n&lt;p&gt;All questions are recent, with no data leakage involved. So don’t come back saying “LeetCode problems are easy for models, this test isn’t meaningful”. It&amp;#39;s just your test questions have been seen by the model before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nyu5vpzx2tcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?auto=webp&amp;s=e45702995060cc687a645562d2df2d39d92ccdf8",
                  "width": 1565,
                  "height": 408
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ba57d01c877fa0d1bec3f4aef3af9baaad55463",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf11fe9849c7a2c0833889c03bd68754c8b4e45",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d5e29a1cd3ba5283fb529c5d5457c3cce7a36a0",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f48f8bee49ad1c403134b86ad6d3fc3d3c55b4",
                    "width": 640,
                    "height": 166
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=44e9301ed958519990d12a001176c79624b7a9fe",
                    "width": 960,
                    "height": 250
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7efed2bc4442573f344c5203ab1b05379a7429be",
                    "width": 1080,
                    "height": 281
                  }
                ],
                "variants": {},
                "id": "Yt8sdbd4WSl3QWw399ju3ntGhqCOHF8RdVFnkafe5Hs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzhns3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kyazoglu",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/",
          "stickied": false,
          "url": "https://i.redd.it/nyu5vpzx2tcf1.png",
          "subreddit_subscribers": 498850,
          "created_utc": 1752484340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m building a local AI agent system using n8n to handle technical SQL Q&amp;A and dashboard generation based on database results — with tool execution via MCP Server.\n\nMy setup:\n\nGPU: NVIDIA A10 (24GB VRAM)\n\nSo I’m limited to small to medium models (&lt;=14B): \n\n* llama3.1:8b-instruct-fp16\n* qwen2.5:14b-instruct\n\nIssue: Even though I explicitly prompt with something like: “Step 1: Call Tool A to run a SQL query. Step 2: Use Tool B to generate a chart.” …the models just don’t follow — they output generic answers instead of invoking the defined tools.:  \nFOR example:  \n\\-------------------------------------------------------------------------------\n\nquestion: can you name top sql id that make low cpu?  \n\\-------------------------------------------------------------------------------\n\nanswer:    \n\\-------------------------------------------------------------------------------  \nStep 0: Execute tool `schema` to get all information about Tables Definition for next step.\n\n{\"name\": \"schema\", \"parameters\": {}}\n\nStep 1: Try to execute tool `Execute_sql_query` based on user question to get new data. Always limit 25.\n\nSince the user asked about performance của database theo aas không tóm tắt, I will assume they want to know the performance metrics of the database for AAS (Application and Service Architecture) without summarization. Here is the SQL query:\n\nSELECT \\* FROM public\\_2.aas\\_performance LIMIT 25;  \n... etc  \n\\----------------------------------------------------------------------------------------  \n\n\nI tested the same prompt with GPT-4, and it executes each step correctly — calls tools properly, reasons well, and behaves exactly as expected.  \nHas anyone found a small-to-mid size local model that can reliably follow structured, tool-calling prompts like GPT-4 does or any technique that can fix this issue",
          "author_fullname": "t2_efnhbt7w0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "local model for SQL Q&amp;A + dashboard agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhlvb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752484127,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a local AI agent system using n8n to handle technical SQL Q&amp;amp;A and dashboard generation based on database results — with tool execution via MCP Server.&lt;/p&gt;\n\n&lt;p&gt;My setup:&lt;/p&gt;\n\n&lt;p&gt;GPU: NVIDIA A10 (24GB VRAM)&lt;/p&gt;\n\n&lt;p&gt;So I’m limited to small to medium models (&amp;lt;=14B): &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;llama3.1:8b-instruct-fp16&lt;/li&gt;\n&lt;li&gt;qwen2.5:14b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Issue: Even though I explicitly prompt with something like: “Step 1: Call Tool A to run a SQL query. Step 2: Use Tool B to generate a chart.” …the models just don’t follow — they output generic answers instead of invoking the defined tools.:&lt;br/&gt;\nFOR example:&lt;br/&gt;\n-------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;question: can you name top sql id that make low cpu?&lt;br/&gt;\n-------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;answer:&lt;br/&gt;\n-------------------------------------------------------------------------------&lt;br/&gt;\nStep 0: Execute tool &lt;code&gt;schema&lt;/code&gt; to get all information about Tables Definition for next step.&lt;/p&gt;\n\n&lt;p&gt;{&amp;quot;name&amp;quot;: &amp;quot;schema&amp;quot;, &amp;quot;parameters&amp;quot;: {}}&lt;/p&gt;\n\n&lt;p&gt;Step 1: Try to execute tool &lt;code&gt;Execute_sql_query&lt;/code&gt; based on user question to get new data. Always limit 25.&lt;/p&gt;\n\n&lt;p&gt;Since the user asked about performance của database theo aas không tóm tắt, I will assume they want to know the performance metrics of the database for AAS (Application and Service Architecture) without summarization. Here is the SQL query:&lt;/p&gt;\n\n&lt;p&gt;SELECT * FROM public_2.aas_performance LIMIT 25;&lt;br/&gt;\n... etc&lt;br/&gt;\n----------------------------------------------------------------------------------------  &lt;/p&gt;\n\n&lt;p&gt;I tested the same prompt with GPT-4, and it executes each step correctly — calls tools properly, reasons well, and behaves exactly as expected.&lt;br/&gt;\nHas anyone found a small-to-mid size local model that can reliably follow structured, tool-calling prompts like GPT-4 does or any technique that can fix this issue&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzhlvb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Practical-Corgi-9906",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhlvb/local_model_for_sql_qa_dashboard_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzhlvb/local_model_for_sql_qa_dashboard_agent/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752484127,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently using Gemini 2.5 pro, and I seem to be using about $100 per month. I plan to increase the usage by 10 fold, so then I thought of using my 4090+3090 on open source models as a possibility cheaper alternative (and protect my assets). I'm currently testing Deep seek r1 70b and 8b. 70b takes a while, 8b seems much faster, but I continued using Gemini because of the context window.\n\nNow I'm just wondering if deepseek r1 is my best bet for programming locally or Kimi 2 is worth more, even if the inference it's much slower? Or something else? \n\nAnd perhaps I should be using some better flavor than pure Deep seek r1?",
          "author_fullname": "t2_cv5ft",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What best model(s) to use for inference using a 4090+3090 for Aider?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzh0cf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752481760,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using Gemini 2.5 pro, and I seem to be using about $100 per month. I plan to increase the usage by 10 fold, so then I thought of using my 4090+3090 on open source models as a possibility cheaper alternative (and protect my assets). I&amp;#39;m currently testing Deep seek r1 70b and 8b. 70b takes a while, 8b seems much faster, but I continued using Gemini because of the context window.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m just wondering if deepseek r1 is my best bet for programming locally or Kimi 2 is worth more, even if the inference it&amp;#39;s much slower? Or something else? &lt;/p&gt;\n\n&lt;p&gt;And perhaps I should be using some better flavor than pure Deep seek r1?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzh0cf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cGalaxy",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752481760,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello!\n\nI have a server on my network housing the RTX Pro 6000. I'd like to run a few models so that I can 1. Generate video (open to the interface used, but it seems like comfyui works well) and 2. Run a chat (likely with openwebui).\n\nMy question is, what is the most efficient way to run the models? Openllama? I prefer to run it dockerized, but it seems you can really fine tune things using pytorch? openllama i have used, but pytorch i am not familiar with. I am willing to run the models baremetal if it is significantly more efficient/performant.\n\nIt would also be beneficial if the program would automatically load/unload models based on their usage as it would be someone non-technical utilizing them and likely not always at the same time with long periods of non-use.\n\nAny tips would be appreciated.  Feel free to roast me as long as I can learn something from it ;)",
          "author_fullname": "t2_7xyy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to run dockerized linux LLM server?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzggo2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752479561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I have a server on my network housing the RTX Pro 6000. I&amp;#39;d like to run a few models so that I can 1. Generate video (open to the interface used, but it seems like comfyui works well) and 2. Run a chat (likely with openwebui).&lt;/p&gt;\n\n&lt;p&gt;My question is, what is the most efficient way to run the models? Openllama? I prefer to run it dockerized, but it seems you can really fine tune things using pytorch? openllama i have used, but pytorch i am not familiar with. I am willing to run the models baremetal if it is significantly more efficient/performant.&lt;/p&gt;\n\n&lt;p&gt;It would also be beneficial if the program would automatically load/unload models based on their usage as it would be someone non-technical utilizing them and likely not always at the same time with long periods of non-use.&lt;/p&gt;\n\n&lt;p&gt;Any tips would be appreciated.  Feel free to roast me as long as I can learn something from it ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzggo2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a_40oz_of_Mickeys",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752479561,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do you have any good uses cases for using the stop-sequence functionality when calling the API?\n\nList them below, please.",
          "author_fullname": "t2_8h2i7wiei",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stop-Sequences - Real World Use Cases",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfwdj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752477324,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have any good uses cases for using the stop-sequence functionality when calling the API?&lt;/p&gt;\n\n&lt;p&gt;List them below, please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzfwdj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Physical_Ad9040",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfwdj/stopsequences_real_world_use_cases/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzfwdj/stopsequences_real_world_use_cases/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752477324,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Regarding the design of tools, I want the LLM to generate files directly for the user. My current approach is:\nDefine a tool: \n```\ngen_file\nargs: {\n  file_name:\n  content:\n  append:\n}\n```\nHowever, I now have a different perspective. Is it really reasonable to use `content` as an argument for a tool call? Do long tool calls pose any problems for LLMs?",
          "author_fullname": "t2_1hlcy0hukk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "about LLM tools design",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfsxt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752476933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Regarding the design of tools, I want the LLM to generate files directly for the user. My current approach is:\nDefine a tool: \n&lt;code&gt;\ngen_file\nargs: {\n  file_name:\n  content:\n  append:\n}\n&lt;/code&gt;\nHowever, I now have a different perspective. Is it really reasonable to use &lt;code&gt;content&lt;/code&gt; as an argument for a tool call? Do long tool calls pose any problems for LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzfsxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dizzy-Meet-3258",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfsxt/about_llm_tools_design/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzfsxt/about_llm_tools_design/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752476933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4](https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4) (paywall)\n\nI don't know how the French and European authorities could accept this.",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple “will seriously consider” buying Mistral | Bloomberg - Mark Gurman",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 117,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfhhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 463,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 463,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Oi81Df9SQch6JRWkXBdb70VbCbc5PHkhZHq2yJRoex0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752475719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4\"&gt;https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4&lt;/a&gt; (paywall)&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know how the French and European authorities could accept this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/syyfccpldscf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/syyfccpldscf1.jpeg?auto=webp&amp;s=6c1efde69cd327275f5e2033e00c0702e28920d1",
                  "width": 1662,
                  "height": 1390
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89bbf9ebe8ad920a9decea85a04e8ddddce9143e",
                    "width": 108,
                    "height": 90
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf248f7dbf4358bb70f07ee9c1da43f8a5a1428d",
                    "width": 216,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbbee1c5b59b74677e14df9bdb2c959132e1da7b",
                    "width": 320,
                    "height": 267
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c267b676d172a191872cfbacda802bec7e6a2e8",
                    "width": 640,
                    "height": 535
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=940d958a3d989315f06b79867efa25519d7abd78",
                    "width": 960,
                    "height": 802
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=121ecef6d4b87299bfabd79e10028f6748fd5beb",
                    "width": 1080,
                    "height": 903
                  }
                ],
                "variants": {},
                "id": "QahBk6E1a44oImzdfLzsg1gZn_nkm-ZxKpD4RbU9wnc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzfhhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 188,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/",
          "stickied": false,
          "url": "https://i.redd.it/syyfccpldscf1.jpeg",
          "subreddit_subscribers": 498850,
          "created_utc": 1752475719,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We’ve launched dedicated GPU clusters (India &amp; US zones) with no waitlist. Mostly serving inference, fine-tuning, and SDXL use cases. \n\n* A100 / H100 / L40S \n* Hourly or monthly billing \n* Accessible via REST or container \n\nIf anyone needs GPUs for open-source models, happy to offer test credits on [cyfuture.ai](https://cyfuture.ai).",
          "author_fullname": "t2_1bv91ygeod",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for affordable dedicated GPUs (A100, H100) outside AWS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfdiw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.21,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752475292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’ve launched dedicated GPU clusters (India &amp;amp; US zones) with no waitlist. Mostly serving inference, fine-tuning, and SDXL use cases. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A100 / H100 / L40S &lt;/li&gt;\n&lt;li&gt;Hourly or monthly billing &lt;/li&gt;\n&lt;li&gt;Accessible via REST or container &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If anyone needs GPUs for open-source models, happy to offer test credits on &lt;a href=\"https://cyfuture.ai\"&gt;cyfuture.ai&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzfdiw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Trash_9030",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfdiw/looking_for_affordable_dedicated_gpus_a100_h100/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzfdiw/looking_for_affordable_dedicated_gpus_a100_h100/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752475292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to do voice dubbing but since I have started I am not being to achieve audible output... The videos are in English I transcrbe then in English then I translate the text in french, then when I try to get the traduced text to be read with the text to speech it gives me a bunch of gibberish, I am asking myself if it's an issue with the M1 processor or the script I don't get it... The videos are short, between 1 min to 3 min... Below is the script I use:\n\n\n#!/usr/bin/env python3\nimport torch\nimport gradio as gr\nimport librosa\nimport numpy as np\nfrom chatterbox.tts import ChatterboxTTS\nimport tempfile\nimport os\nimport importlib.util  # For dependency checking\n\n# Define sampling rate (Chatterbox uses 22.05kHz)\nSAMPLING_RATE = 22050\n\n# Check if soundfile is available\nif importlib.util.find_spec(\"soundfile\"):\n    import soundfile as sf\n    has_soundfile = True\nelse:\n    print(\"Warning: soundfile not installed. Using scipy.io.wavfile instead.\")\n    from scipy.io import wavfile\n    has_soundfile = False\n\n# Initialize TTS model\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\ntts_model = ChatterboxTTS.from_pretrained(device=device)\n\ndef preprocess_french_text(text):\n    \"\"\"Preprocess French text for better TTS pronunciation\"\"\"\n    # Simple normalization - expand common abbreviations\n    replacements = {\n        \"M.\": \"Monsieur\",\n        \"Mme\": \"Madame\",\n        \"Mlle\": \"Mademoiselle\",\n        \"Dr.\": \"Docteur\",\n        \"St.\": \"Saint\",\n        \"n°\": \"numéro\",\n        \"&amp;\": \"et\"\n    }\n    \n    for abbr, full in replacements.items():\n        text = text.replace(abbr, full)\n    \n    return text\n\ndef preprocess_voice_sample(voice_path):\n    \"\"\"Preprocess voice sample to meet Chatterbox requirements\"\"\"\n    if not voice_path or not os.path.exists(voice_path):\n        return None\n    \n    try:\n        # Load audio and convert to mono\n        y, sr = librosa.load(voice_path, sr=SAMPLING_RATE, mono=True)\n        \n        # Trim to 5 seconds (Chatterbox's optimal length)\n        max_samples = 5 * SAMPLING_RATE\n        if len(y) &gt; max_samples:\n            y = y[:max_samples]\n        \n        # Save processed sample to temporary file\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n            if has_soundfile:\n                sf.write(tmpfile.name, y, SAMPLING_RATE)\n            else:\n                wavfile.write(tmpfile.name, SAMPLING_RATE, (y * 32767).astype(np.int16))\n            return tmpfile.name\n    except Exception as e:\n        print(f\"Voice preprocessing error: {e}\")\n        return voice_path  # Fallback to original\n\ndef ensure_mono(audio):\n    \"\"\"Convert audio to mono (1D array) if it's stereo\"\"\"\n    if audio.ndim &gt; 1:\n        return np.mean(audio, axis=1)\n    return audio\n\ndef generate_tts_segment(text, voice_sample_path=None, exaggeration=0.5, cfg_weight=0.7, pace=1.0):\n    \"\"\"Generate French TTS audio for text segment\"\"\"\n    # Preprocess French text\n    text = preprocess_french_text(text)\n    \n    params = {\n        \"text\": text,\n        \"exaggeration\": exaggeration,\n        \"cfg_weight\": cfg_weight\n    }\n    \n    if voice_sample_path and os.path.exists(voice_sample_path):\n        params[\"audio_prompt_path\"] = voice_sample_path\n    \n    # Generate audio (returns a PyTorch tensor)\n    audio_tensor = tts_model.generate(**params)\n    \n    # Convert tensor to numpy array\n    audio = audio_tensor.cpu().numpy().astype(np.float32)\n    \n    # Ensure mono audio\n    audio = ensure_mono(audio)\n    \n    # Normalize audio to avoid clipping\n    max_val = np.max(np.abs(audio))\n    if max_val &gt; 0:\n        audio = audio / max_val\n    \n    # Apply pace adjustment\n    if pace != 1.0:\n        audio = librosa.effects.time_stretch(audio, rate=pace)\n    \n    return audio\n\ndef process_text_file(text_file, voice_sample=None, exaggeration=0.5, cfg_weight=0.7, pause_duration=0.5, pace=1.0):\n    \"\"\"Process text file and generate concatenated audio\"\"\"\n    # Get actual file path\n    txt_path = text_file.name\n    \n    # Preprocess voice sample if provided\n    preprocessed_voice_path = None\n    if voice_sample:\n        preprocessed_voice_path = preprocess_voice_sample(voice_sample)\n    \n    try:\n        with open(txt_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n    except Exception as e:\n        yield f\"Error opening text file: {str(e)}\", None\n        return\n    \n    # Split text into paragraphs\n    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n    \n    full_audio = np.array([], dtype=np.float32)\n    pause_samples = int(pause_duration * SAMPLING_RATE)\n    \n    for i, paragraph in enumerate(paragraphs):\n        try:\n            # Generate audio for paragraph\n            segment = generate_tts_segment(\n                text=paragraph,\n                voice_sample_path=preprocessed_voice_path,\n                exaggeration=exaggeration,\n                cfg_weight=cfg_weight,\n                pace=pace\n            )\n            full_audio = np.concatenate([full_audio, segment])\n            \n            # Add pause between paragraphs (except after last one)\n            if i &lt; len(paragraphs) - 1:\n                full_audio = np.concatenate([full_audio, np.zeros(pause_samples)])\n        except Exception as e:\n            yield f\"Error processing paragraph {i+1}: {str(e)}\", None\n            return\n        \n        yield f\"Processing paragraph {i+1}/{len(paragraphs)}\", None\n        \n    # Clean up temporary voice file\n    if preprocessed_voice_path and os.path.exists(preprocessed_voice_path):\n        try:\n            os.remove(preprocessed_voice_path)\n        except Exception:\n            pass  # Ignore cleanup errors\n        \n    # Save to temporary file\n    try:\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n            output_path = tmpfile.name\n            if has_soundfile:\n                sf.write(output_path, full_audio, SAMPLING_RATE)\n            else:\n                wavfile.write(output_path, SAMPLING_RATE, (full_audio * 32767).astype(np.int16))\n        yield \"Audio generated successfully!\", output_path\n    except Exception as e:\n        yield f\"Audio save error: {str(e)}\", None\n\n# Gradio UI\nwith gr.Blocks(title=\"French Text Audio Synthesizer\") as ui:\n    gr.Markdown(\"# 🎧 French Text-to-Speech Generator\")\n    gr.Markdown(\"Generate French audio from .txt files with natural pauses\")\n    \n    with gr.Row():\n        with gr.Column():\n            text_input = gr.File(label=\"Text File\", file_types=[\".txt\"])\n            voice_input = gr.Audio(\n                label=\"Voice Sample (Optional)\",\n                type=\"filepath\",\n                sources=[\"upload\"],\n                format=\"wav\"\n            )\n            emotion_slider = gr.Slider(0.0, 1.0, 0.5, label=\"Emotion Intensity\")\n            pause_slider = gr.Slider(0.0, 2.0, 0.5, label=\"Pause Duration (seconds)\")\n            pace_slider = gr.Slider(0.5, 1.5, 1.0, label=\"Speech Pace\")\n            generate_btn = gr.Button(\"Generate Audio\")\n        \n        with gr.Column():\n            status = gr.Textbox(label=\"Status\", interactive=False)\n            audio_output = gr.Audio(label=\"Generated Audio\", type=\"filepath\")\n    \n    generate_btn.click(\n        fn=process_text_file,\n        inputs=[text_input, voice_input, emotion_slider, pause_slider, pace_slider],\n        outputs=[status, audio_output]\n    )\n\nif __name__ == \"__main__\":\n    ui.launch(server_port=7860)\n",
          "author_fullname": "t2_77qlzs20",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Xttsv2 model, Chatterbox on MacBook air 8 gb",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzf6zi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752474601,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to do voice dubbing but since I have started I am not being to achieve audible output... The videos are in English I transcrbe then in English then I translate the text in french, then when I try to get the traduced text to be read with the text to speech it gives me a bunch of gibberish, I am asking myself if it&amp;#39;s an issue with the M1 processor or the script I don&amp;#39;t get it... The videos are short, between 1 min to 3 min... Below is the script I use:&lt;/p&gt;\n\n&lt;h1&gt;!/usr/bin/env python3&lt;/h1&gt;\n\n&lt;p&gt;import torch\nimport gradio as gr\nimport librosa\nimport numpy as np\nfrom chatterbox.tts import ChatterboxTTS\nimport tempfile\nimport os\nimport importlib.util  # For dependency checking&lt;/p&gt;\n\n&lt;h1&gt;Define sampling rate (Chatterbox uses 22.05kHz)&lt;/h1&gt;\n\n&lt;p&gt;SAMPLING_RATE = 22050&lt;/p&gt;\n\n&lt;h1&gt;Check if soundfile is available&lt;/h1&gt;\n\n&lt;p&gt;if importlib.util.find_spec(&amp;quot;soundfile&amp;quot;):\n    import soundfile as sf\n    has_soundfile = True\nelse:\n    print(&amp;quot;Warning: soundfile not installed. Using scipy.io.wavfile instead.&amp;quot;)\n    from scipy.io import wavfile\n    has_soundfile = False&lt;/p&gt;\n\n&lt;h1&gt;Initialize TTS model&lt;/h1&gt;\n\n&lt;p&gt;device = &amp;quot;mps&amp;quot; if torch.backends.mps.is_available() else &amp;quot;cpu&amp;quot;\ntts_model = ChatterboxTTS.from_pretrained(device=device)&lt;/p&gt;\n\n&lt;p&gt;def preprocess_french_text(text):\n    &amp;quot;&amp;quot;&amp;quot;Preprocess French text for better TTS pronunciation&amp;quot;&amp;quot;&amp;quot;\n    # Simple normalization - expand common abbreviations\n    replacements = {\n        &amp;quot;M.&amp;quot;: &amp;quot;Monsieur&amp;quot;,\n        &amp;quot;Mme&amp;quot;: &amp;quot;Madame&amp;quot;,\n        &amp;quot;Mlle&amp;quot;: &amp;quot;Mademoiselle&amp;quot;,\n        &amp;quot;Dr.&amp;quot;: &amp;quot;Docteur&amp;quot;,\n        &amp;quot;St.&amp;quot;: &amp;quot;Saint&amp;quot;,\n        &amp;quot;n°&amp;quot;: &amp;quot;numéro&amp;quot;,\n        &amp;quot;&amp;amp;&amp;quot;: &amp;quot;et&amp;quot;\n    }&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for abbr, full in replacements.items():\n    text = text.replace(abbr, full)\n\nreturn text\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def preprocess_voice_sample(voice_path):\n    &amp;quot;&amp;quot;&amp;quot;Preprocess voice sample to meet Chatterbox requirements&amp;quot;&amp;quot;&amp;quot;\n    if not voice_path or not os.path.exists(voice_path):\n        return None&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;try:\n    # Load audio and convert to mono\n    y, sr = librosa.load(voice_path, sr=SAMPLING_RATE, mono=True)\n\n    # Trim to 5 seconds (Chatterbox&amp;#39;s optimal length)\n    max_samples = 5 * SAMPLING_RATE\n    if len(y) &amp;gt; max_samples:\n        y = y[:max_samples]\n\n    # Save processed sample to temporary file\n    with tempfile.NamedTemporaryFile(suffix=&amp;quot;.wav&amp;quot;, delete=False) as tmpfile:\n        if has_soundfile:\n            sf.write(tmpfile.name, y, SAMPLING_RATE)\n        else:\n            wavfile.write(tmpfile.name, SAMPLING_RATE, (y * 32767).astype(np.int16))\n        return tmpfile.name\nexcept Exception as e:\n    print(f&amp;quot;Voice preprocessing error: {e}&amp;quot;)\n    return voice_path  # Fallback to original\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def ensure_mono(audio):\n    &amp;quot;&amp;quot;&amp;quot;Convert audio to mono (1D array) if it&amp;#39;s stereo&amp;quot;&amp;quot;&amp;quot;\n    if audio.ndim &amp;gt; 1:\n        return np.mean(audio, axis=1)\n    return audio&lt;/p&gt;\n\n&lt;p&gt;def generate_tts_segment(text, voice_sample_path=None, exaggeration=0.5, cfg_weight=0.7, pace=1.0):\n    &amp;quot;&amp;quot;&amp;quot;Generate French TTS audio for text segment&amp;quot;&amp;quot;&amp;quot;\n    # Preprocess French text\n    text = preprocess_french_text(text)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;params = {\n    &amp;quot;text&amp;quot;: text,\n    &amp;quot;exaggeration&amp;quot;: exaggeration,\n    &amp;quot;cfg_weight&amp;quot;: cfg_weight\n}\n\nif voice_sample_path and os.path.exists(voice_sample_path):\n    params[&amp;quot;audio_prompt_path&amp;quot;] = voice_sample_path\n\n# Generate audio (returns a PyTorch tensor)\naudio_tensor = tts_model.generate(**params)\n\n# Convert tensor to numpy array\naudio = audio_tensor.cpu().numpy().astype(np.float32)\n\n# Ensure mono audio\naudio = ensure_mono(audio)\n\n# Normalize audio to avoid clipping\nmax_val = np.max(np.abs(audio))\nif max_val &amp;gt; 0:\n    audio = audio / max_val\n\n# Apply pace adjustment\nif pace != 1.0:\n    audio = librosa.effects.time_stretch(audio, rate=pace)\n\nreturn audio\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def process_text_file(text_file, voice_sample=None, exaggeration=0.5, cfg_weight=0.7, pause_duration=0.5, pace=1.0):\n    &amp;quot;&amp;quot;&amp;quot;Process text file and generate concatenated audio&amp;quot;&amp;quot;&amp;quot;\n    # Get actual file path\n    txt_path = text_file.name&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Preprocess voice sample if provided\npreprocessed_voice_path = None\nif voice_sample:\n    preprocessed_voice_path = preprocess_voice_sample(voice_sample)\n\ntry:\n    with open(txt_path, &amp;#39;r&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f:\n        text = f.read()\nexcept Exception as e:\n    yield f&amp;quot;Error opening text file: {str(e)}&amp;quot;, None\n    return\n\n# Split text into paragraphs\nparagraphs = [p.strip() for p in text.split(&amp;#39;\\n\\n&amp;#39;) if p.strip()]\n\nfull_audio = np.array([], dtype=np.float32)\npause_samples = int(pause_duration * SAMPLING_RATE)\n\nfor i, paragraph in enumerate(paragraphs):\n    try:\n        # Generate audio for paragraph\n        segment = generate_tts_segment(\n            text=paragraph,\n            voice_sample_path=preprocessed_voice_path,\n            exaggeration=exaggeration,\n            cfg_weight=cfg_weight,\n            pace=pace\n        )\n        full_audio = np.concatenate([full_audio, segment])\n\n        # Add pause between paragraphs (except after last one)\n        if i &amp;lt; len(paragraphs) - 1:\n            full_audio = np.concatenate([full_audio, np.zeros(pause_samples)])\n    except Exception as e:\n        yield f&amp;quot;Error processing paragraph {i+1}: {str(e)}&amp;quot;, None\n        return\n\n    yield f&amp;quot;Processing paragraph {i+1}/{len(paragraphs)}&amp;quot;, None\n\n# Clean up temporary voice file\nif preprocessed_voice_path and os.path.exists(preprocessed_voice_path):\n    try:\n        os.remove(preprocessed_voice_path)\n    except Exception:\n        pass  # Ignore cleanup errors\n\n# Save to temporary file\ntry:\n    with tempfile.NamedTemporaryFile(suffix=&amp;quot;.wav&amp;quot;, delete=False) as tmpfile:\n        output_path = tmpfile.name\n        if has_soundfile:\n            sf.write(output_path, full_audio, SAMPLING_RATE)\n        else:\n            wavfile.write(output_path, SAMPLING_RATE, (full_audio * 32767).astype(np.int16))\n    yield &amp;quot;Audio generated successfully!&amp;quot;, output_path\nexcept Exception as e:\n    yield f&amp;quot;Audio save error: {str(e)}&amp;quot;, None\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Gradio UI&lt;/h1&gt;\n\n&lt;p&gt;with gr.Blocks(title=&amp;quot;French Text Audio Synthesizer&amp;quot;) as ui:\n    gr.Markdown(&amp;quot;# 🎧 French Text-to-Speech Generator&amp;quot;)\n    gr.Markdown(&amp;quot;Generate French audio from .txt files with natural pauses&amp;quot;)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with gr.Row():\n    with gr.Column():\n        text_input = gr.File(label=&amp;quot;Text File&amp;quot;, file_types=[&amp;quot;.txt&amp;quot;])\n        voice_input = gr.Audio(\n            label=&amp;quot;Voice Sample (Optional)&amp;quot;,\n            type=&amp;quot;filepath&amp;quot;,\n            sources=[&amp;quot;upload&amp;quot;],\n            format=&amp;quot;wav&amp;quot;\n        )\n        emotion_slider = gr.Slider(0.0, 1.0, 0.5, label=&amp;quot;Emotion Intensity&amp;quot;)\n        pause_slider = gr.Slider(0.0, 2.0, 0.5, label=&amp;quot;Pause Duration (seconds)&amp;quot;)\n        pace_slider = gr.Slider(0.5, 1.5, 1.0, label=&amp;quot;Speech Pace&amp;quot;)\n        generate_btn = gr.Button(&amp;quot;Generate Audio&amp;quot;)\n\n    with gr.Column():\n        status = gr.Textbox(label=&amp;quot;Status&amp;quot;, interactive=False)\n        audio_output = gr.Audio(label=&amp;quot;Generated Audio&amp;quot;, type=&amp;quot;filepath&amp;quot;)\n\ngenerate_btn.click(\n    fn=process_text_file,\n    inputs=[text_input, voice_input, emotion_slider, pause_slider, pace_slider],\n    outputs=[status, audio_output]\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    ui.launch(server_port=7860)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzf6zi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Layonkizungu",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzf6zi/xttsv2_model_chatterbox_on_macbook_air_8_gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzf6zi/xttsv2_model_chatterbox_on_macbook_air_8_gb/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752474601,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.\n\nQ: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?",
          "author_fullname": "t2_1fp7huwh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can VRAM be combined of 2 brands",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lze20x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752470433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.&lt;/p&gt;\n\n&lt;p&gt;Q: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lze20x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tonyleungnl",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752470433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It's very cool to watch it do it's thing. Make sure to use the --diffusion-visual flag. It's still a PR but has been approved so it should be merged soon.",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Diffusion model support in llama.cpp.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lze1r3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=77e8aaca890b1dc8486701afa4da4f4e06d486be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752470404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It&amp;#39;s very cool to watch it do it&amp;#39;s thing. Make sure to use the --diffusion-visual flag. It&amp;#39;s still a PR but has been approved so it should be merged soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14644",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?auto=webp&amp;s=e183ff7e541a319425a36dcaf9b80b74c4ff9243",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f661fc69800730ffa673f5aa97b47d5b9e191899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4c5410f2c6927b2130b0b8edfce13fc0ce8cd59",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aab06d8dbc7317d7ae49105599d33bc04e0c66cf",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c75c6786f093153f6a5dc5065d5f9e2b741b5086",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a1002549b1b328880a986274b59212fd91c0e1f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01b756f299543d2fda31db392dfcbc407ad0faa7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lze1r3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14644",
          "subreddit_subscribers": 498850,
          "created_utc": 1752470404,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created [**TorchLeet**](https://github.com/Exorust/TorchLeet)! It's a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.\n\nIt’s split into:\n\n* **PyTorch Problems** (Basic → Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability\n* **LLM Problems**: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.\n\n  \nI'd love feedback from the community and help taking this forward!",
          "author_fullname": "t2_7zqffw4e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Practice Pytorch like Leetcode? (Also with cool LLM questions)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzdu0l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752469640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created &lt;a href=\"https://github.com/Exorust/TorchLeet\"&gt;&lt;strong&gt;TorchLeet&lt;/strong&gt;&lt;/a&gt;! It&amp;#39;s a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.&lt;/p&gt;\n\n&lt;p&gt;It’s split into:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch Problems&lt;/strong&gt; (Basic → Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM Problems&lt;/strong&gt;: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love feedback from the community and help taking this forward!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?auto=webp&amp;s=6fcfa2d9d7c19f8fa35afeefcbbce3bf504c2b85",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33bbaed8c03eedf7c1200615b0ca7a0d1815ce63",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c56109fdcb24e669c74e98d923ae3f1642d2e040",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08a19b35605d99c30080a37ea914e073131c141a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8084972ed24e594f6135f70051578b2e4855e936",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc05f2d37492136cc66d813fe3f3a1397d056030",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1a139fb45b571bcfb4f1178fa34f58b45cb9d7eb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzdu0l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "exorust_fire",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752469640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been looking all over for a frontend to simplify the use of FishSpeech, but I haven't been able to find one.\n\nI was wondering if anyone has found one ...",
          "author_fullname": "t2_hyklw8a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There will be some frontend for FishSpeech?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzdgc8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752468330,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking all over for a frontend to simplify the use of FishSpeech, but I haven&amp;#39;t been able to find one.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has found one ...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzdgc8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vk3r",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzdgc8/there_will_be_some_frontend_for_fishspeech/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzdgc8/there_will_be_some_frontend_for_fishspeech/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752468330,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:\n\n|Model|dense layer#|MoE layer#|shared|active/routed|Shared|Active|Params|Active%|fp16 kv@128k|kv%|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|DeepSeek-MoE-16B|1|27|2|6/64|1.42B|2.83B|16.38B|17.28%|28GB|85.47%|\n|DeepSeek-V2-Lite|1|26|2|6/64|1.31B|2.66B|15.71B|16.93%|3.8GB|12.09%|\n|DeepSeek-V2|1|59|2|6/160|12.98B|21.33B|235.74B|8.41%|8.44GB|1.78%|\n|DeepSeek-V3|3|58|1|8/256|17.01B|37.45B|671.03B|5.58%|8.578GB|0.64%|\n|Kimi-K2|1|60|1|8/384|11.56B|32.70B|1026.41B|3.19%|8.578GB|0.42%|\n|Qwen3-30B-A3B|0|48|0|8/128|1.53B|3.34B|30.53B|10.94%|12GB|19.65%|\n|Qwen3-235B-A22B|0|94|0|8/128|7.95B|22.14B|235.09B|9.42%|23.5GB|4.998%|\n|Llama-4-Scout-17B-16E|0|48|1|1/16|11.13B|17.17B|107.77B|15.93%|24GB|11.13%|\n|Llama-4-Maverick-17B-128E|24|24|1|1/128|14.15B|17.17B|400.71B|4.28%|24GB|2.99%|\n|Mixtral-8x7B|0|32|0|2/8|1.60B|12.88B|46.70B|27.58%|24GB|25.696%|\n|Mixtral-8x22B|0|56|0|2/8|5.33B|39.15B|140.62B|27.84%|28GB|9.956%|\n\nLooks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. \n\nModels using their own architecture is Kimi-VL and Kimi-Audio. \n\nEdited: Per u/Aaaaaaaaaeeeee 's request. I added a column called \"Shared\" which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.",
          "author_fullname": "t2_s6sfw4yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi-K2 is a DeepSeek V3 with more experts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzcuom",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 193,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 193,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752495937,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752466353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;dense layer#&lt;/th&gt;\n&lt;th align=\"left\"&gt;MoE layer#&lt;/th&gt;\n&lt;th align=\"left\"&gt;shared&lt;/th&gt;\n&lt;th align=\"left\"&gt;active/routed&lt;/th&gt;\n&lt;th align=\"left\"&gt;Shared&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active&lt;/th&gt;\n&lt;th align=\"left\"&gt;Params&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active%&lt;/th&gt;\n&lt;th align=\"left\"&gt;fp16 kv@128k&lt;/th&gt;\n&lt;th align=\"left\"&gt;kv%&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-MoE-16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;27&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/64&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.42B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.83B&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.28%&lt;/td&gt;\n&lt;td align=\"left\"&gt;28GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;85.47%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V2-Lite&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/64&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.31B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.66B&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.71B&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.93%&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.8GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.09%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;59&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/160&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.98B&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.33B&lt;/td&gt;\n&lt;td align=\"left\"&gt;235.74B&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.41%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.44GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.78%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;58&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/256&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.01B&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.45B&lt;/td&gt;\n&lt;td align=\"left\"&gt;671.03B&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.58%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.578GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.64%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi-K2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/384&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.56B&lt;/td&gt;\n&lt;td align=\"left\"&gt;32.70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1026.41B&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.19%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.578GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.42%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;48&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.53B&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.34B&lt;/td&gt;\n&lt;td align=\"left\"&gt;30.53B&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.94%&lt;/td&gt;\n&lt;td align=\"left\"&gt;12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.65%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;94&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.95B&lt;/td&gt;\n&lt;td align=\"left\"&gt;22.14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;235.09B&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.42%&lt;/td&gt;\n&lt;td align=\"left\"&gt;23.5GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.998%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-4-Scout-17B-16E&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;48&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/16&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.13B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;107.77B&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.93%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.13%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.15B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400.71B&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.28%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.99%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mixtral-8x7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;32&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.60B&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.88B&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.58%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.696%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mixtral-8x22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;56&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.33B&lt;/td&gt;\n&lt;td align=\"left\"&gt;39.15B&lt;/td&gt;\n&lt;td align=\"left\"&gt;140.62B&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.84%&lt;/td&gt;\n&lt;td align=\"left\"&gt;28GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.956%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Looks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. &lt;/p&gt;\n\n&lt;p&gt;Models using their own architecture is Kimi-VL and Kimi-Audio. &lt;/p&gt;\n\n&lt;p&gt;Edited: Per &lt;a href=\"/u/Aaaaaaaaaeeeee\"&gt;u/Aaaaaaaaaeeeee&lt;/a&gt; &amp;#39;s request. I added a column called &amp;quot;Shared&amp;quot; which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzcuom",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Warning2146",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752466353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What would you build with that? does it give you something that is entry level, mid and top tier (consumer grade)\n\nOr does it make sense to step up to 10k? where does the incremental benefit diminish significantly as the budget increases?\n\nEdit: I think i would at a bare minimum run a 5090 on it? does that future proof most local LLM models? i would want to run things like hunyuan (tencent vid), audiogen, musicgen (Meta), musetalk, Qwen, Whisper, image gen tools.\n\ndo most of these things run below 48gb vram? i suppose that is the bottleneck? Does that mean if i want to future proof, i think something a little better. i would also want to use the rig for gaming",
          "author_fullname": "t2_4dhoqhre",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of rig would you build with a 5k budget for local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzbadq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752463759,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752461403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would you build with that? does it give you something that is entry level, mid and top tier (consumer grade)&lt;/p&gt;\n\n&lt;p&gt;Or does it make sense to step up to 10k? where does the incremental benefit diminish significantly as the budget increases?&lt;/p&gt;\n\n&lt;p&gt;Edit: I think i would at a bare minimum run a 5090 on it? does that future proof most local LLM models? i would want to run things like hunyuan (tencent vid), audiogen, musicgen (Meta), musetalk, Qwen, Whisper, image gen tools.&lt;/p&gt;\n\n&lt;p&gt;do most of these things run below 48gb vram? i suppose that is the bottleneck? Does that mean if i want to future proof, i think something a little better. i would also want to use the rig for gaming&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzbadq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "songhaegyo",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzbadq/what_kind_of_rig_would_you_build_with_a_5k_budget/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzbadq/what_kind_of_rig_would_you_build_with_a_5k_budget/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752461403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2u7dh8n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building “Auto-Analyst” — A data analytics AI agentic system. LLM Agnostic can be used locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzbad8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=140&amp;height=79&amp;crop=140:79,smart&amp;auto=webp&amp;s=7dc759be09089b99a98b8d7055e6ee537f839db7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752461401,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "firebird-technologies.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.firebird-technologies.com/p/building-auto-analyst-a-data-analytics",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?auto=webp&amp;s=978512887e76ff70477002d96947b7d9bcfa4c77",
                  "width": 788,
                  "height": 445
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7b0b8070f32c6abef26fb7d071c276a90b8747b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7058de702adf3cc481f241fd2054477ddbe04f4",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00741c3f8a782a48cf37c001446ef9bc418eccc4",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ff25f0b3f0b37cd0f676be8ed2a53bfd7286a81",
                    "width": 640,
                    "height": 361
                  }
                ],
                "variants": {},
                "id": "vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzbad8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phicreative1997",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzbad8/building_autoanalyst_a_data_analytics_ai_agentic/",
          "stickied": false,
          "url": "https://www.firebird-technologies.com/p/building-auto-analyst-a-data-analytics",
          "subreddit_subscribers": 498850,
          "created_utc": 1752461401,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for something I can run locally that's actually close to gpt-4o or claude in terms of quality.\n\nKinda tight on money right now so I can't afford gpt plus or claude pro :/  \n  \nI have to write a bunch of posts throughout the day, and the free gpt-4o hits its limit way too fast.\n\nIs there anything similar out there that gives quality output like gpt-4o or claude and can run locally?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Actual alternative to gpt-4o or claude?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzb7fh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752461154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for something I can run locally that&amp;#39;s actually close to gpt-4o or claude in terms of quality.&lt;/p&gt;\n\n&lt;p&gt;Kinda tight on money right now so I can&amp;#39;t afford gpt plus or claude pro :/  &lt;/p&gt;\n\n&lt;p&gt;I have to write a bunch of posts throughout the day, and the free gpt-4o hits its limit way too fast.&lt;/p&gt;\n\n&lt;p&gt;Is there anything similar out there that gives quality output like gpt-4o or claude and can run locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzb7fh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzb7fh/any_actual_alternative_to_gpt4o_or_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzb7fh/any_actual_alternative_to_gpt4o_or_claude/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752461154,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone read any good papers on RFT / RL techniques for finetuning \"reasoning\" models for tool calling? I'm really interested in learning more. I have read this paper\nhttps://arxiv.org/html/2412.16849v1 \n-- but really don't have a good lay of the land regarding this space.",
          "author_fullname": "t2_izj21",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning / RL post training for tool calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzb04f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752460532,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone read any good papers on RFT / RL techniques for finetuning &amp;quot;reasoning&amp;quot; models for tool calling? I&amp;#39;m really interested in learning more. I have read this paper\n&lt;a href=\"https://arxiv.org/html/2412.16849v1\"&gt;https://arxiv.org/html/2412.16849v1&lt;/a&gt; \n-- but really don&amp;#39;t have a good lay of the land regarding this space.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzb04f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "soorg_nalyd",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzb04f/finetuning_rl_post_training_for_tool_calling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzb04f/finetuning_rl_post_training_for_tool_calling/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752460532,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that's why I didn't go the fine tune route. It will have no modern bias and will only be able to reason within the time period it's trained on. It's super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.",
          "author_fullname": "t2_1ink6kzg93",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Training an LLM only on books from the 1800's - no modern bias",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzampg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 714,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 714,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=60dd42f39ab2b858d7a5af3cc6c9b33d8a7c0ec8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752459413,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, im working on something that I havent seen anyone else do before, I trained nanoGPT on only books from a specifc time period and region of the world. I chose to do 1800-1850 London. My dataset was only 187mb (around 50 books). Right now the trained model produces random incoherent sentences but they do kind of feel like 1800s style sentences. My end goal is to create an LLM that doesnt pretend to be historical but just is, that&amp;#39;s why I didn&amp;#39;t go the fine tune route. It will have no modern bias and will only be able to reason within the time period it&amp;#39;s trained on. It&amp;#39;s super random and has no utility but I think if I train using a big dataset (like 600 books) the result will be super sick.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/haykgrigo3/TimeCapsuleLLM",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?auto=webp&amp;s=9ce4bc74d599d7799104a933627053a9a10d7d4b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d44946a1ed6e59d2f29fe66c42efbdf9beadf176",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2de1efb7c3a13836541de0a4e25083dafe4cb75",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=200239be7df394436a63a82488ec3fea1a1981d4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e60398c5e2e84881134a46e0acf601c56ba81942",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fe73c59013abcf68f28e8e3ab9df30778338705",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15f1bb851a628752877d4bae276313f481c7dcf5",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "AiV4MAC3PG2xPq-j4g8nw6ZuH5_-LU4f7enDtlyUQUo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzampg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Trick-177",
          "discussion_type": null,
          "num_comments": 165,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzampg/training_an_llm_only_on_books_from_the_1800s_no/",
          "stickied": false,
          "url": "https://github.com/haykgrigo3/TimeCapsuleLLM",
          "subreddit_subscribers": 498850,
          "created_utc": 1752459413,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying Fish Speech Open Audio S1 mini.   \n  \nThis one: [https://github.com/fishaudio/fish-speech](https://github.com/fishaudio/fish-speech)   \n  \nIn the web ui, there is no pacing option. Is there anyway we can control the pacing?   \n  \nWhen you upload a referenced audio, put a text prompt and generate the audio, I want output to speak slow or fast sometimes.   \n  \nCan we add a custom pacing control option?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you add pacing control option in TTS ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lza5bu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752457998,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying Fish Speech Open Audio S1 mini.   &lt;/p&gt;\n\n&lt;p&gt;This one: &lt;a href=\"https://github.com/fishaudio/fish-speech\"&gt;https://github.com/fishaudio/fish-speech&lt;/a&gt;   &lt;/p&gt;\n\n&lt;p&gt;In the web ui, there is no pacing option. Is there anyway we can control the pacing?   &lt;/p&gt;\n\n&lt;p&gt;When you upload a referenced audio, put a text prompt and generate the audio, I want output to speak slow or fast sometimes.   &lt;/p&gt;\n\n&lt;p&gt;Can we add a custom pacing control option?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?auto=webp&amp;s=ad23502cf6602bb61e1a9ac506d7031701a2b6dc",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=95436e0dba572c9aee2a59c53fd67015c882eebb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf4d62f675632ca88ba7e85fa82e4aadf74b32ef",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a660e607df8576c404ccecb4558748f67e07102c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5442b0d30b6b17b82289f8cfe1d046d52a045429",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e538b19864e18862b4e2246be24a66c0195a2e0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e082495a45e9097df6d4b048ab8a643fc4960e6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "dGuJLatJajGp-nrEX3yz6d31fPziHLA-BG1PXpYICXk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lza5bu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lza5bu/can_you_add_pacing_control_option_in_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lza5bu/can_you_add_pacing_control_option_in_tts/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752457998,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking for LLMs to generate questions and answers from physics textbook chapters. The chapters I’ll provide can be up to 10 pages long and may include images. I’ve tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didn’t work either as it rejects the input file, saying it’s too large. Which LLM model would you recommend me to try next? It doesn’t have to be free. ",
          "author_fullname": "t2_3g2onktn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which LLM should I use to generate high quality Q&amp;A from physics textbook chapters?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz81ea",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752451840,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking for LLMs to generate questions and answers from physics textbook chapters. The chapters I’ll provide can be up to 10 pages long and may include images. I’ve tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didn’t work either as it rejects the input file, saying it’s too large. Which LLM model would you recommend me to try next? It doesn’t have to be free. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz81ea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WhiteTentacle",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752451840,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Planning to build a budget local llm server with 2 mi50s. \n\nSince they need to have a Radeon 7 bios flashed on to provide display output, I was wondering if I can just use a cpu with an igpu to ignore that part. Something like a 5600g or ancintel cpu without the F suffix",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Will this work?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz7vh3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752451366,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Planning to build a budget local llm server with 2 mi50s. &lt;/p&gt;\n\n&lt;p&gt;Since they need to have a Radeon 7 bios flashed on to provide display output, I was wondering if I can just use a cpu with an igpu to ignore that part. Something like a 5600g or ancintel cpu without the F suffix&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz7vh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz7vh3/will_this_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz7vh3/will_this_work/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752451366,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Let's say we have a 30b, 24b, 14b, 7b model that exceeds in quality but the context window is like... 8k or worse, 4k. What can you possibly do in this case?\n\nBack in 2022 I used a unkown gpt plugin involving PDF files are permanent memory that didn't used the context window, even now it would be really useful if there was also a manner of insering some sort of text, pdf or text document file for the model to get \"fixed on\", like it's permanent focus (like a bot Card for example, where the biography would be stored instead of resent at every request and then combined to the whole context of the chat).\n\nResume: Method of increasing context lengh or using document for loading what chat context is focused on.",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Safe methods of increasing Context Window of models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz5sm6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752445683,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say we have a 30b, 24b, 14b, 7b model that exceeds in quality but the context window is like... 8k or worse, 4k. What can you possibly do in this case?&lt;/p&gt;\n\n&lt;p&gt;Back in 2022 I used a unkown gpt plugin involving PDF files are permanent memory that didn&amp;#39;t used the context window, even now it would be really useful if there was also a manner of insering some sort of text, pdf or text document file for the model to get &amp;quot;fixed on&amp;quot;, like it&amp;#39;s permanent focus (like a bot Card for example, where the biography would be stored instead of resent at every request and then combined to the whole context of the chat).&lt;/p&gt;\n\n&lt;p&gt;Resume: Method of increasing context lengh or using document for loading what chat context is focused on.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz5sm6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz5sm6/safe_methods_of_increasing_context_window_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz5sm6/safe_methods_of_increasing_context_window_of/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752445683,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Similiar in principle to the '[6x GPU Build. 4x RTX 3090 and 2x MI60. Epyc 7002](https://www.reddit.com/r/LocalLLaMA/comments/1g6ixae/6x_gpu_build_4x_rtx_3090_and_2x_mi60_epyc_7002/)' someone posted several months ago\n\nBut that thing is ginormous (dual PSU on either side of motherboard) so likely the 636mm wide version.\n\nI have a 12U (19inch) rack I want to install this into. The total rack width is 520mm BUT with the T-nut mounting posts on either side the opening narrows down to approx 480mm.\n\nI've found some frames but they are all 500mm wide. Which could 'fit' inside the rack but then wont be able to slide in and out on the shelf I will mount it on (for easy maintenance or fiddling)\n\nCan anyone point me to a 480mm wide dual layer frame that can slide into a 19inch rack shelf?\n\nThe only other solution I can think of is to hack a dual stack from two of these:\n\nhttps://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;format=pjpg&amp;auto=webp&amp;s=64aa05e469107d24131f92557562e0ae80c5f44a\n\nPS this build is just for fun/learning/tinkering with LLMs (and also replace one of my ex-office-PC proxmox nodes, Has nothing to do with work/career/business so I have already gone massively over on the 'just for fun'' budget by deciding on a Threadripper Pro 3945WX on a Gigabyte MC62-G40 board with 128GB RAM. And cant justify a more professional chassis solution\n\nThe MC62 &amp; TR 3945WX seemed the most cost effective way to get multiple PICE4.0 x16s for when I finally figure out how and why I would train models myself. I plan on selling my old RTX 4090 and pour that into some 3090's for the cheaper VRAM increase.\n\n\\---------------------------------------------------------------------------------------------------\n\n\\[unrelated to LLM's - also wondering about running a VM/LXC for PCVR with my RTX 5090 passed through and ditch the windows machine altogether? So if someone here has experience of VM/LXC linux &gt; Monado &gt; WiVRn versus Win11 &gt; Virtual Desktop performance for Quest3 simracing. Could you talk me through the benefit/downside of virtualising my PCVR rig so I could use the 5090 for VR AND LLMs. So eventually rather than fitting out multiple 3090's add another 5090 with the 4090+7800X3D+32GBDDR5+B650i mobo sale proceeds?",
          "author_fullname": "t2_if95iuzc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "480mm wide multi GPU frame - can only find 500+mm frames",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qa3ttf30tpcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/qa3ttf30tpcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53d3b00e8f63b48c9790997de4baf9df85d2adbb"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/qa3ttf30tpcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4452c2305b9bd0e32bcf1b3533b6a20957368b2"
                }
              ],
              "s": {
                "y": 220,
                "x": 220,
                "u": "https://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;format=pjpg&amp;auto=webp&amp;s=64aa05e469107d24131f92557562e0ae80c5f44a"
              },
              "id": "qa3ttf30tpcf1"
            }
          },
          "name": "t3_1lz5cwa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vcAl6hE0vLfYguTcveTyE8ouQSXIpZ8trLcegu0T6lk.jpg",
          "edited": 1752482948,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752444529,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Similiar in principle to the &amp;#39;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1g6ixae/6x_gpu_build_4x_rtx_3090_and_2x_mi60_epyc_7002/\"&gt;6x GPU Build. 4x RTX 3090 and 2x MI60. Epyc 7002&lt;/a&gt;&amp;#39; someone posted several months ago&lt;/p&gt;\n\n&lt;p&gt;But that thing is ginormous (dual PSU on either side of motherboard) so likely the 636mm wide version.&lt;/p&gt;\n\n&lt;p&gt;I have a 12U (19inch) rack I want to install this into. The total rack width is 520mm BUT with the T-nut mounting posts on either side the opening narrows down to approx 480mm.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found some frames but they are all 500mm wide. Which could &amp;#39;fit&amp;#39; inside the rack but then wont be able to slide in and out on the shelf I will mount it on (for easy maintenance or fiddling)&lt;/p&gt;\n\n&lt;p&gt;Can anyone point me to a 480mm wide dual layer frame that can slide into a 19inch rack shelf?&lt;/p&gt;\n\n&lt;p&gt;The only other solution I can think of is to hack a dual stack from two of these:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=64aa05e469107d24131f92557562e0ae80c5f44a\"&gt;https://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=64aa05e469107d24131f92557562e0ae80c5f44a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;PS this build is just for fun/learning/tinkering with LLMs (and also replace one of my ex-office-PC proxmox nodes, Has nothing to do with work/career/business so I have already gone massively over on the &amp;#39;just for fun&amp;#39;&amp;#39; budget by deciding on a Threadripper Pro 3945WX on a Gigabyte MC62-G40 board with 128GB RAM. And cant justify a more professional chassis solution&lt;/p&gt;\n\n&lt;p&gt;The MC62 &amp;amp; TR 3945WX seemed the most cost effective way to get multiple PICE4.0 x16s for when I finally figure out how and why I would train models myself. I plan on selling my old RTX 4090 and pour that into some 3090&amp;#39;s for the cheaper VRAM increase.&lt;/p&gt;\n\n&lt;p&gt;---------------------------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;[unrelated to LLM&amp;#39;s - also wondering about running a VM/LXC for PCVR with my RTX 5090 passed through and ditch the windows machine altogether? So if someone here has experience of VM/LXC linux &amp;gt; Monado &amp;gt; WiVRn versus Win11 &amp;gt; Virtual Desktop performance for Quest3 simracing. Could you talk me through the benefit/downside of virtualising my PCVR rig so I could use the 5090 for VR AND LLMs. So eventually rather than fitting out multiple 3090&amp;#39;s add another 5090 with the 4090+7800X3D+32GBDDR5+B650i mobo sale proceeds?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz5cwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "munkiemagik",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz5cwa/480mm_wide_multi_gpu_frame_can_only_find_500mm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz5cwa/480mm_wide_multi_gpu_frame_can_only_find_500mm/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752444529,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google has the on-device model Gemma 3 1B that I am using for my scam detection Android app. Google has instructions for RAG here - [https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android](https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android)\n\nBut that gets too slow for loading even 1000 chunks. Anybody knows how to compute the chunk embeddings offline, store it in sqlite and then load that into the Gemma 3 instead?",
          "author_fullname": "t2_1inf92eupz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Computing embeddings offline for Gemma 3 1B (on-device model)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz4sk3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752443051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google has the on-device model Gemma 3 1B that I am using for my scam detection Android app. Google has instructions for RAG here - &lt;a href=\"https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android\"&gt;https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But that gets too slow for loading even 1000 chunks. Anybody knows how to compute the chunk embeddings offline, store it in sqlite and then load that into the Gemma 3 instead?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?auto=webp&amp;s=b78731184d9920fa4900b6590e113d2772fa64ed",
                  "width": 1440,
                  "height": 900
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1cc13c1cb1062998d0e6a2cc88bc3272f2368f7",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1812be5c0e49c65e85787f4dbb2922a543943e79",
                    "width": 216,
                    "height": 135
                  },
                  {
                    "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca7983e470f1e5cbc5edcd5c5e1c7e5b70227953",
                    "width": 320,
                    "height": 200
                  },
                  {
                    "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=293ebb5606c7edf7f2570aa914eb4ddb55f1e615",
                    "width": 640,
                    "height": 400
                  },
                  {
                    "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1bd156ecd3df7024382f9e145cda17bcaf6bc79",
                    "width": 960,
                    "height": 600
                  },
                  {
                    "url": "https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3b1fd853b19889a23a601c33fae7d2323e8bdb0",
                    "width": 1080,
                    "height": 675
                  }
                ],
                "variants": {},
                "id": "iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz4sk3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Basic-Donut1740",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752443051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "📢 Paid Research Interview Opportunity for AI Agent Developers\n\nHi everyone – I’m Mingyao, a researcher from the University of Washington, conducting a study on how **individual AI agent developers handle privacy and security** when building autonomous systems using tools like LangChain, GPT, AutoGPT, etc.\n\n🧠 Why it matters:\nWe aim to uncover developers’ challenges and practices in privacy &amp; security so we can help shape **better design tools, standards, and workflows** that benefit the whole ecosystem — including builders and clients.\n\n💬 We’re conducting 30–60 minute 1:1 interviews via Zoom\n💵 $15/hour compensation\n👤 Looking for: **Solo or small team** developers who’ve built AI agents for real-world use\n📅 Flexible scheduling — just reply or email me!\n\n📧 Contact: mx37@uw.edu / yutingy@umich.edu \n\nhttp://linkedin.com/in/mingyao-xu-bb8b46297\n\nYour insights will directly help improve tools that developers like you use every day. I’ll be happy to share key findings with the group if there’s interest!\n\nThanks and excited to connect 🙌 ",
          "author_fullname": "t2_1j0s2bg4mw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "📢 [Paid Study] Interviewing Individual AI Agent Developers – Share Your Experience + $15/hr",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz4f51",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752442087,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;📢 Paid Research Interview Opportunity for AI Agent Developers&lt;/p&gt;\n\n&lt;p&gt;Hi everyone – I’m Mingyao, a researcher from the University of Washington, conducting a study on how &lt;strong&gt;individual AI agent developers handle privacy and security&lt;/strong&gt; when building autonomous systems using tools like LangChain, GPT, AutoGPT, etc.&lt;/p&gt;\n\n&lt;p&gt;🧠 Why it matters:\nWe aim to uncover developers’ challenges and practices in privacy &amp;amp; security so we can help shape &lt;strong&gt;better design tools, standards, and workflows&lt;/strong&gt; that benefit the whole ecosystem — including builders and clients.&lt;/p&gt;\n\n&lt;p&gt;💬 We’re conducting 30–60 minute 1:1 interviews via Zoom\n💵 $15/hour compensation\n👤 Looking for: &lt;strong&gt;Solo or small team&lt;/strong&gt; developers who’ve built AI agents for real-world use\n📅 Flexible scheduling — just reply or email me!&lt;/p&gt;\n\n&lt;p&gt;📧 Contact: &lt;a href=\"mailto:mx37@uw.edu\"&gt;mx37@uw.edu&lt;/a&gt; / &lt;a href=\"mailto:yutingy@umich.edu\"&gt;yutingy@umich.edu&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://linkedin.com/in/mingyao-xu-bb8b46297\"&gt;http://linkedin.com/in/mingyao-xu-bb8b46297&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Your insights will directly help improve tools that developers like you use every day. I’ll be happy to share key findings with the group if there’s interest!&lt;/p&gt;\n\n&lt;p&gt;Thanks and excited to connect 🙌 &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lz4f51",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TalkComfortable9144",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz4f51/paid_study_interviewing_individual_ai_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz4f51/paid_study_interviewing_individual_ai_agent/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752442087,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Now use MCP in [Kimi.com](http://Kimi.com) :)  \nLogin into the Kimi for experience and file support, without login file support is not available.\n\nSupport added in the version v0.5.3  \n  \nAdded Settings panel for custom delays for auto execute, auto submit, and auto insert.  \nImproved system prompt for better performance.\n\nMCP SuperAssistant adds support for mcp in ChatGPT, Google Gemini, Perplexity, Grok, Google AI Studio, OpenRouter Chat, DeepSeek, T3 Chat, GitHub Copilot, Mistral AI, Kimi  \n  \nChrome extension version updated to 0.5.3  \nChrome: [https://chromewebstore.google.com/detail/mcp-superassistant/kngiafgkdnlkgmefdafaibkibegkcaef?hl=en](https://chromewebstore.google.com/detail/mcp-superassistant/kngiafgkdnlkgmefdafaibkibegkcaef?hl=en)  \nFirefox: [https://addons.mozilla.org/en-US/firefox/addon/mcp-superassistant/](https://addons.mozilla.org/en-US/firefox/addon/mcp-superassistant/)  \nGithub: [https://github.com/srbhptl39/MCP-SuperAssistant](https://github.com/srbhptl39/MCP-SuperAssistant)  \nWebsite: [https://mcpsuperassistant.ai](https://mcpsuperassistant.ai)  \n  \nPeace Out✌🏻  \n  \n",
          "author_fullname": "t2_2lzgda1k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Added MCP Support to Kimi.com via MCP SuperAssistant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz3n8n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/jb41717pfpcf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1840,
              "scrubber_media_url": "https://v.redd.it/jb41717pfpcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/jb41717pfpcf1/DASHPlaylist.mpd?a=1755113126%2CYWQ0ODQwMmExNzY1ODBkNzc0MTVhZmM4ZGJmZTllODYyNTU0OWM2ZTFlMzc2YzM3ODg2YTk0NDM5ZjAxOTg1Yw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 89,
              "hls_url": "https://v.redd.it/jb41717pfpcf1/HLSPlaylist.m3u8?a=1755113126%2CMmE2NWJmZjk1NDM0MjIyYTYxZTQwMTBhZDU1ZDMxMDA3OTNmNmE5NjkwZDg5ODhkYTIxOGI1NjVmZjAyNDg3MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=140&amp;height=82&amp;crop=140:82,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ebd0b674f09b8b19e1fb494f57895e3b70702e7e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752440156,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now use MCP in &lt;a href=\"http://Kimi.com\"&gt;Kimi.com&lt;/a&gt; :)&lt;br/&gt;\nLogin into the Kimi for experience and file support, without login file support is not available.&lt;/p&gt;\n\n&lt;p&gt;Support added in the version v0.5.3  &lt;/p&gt;\n\n&lt;p&gt;Added Settings panel for custom delays for auto execute, auto submit, and auto insert.&lt;br/&gt;\nImproved system prompt for better performance.&lt;/p&gt;\n\n&lt;p&gt;MCP SuperAssistant adds support for mcp in ChatGPT, Google Gemini, Perplexity, Grok, Google AI Studio, OpenRouter Chat, DeepSeek, T3 Chat, GitHub Copilot, Mistral AI, Kimi  &lt;/p&gt;\n\n&lt;p&gt;Chrome extension version updated to 0.5.3&lt;br/&gt;\nChrome: &lt;a href=\"https://chromewebstore.google.com/detail/mcp-superassistant/kngiafgkdnlkgmefdafaibkibegkcaef?hl=en\"&gt;https://chromewebstore.google.com/detail/mcp-superassistant/kngiafgkdnlkgmefdafaibkibegkcaef?hl=en&lt;/a&gt;&lt;br/&gt;\nFirefox: &lt;a href=\"https://addons.mozilla.org/en-US/firefox/addon/mcp-superassistant/\"&gt;https://addons.mozilla.org/en-US/firefox/addon/mcp-superassistant/&lt;/a&gt;&lt;br/&gt;\nGithub: &lt;a href=\"https://github.com/srbhptl39/MCP-SuperAssistant\"&gt;https://github.com/srbhptl39/MCP-SuperAssistant&lt;/a&gt;&lt;br/&gt;\nWebsite: &lt;a href=\"https://mcpsuperassistant.ai\"&gt;https://mcpsuperassistant.ai&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Peace Out✌🏻  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/jb41717pfpcf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?format=pjpg&amp;auto=webp&amp;s=6d1dfcac71ebe96cac82686e96fdc5f68b8dbc43",
                  "width": 1840,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=abf24aff4cb544a6fbc7cd15d2c121b4326c9974",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f7032512d44ddfdcc08d86acbeb6f407018fb1bf",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a6ba61545d19cc08c3673d1a22f94af9379716bf",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98960cea0cffdca9cf42d79f29bfa5d599591615",
                    "width": 640,
                    "height": 375
                  },
                  {
                    "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d6941cc9e5719ce8a4ea38840e400c4c1cef1bc2",
                    "width": 960,
                    "height": 563
                  },
                  {
                    "url": "https://external-preview.redd.it/N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a1d0e9c49b7836a5b8903eadf6e18239a3678bb3",
                    "width": 1080,
                    "height": 633
                  }
                ],
                "variants": {},
                "id": "N284YTEwN3BmcGNmMXWHl8ryF0G-UJ9iQeTnkvyOl6nNy-YVT3WaTg4-v78Y"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lz3n8n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EfficientApartment52",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz3n8n/added_mcp_support_to_kimicom_via_mcp/",
          "stickied": false,
          "url": "https://v.redd.it/jb41717pfpcf1",
          "subreddit_subscribers": 498850,
          "created_utc": 1752440156,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/jb41717pfpcf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1840,
              "scrubber_media_url": "https://v.redd.it/jb41717pfpcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/jb41717pfpcf1/DASHPlaylist.mpd?a=1755113126%2CYWQ0ODQwMmExNzY1ODBkNzc0MTVhZmM4ZGJmZTllODYyNTU0OWM2ZTFlMzc2YzM3ODg2YTk0NDM5ZjAxOTg1Yw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 89,
              "hls_url": "https://v.redd.it/jb41717pfpcf1/HLSPlaylist.m3u8?a=1755113126%2CMmE2NWJmZjk1NDM0MjIyYTYxZTQwMTBhZDU1ZDMxMDA3OTNmNmE5NjkwZDg5ODhkYTIxOGI1NjVmZjAyNDg3MQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I've tried using different (free) ways to parse academic pdfs \\*locally\\*, so I can get the author's name, publication year, and abbreviated title. The two approaches are:\n\n(1) GROBID (lightweight)\n\n(2) PyPDF2 + pytesseract + pdf2image\n\nNeither of them are great, with success rate of around 60% (full correctness). Any other approaches out there worth a go?\n\n",
          "author_fullname": "t2_bqf3y48i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local free PDF parser for academic pdfs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz2zt2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752438557,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve tried using different (free) ways to parse academic pdfs *locally*, so I can get the author&amp;#39;s name, publication year, and abbreviated title. The two approaches are:&lt;/p&gt;\n\n&lt;p&gt;(1) GROBID (lightweight)&lt;/p&gt;\n\n&lt;p&gt;(2) PyPDF2 + pytesseract + pdf2image&lt;/p&gt;\n\n&lt;p&gt;Neither of them are great, with success rate of around 60% (full correctness). Any other approaches out there worth a go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz2zt2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Objective_Science965",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz2zt2/local_free_pdf_parser_for_academic_pdfs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz2zt2/local_free_pdf_parser_for_academic_pdfs/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752438557,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do you know if we can use the api key of kimi k2 in a cli like Claude code ?",
          "author_fullname": "t2_1r4xvzcirf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi k2 on cli ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz2i5h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752437377,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you know if we can use the api key of kimi k2 in a cli like Claude code ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz2i5h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Equivalent-Fig1588",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz2i5h/kimi_k2_on_cli/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz2i5h/kimi_k2_on_cli/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752437377,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "HI there guys, hoping you're doing fine.\n\nAs always related to PPL benchmarks, take them with a grain of salt as it may not represent the quality of the model itself, but it may help as a guide at how much a model could get affected by quantization.\n\nAs it has been mentioned sometimes, and a bit of spoiler, quantization on DeepSeek models is pretty impressive, because either quantization methods nowadays are really good and/or DeepSeek being natively FP8, it changes the paradigm a bit.\n\nAlso many thanks to ubergarm (u/VoidAlchemy) for his data on his quants and Q8\\_0/FP8 baseline!\n\nFor the quants that aren't from him, I did run them with the same command he did, with wiki.text.raw:\n\n    ./llama-perplexity -m 'model_name.gguf' \\\n    -c 512 --no-mmap -ngl 999 \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA0\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA1\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA2\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA3\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA4\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA5\" \\\n    -ot \"blk.(layers_depending_on_model).ffn.=CUDA6\" \\\n    -ot exps=CPU \\\n    -fa -mg 0 -mla 3 -amb 256 -fmoe \\\n    -f wiki.test.raw\n\n\\--------------------------\n\nFor baselines, we have this data:\n\n* DeepSeek R1 0528 Q8: 3.2119\n* DeepSeek V3 0324 Q8 and q8\\_cache (important\\*): 3.2454\n* DeepSeek V3 0324 Q8 and F16 cache extrapolated\\*: 3.2443\n\n\\*Based on [https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241), on R1 0528 at Q8\\_0, the difference between F16 and Q8\\_0 cache is:\n\n* `-ctk fp16` `3.2119 +/- 0.01697`\n* `-ctk q8_0` `3.2130 +/- 0.01698`\n\nSo then, F16 cache is 0.03% better than Q8\\_0 for this model. Extrapolating that to V3, then V3 0324 Q8 at F16 should have 3.2443 PPL.\n\nQuants tested for R1 0528:\n\n* IQ1\\_S\\_R4 (ubergarm)\n* UD-TQ1\\_0\n* IQ2\\_KT (ubergarm)\n* IQ2\\_K\\_R4 (ubergarm)\n* Q2\\_K\\_XL\n* IQ3\\_XXS\n* IQ3\\_KS (ubergarm, my bad here as I named it IQ3\\_KT)\n* Q3\\_K\\_XL\n* IQ3\\_K\\_R4 (ubergarm)\n* IQ4\\_XS\n* q4\\_0 (pure)\n* IQ4\\_KS\\_R4 (ubergarm)\n* Q8\\_0 (ubergarm)\n\nQuants tested for V3 0324:\n\n* Q1\\_S\\_R4 (ubergarm)\n* IQ2\\_K\\_R4 (ubergarm)\n* Q2\\_K\\_XL\n* IQ3\\_XXS\n* Q3\\_K\\_XL\n* IQ3\\_K\\_R4 (ubergarm)\n* IQ3\\_K\\_R4\\_Pure (ubergarm)\n* IQ4\\_XS\n* IQ4\\_K\\_R4 (ubergarm)\n* Q8\\_0 (ubergarm)\n\nSo here we go:\n\n# DeepSeek R1 0528\n\n[R1 0528 comparison \\(IQ3\\_KT is IQ3\\_KS, my bad\\)](https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;format=png&amp;auto=webp&amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb)\n\nAs can you see, near 3.3bpw and above it gets quite good!. So now using different baselines to compare, using 100% for Q2\\_K\\_XL, Q3\\_K\\_XL, IQ4\\_XS and Q8\\_0.\n\n[R1 0528 Q2\\_K\\_XL](https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542)\n\n[R1 0528 Q3\\_K\\_XL](https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0)\n\n[R1 0528 IQ4\\_XS](https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=1723d977f7c034496eb7a95bed576b6b53572542)\n\n[R1 0528 Q8\\_0](https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a)\n\nSo with a table format, it looks like this (ordered by best to worse PPL)\n\n|Model|Size (GB)|BPW|PPL|\n|:-|:-|:-|:-|\n|Q8\\_0|665.3|8.000|3.2119|\n|IQ4\\_KS\\_R4|367.8|4.701|3.2286|\n|IQ4\\_XS|333.1|4.260|3.2598|\n|q4\\_0|352.6|4.508|3.2895|\n|IQ3\\_K\\_R4|300.9|3.847|3.2730|\n|IQ3\\_KT|272.5|3.483|3.3056|\n|Q3\\_K\\_XL|275.6|3.520|3.3324|\n|IQ3\\_XXS|254.2|3.250|3.3805|\n|IQ2\\_K\\_R4|220.0|2.799|3.5069|\n|Q2\\_K\\_XL|233.9|2.990|3.6062|\n|IQ2\\_KT|196.7|2.514|3.6378|\n|UD-TQ1\\_0|150.8|1.927|4.7567|\n|IQ1\\_S\\_R4|130.2|1.664|4.8805|\n\n# DeepSeek V3 0324\n\n[V3 0324 Comparison](https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;format=png&amp;auto=webp&amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81)\n\nHere Q2\\_K\\_XL performs really good, even better than R1 Q2\\_K\\_XL. Reason is unkown for now. ALso, IQ3\\_XXS is not here as it failed the test with nan, also unkown.\n\n[V3 0324 Q2\\_K\\_XL](https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357)\n\n[V3 0324 Q3\\_K\\_XL](https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=5389b135a13c86ff471d38540909a7586e2282ff)\n\n[V3 0324 IQ4\\_XS](https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9)\n\n[V3 0324 Q8\\_0](https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea)\n\nSo with a table format, from best to lower PPL:\n\n|Model|Size (GB)|BPW|PPL|\n|:-|:-|:-|:-|\n|Q8\\_0|665.3|8.000|3.2454|\n|IQ4\\_K\\_R4|386.2|4.936|3.2596|\n|IQ4\\_XS|333.1|4.260|3.2598|\n|IQ3\\_K\\_R4\\_Pure|352.5|4.505|3.2942|\n|IQ3\\_K\\_R4|324.0|4.141|3.3193|\n|Q3\\_K\\_XL|281.5|3.600|3.3690|\n|Q2\\_K\\_XL|233.9|2.990|3.5264|\n|IQ2\\_K\\_R4|226.0|2.889|3.5614|\n|IQ1\\_S\\_R4|130.2|1.664|5.1292|\n|IQ3\\_XXS|254.2|3.250|NaN (failed)|\n\n\\-----------------------------------------\n\nFinally, a small comparison between R1 0528 and V3 0324\n\nhttps://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;format=png&amp;auto=webp&amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779\n\n\\-------------------------------------\n\nSo that's all! Again, PPL is not in a indicator of everything, so take everything with a grain of salt.",
          "author_fullname": "t2_j1kqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Some small PPL benchmarks on DeepSeek R1 0528 quants, from Unlosh and ubergarm, from 1.6bpw (1Q_S_R4) to 4.7bpw (IQ4_KS_R4) (and Q8/FP8 baseline). Also a few V3 0324 ones.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8oart9461pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=615a76c2eb11dbcbba71bcde5e8b14a9b26955e3"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1c9139d1b2905964e6745bf2fd14259b9a426ef3"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=936018d1934011acf9e5a0fc6e5d1e12d392b3c9"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4aa90341e104747b8a053980c3ab9602ade859c"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bb633b4c9c3893c7f7c42033f9e4f89a812c598"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/8oart9461pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83fc06d5dd5479b8d4a44ba237e86a2bd273256b"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=1723d977f7c034496eb7a95bed576b6b53572542"
              },
              "id": "8oart9461pcf1"
            },
            "7rmqc55d2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87ab3d79391658c24f8eb175eb96cd85e9a00da6"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=956f56f00765d9ad2c0793039e5de22c7092e14f"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc7798ea477051b35e7007cb17e3c41394c51212"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b29a30d6cb56832e224a01ba1c5b7550e96bfc2"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12171abe12ae972d53b8deabb974efd754c7e52c"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=180b1fd5de5280cacff280251987f6c88c055812"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=5389b135a13c86ff471d38540909a7586e2282ff"
              },
              "id": "7rmqc55d2pcf1"
            },
            "l1nuh3r22pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 77,
                  "x": 108,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b9568c5d650c2cd7c0446713e1cea84ab15e770"
                },
                {
                  "y": 155,
                  "x": 216,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecec583e68c2f6b061401ecfc9ae823d8ccf9330"
                },
                {
                  "y": 230,
                  "x": 320,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=47ab80310d3fcdbbc3296ef62140f3a2a623564e"
                },
                {
                  "y": 460,
                  "x": 640,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f58a9f33959d704d9383bed69b04b4d8823d3bfc"
                },
                {
                  "y": 690,
                  "x": 960,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55ea27ccead37bc0748ca76af9beb6cef79b2bd6"
                },
                {
                  "y": 777,
                  "x": 1080,
                  "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0184ecd6a22d35f76c80d17a07a20fbd976c79a2"
                }
              ],
              "s": {
                "y": 2979,
                "x": 4139,
                "u": "https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;format=png&amp;auto=webp&amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81"
              },
              "id": "l1nuh3r22pcf1"
            },
            "teu0yiof2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9573603bf90a5ff2ef935a4a813d1bb8c111ff24"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ac9b2792909e2e52b968b2584ad6829dd49793a"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba5ecaf0e1bf7a286da0f5d65f0099deaf5577f0"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=33b414c7be5afc38a8a4bdc20cecaf50a65b7b13"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=396b89d26216bb1c9604ce42988c34519ad44938"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=936016288947f75add8ea488474c6ccf0b60468b"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea"
              },
              "id": "teu0yiof2pcf1"
            },
            "i5tb2cx41pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ccd1eb06c1618b728e8a0604331877f92d9acf4"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6a3f76ae75b7812bf935d8472fe5c847bf221fc"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=71f6875de4d3c999cba1ed1e72a875f02940c517"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f502525fe41dc6b42397b8d9c585cbee2ac18028"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b248d089bf7ed3769b453c8696436b9fdaaf8458"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=836822fc69fbd55912f352f05dc701c9bbfef2f9"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0"
              },
              "id": "i5tb2cx41pcf1"
            },
            "dszt1qw71pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf982e76cfe91939c03204063797f16a8434b727"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c56734e22cf419ace70839ef50bfe69d9b523e8e"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9ca8ee8ae47c017f473ea4c5b681827b319b10b"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=31b498d68bb4c16bc4fb62e800f5feea76bd531b"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=45e796875cf5c42de8161e1c2ae74e8ef6daedc0"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b1ec5abb73f5e058697589587ac177114c9f33c"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a"
              },
              "id": "dszt1qw71pcf1"
            },
            "tfu0yvn21pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2afda21ba96b06391f3628938bdc7dea4a34001f"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d364400f8233f61d0041646a762803b428fe8388"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3004c72b817e4b50b842bf597633c3ad280437de"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f78198a60a934140599f55840257d46e1081ff3c"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d64fc5371534bbf7ffbf4c9e22abf3c2d855108"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77d97017393854e480b59a604456ed04cddcc741"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542"
              },
              "id": "tfu0yvn21pcf1"
            },
            "s50qgpnr2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 77,
                  "x": 108,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=feb259d1b93d378f1fc631775da3c556d54ab2c3"
                },
                {
                  "y": 154,
                  "x": 216,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c1c022151fbf8eee7c87904451af64a5f70873e"
                },
                {
                  "y": 228,
                  "x": 320,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=49532cb0b7328f5418bffbe6e1746b5627742b4c"
                },
                {
                  "y": 457,
                  "x": 640,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=68d831e0f1ae10968624015e06e0ddb82149726d"
                },
                {
                  "y": 685,
                  "x": 960,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1562dbd9c9e944eeac38e77d6f927fd93a9e32f"
                },
                {
                  "y": 771,
                  "x": 1080,
                  "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d504b01aba8ee14725e22e75fc13ff875e387897"
                }
              ],
              "s": {
                "y": 2975,
                "x": 4164,
                "u": "https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;format=png&amp;auto=webp&amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779"
              },
              "id": "s50qgpnr2pcf1"
            },
            "6bheilba2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1569b2dae00bec9751f2e7d88e8f29915d3048cd"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a602548870ac5cf6308a610bb48aa701c3604ed0"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=195d11ff9a75f766e7e8af3ad94c6d80f27f23f7"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=171355dcfe1b8e13e22e52b9273acefdcb621c9d"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=052d6b680a3a36129025d576418b9a3a6dfaa16f"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/6bheilba2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7342a6d1ff58f621faccfdaba720a1dbc94ff78"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357"
              },
              "id": "6bheilba2pcf1"
            },
            "ioqbx5iv0pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 77,
                  "x": 108,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88177c33bbced92cd375a627a732dcdb99ada4c3"
                },
                {
                  "y": 155,
                  "x": 216,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=afcadfe2e58344c29a65d3f25c56f50c21f82a74"
                },
                {
                  "y": 230,
                  "x": 320,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6228278b0a3825f057c1380f6faf30cc8e6dce2c"
                },
                {
                  "y": 461,
                  "x": 640,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbf3294de2c4b4f094213870c79018afa953b1e0"
                },
                {
                  "y": 691,
                  "x": 960,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=98e2a31b9a2f976126b5ed3721512a573c83ef24"
                },
                {
                  "y": 778,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e0f867696a8e177b4b8751cf2500d16e53e57b6"
                }
              ],
              "s": {
                "y": 2979,
                "x": 4135,
                "u": "https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;format=png&amp;auto=webp&amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb"
              },
              "id": "ioqbx5iv0pcf1"
            },
            "yih3wq9e2pcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7ae430a69347c77320a2f651f4376e13371e0b7"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2c8a06a57a5cb3e3792d7a1866f9916c96b4518"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e51ea3017eca682a4c9a3d64a5210b8c7f183c3b"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=43e84a68887262c6667e3431a3f70d4306376485"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad6f4ac874038b2e6b39a2dce2002d94354d9478"
                },
                {
                  "y": 716,
                  "x": 1080,
                  "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28096bc4f5f8b1ee053a97b51c9047e079854d30"
                }
              ],
              "s": {
                "y": 2364,
                "x": 3565,
                "u": "https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;format=png&amp;auto=webp&amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9"
              },
              "id": "yih3wq9e2pcf1"
            }
          },
          "name": "t3_1lz1s8x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8688e9a81e04dfb9541beabf3f5c89f9d553f07b",
          "edited": 1752441951,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752435641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HI there guys, hoping you&amp;#39;re doing fine.&lt;/p&gt;\n\n&lt;p&gt;As always related to PPL benchmarks, take them with a grain of salt as it may not represent the quality of the model itself, but it may help as a guide at how much a model could get affected by quantization.&lt;/p&gt;\n\n&lt;p&gt;As it has been mentioned sometimes, and a bit of spoiler, quantization on DeepSeek models is pretty impressive, because either quantization methods nowadays are really good and/or DeepSeek being natively FP8, it changes the paradigm a bit.&lt;/p&gt;\n\n&lt;p&gt;Also many thanks to ubergarm (&lt;a href=\"/u/VoidAlchemy\"&gt;u/VoidAlchemy&lt;/a&gt;) for his data on his quants and Q8_0/FP8 baseline!&lt;/p&gt;\n\n&lt;p&gt;For the quants that aren&amp;#39;t from him, I did run them with the same command he did, with wiki.text.raw:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-perplexity -m &amp;#39;model_name.gguf&amp;#39; \\\n-c 512 --no-mmap -ngl 999 \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA0&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA1&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA2&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA3&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA4&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA5&amp;quot; \\\n-ot &amp;quot;blk.(layers_depending_on_model).ffn.=CUDA6&amp;quot; \\\n-ot exps=CPU \\\n-fa -mg 0 -mla 3 -amb 256 -fmoe \\\n-f wiki.test.raw\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;--------------------------&lt;/p&gt;\n\n&lt;p&gt;For baselines, we have this data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DeepSeek R1 0528 Q8: 3.2119&lt;/li&gt;\n&lt;li&gt;DeepSeek V3 0324 Q8 and q8_cache (important*): 3.2454&lt;/li&gt;\n&lt;li&gt;DeepSeek V3 0324 Q8 and F16 cache extrapolated*: 3.2443&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;*Based on &lt;a href=\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241\"&gt;https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/discussions/2#686fdceb17516435632a4241&lt;/a&gt;, on R1 0528 at Q8_0, the difference between F16 and Q8_0 cache is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;-ctk fp16&lt;/code&gt; &lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;-ctk q8_0&lt;/code&gt; &lt;code&gt;3.2130 +/- 0.01698&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So then, F16 cache is 0.03% better than Q8_0 for this model. Extrapolating that to V3, then V3 0324 Q8 at F16 should have 3.2443 PPL.&lt;/p&gt;\n\n&lt;p&gt;Quants tested for R1 0528:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;IQ1_S_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;UD-TQ1_0&lt;/li&gt;\n&lt;li&gt;IQ2_KT (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q2_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_XXS&lt;/li&gt;\n&lt;li&gt;IQ3_KS (ubergarm, my bad here as I named it IQ3_KT)&lt;/li&gt;\n&lt;li&gt;Q3_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ4_XS&lt;/li&gt;\n&lt;li&gt;q4_0 (pure)&lt;/li&gt;\n&lt;li&gt;IQ4_KS_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Quants tested for V3 0324:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Q1_S_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ2_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q2_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_XXS&lt;/li&gt;\n&lt;li&gt;Q3_K_XL&lt;/li&gt;\n&lt;li&gt;IQ3_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ3_K_R4_Pure (ubergarm)&lt;/li&gt;\n&lt;li&gt;IQ4_XS&lt;/li&gt;\n&lt;li&gt;IQ4_K_R4 (ubergarm)&lt;/li&gt;\n&lt;li&gt;Q8_0 (ubergarm)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So here we go:&lt;/p&gt;\n\n&lt;h1&gt;DeepSeek R1 0528&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ioqbx5iv0pcf1.png?width=4135&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f1a3feb6e2143aaa739d1c4d61d45df80494abb\"&gt;R1 0528 comparison (IQ3_KT is IQ3_KS, my bad)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As can you see, near 3.3bpw and above it gets quite good!. So now using different baselines to compare, using 100% for Q2_K_XL, Q3_K_XL, IQ4_XS and Q8_0.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tfu0yvn21pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f2b75d15eecfd49481db1a066b04fb57f5ac3542\"&gt;R1 0528 Q2_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i5tb2cx41pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02a12f2c12b6ef657397b60fc8e87d022bc6c5b0\"&gt;R1 0528 Q3_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8oart9461pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1723d977f7c034496eb7a95bed576b6b53572542\"&gt;R1 0528 IQ4_XS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dszt1qw71pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a77fc375c2e197346034a962fdff96ddea5ac49a\"&gt;R1 0528 Q8_0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So with a table format, it looks like this (ordered by best to worse PPL)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size (GB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;BPW&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;665.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.000&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2119&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_KS_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;367.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.701&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2286&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_XS&lt;/td&gt;\n&lt;td align=\"left\"&gt;333.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.260&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2598&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;352.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.508&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2895&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;300.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.847&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2730&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_KT&lt;/td&gt;\n&lt;td align=\"left\"&gt;272.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.483&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3056&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q3_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;275.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.520&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3324&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_XXS&lt;/td&gt;\n&lt;td align=\"left\"&gt;254.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.250&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3805&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ2_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;220.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.799&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5069&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q2_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;233.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.990&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.6062&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ2_KT&lt;/td&gt;\n&lt;td align=\"left\"&gt;196.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.514&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.6378&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UD-TQ1_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;150.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.927&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.7567&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ1_S_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;130.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.664&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.8805&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;DeepSeek V3 0324&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/l1nuh3r22pcf1.png?width=4139&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16bd4c33d941c65b4fa439bf621e0e7f69195f81\"&gt;V3 0324 Comparison&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here Q2_K_XL performs really good, even better than R1 Q2_K_XL. Reason is unkown for now. ALso, IQ3_XXS is not here as it failed the test with nan, also unkown.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6bheilba2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e278431b88fa49e69f8e32bd2bf881fd7e57357\"&gt;V3 0324 Q2_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7rmqc55d2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5389b135a13c86ff471d38540909a7586e2282ff\"&gt;V3 0324 Q3_K_XL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yih3wq9e2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23fdbeaec51b4e226da035042bfcf80da5a5f4e9\"&gt;V3 0324 IQ4_XS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/teu0yiof2pcf1.png?width=3565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e69256c7c5d098956ed1063c4bdb029aa9631ea\"&gt;V3 0324 Q8_0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So with a table format, from best to lower PPL:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size (GB)&lt;/th&gt;\n&lt;th align=\"left\"&gt;BPW&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;665.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.000&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2454&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;386.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.936&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2596&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ4_XS&lt;/td&gt;\n&lt;td align=\"left\"&gt;333.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.260&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2598&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_K_R4_Pure&lt;/td&gt;\n&lt;td align=\"left\"&gt;352.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.505&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.2942&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;324.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.141&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3193&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q3_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;281.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.600&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.3690&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Q2_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;233.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.990&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5264&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ2_K_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;226.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.889&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.5614&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ1_S_R4&lt;/td&gt;\n&lt;td align=\"left\"&gt;130.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.664&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.1292&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;IQ3_XXS&lt;/td&gt;\n&lt;td align=\"left\"&gt;254.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.250&lt;/td&gt;\n&lt;td align=\"left\"&gt;NaN (failed)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;-----------------------------------------&lt;/p&gt;\n\n&lt;p&gt;Finally, a small comparison between R1 0528 and V3 0324&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779\"&gt;https://preview.redd.it/s50qgpnr2pcf1.png?width=4164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bf3e1a6544913d76462b6486b76ad570c6eb779&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;-------------------------------------&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s all! Again, PPL is not in a indicator of everything, so take everything with a grain of salt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?auto=webp&amp;s=8216ca1903562c8f1a410d204c99985dbb7b3108",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=916df2f54c0cd22cd11ed7809b3c140706edfcee",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e1dbcdca4fa3782c38fd3d119431d344a540668",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7924ff592a988d2740e535ed06a0e5c6b9fe1ae5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34bb099dc9d99a1928035eecc7f6474b2da46ba7",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bdda7d98fc08d38df0a76903dbc3720bcf1ae1d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfaf0e47961251e127029f745bd8fda90457859c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "7ISepN1ZhP4X7ew10cBPIuuqsS75KZVYV_G0DmVulbM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lz1s8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panchovix",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752435641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The way llm agents are going, everything is going to be rebuilt for them. \n",
          "author_fullname": "t2_1j530lromt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We're all context for llms",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz1rv1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.22,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752435615,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The way llm agents are going, everything is going to be rebuilt for them. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lz1rv1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Proud-Victory2562",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz1rv1/were_all_context_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz1rv1/were_all_context_for_llms/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752435615,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "HI folks, when I put a simple markdown (.md) file in the local docs folder (it as full permissions) it tries to embed, but never moves off 0% -- im not sure if something is broke or im doing something wrong -- can anyone help?",
          "author_fullname": "t2_1st7s293ne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Problems with LocalDocs on GPT4All",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz1fjz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752434793,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HI folks, when I put a simple markdown (.md) file in the local docs folder (it as full permissions) it tries to embed, but never moves off 0% -- im not sure if something is broke or im doing something wrong -- can anyone help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz1fjz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "slrg1968",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz1fjz/problems_with_localdocs_on_gpt4all/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz1fjz/problems_with_localdocs_on_gpt4all/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752434793,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, \n\nHere is a question that has been in my head for some time. Would it be possible to lighten an LLM by removing content? \n\nI know it's a question that for someone really knowledgeable will be crazy and stupid. \n\nThe idea would be, if possible, to remove information that is not relevant to the user on a topic. \n\nLet's give an example: let's say we have a 3B model of parameters that needs 10 gigabytes of VRAM and only a graph of 8 gigabytes of VRAM. We could refine the model or distill it to remove information, for example, from sports and the final result would be 2.7 B of parameters. It is a theoretical question and not a real case, the numbers are invented. \n\n\nBasically, see if there is a technique that allows you to reduce the size of a model (not quantize) by removing content not necessary for its use and thus improving its performance (less size, more layers in GPU) T\n\nhank you very much and a little patience for those of us who ask stupid questions. \n\nThanks a lot. \n\nGreetings.",
          "author_fullname": "t2_1mbsf8cel3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Madness, the ignorant's question. Would it be possible to lighten an LLM model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz17w8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752434266,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;Here is a question that has been in my head for some time. Would it be possible to lighten an LLM by removing content? &lt;/p&gt;\n\n&lt;p&gt;I know it&amp;#39;s a question that for someone really knowledgeable will be crazy and stupid. &lt;/p&gt;\n\n&lt;p&gt;The idea would be, if possible, to remove information that is not relevant to the user on a topic. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s give an example: let&amp;#39;s say we have a 3B model of parameters that needs 10 gigabytes of VRAM and only a graph of 8 gigabytes of VRAM. We could refine the model or distill it to remove information, for example, from sports and the final result would be 2.7 B of parameters. It is a theoretical question and not a real case, the numbers are invented. &lt;/p&gt;\n\n&lt;p&gt;Basically, see if there is a technique that allows you to reduce the size of a model (not quantize) by removing content not necessary for its use and thus improving its performance (less size, more layers in GPU) T&lt;/p&gt;\n\n&lt;p&gt;hank you very much and a little patience for those of us who ask stupid questions. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot. &lt;/p&gt;\n\n&lt;p&gt;Greetings.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz17w8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Macestudios32",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752434266,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,  \nI’m working on building a search engine for a retail platform with a product catalog that includes things like title, description, size, color, and categories (e.g., “men’s clothing &gt; shirts” or “women’s shoes”).\n\nI'm still new to search, embeddings, and reranking, and I’ve got a bunch of questions. Would really appreciate any feedback or direction!\n\n**1. BM25 preprocessing:**  \nFor the BM25 part, I’m wondering what’s the right preprocessing pipeline. Should I:\n\n* Lowercase everything?\n* Normalize Turkish characters like \"ç\" to \"c\", \"ş\" to \"s\"?\n* Do stemming or lemmatization?\n* Only keep keywords?\n\nAny tips or open-source Turkish tokenizers that actually work well?\n\n**2. Embedding inputs:**  \nWhen embedding products (using models like GPT or other multilingual LLMs), I usually feed them like this:\n\n    product title: ...  \n    product description: ...  \n    color: ...  \n    size: ...\n    \n\nI read somewhere (even here) that these key-value labels (\"product title:\", etc.) might not help and could even hurt  that LLM-based models can infer structure without them. Is that really true? Is there another sota way to do it?\n\nAlso, should I normalize Turkish characters here too, or just leave them as-is?\n\n**3. Reranking:**  \nI tried ColBERT but wasn’t impressed. I had much better results with Qwen-Reranker-4B, but it’s too slow when I’m comparing query to even 25 products. Are there any smaller/faster rerankers that still perform decently for Turkish/multilingual content and can bu used it production? ColBERT is fast because of it's architecture but Reranker much reliable but slower :/\n\nAny advice, practical tips, or general pointers are more than welcome! Especially curious about how people handle multilingual search pipelines (Turkish in my case) and what preprocessing tricks really matter in practice.\n\nThanks in advance 🙏",
          "author_fullname": "t2_r3c0w369",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need advice on search pipeline for retail products (BM25 + embeddings + reranking)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz0hk3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752432488,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\nI’m working on building a search engine for a retail platform with a product catalog that includes things like title, description, size, color, and categories (e.g., “men’s clothing &amp;gt; shirts” or “women’s shoes”).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still new to search, embeddings, and reranking, and I’ve got a bunch of questions. Would really appreciate any feedback or direction!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. BM25 preprocessing:&lt;/strong&gt;&lt;br/&gt;\nFor the BM25 part, I’m wondering what’s the right preprocessing pipeline. Should I:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Lowercase everything?&lt;/li&gt;\n&lt;li&gt;Normalize Turkish characters like &amp;quot;ç&amp;quot; to &amp;quot;c&amp;quot;, &amp;quot;ş&amp;quot; to &amp;quot;s&amp;quot;?&lt;/li&gt;\n&lt;li&gt;Do stemming or lemmatization?&lt;/li&gt;\n&lt;li&gt;Only keep keywords?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any tips or open-source Turkish tokenizers that actually work well?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Embedding inputs:&lt;/strong&gt;&lt;br/&gt;\nWhen embedding products (using models like GPT or other multilingual LLMs), I usually feed them like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;product title: ...  \nproduct description: ...  \ncolor: ...  \nsize: ...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I read somewhere (even here) that these key-value labels (&amp;quot;product title:&amp;quot;, etc.) might not help and could even hurt  that LLM-based models can infer structure without them. Is that really true? Is there another sota way to do it?&lt;/p&gt;\n\n&lt;p&gt;Also, should I normalize Turkish characters here too, or just leave them as-is?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Reranking:&lt;/strong&gt;&lt;br/&gt;\nI tried ColBERT but wasn’t impressed. I had much better results with Qwen-Reranker-4B, but it’s too slow when I’m comparing query to even 25 products. Are there any smaller/faster rerankers that still perform decently for Turkish/multilingual content and can bu used it production? ColBERT is fast because of it&amp;#39;s architecture but Reranker much reliable but slower :/&lt;/p&gt;\n\n&lt;p&gt;Any advice, practical tips, or general pointers are more than welcome! Especially curious about how people handle multilingual search pipelines (Turkish in my case) and what preprocessing tricks really matter in practice.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lz0hk3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zedeleyici3401",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752432488,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "“We have discovered a novel method to lock Open Weights for models to prevent fine tuning and safety reversal with the only side effect being the weights cannot be quantized. This is due to the method building off of quantization aware training, in effect, reversing that process.\n\nAny attempt to fine tune, adjust safe guards or quantization will result in severe degradation of the model: Benchmark results drop by over half, and the model tends to just output, “I’m doing this for your own safety.”\n\nAn example of this behavior can be seen simulated here: https://www.goody2.ai/\n\nEDIT: this is parody and satire at Open AI’s expense. I would this the (probably) in the title coupled with excessively negative results for most of us here would make that obvious. Still, I won’t be surprised if this is roughly what they announce.",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI’s announcement of their new Open Weights (Probably)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lz0b1p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.41,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752438171,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752432039,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;“We have discovered a novel method to lock Open Weights for models to prevent fine tuning and safety reversal with the only side effect being the weights cannot be quantized. This is due to the method building off of quantization aware training, in effect, reversing that process.&lt;/p&gt;\n\n&lt;p&gt;Any attempt to fine tune, adjust safe guards or quantization will result in severe degradation of the model: Benchmark results drop by over half, and the model tends to just output, “I’m doing this for your own safety.”&lt;/p&gt;\n\n&lt;p&gt;An example of this behavior can be seen simulated here: &lt;a href=\"https://www.goody2.ai/\"&gt;https://www.goody2.ai/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;EDIT: this is parody and satire at Open AI’s expense. I would this the (probably) in the title coupled with excessively negative results for most of us here would make that obvious. Still, I won’t be surprised if this is roughly what they announce.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?auto=webp&amp;s=dd82c52e3c0cbbbac9e33e14f8578180d0118b5e",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49a3c770178c2a312b52ba291a11497cae75ca76",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=57145738b3d0dc87f706fb8cd010d6539abf2ac7",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c39ff2172cb1e4a98349ee643f7c1a9d4212191",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd4b29b2b67e9d16a6993f4e85364dfc138327dc",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e74d5984363a988ca297c3a069cfc25bd8835f89",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28a0a1c5f0163600d5722f010f9592a10aed738f",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "KJ7pkyeZcmLTiq2mOGe-ze2YhgYpuhFj7rAwbWEG8d8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lz0b1p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lz0b1p/openais_announcement_of_their_new_open_weights/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz0b1p/openais_announcement_of_their_new_open_weights/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752432039,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I use the Kimi app on my iPhone but it seems like the thinking options only offers like kimi 1.5? Do I do something wrong here or do I have to activate it?",
          "author_fullname": "t2_1t4njpvo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi k2 not available on iPhone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyyu6i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752428483,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use the Kimi app on my iPhone but it seems like the thinking options only offers like kimi 1.5? Do I do something wrong here or do I have to activate it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyyu6i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatrandomGuyxoxo",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyyu6i/kimi_k2_not_available_on_iphone/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyyu6i/kimi_k2_not_available_on_iphone/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752428483,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i need a good LLM i can run on these specs. should i wait for grok 3?\n\nhttps://preview.redd.it/4ky5a383hocf1.png?width=993&amp;format=png&amp;auto=webp&amp;s=a676584defa0b0894ca945493a2b5ca4413aa1f7\n\n",
          "author_fullname": "t2_gn3aqjvm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "i need the best local llm i can run on my gaming pc",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ky5a383hocf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 28,
                  "x": 108,
                  "u": "https://preview.redd.it/4ky5a383hocf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cecb41294a4b31eb39701e074911fa33b8d8248"
                },
                {
                  "y": 56,
                  "x": 216,
                  "u": "https://preview.redd.it/4ky5a383hocf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ca93b4af094cd8d0c2c1e9c31929b6548b58ddff"
                },
                {
                  "y": 83,
                  "x": 320,
                  "u": "https://preview.redd.it/4ky5a383hocf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=52994e1cd94c84becb8fe22d73d466517de5e5da"
                },
                {
                  "y": 166,
                  "x": 640,
                  "u": "https://preview.redd.it/4ky5a383hocf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae0bd2e69adad43de076ed311fa5208e8f3a33b3"
                },
                {
                  "y": 249,
                  "x": 960,
                  "u": "https://preview.redd.it/4ky5a383hocf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=333d9917b8c831656ac1c913b9fb566096309d40"
                }
              ],
              "s": {
                "y": 258,
                "x": 993,
                "u": "https://preview.redd.it/4ky5a383hocf1.png?width=993&amp;format=png&amp;auto=webp&amp;s=a676584defa0b0894ca945493a2b5ca4413aa1f7"
              },
              "id": "4ky5a383hocf1"
            }
          },
          "name": "t3_1lyyryy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.27,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/1ltVnxF58OtW0jV7EwNKr6Hm2DP-TXLcklsmPU62Fs4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752428339,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i need a good LLM i can run on these specs. should i wait for grok 3?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ky5a383hocf1.png?width=993&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a676584defa0b0894ca945493a2b5ca4413aa1f7\"&gt;https://preview.redd.it/4ky5a383hocf1.png?width=993&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a676584defa0b0894ca945493a2b5ca4413aa1f7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyyryy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Interesting_Pay7816",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyyryy/i_need_the_best_local_llm_i_can_run_on_my_gaming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyyryy/i_need_the_best_local_llm_i_can_run_on_my_gaming/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752428339,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI come from a Python background where I use Pydantic AI a lot, especially for handling structured data and validation. I’m starting a new project in TypeScript and I’m looking for libraries or frameworks that can help me achieve similar functionality, specifically for structured output and data validation.\n\nDoes anyone know of any great TypeScript tools that provide a Pydantic AI like experience?  \nAny resources, recommendations, or example projects would be really appreciated!",
          "author_fullname": "t2_t1w7nq7d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to get LLM structured outputs in TS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyyoff",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752428107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I come from a Python background where I use Pydantic AI a lot, especially for handling structured data and validation. I’m starting a new project in TypeScript and I’m looking for libraries or frameworks that can help me achieve similar functionality, specifically for structured output and data validation.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any great TypeScript tools that provide a Pydantic AI like experience?&lt;br/&gt;\nAny resources, recommendations, or example projects would be really appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyyoff",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "too_much_lag",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyyoff/how_to_get_llm_structured_outputs_in_ts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyyoff/how_to_get_llm_structured_outputs_in_ts/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752428107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Got tired of waiting for k2 ggufs and found this guy:  \n[https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main](https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main)\n\nThere is a typo in the commands but it seems to work great, and really easy to get going:  \npip install ftllm  \nftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40\n\nand just like that I'm getting 7-10T/s on my 5090 + DDR5 Xeon machine",
          "author_fullname": "t2_9hl4ymvj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Never seen fastllm mentioned here, anyone using it? (kimi k2 local)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyyhwz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752428193,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752427662,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got tired of waiting for k2 ggufs and found this guy:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main\"&gt;https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There is a typo in the commands but it seems to work great, and really easy to get going:&lt;br/&gt;\npip install ftllm&lt;br/&gt;\nftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40&lt;/p&gt;\n\n&lt;p&gt;and just like that I&amp;#39;m getting 7-10T/s on my 5090 + DDR5 Xeon machine&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?auto=webp&amp;s=bae6f5c013ad93b7ca44d907a27215b9cd031d97",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ae33582b4b0826302a3cd3ed7609af7df200f8d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c75bfa0830517e198d439f93aa7ff4a96c4340e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eca931e08f88125c81e49d17938fac710ad11893",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6743d4fea5b1dddfd1c329d591498a0f9454d56",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4b0fb1bec8773b706d6c25c2eff96884f1c19a3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f63241ebc2da005589f7190f60055879ec108c46",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyyhwz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious_Cut_6144",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752427662,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\nI’m interested in running a self-hosted local LLM for coding assistance—something similar to what Cursor offers, but fully local for privacy and experimentation. Ideally, I’d like it to support code completion, inline suggestions, and maybe even multi-file context.\n\nWhat kind of hardware would I realistically need to run this smoothly?\nSome specific questions:\n\t•\tIs a consumer-grade GPU (like an RTX 4070/4080) enough for models like Code Llama or Phi-3?\n\t•\tHow much RAM is recommended for practical use?\n\t•\tAre there any CPU-only setups that work decently, or is GPU basically required for real-time performance?\n\t•\tAny tips for keeping power consumption/noise low while running this 24/7?\n\nWould love to hear from anyone who’s running something like this already—what’s your setup and experience been like?\n\nThanks in advance!",
          "author_fullname": "t2_37p33r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of hardware would I need to self-host a local LLM for coding (like Cursor)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyyelr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752427438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nI’m interested in running a self-hosted local LLM for coding assistance—something similar to what Cursor offers, but fully local for privacy and experimentation. Ideally, I’d like it to support code completion, inline suggestions, and maybe even multi-file context.&lt;/p&gt;\n\n&lt;p&gt;What kind of hardware would I realistically need to run this smoothly?\nSome specific questions:\n    • Is a consumer-grade GPU (like an RTX 4070/4080) enough for models like Code Llama or Phi-3?\n    • How much RAM is recommended for practical use?\n    • Are there any CPU-only setups that work decently, or is GPU basically required for real-time performance?\n    • Any tips for keeping power consumption/noise low while running this 24/7?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from anyone who’s running something like this already—what’s your setup and experience been like?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyyelr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ClassicHabit",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752427438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've noticed that when using Jan's built-in Hub, the list of available models seems very limited. Even though there are many GGUF models available on Hugging Face (with proper formatting and quantization), they often don't appear in the search results inside Jan.\n\nI can download them manually by downloading them fron Hugging Face, but it would be a lot more convenient if Jan just showed all compatible GGUF models by default. Do you think there a limitation in the Hub search functionality? Is this a known issue?",
          "author_fullname": "t2_lex91qqzy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jan doesn't show all available GGUF models from Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyybq8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752427241,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed that when using Jan&amp;#39;s built-in Hub, the list of available models seems very limited. Even though there are many GGUF models available on Hugging Face (with proper formatting and quantization), they often don&amp;#39;t appear in the search results inside Jan.&lt;/p&gt;\n\n&lt;p&gt;I can download them manually by downloading them fron Hugging Face, but it would be a lot more convenient if Jan just showed all compatible GGUF models by default. Do you think there a limitation in the Hub search functionality? Is this a known issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyybq8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SensitiveDisk0",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyybq8/jan_doesnt_show_all_available_gguf_models_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyybq8/jan_doesnt_show_all_available_gguf_models_from/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752427241,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi. I been trying to automatically log the inputs and outputs in the CLI  and API webgui in llama.cpp. Looking for an efficient one. ",
          "author_fullname": "t2_t3o897gj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Easy way to log input/output in llama.cpp?  (server and chat)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyy4k8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752426758,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I been trying to automatically log the inputs and outputs in the CLI  and API webgui in llama.cpp. Looking for an efficient one. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyy4k8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aayehh",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyy4k8/easy_way_to_log_inputoutput_in_llamacpp_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyy4k8/easy_way_to_log_inputoutput_in_llamacpp_server/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752426758,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech\n\nhttps://arxiv.org/abs/2506.21619\n\nFeatures:\n\n- **Fully local with open weights.**\n- Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.\n- Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.\n- Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.\n- Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard \"free length\" mode where it automatically lets the audio become as long as necessary.\n- Supported text to speech languages that it can output: English and Chinese. Like most models.\n\nHere's a few real-world use cases:\n\n- Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that's the perfect length for dubbing.\n- Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.\n- Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.\n\n## So how did it leak?\n\n- They have been preparing a website at https://index-tts2.github.io/ which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they've been preparing, along with demo files.\n- Here's an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4\n- The entire presentation page is here: https://index-tts.github.io/index-tts2.github.io/\n- To download all demos and watch the HTML presentation locally, you can also \"git clone https://github.com/index-tts/index-tts2.github.io.git\".\n\nI can't wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual *acting!* Bravo, Bilibili, the company behind this research!\n\nThey are planning to release it \"soon\", and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I'd say it's coming this month or the next.\n\nTheir previous model was Apache 2 license, both for the source code and the weights. Let's hope the next model is the same awesome license.",
          "author_fullname": "t2_4a13s1mr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IndexTTS2, the most realistic and expressive text-to-speech model so far, has leaked their demos ahead of the official launch! And... wow!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyy39n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 574,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 574,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752437979,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752426670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21619\"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fully local with open weights.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Zero-shot voice cloning. You just provide one audio file (in any language) and it will extremely accurately clone the voice style and rhythm. It sounds much more accurate than MaskGCT and F5-TTS, two of the other state-of-the-art local models.&lt;/li&gt;\n&lt;li&gt;Optional: Zero-shot emotion cloning by providing a second audio file that contains the emotional state to emulate. This affects things thing whispering, screaming, fear, desire, anger, etc. This is a world-first.&lt;/li&gt;\n&lt;li&gt;Optional: Text control of emotions, without needing a 2nd audio file. You can just write what emotions should be used.&lt;/li&gt;\n&lt;li&gt;Optional: Full control over how long the output will be, which makes it perfect for dubbing movies. This is a world-first. Alternatively you can run it in standard &amp;quot;free length&amp;quot; mode where it automatically lets the audio become as long as necessary.&lt;/li&gt;\n&lt;li&gt;Supported text to speech languages that it can output: English and Chinese. Like most models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s a few real-world use cases:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Take an Anime, clone the voice of the original character, clone the emotion of the original performance, and make them read the English script, and tell it how long the performance should last. You will now have the exact same voice and emotions reading the English translation with a good performance that&amp;#39;s the perfect length for dubbing.&lt;/li&gt;\n&lt;li&gt;Take one voice sample, and make it say anything, with full text-based control of what emotions the speaker should perform.&lt;/li&gt;\n&lt;li&gt;Take two voice samples, one being the speaker voice and the other being the emotional performance, and then make it say anything with full text-based control.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;So how did it leak?&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;They have been preparing a website at &lt;a href=\"https://index-tts2.github.io/\"&gt;https://index-tts2.github.io/&lt;/a&gt; which is not public yet, but their repo for the site is already public. Via that repo you can explore the presentation they&amp;#39;ve been preparing, along with demo files.&lt;/li&gt;\n&lt;li&gt;Here&amp;#39;s an example demo file with dubbing from Chinese to English, showing how damn good this TTS model is at conveying emotions. The voice performance it gives is good enough that I could happily watch an entire movie or TV show dubbed with this AI model: &lt;a href=\"https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4\"&gt;https://index-tts.github.io/index-tts2.github.io/ex6/Empresses_in_the_Palace_1.mp4&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;The entire presentation page is here: &lt;a href=\"https://index-tts.github.io/index-tts2.github.io/\"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;To download all demos and watch the HTML presentation locally, you can also &amp;quot;git clone &lt;a href=\"https://github.com/index-tts/index-tts2.github.io.git\"&gt;https://github.com/index-tts/index-tts2.github.io.git&lt;/a&gt;&amp;quot;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I can&amp;#39;t wait to play around with this. Absolutely crazy how realistic these AI voice emotions are! This is approaching actual &lt;em&gt;acting!&lt;/em&gt; Bravo, Bilibili, the company behind this research!&lt;/p&gt;\n\n&lt;p&gt;They are planning to release it &amp;quot;soon&amp;quot;, and considering the state of everything (paper came out on June 23rd, and the website is practically finished) I&amp;#39;d say it&amp;#39;s coming this month or the next.&lt;/p&gt;\n\n&lt;p&gt;Their previous model was Apache 2 license, both for the source code and the weights. Let&amp;#39;s hope the next model is the same awesome license.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lyy39n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pilkyton",
          "discussion_type": null,
          "num_comments": 129,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyy39n/indextts2_the_most_realistic_and_expressive/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752426670,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With 64GB RAM I could run [dots](https://huggingface.co/unsloth/dots.llm1.inst-GGUF) with `mmap` at Q4 with some hiccups (offloading a small part of the model to the SSD). I had [mixed feelings](https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/comment/n13cnzx/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) about the model:\n\n&gt;I've been playing around with Dots at Q4\\_K\\_XL a bit, and it's one of those models that gives me mixed feelings. It's super-impressive at times, one of the best performing models I've ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.\n\nI upgraded to 128GB RAM and tried dots again at Q5\\_K\\_XL, and (unless I did something wrong before) it was noticeable better. I got curious and also tried Q6\\_K\\_XL (highest quant I can fit now) and it was even more noticeable better. \n\nI have no mixed feelings anymore. Compared to especially Q4, Q6 feels almost like a new model. It almost always impress me now, it feels very solid and overall powerful. I think this is now my new favorite overall model.\n\nI'm a little surprised that the difference between Q4, Q5 and Q6 is this large. I thought I would only see this sort of quality gap below Q4, starting at Q3. Has anyone else experienced this too with this model, or any other model for that matter?\n\nI can only fit the even larger model Qwen3-235b at Q4, I wonder if the quality difference is also this big at Q5/Q6 here?",
          "author_fullname": "t2_qhlcbiy3k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "dots.llm1 appears to be very sensitive to quantization?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyy0yi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752426930,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752426515,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With 64GB RAM I could run &lt;a href=\"https://huggingface.co/unsloth/dots.llm1.inst-GGUF\"&gt;dots&lt;/a&gt; with &lt;code&gt;mmap&lt;/code&gt; at Q4 with some hiccups (offloading a small part of the model to the SSD). I had &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/comment/n13cnzx/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;mixed feelings&lt;/a&gt; about the model:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;ve been playing around with Dots at Q4_K_XL a bit, and it&amp;#39;s one of those models that gives me mixed feelings. It&amp;#39;s super-impressive at times, one of the best performing models I&amp;#39;ve ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I upgraded to 128GB RAM and tried dots again at Q5_K_XL, and (unless I did something wrong before) it was noticeable better. I got curious and also tried Q6_K_XL (highest quant I can fit now) and it was even more noticeable better. &lt;/p&gt;\n\n&lt;p&gt;I have no mixed feelings anymore. Compared to especially Q4, Q6 feels almost like a new model. It almost always impress me now, it feels very solid and overall powerful. I think this is now my new favorite overall model.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little surprised that the difference between Q4, Q5 and Q6 is this large. I thought I would only see this sort of quality gap below Q4, starting at Q3. Has anyone else experienced this too with this model, or any other model for that matter?&lt;/p&gt;\n\n&lt;p&gt;I can only fit the even larger model Qwen3-235b at Q4, I wonder if the quality difference is also this big at Q5/Q6 here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?auto=webp&amp;s=f0d8a1e0eca0ded72123d7800bbfdc68ebd71f4a",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ba3d79ad811c3a5dd1850b68eecf457670859c35",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6e12ba77bdf5d9d77ee38b4a963c471b423fa3f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1f1774424e25aa6e286c88fa430ff3f6d1f4ab1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39a200fa54cb5b74c34bfbef3203d2dda0951238",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8836a8bbf032eca97e032f8e95ecba04afbd729d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6136cce018d4c1fdd79c7c6109223f99e6ccc25a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyy0yi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Admirable-Star7088",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752426515,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As promised in the banana thread. OP delivers.\n\n**Benchmarks**\n\nThe following benchmarks were taken using official Qwen3 models from Huggingface's Qwen repo for consistency:\n\nMoE:\n\n* Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel\n* Qwen3 30B A3B BF16 in Tensor Parallel\n* Qwen3 30B A3B BF16 on a single GPU\n* Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel\n* Qwen3 30B A3B GPTQ Int4 quant on a single GPU\n\nDense:\n\n* Qwen3 32B BF16 on a single GPU\n* Qwen3 32B BF16 in Tensor Parallel\n* Qwen3 14B BF16 on a single GPU\n* Qwen3 14B BF16 in Tensor Parallel\n\nAll benchmarking was done with `vllm bench throughput ...` using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.\n\n**Hardware**\n\n2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, ~~512GB~~ 768GB DDR5 5200 MT/s, PCIe 5.0 x16.\n\n**Software**\n\n* Ubuntu 24.04.2\n* NVidia drivers 575.57.08\n* CUDA 12.9\n\nThis was the magic Torch incantation that got everything working: \n    \n    pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128\n\nOtherwise these instructions worked well despite being for WSL: https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3\n\n\n**MoE Results**\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\n    Throughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\n    Throughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192\n    Throughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input**\n\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384\n    Throughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s\n    Total num prompt tokens:  16383966\n    Total num output tokens:  128000\n\n\n\n\n**Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024\n    Throughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096\n    Throughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024\n    Throughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096\n    Throughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n\n\n\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\n    Throughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\n    Throughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n\n\n\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024\n    Throughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096\n    Throughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Dense Model Results**\n\n**Qwen3 32B BF16 @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024\n    Throughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096\n    Throughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 8k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192\n    Throughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n\n**Qwen3 32B BF16 @ 1k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2\n    Throughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 4k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \n    Throughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 32B BF16 @ 8k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \n    Throughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 1k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024\n    Throughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 4k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096\n    Throughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 8k input | single GPU**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192\n    Throughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 1k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 \n    Throughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s\n    Total num prompt tokens:  1021646\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 4k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \n    Throughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s\n    Total num prompt tokens:  4091212\n    Total num output tokens:  128000\n\n**Qwen3 14B BF16 @ 8k input | Tensor Parallel**\n\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \n    Throughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s\n    Total num prompt tokens:  8189599\n    Total num output tokens:  128000",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 30B and 235B on dual RTX PRO 6000 Blackwell Workstation Edition",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyxf1f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752473261,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752425040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As promised in the banana thread. OP delivers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The following benchmarks were taken using official Qwen3 models from Huggingface&amp;#39;s Qwen repo for consistency:&lt;/p&gt;\n\n&lt;p&gt;MoE:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B BF16 in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B BF16 on a single GPU&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant on a single GPU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Dense:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 32B BF16 on a single GPU&lt;/li&gt;\n&lt;li&gt;Qwen3 32B BF16 in Tensor Parallel&lt;/li&gt;\n&lt;li&gt;Qwen3 14B BF16 on a single GPU&lt;/li&gt;\n&lt;li&gt;Qwen3 14B BF16 in Tensor Parallel&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All benchmarking was done with &lt;code&gt;vllm bench throughput ...&lt;/code&gt; using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, &lt;del&gt;512GB&lt;/del&gt; 768GB DDR5 5200 MT/s, PCIe 5.0 x16.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ubuntu 24.04.2&lt;/li&gt;\n&lt;li&gt;NVidia drivers 575.57.08&lt;/li&gt;\n&lt;li&gt;CUDA 12.9&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This was the magic Torch incantation that got everything working: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Otherwise these instructions worked well despite being for WSL: &lt;a href=\"https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3\"&gt;https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;MoE Results&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\nThroughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\nThroughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192\nThroughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384\nThroughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s\nTotal num prompt tokens:  16383966\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024\nThroughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096\nThroughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024\nThroughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096\nThroughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\nThroughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\nThroughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024\nThroughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096\nThroughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Dense Model Results&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024\nThroughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096\nThroughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192\nThroughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2\nThroughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \nThroughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \nThroughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024\nThroughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096\nThroughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192\nThroughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 \nThroughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s\nTotal num prompt tokens:  1021646\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \nThroughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s\nTotal num prompt tokens:  4091212\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \nThroughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s\nTotal num prompt tokens:  8189599\nTotal num output tokens:  128000\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyxf1f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752425040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm releasing a new version of my [audiobook creator app](https://github.com/prakharsr/audiobook-creator) which now supports Kokoro and Orpheus. This release adds support for [Orpheus TTS](https://github.com/canopyai/Orpheus-TTS) which supports high-quality audio and more expressive speech. This version also adds support for adding emotion tags automatically using an LLM. Audio generation using Orpheus is done using my dedicated [Orpheus TTS FastAPI Server](https://github.com/prakharsr/Orpheus-TTS-FastAPI) repository.\n\nListen to a sample audiobook generated using this app: [https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus](https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus)\n\nApp Features:\n\n* **Advanced TTS Engine Support**: Seamlessly switch between Kokoro and Orpheus TTS engines via environment configuration\n* **Async Parallel Processing**: Optimized for concurrent request handling with significant performance improvements and faster audiobook generation.\n* **Gradio UI App**: Create audiobooks easily with an easy to use, intuitive UI made with Gradio.\n* **M4B Audiobook Creation**: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.\n* **Multi-Format Input Support**: Converts books from various formats (EPUB, PDF, etc.) into plain text.\n* **Multi-Format Output Support**: Supports various output formats: AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B.\n* **Docker Support**: Use pre-built docker images/ build using docker compose to save time and for a smooth user experience.\n* **Emotion Tags Addition**: Emotion tags which are supported in Orpheus TTS can be added to the book's text intelligently using an LLM to enhance character voice expression.\n* **Character Identification**: Identifies characters and infers their attributes (gender, age) using advanced NLP techniques and LLMs.\n* **Customizable Audiobook Narration**: Supports single-voice or multi-voice narration with narrator gender preference for enhanced listening experiences.\n* **Progress Tracking**: Includes progress bars and execution time measurements for efficient monitoring.\n* **Open Source**: Licensed under GPL v3.\n\nCheckout the Audiobook Creator Repo here: [https://github.com/prakharsr/audiobook-creator](https://github.com/prakharsr/audiobook-creator)\n\nLet me know how the audiobooks sound and if you like the app :)",
          "author_fullname": "t2_hi3epx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Audiobook Creator - v1.4 - Added support for Orpheus along with Kokoro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyw5u2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752421959,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m releasing a new version of my &lt;a href=\"https://github.com/prakharsr/audiobook-creator\"&gt;audiobook creator app&lt;/a&gt; which now supports Kokoro and Orpheus. This release adds support for &lt;a href=\"https://github.com/canopyai/Orpheus-TTS\"&gt;Orpheus TTS&lt;/a&gt; which supports high-quality audio and more expressive speech. This version also adds support for adding emotion tags automatically using an LLM. Audio generation using Orpheus is done using my dedicated &lt;a href=\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\"&gt;Orpheus TTS FastAPI Server&lt;/a&gt; repository.&lt;/p&gt;\n\n&lt;p&gt;Listen to a sample audiobook generated using this app: &lt;a href=\"https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus\"&gt;https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;App Features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Advanced TTS Engine Support&lt;/strong&gt;: Seamlessly switch between Kokoro and Orpheus TTS engines via environment configuration&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Optimized for concurrent request handling with significant performance improvements and faster audiobook generation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gradio UI App&lt;/strong&gt;: Create audiobooks easily with an easy to use, intuitive UI made with Gradio.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;M4B Audiobook Creation&lt;/strong&gt;: Creates compatible audiobooks with covers, metadata, chapter timestamps etc. in M4B format.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multi-Format Input Support&lt;/strong&gt;: Converts books from various formats (EPUB, PDF, etc.) into plain text.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Multi-Format Output Support&lt;/strong&gt;: Supports various output formats: AAC, M4A, MP3, WAV, OPUS, FLAC, PCM, M4B.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Docker Support&lt;/strong&gt;: Use pre-built docker images/ build using docker compose to save time and for a smooth user experience.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Emotion Tags Addition&lt;/strong&gt;: Emotion tags which are supported in Orpheus TTS can be added to the book&amp;#39;s text intelligently using an LLM to enhance character voice expression.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Character Identification&lt;/strong&gt;: Identifies characters and infers their attributes (gender, age) using advanced NLP techniques and LLMs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Customizable Audiobook Narration&lt;/strong&gt;: Supports single-voice or multi-voice narration with narrator gender preference for enhanced listening experiences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Includes progress bars and execution time measurements for efficient monitoring.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Licensed under GPL v3.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Checkout the Audiobook Creator Repo here: &lt;a href=\"https://github.com/prakharsr/audiobook-creator\"&gt;https://github.com/prakharsr/audiobook-creator&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know how the audiobooks sound and if you like the app :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?auto=webp&amp;s=8dbdf77b998c2a437d59906bf87790938f4fbabf",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f5d797fded6eb1be79f568494cf1d46669cab00",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=28a2d439698d177ba57f4500a952608a6b94f98e",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29ec3cca77e1d909a8119f3c0f5a41250a70bb40",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7040e5d6ed60ffca60c3256cbabfb1663f065b5a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=19b88df8422100e720ad41fa74806597ac879879",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=154b520599f685ee02d84d0900386f6fdc48eb30",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "UoHhOkeVuwQG6KOjlcba2eN3oWFHe5ObpsY1_6Psfzk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyw5u2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "prakharsr",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyw5u2/audiobook_creator_v14_added_support_for_orpheus/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752421959,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys recently LM studio add support for tool use for local running llms. I wanting to add the option for my local running llm to do searching with my default browser for more up to date information. \n\nBut I have no clue how I want to keep in contained to the LM studio UI if possible. ",
          "author_fullname": "t2_2k690kue",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Like some help setting up MCP sever for LM studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyvyhq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752421454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys recently LM studio add support for tool use for local running llms. I wanting to add the option for my local running llm to do searching with my default browser for more up to date information. &lt;/p&gt;\n\n&lt;p&gt;But I have no clue how I want to keep in contained to the LM studio UI if possible. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyvyhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Night5124",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvyhq/like_some_help_setting_up_mcp_sever_for_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvyhq/like_some_help_setting_up_mcp_sever_for_lm_studio/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752421454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm releasing a v1.0 of my [Orpheus TTS FastAPI Server](https://github.com/prakharsr/Orpheus-TTS-FastAPI). Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the [Orpheus TTS](https://github.com/canopyai/Orpheus-TTS) model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the `orpheus-speech` python package.\n\nThe project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:\n\n1. Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.\n2. **Intelligent Retry Logic:** Automatic retry on audio decoding errors for improved reliability. The original implementation in `orpheus-speech` skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.\n3. **Token Repetition Detection**: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in `orpheus-speech` sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.\n4. **Async Parallel Processing**: Processes multiple text chunks simultaneously for faster generation. The original implementation in `orpheus-speech` was synchronous, this is now fixed by adding support for concurrent async calls.\n5. **Text Chunking**: Automatic intelligent text splitting for long content.\n\nLink to the repo: [https://github.com/prakharsr/Orpheus-TTS-FastAPI](https://github.com/prakharsr/Orpheus-TTS-FastAPI)\n\nLet me know how it works and also checkout my [Audiobook Creator Project here](https://github.com/prakharsr/audiobook-creator) which supports Kokoro and Orpheus.",
          "author_fullname": "t2_hi3epx7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Orpheus TTS FastAPI Server Release v1.0 (Async and Audio Issues Fixes)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyvsqv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752421053,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m releasing a v1.0 of my &lt;a href=\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\"&gt;Orpheus TTS FastAPI Server&lt;/a&gt;. Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the &lt;a href=\"https://github.com/canopyai/Orpheus-TTS\"&gt;Orpheus TTS&lt;/a&gt; model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the &lt;code&gt;orpheus-speech&lt;/code&gt; python package.&lt;/p&gt;\n\n&lt;p&gt;The project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Intelligent Retry Logic:&lt;/strong&gt; Automatic retry on audio decoding errors for improved reliability. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Token Repetition Detection&lt;/strong&gt;: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Processes multiple text chunks simultaneously for faster generation. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; was synchronous, this is now fixed by adding support for concurrent async calls.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatic intelligent text splitting for long content.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Link to the repo: &lt;a href=\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\"&gt;https://github.com/prakharsr/Orpheus-TTS-FastAPI&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know how it works and also checkout my &lt;a href=\"https://github.com/prakharsr/audiobook-creator\"&gt;Audiobook Creator Project here&lt;/a&gt; which supports Kokoro and Orpheus.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?auto=webp&amp;s=0cc860c55c5d39a8725b904b4561a9f80e0f99d0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45d52cf1200f1189b715c76836164ff9cecf79b9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b408822642b2470faba4b74a50ce8516a253a9e3",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=948de85aa715b4f5e80a8759acad2fb62df83251",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0c92ff268493c06d6bca23d519a6e658e83f3c0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4c66abb6a8bf083ae94087d05f70bde8121c8436",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec93d51f819b3e6c11763f4435e0e77ffbdb9b84",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyvsqv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "prakharsr",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752421053,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. \n\nI had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. \n\nI prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Let’s talk about models you believed are more Hyped than Hot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyvkhr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752420473,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. &lt;/p&gt;\n\n&lt;p&gt;I had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. &lt;/p&gt;\n\n&lt;p&gt;I prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvkhr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752420473,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like \"here's a scenario, what do you think is the most realistic thing to happen?\" or \"what do you think would be a good solution to this issue?\". I found it quite bad in this regard.\n\n* It frequently made things up, even when specifically instructed not to do so. **It then clarified it was trying to come up with a helpful looking answer using fragmented data**, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.\n\n* If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion using sources that do not exist. **At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.** It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.\n\n* Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.\n\n* If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like \"why are you not revealing the character's thoughts here?\" or \"why are you not taking X into account?\". Free ChatGPT is actually much better in this regard.\n\n* Out of all the AI chat bots i have tried, it has possibly the most restrictive content filters i have seen. It's very prudish.\n\nEdit : Im using Kimi k2 on www.kimi.com btw.",
          "author_fullname": "t2_11hnt7w9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Kimi K2 for writing and reasoning, and was not impressed.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyvah4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 62,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 62,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752461908,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here&amp;#39;s a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it&amp;#39;s own idea.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character&amp;#39;s thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Out of all the AI chat bots i have tried, it has possibly the most restrictive content filters i have seen. It&amp;#39;s very prudish.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Edit : Im using Kimi k2 on &lt;a href=\"http://www.kimi.com\"&gt;www.kimi.com&lt;/a&gt; btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvah4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlompSpark",
          "discussion_type": null,
          "num_comments": 96,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752419788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone has tried something like that? I just put: create a google chrome extension that blocks websites. it's just something that takes a list of websites and blocks them. The extension does not work in both codes provided by the LLMs.",
          "author_fullname": "t2_cihv7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Testing ChatGPT and Claude capabilities to \"simple projects\": Block Site extension for Google Chrome",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyv7s7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419608,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone has tried something like that? I just put: create a google chrome extension that blocks websites. it&amp;#39;s just something that takes a list of websites and blocks them. The extension does not work in both codes provided by the LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyv7s7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "helioscarbex",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv7s7/testing_chatgpt_and_claude_capabilities_to_simple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyv7s7/testing_chatgpt_and_claude_capabilities_to_simple/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752419608,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.\n\nThe app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.\n\nI tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!\n\nHere's a brief guide to make this work with ollama:\n\n* Download the app from GitHub: [https://github.com/coreply/coreply](https://github.com/coreply/coreply)\n* Download `gemma3:4b-it-qat` in ollama\n* Set environment variable `OLLAMA_HOST` to [`0.0.0.0`](http://0.0.0.0) on the computer running ollama and restart ollama\n* In the Coreply app, set the API URL to `http://192.168.xxx.xxx:11434/v1/`(replace [`192.168.xxx.xxx`](http://192.168.xxx.xxx) with the IP address of the ollama machine), Model name `gemma3:4b-it-qat`\n* Grant permissions and turn on the app. Enjoy your texting suggestions!\n\nMy laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.\n\nLet me know how's your experience with it!",
          "author_fullname": "t2_l6eo8ggy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I use Gemma 3 to help me reply my texts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyv750",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 73,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755113126%2COWQxNGRkNjBiN2U2YmFmMTI0MDlmYjJiNzIzNDcwZGJkMGNhYWM1MjZjOTk5NDkyYmQzMjA4ZTFlOTE3ODZmYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755113126%2CZTA5M2QxZmIzNzQ0NTk1MzIxMDQwZDcxZmZiMTAyYjNjZDFmNGM5MWNmMDg0MDliYWE4NGU2NjVkOWM2MTFlOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 73,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=140&amp;height=34&amp;crop=140:34,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ea93f7d06620c65936cfa4562d1f7121ab5794f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752419560,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since there&amp;#39;re code completions, I wish I could have something similar when texting people. Now there&amp;#39;s finally a decent method for that.&lt;/p&gt;\n\n&lt;p&gt;The app works on any endpoint that&amp;#39;s OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt;\n\n&lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief guide to make this work with ollama:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Download the app from GitHub: &lt;a href=\"https://github.com/coreply/coreply\"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt;\n&lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href=\"http://0.0.0.0\"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt;\n&lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href=\"http://192.168.xxx.xxx\"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My laptop isn&amp;#39;t powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt;\n\n&lt;p&gt;Let me know how&amp;#39;s your experience with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/48w6qb1mincf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?format=pjpg&amp;auto=webp&amp;s=4315959609b1452ddac1019bc82dfcd0d2ecd932",
                  "width": 1080,
                  "height": 264
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca55479fe3e543c483ca9fd1a6e1c7663b1e1469",
                    "width": 108,
                    "height": 26
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bb9e1e7f65de33e3f9e6aac96c0c769244d7247e",
                    "width": 216,
                    "height": 52
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cb1d98d3e21b6fb43e522b5e2c12bfd0b0606cc7",
                    "width": 320,
                    "height": 78
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=150f56510dfbd5b6d84de28d6521c41bb3feede9",
                    "width": 640,
                    "height": 156
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0f618077403fd8aaf60fa19cfeb0ddd4cf30fc8d",
                    "width": 960,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b18333794ad268d43549283f55cc34c53f54a5c2",
                    "width": 1080,
                    "height": 264
                  }
                ],
                "variants": {},
                "id": "NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyv750",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sean01-eth",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/",
          "stickied": false,
          "url": "https://v.redd.it/48w6qb1mincf1",
          "subreddit_subscribers": 498850,
          "created_utc": 1752419560,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755113126%2COWQxNGRkNjBiN2U2YmFmMTI0MDlmYjJiNzIzNDcwZGJkMGNhYWM1MjZjOTk5NDkyYmQzMjA4ZTFlOTE3ODZmYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755113126%2CZTA5M2QxZmIzNzQ0NTk1MzIxMDQwZDcxZmZiMTAyYjNjZDFmNGM5MWNmMDg0MDliYWE4NGU2NjVkOWM2MTFlOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, noobie here.\n\nI am using OBS and there is a plugin called 'localvocal'.  \nI can choose there several LLMs etc.  \nWhich one should be the best for my use case? How can I add other LLMs from huggingface?\n\nAny help is appreciated, thank you!",
          "author_fullname": "t2_7mkjq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM model for live translation into subtitles [RU-EN]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyv5uc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, noobie here.&lt;/p&gt;\n\n&lt;p&gt;I am using OBS and there is a plugin called &amp;#39;localvocal&amp;#39;.&lt;br/&gt;\nI can choose there several LLMs etc.&lt;br/&gt;\nWhich one should be the best for my use case? How can I add other LLMs from huggingface?&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyv5uc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TuGuX",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv5uc/llm_model_for_live_translation_into_subtitles_ruen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyv5uc/llm_model_for_live_translation_into_subtitles_ruen/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752419469,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been building an AI assistant (Caelum) that can control a system using natural language, but I didn’t want it running raw shell commands or hallucinating `subprocess` calls. That’s unreliable and messy, so I built a structured `do()` system with plugin routing, safety flags, and argument parsing. Each command is a plugin, and you can write one in like 10–15 lines of code. Plugins auto-register and are isolated, so there’s no hardcoded logic or brittle wrappers.\n\nRight now it supports 39 commands, all modular, and you can interact with it using structured phrases or natural language if you add a mapping layer. It’s async-friendly, works with local agents, and is designed to grow without becoming a spaghetti monster.\n\nI originally posted this in another thread and realized quickly that it was the wrong crowd. This isn’t a CLI enhancement. It’s a system automation backbone that gives LLMs a safe, predictable way to control the OS through plugins, not shell access.\n\nIf you’re working on local agents or LLM-powered tools and want something that bridges into actual system control without chaos, I’d be happy to talk more about how it works. \n\n[https://github.com/BlackBeardJW/caelum-sys](https://github.com/BlackBeardJW/caelum-sys)  \n[https://pypi.org/project/caelum-sys/](https://pypi.org/project/caelum-sys/)",
          "author_fullname": "t2_dhf7dmo0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a plugin-based system automation layer for LLMs, safe, modular, and dead simple to extend",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyuxj5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752418915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been building an AI assistant (Caelum) that can control a system using natural language, but I didn’t want it running raw shell commands or hallucinating &lt;code&gt;subprocess&lt;/code&gt; calls. That’s unreliable and messy, so I built a structured &lt;code&gt;do()&lt;/code&gt; system with plugin routing, safety flags, and argument parsing. Each command is a plugin, and you can write one in like 10–15 lines of code. Plugins auto-register and are isolated, so there’s no hardcoded logic or brittle wrappers.&lt;/p&gt;\n\n&lt;p&gt;Right now it supports 39 commands, all modular, and you can interact with it using structured phrases or natural language if you add a mapping layer. It’s async-friendly, works with local agents, and is designed to grow without becoming a spaghetti monster.&lt;/p&gt;\n\n&lt;p&gt;I originally posted this in another thread and realized quickly that it was the wrong crowd. This isn’t a CLI enhancement. It’s a system automation backbone that gives LLMs a safe, predictable way to control the OS through plugins, not shell access.&lt;/p&gt;\n\n&lt;p&gt;If you’re working on local agents or LLM-powered tools and want something that bridges into actual system control without chaos, I’d be happy to talk more about how it works. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/BlackBeardJW/caelum-sys\"&gt;https://github.com/BlackBeardJW/caelum-sys&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://pypi.org/project/caelum-sys/\"&gt;https://pypi.org/project/caelum-sys/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?auto=webp&amp;s=2a2a988a09da5bc5ad4f13e5a79bd3559c0d9808",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=603529e7fcabe20144806792dd5e7c2476b13cfe",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64e3526af339fa9467fd5d5e7d75032005389d24",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb3942c412a45a44ec1098ffb7f97bab33ea85e4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=417f48a9f26f6fdf936c7f469c3797e861d17912",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=de6ca03a4ece691622a59b07094afb40147cfb4a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4bd124f47d49ba62d35cdffc6ded4e2d57584d7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyuxj5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BlackBeardJW",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyuxj5/built_a_pluginbased_system_automation_layer_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyuxj5/built_a_pluginbased_system_automation_layer_for/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752418915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I’m getting serious AI fever.\n\nI know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.\n\nIs it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?\n\nI know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.\n\nIs it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?\n\nShould I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?\n\n",
          "author_fullname": "t2_qq6spcu23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI fever D:",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyu7bf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752417048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I’m getting serious AI fever.&lt;/p&gt;\n\n&lt;p&gt;I know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.&lt;/p&gt;\n\n&lt;p&gt;Is it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?&lt;/p&gt;\n\n&lt;p&gt;I know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.&lt;/p&gt;\n\n&lt;p&gt;Is it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?&lt;/p&gt;\n\n&lt;p&gt;Should I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyu7bf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Czydera",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752417048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nSoon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I've done... helps.\n\nSending code to an internet service usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.\n\nI am looking at the p16s Gen 4 AMD  with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.\n\nThe first one will be more affordable than the second one...\n\nWould current big models run normally on a laptop like that P16s?\n\nThank you all in advance.",
          "author_fullname": "t2_albb0nfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for my next laptop soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lytioc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752472802,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752415228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Soon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I&amp;#39;ve done... helps.&lt;/p&gt;\n\n&lt;p&gt;Sending code to an internet service usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.&lt;/p&gt;\n\n&lt;p&gt;I am looking at the p16s Gen 4 AMD  with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.&lt;/p&gt;\n\n&lt;p&gt;The first one will be more affordable than the second one...&lt;/p&gt;\n\n&lt;p&gt;Would current big models run normally on a laptop like that P16s?&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lytioc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robotecnik",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752415228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD's GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?\n",
          "author_fullname": "t2_9wlqkcl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can I figure out the speed in tokens per second that my model will run on the CPU?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyt372",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752414028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD&amp;#39;s GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyt372",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Holiday-Picture6796",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752414028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With every religious text or practice of import in all languages each, etc? Anyone know of any \"godly ai\"' .. or is that unnecessary because the current models already have all the texts?",
          "author_fullname": "t2_2oqnjla8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is anyone training a religion model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyt0zp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752413859,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With every religious text or practice of import in all languages each, etc? Anyone know of any &amp;quot;godly ai&amp;quot;&amp;#39; .. or is that unnecessary because the current models already have all the texts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyt0zp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SeasonNo3107",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyt0zp/is_anyone_training_a_religion_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyt0zp/is_anyone_training_a_religion_model/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752413859,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So 6 months ago I discussed some information about the at the time not launched [910C accelerator here](https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/).\n\nThe details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/\n\nAnd semianalysis (regarding the 800 tflop bf16 performance) https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/\n\nSince then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.\n\nOn the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.\n\nRecently attended a conference where senior Huawei executives verbally discussed their future plans:\n\n1. They are aiming for a launch of the 920 in H2 2026 or H1 2027 \n\n2. The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?\n\n3. The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.\n\n4. A quote near verbatim \"the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.\" It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.",
          "author_fullname": "t2_nm52x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Rumor] Huawei 920 accelerator coming H2 2026",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lysqk7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752413617,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752413033,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So 6 months ago I discussed some information about the at the time not launched &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/\"&gt;910C accelerator here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) &lt;a href=\"https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/\"&gt;https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And semianalysis (regarding the 800 tflop bf16 performance) &lt;a href=\"https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/\"&gt;https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Since then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.&lt;/p&gt;\n\n&lt;p&gt;Recently attended a conference where senior Huawei executives verbally discussed their future plans:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;They are aiming for a launch of the 920 in H2 2026 or H1 2027 &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A quote near verbatim &amp;quot;the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.&amp;quot; It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lysqk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "44seconds",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752413033,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "EDIT: The issue turned out to be an old version of llama.cpp. Upgrading to the latest version as of now (b5890) resulted in 3.3t/s!\n\nEDIT 2.1: I got this up to ~~4.5t/s~~ 5.0t/s. Details added to the bottom of the post!\n\nPreface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.\n\nAs per the title, I have attempted to run Qwen3-235B-A22B using `llama-server` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?\n\nMachine specification are:\n\n    CPU: i3-12100F (12th Gen Intel)\n    RAM: 128GB (4*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\n    Motherboard: MSI PRO B660M-A WIFI DDR4\n    GPU: GeForce RTX 3090 24GB VRAM\n\n(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)\n\n`llama-server` launch options:\n\n    llama-server \\\n      --host 0.0.0.0 \\\n      --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n      --ctx-size 16384 \\\n      --n-gpu-layers 99 \\\n      --flash-attn \\\n      --threads 3 \\\n      -ot \"exps=CPU\" \\\n      --seed 3407 \\\n      --prio 3 \\\n      --temp 0.6 \\\n      --min-p 0.0 \\\n      --top-p 0.95 \\\n      --top-k 20 \\\n      --no-mmap \\\n      --no-warmup \\\n      --mlock\n\nAny advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)\n\n---\n\nManaged to achieve 5.0t/s!\n\n```\nllama-server \\\n  --host 0.0.0.0 \\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n  --ctx-size 16384 \\\n  --n-gpu-layers 99 \\\n  --flash-attn \\\n  --threads 4 \\\n  --seed 3407 \\\n  --prio 3 \\\n  --temp 0.6 \\\n  --min-p 0.0 \\\n  --top-p 0.95 \\\n  --top-k 20 \\\n  --no-warmup \\\n  -ub 1 \\\n  -ot 'blk\\.()\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(19)\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(2[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(3[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(4[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(5[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(6[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(7[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(8[0-9])\\.ffn_.*_exps\\.weight=CPU' \\\n  -ot 'blk\\.(9[0-9])\\.ffn_.*_exps\\.weight=CPU'\n```\n\nThis results in 23.76GB VRAM used and 5.0t/s.\n\n```\nprompt eval time =    5383.36 ms /    29 tokens (  185.63 ms per token,     5.39 t\n       eval time =  359004.62 ms /  1783 tokens (  201.35 ms per token,     4.97 t\n      total time =  364387.98 ms /  1812 tokens\n```",
          "author_fullname": "t2_tfa1mcp1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B @ 0.7t/s. Hardware or configuration bottleneck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lysmo9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752443952,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752412727,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: The issue turned out to be an old version of llama.cpp. Upgrading to the latest version as of now (b5890) resulted in 3.3t/s!&lt;/p&gt;\n\n&lt;p&gt;EDIT 2.1: I got this up to &lt;del&gt;4.5t/s&lt;/del&gt; 5.0t/s. Details added to the bottom of the post!&lt;/p&gt;\n\n&lt;p&gt;Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.&lt;/p&gt;\n\n&lt;p&gt;As per the title, I have attempted to run Qwen3-235B-A22B using &lt;code&gt;llama-server&lt;/code&gt; on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?&lt;/p&gt;\n\n&lt;p&gt;Machine specification are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CPU: i3-12100F (12th Gen Intel)\nRAM: 128GB (4*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\nMotherboard: MSI PRO B660M-A WIFI DDR4\nGPU: GeForce RTX 3090 24GB VRAM\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; launch options:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server \\\n  --host 0.0.0.0 \\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n  --ctx-size 16384 \\\n  --n-gpu-layers 99 \\\n  --flash-attn \\\n  --threads 3 \\\n  -ot &amp;quot;exps=CPU&amp;quot; \\\n  --seed 3407 \\\n  --prio 3 \\\n  --temp 0.6 \\\n  --min-p 0.0 \\\n  --top-p 0.95 \\\n  --top-k 20 \\\n  --no-mmap \\\n  --no-warmup \\\n  --mlock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Any advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Managed to achieve 5.0t/s!&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nllama-server \\\n  --host 0.0.0.0 \\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n  --ctx-size 16384 \\\n  --n-gpu-layers 99 \\\n  --flash-attn \\\n  --threads 4 \\\n  --seed 3407 \\\n  --prio 3 \\\n  --temp 0.6 \\\n  --min-p 0.0 \\\n  --top-p 0.95 \\\n  --top-k 20 \\\n  --no-warmup \\\n  -ub 1 \\\n  -ot &amp;#39;blk\\.()\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(19)\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(2[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(3[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(4[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(5[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(6[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(7[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(8[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39; \\\n  -ot &amp;#39;blk\\.(9[0-9])\\.ffn_.*_exps\\.weight=CPU&amp;#39;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This results in 23.76GB VRAM used and 5.0t/s.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nprompt eval time =    5383.36 ms /    29 tokens (  185.63 ms per token,     5.39 t\n       eval time =  359004.62 ms /  1783 tokens (  201.35 ms per token,     4.97 t\n      total time =  364387.98 ms /  1812 tokens\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lysmo9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ConnectionOutside485",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752412727,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If there is any reference or if anyone has clear idea please do reply.\n\n\nI have a 64gb ram 8core machine.\n3billion parameters models response running via ollama is slower than 600gb models api response.\nHow insane is that.?\n\nQuestion: how do you decide on infra\nIf a model is 600B params, each param is one byte so it goes to nearly 600gb.\nNow what kinda of system requirements does this model need to be running? \nShould a cpu be able to do 600 billion calculations per second or something?\n\nWhat kinda ram requirements does this need?\nSay if this is not a moe model, does it need 600Gb of ram to get started with this?\n\nNow how does the system requirements ram and cpu differ for moe and non moe models.\n\n",
          "author_fullname": "t2_vewp49tm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does it take to run llms?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqwil",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752407430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If there is any reference or if anyone has clear idea please do reply.&lt;/p&gt;\n\n&lt;p&gt;I have a 64gb ram 8core machine.\n3billion parameters models response running via ollama is slower than 600gb models api response.\nHow insane is that.?&lt;/p&gt;\n\n&lt;p&gt;Question: how do you decide on infra\nIf a model is 600B params, each param is one byte so it goes to nearly 600gb.\nNow what kinda of system requirements does this model need to be running? \nShould a cpu be able to do 600 billion calculations per second or something?&lt;/p&gt;\n\n&lt;p&gt;What kinda ram requirements does this need?\nSay if this is not a moe model, does it need 600Gb of ram to get started with this?&lt;/p&gt;\n\n&lt;p&gt;Now how does the system requirements ram and cpu differ for moe and non moe models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyqwil",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Impossible_Nose_2956",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqwil/what_does_it_take_to_run_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqwil/what_does_it_take_to_run_llms/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752407430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could it be because V-JEPA2 gave them strong confidence? [https://arxiv.org/abs/2506.09985](https://arxiv.org/abs/2506.09985)",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why has Meta started throwing billions at AI now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqhqq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.51,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752406004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could it be because V-JEPA2 gave them strong confidence? &lt;a href=\"https://arxiv.org/abs/2506.09985\"&gt;https://arxiv.org/abs/2506.09985&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyqhqq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqhqq/why_has_meta_started_throwing_billions_at_ai_now/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqhqq/why_has_meta_started_throwing_billions_at_ai_now/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752406004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working on a hackathon project and used 'exa' for AI web search. It's so dogwater, it literally kept making up sources and didn't even TRY to parse the output. If I have to put EXTRA work into LEARNING to use your damn service, what am i paying you for??? Like come on man... at least make it easier, if I knew it was like that i'd just make my own service.",
          "author_fullname": "t2_7l6ugv3d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are these random AI services?? Why are they so bad?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqefd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.32,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752405674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a hackathon project and used &amp;#39;exa&amp;#39; for AI web search. It&amp;#39;s so dogwater, it literally kept making up sources and didn&amp;#39;t even TRY to parse the output. If I have to put EXTRA work into LEARNING to use your damn service, what am i paying you for??? Like come on man... at least make it easier, if I knew it was like that i&amp;#39;d just make my own service.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lyqefd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Affectionate-Divide8",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqefd/what_are_these_random_ai_services_why_are_they_so/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqefd/what_are_these_random_ai_services_why_are_they_so/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752405674,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tried vertex.. 35 tps\n\nHuggingFace\nwith q6 from unsloth 48 tps\noriginal from Google 35 tps\n\nI need 100tps.. please help\n\nI know not much about inference infrastructure.\n",
          "author_fullname": "t2_86t4cp3p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed for MedGemma 27B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq7mc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried vertex.. 35 tps&lt;/p&gt;\n\n&lt;p&gt;HuggingFace\nwith q6 from unsloth 48 tps\noriginal from Google 35 tps&lt;/p&gt;\n\n&lt;p&gt;I need 100tps.. please help&lt;/p&gt;\n\n&lt;p&gt;I know not much about inference infrastructure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyq7mc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FewOwl9332",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq7mc/help_needed_for_medgemma_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq7mc/help_needed_for_medgemma_27b/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752404980,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.\n\nI've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.\n\nI did look at [https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix](https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix) but it is somewhat out of date now.\n\nI have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)\n\nLooking for help with:\n\n* Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?\n* Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?\n* Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?\n\nI have some constraints:\n\n* Must be air-gapped\n* I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.\n* Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure\n\nWould love to hear from anyone who’s done this in production or lab.\n\nThanks in advance!\n\n",
          "author_fullname": "t2_s8xklsb6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM to back Elastic AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq22j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve shortlisted Meta&amp;#39;s LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic&amp;#39;s model requirements . I&amp;#39;m planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt;\n\n&lt;p&gt;I did look at &lt;a href=\"https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix\"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt;\n\n&lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt;\n\n&lt;p&gt;Looking for help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt;\n&lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt;\n&lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have some constraints:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be air-gapped&lt;/li&gt;\n&lt;li&gt;I can&amp;#39;t use Chinese, Israeli or similar products. CISO doesn&amp;#39;t allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt;\n&lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear from anyone who’s done this in production or lab.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?auto=webp&amp;s=9a1b4684102bb8c94296cbfa71ad3a31d0c0f257",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7dcc983e13bd8aed1654a54d718d49f54cdaae",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db7b95b72b6ba957c849b5433b50158cc281a2e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=54ac1a61a829459657d4fbc629848f4a7b86377b",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19ae5d46a63ca2b66411c4548a6cff1870e42360",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5dbba47c48a53ff43f7f01db90cc69cd595f0e8",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=994bf868b62aa1188f41e6e7f154a8365d35c7fe",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyq22j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OldManCyberNinja",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752404403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone!\n\nWanted to ask a question that's been on my mind recently.\n\nI've done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.\n\nBut how is LLM evaluation done in real life (i.e. in industry)? If I'm a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it's doing a good job?\n\nIs it only product related metrics like customer satisfaction and existing benchmarks like in the industry? ",
          "author_fullname": "t2_owqzu42m8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM evaluation in real life?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq1yh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;Wanted to ask a question that&amp;#39;s been on my mind recently.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.&lt;/p&gt;\n\n&lt;p&gt;But how is LLM evaluation done in real life (i.e. in industry)? If I&amp;#39;m a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it&amp;#39;s doing a good job?&lt;/p&gt;\n\n&lt;p&gt;Is it only product related metrics like customer satisfaction and existing benchmarks like in the industry? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyq1yh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Plastic-Bus-7003",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/",
          "subreddit_subscribers": 498850,
          "created_utc": 1752404393,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’m developing a tool that allows you to create full applications by simply describing what you want in plain English—no complicated setup, no boilerplate code.\n\nHere’s what it currently offers:\n\t•\tSupports over 10 programming languages\n\t•\tLets you connect your GitHub repository\n\t•\tCan fix bugs or make improvements in your existing projects\n\t•\tWorks like Bolt.new or similar AI dev platforms, but with:\n\t•\tFaster response times\n\t•\tNo repetitive errors\n\t•\tNo excessive token usage\n\nIt’s currently in the development phase, but I plan to launch it for free to everyone at the start.\n\nI’m looking for honest feedback. What features would you find useful? What problems should I prioritize solving?\n\nYour input will directly influence how I shape this tool. Looking forward to hearing your thoughts in the comments.",
          "author_fullname": "t2_u5t9kq5vv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building an App That Builds Apps – Feedback Appreciated",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyptl7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.11,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OYZ5M27a9upsQcZRuC8QqWr8XyHjw-uCoGp77BH_QKk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752403499,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m developing a tool that allows you to create full applications by simply describing what you want in plain English—no complicated setup, no boilerplate code.&lt;/p&gt;\n\n&lt;p&gt;Here’s what it currently offers:\n    • Supports over 10 programming languages\n    • Lets you connect your GitHub repository\n    • Can fix bugs or make improvements in your existing projects\n    • Works like Bolt.new or similar AI dev platforms, but with:\n    • Faster response times\n    • No repetitive errors\n    • No excessive token usage&lt;/p&gt;\n\n&lt;p&gt;It’s currently in the development phase, but I plan to launch it for free to everyone at the start.&lt;/p&gt;\n\n&lt;p&gt;I’m looking for honest feedback. What features would you find useful? What problems should I prioritize solving?&lt;/p&gt;\n\n&lt;p&gt;Your input will directly influence how I shape this tool. Looking forward to hearing your thoughts in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0t2fav6bfmcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?auto=webp&amp;s=7851a3fd0e9a8f4050435c357dd9c8e1b79a5b08",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36351b492606d376ec84f6c3266514145d42d6c5",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e8b5638135d991191e0382cc144b86ce44d89ece",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ee54bc62ef610c65ae8dccc836b6ecc130dc999",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e5c7aff51a855990bc95d8e61e471afcef695f3",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5728de62d148c87ec6be7ba70dec9a2f1481ae2b",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ca6d3ef9e20be3411788c7a738bb7ef1aa547740",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "osOMZVrkvVKxIqXVX5EQh7Ig9gY66_BSUdZF017hArA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lyptl7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prestigious_Skin6507",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyptl7/building_an_app_that_builds_apps_feedback/",
          "stickied": false,
          "url": "https://i.redd.it/0t2fav6bfmcf1.jpeg",
          "subreddit_subscribers": 498850,
          "created_utc": 1752403499,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2q7ua5a8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyozcn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=65fdf939c9fee2a51d4ce7ef5c5a36373a796f6a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752400163,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "muthuishere.medium.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?auto=webp&amp;s=ef577e27fadc95bfe3b1744d7ac8872e6e554c70",
                  "width": 610,
                  "height": 936
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc6872452b4829cdf316ac0294b8c4c189b660af",
                    "width": 108,
                    "height": 165
                  },
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=389d4bc4bbd718d16dd11308be232fde6ab93a01",
                    "width": 216,
                    "height": 331
                  },
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6b426f7ec6446f2ee47476b0c04d150cea9b8a5",
                    "width": 320,
                    "height": 491
                  }
                ],
                "variants": {},
                "id": "1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyozcn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "muthuishere2101",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/",
          "stickied": false,
          "url": "https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b",
          "subreddit_subscribers": 498850,
          "created_utc": 1752400163,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}