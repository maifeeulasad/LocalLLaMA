{
  "kind": "Listing",
  "data": {
    "after": "t3_1mjyuv5",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any suggestions for notetaker tool like a plugin in Microsoft notebook where I can just record audio of my notes and it transcribes and summarizes etc?\n\nI have so many meetings each day that I cant record. After the meeting I want to record my summary before I forget. Any suggestiins on how to manage it?",
          "author_fullname": "t2_en8akaci",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Notetaker tool",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkhrs7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754615495,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions for notetaker tool like a plugin in Microsoft notebook where I can just record audio of my notes and it transcribes and summarizes etc?&lt;/p&gt;\n\n&lt;p&gt;I have so many meetings each day that I cant record. After the meeting I want to record my summary before I forget. Any suggestiins on how to manage it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkhrs7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Brother-2237",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkhrs7/notetaker_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkhrs7/notetaker_tool/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754615495,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Let's assume it's a 2Billion parameter model to fork\n\nI am curious what kind of compute and horsepower it would take to update an LLM with new information. \n\nYes, RAG/VectorDB's work as an interim step in ensuring valid responses, but the scenario I'm exploring has verified good data via fuzzy questions and returns accurate answers, so now we're ready to update it. \n\nWith that, what does the process look like?\n\n",
          "author_fullname": "t2_12koak",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does it take to regenerate or update a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkhgva",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754614655,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s assume it&amp;#39;s a 2Billion parameter model to fork&lt;/p&gt;\n\n&lt;p&gt;I am curious what kind of compute and horsepower it would take to update an LLM with new information. &lt;/p&gt;\n\n&lt;p&gt;Yes, RAG/VectorDB&amp;#39;s work as an interim step in ensuring valid responses, but the scenario I&amp;#39;m exploring has verified good data via fuzzy questions and returns accurate answers, so now we&amp;#39;re ready to update it. &lt;/p&gt;\n\n&lt;p&gt;With that, what does the process look like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkhgva",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "techtornado",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkhgva/what_does_it_take_to_regenerate_or_update_a_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkhgva/what_does_it_take_to_regenerate_or_update_a_model/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754614655,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I just got a new M4 Macbook in hopes of running models locally. The Qwen3:30b model takes 1-2 minutes to respond to SIMPLE requests (using chat-completions API through Ollama).\n\nThat's not just the first request, but each request. Is it really always this slow?\n\nMy stack for reference:  \n\\- Python script  \n\\- PydanticAI Agent  \n\\- Synchronous chat completions with simple question and output object\n\nOLLAMA\\_MAX\\_LOADED\\_MODELS=1  \nOLLAMA\\_CONTEXT\\_LENGTH=4096\n\nAm I doing something wrong? Why are these models so unworkably slow?",
          "author_fullname": "t2_1fmngmtx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it really this unbearably slow?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkhga1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754614608,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I just got a new M4 Macbook in hopes of running models locally. The Qwen3:30b model takes 1-2 minutes to respond to SIMPLE requests (using chat-completions API through Ollama).&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s not just the first request, but each request. Is it really always this slow?&lt;/p&gt;\n\n&lt;p&gt;My stack for reference:&lt;br/&gt;\n- Python script&lt;br/&gt;\n- PydanticAI Agent&lt;br/&gt;\n- Synchronous chat completions with simple question and output object&lt;/p&gt;\n\n&lt;p&gt;OLLAMA_MAX_LOADED_MODELS=1&lt;br/&gt;\nOLLAMA_CONTEXT_LENGTH=4096&lt;/p&gt;\n\n&lt;p&gt;Am I doing something wrong? Why are these models so unworkably slow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkhga1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shvyxxn",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkhga1/is_it_really_this_unbearably_slow/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkhga1/is_it_really_this_unbearably_slow/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754614608,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1uklydw3g2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI new open-source model is basically Phi-5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkhbs9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754614255,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "news.ycombinator.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://news.ycombinator.com/item?id=44828884",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mkhbs9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ik-when-that-hotline",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/",
          "stickied": false,
          "url": "https://news.ycombinator.com/item?id=44828884",
          "subreddit_subscribers": 513415,
          "created_utc": 1754614255,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "IT'S FULLY FUNCTIONAL TOO AND ISNT EVEN LOBOTOMIZED. Download it now before they take it down due to \"safety concerns\": https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/tree/main",
          "author_fullname": "t2_1tflldnxyz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "INSANE NEWS: FULLY UNCENSORED (abliterated) GPT OSS 20B NOW AVAILABLE ON HUGGINGFACE!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkh8qe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754614017,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;IT&amp;#39;S FULLY FUNCTIONAL TOO AND ISNT EVEN LOBOTOMIZED. Download it now before they take it down due to &amp;quot;safety concerns&amp;quot;: &lt;a href=\"https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/tree/main\"&gt;https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF/tree/main&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?auto=webp&amp;s=e218292ffeeb286451b680d4a561fd1da0df6d8c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcbbd0387dd8ac9b2f7f0fb4f258aade8378636b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1542424259385e9713df82d41658936838f99dfd",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1dd3d715c01ba930a04e431096f9eb1af736210",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8ef5dae58a1931f159f19948400500dc5e8110f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8336670c18f559983499054a334974b56d192c99",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49220accb219215b3165b31165096648f210592a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mkh8qe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DementedAndCute",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkh8qe/insane_news_fully_uncensored_abliterated_gpt_oss/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkh8qe/insane_news_fully_uncensored_abliterated_gpt_oss/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754614017,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They did it. The process of enshittification of AI has began. As soon as they release ChatGPT 5, they disable the o3.\n\nI normally run locally the GWEN and DS. But, specially on travels I used the o3. The new model is so, so, so bad. I won't pay U$200 just to get access to a model that probably is a new skin of o3.\n\nWe cannot trust the companies. From now I'll rely only in Local and create access to it as a particular server through tailscale. The major problem for me is the search, how you guys are doing it? Any setup to make it as useful as the search through o3? This is the main bottleneck for me.",
          "author_fullname": "t2_1hra1kibwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM is more important than never and improving local models with research.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkgy0t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754613192,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They did it. The process of enshittification of AI has began. As soon as they release ChatGPT 5, they disable the o3.&lt;/p&gt;\n\n&lt;p&gt;I normally run locally the GWEN and DS. But, specially on travels I used the o3. The new model is so, so, so bad. I won&amp;#39;t pay U$200 just to get access to a model that probably is a new skin of o3.&lt;/p&gt;\n\n&lt;p&gt;We cannot trust the companies. From now I&amp;#39;ll rely only in Local and create access to it as a particular server through tailscale. The major problem for me is the search, how you guys are doing it? Any setup to make it as useful as the search through o3? This is the main bottleneck for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkgy0t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent_Pin7635",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkgy0t/local_llm_is_more_important_than_never_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkgy0t/local_llm_is_more_important_than_never_and/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754613192,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I downloaded 2 8bit models (both use 32-33gb of ram)\n\nThe first one was Qwen3 30B A3B Instruct 2507 8bit. This model is much nicer it seems more \"Human like\" ie like a Nexus 6 vs a Nexus 4 etc.. The answers and modeled behaviors are much more interesting and personable. faster ie 72 tokens per second\n\nThe second one Qwen3 32B 8bit 8BIT seems more like just getting wikipedia answers, more of a formal Rigid feel to its behavior. slower ie less tokens per second 15 tokens per second.\n\nSo is the first one more advanced version? Why is it so different in how it behaves it defiantly is the one I will stick with.  Significantly nicer \"Attitude as well\" \n\nAnyhow this AI stuff is do damn interesting downloading more models to check out. I am using LM-Studio because it supports MLX.\n\nSo what's going on with these models?",
          "author_fullname": "t2_q4dwaqj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "two models big difference in how it converses/answers. ie Qwen3 30B A3B vs Qwen3 32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkgv1l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754612966,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded 2 8bit models (both use 32-33gb of ram)&lt;/p&gt;\n\n&lt;p&gt;The first one was Qwen3 30B A3B Instruct 2507 8bit. This model is much nicer it seems more &amp;quot;Human like&amp;quot; ie like a Nexus 6 vs a Nexus 4 etc.. The answers and modeled behaviors are much more interesting and personable. faster ie 72 tokens per second&lt;/p&gt;\n\n&lt;p&gt;The second one Qwen3 32B 8bit 8BIT seems more like just getting wikipedia answers, more of a formal Rigid feel to its behavior. slower ie less tokens per second 15 tokens per second.&lt;/p&gt;\n\n&lt;p&gt;So is the first one more advanced version? Why is it so different in how it behaves it defiantly is the one I will stick with.  Significantly nicer &amp;quot;Attitude as well&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;Anyhow this AI stuff is do damn interesting downloading more models to check out. I am using LM-Studio because it supports MLX.&lt;/p&gt;\n\n&lt;p&gt;So what&amp;#39;s going on with these models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkgv1l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "meshreplacer",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754612966,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "GPT 4.1/4o and other models always supported logprobs via the API, but with GPT-5 that capability seems to be gone! Try it yourself and you'll get the error `You are not allowed to request logprobs from this model`\n\n**What are logprobs?** Logprobs expose the probability distribution for each generated token. For the example “The dog chased the”, the next token might be “cat” (70%), “ball” (25%), or “squirrel” (5%). All LLMs have this capability internally, but closed providers like OpenAI and Anthropic sometimes choose to hide these from API users.\n\n**Why did OpenAI remove them?** Most likely to prevent model distillation. There's rumours other labs train on the outputs of OpenAI models (Anthropic even cut off OpenAI's API key, they don't seem to trust each other). While you can distill without logprobs, having access to the full probability distribution significantly improves distillation quality and training efficiency. I’d guess this is a move to prevent competitors from training on their GPT-5 model outputs.\n\n**Technical impact: Evals and G-Eval** The biggest loss (for me at least) is for evaluation workflows. G-Eval (Liu et al.) uses logprobs to weight judge outputs based on model confidence. Instead of binary pass/fail, you get calibrated scores. Consider a eval where the model is uncertain: 51% chance of pass and 49% chance of failure:\n\n* Classic LLM-judge: 51% confident → \"pass\" (binary)\n* G-Eval: 51% pass, 49% fail → 0.51 score (calibrated)\n\nIn the G-Eval paper consistently outperforms other eval techniques, and logprobs are required.\n\n**How we detected this** I build [Kiln](https://github.com/Kiln-AI/Kiln) \\- and open and free tool for evals, synthetic data gen, and fine-tuning. We run automated capability tests on every model before adding them. This makes it much easier to select the right model for a given task. Our logprobs/evals tests immediately caught this change. As far as I'm aware, this wasn't mentioned in any release notes (but I might have missed it).\n\nHere are details on the testing we run on every model to catch issues like this: [https://getkiln.ai/blog/i\\_wrote\\_2000\\_llm\\_test\\_cases\\_so\\_you\\_dont\\_have\\_to](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to)\n\nAnd here's our full model library with the results: [https://getkiln.ai/model\\_library](https://getkiln.ai/model_library)",
          "author_fullname": "t2_slbscky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-5 removed logprob support from the API - technical breakdown and implications",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkg7m7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754611174,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GPT 4.1/4o and other models always supported logprobs via the API, but with GPT-5 that capability seems to be gone! Try it yourself and you&amp;#39;ll get the error &lt;code&gt;You are not allowed to request logprobs from this model&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are logprobs?&lt;/strong&gt; Logprobs expose the probability distribution for each generated token. For the example “The dog chased the”, the next token might be “cat” (70%), “ball” (25%), or “squirrel” (5%). All LLMs have this capability internally, but closed providers like OpenAI and Anthropic sometimes choose to hide these from API users.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why did OpenAI remove them?&lt;/strong&gt; Most likely to prevent model distillation. There&amp;#39;s rumours other labs train on the outputs of OpenAI models (Anthropic even cut off OpenAI&amp;#39;s API key, they don&amp;#39;t seem to trust each other). While you can distill without logprobs, having access to the full probability distribution significantly improves distillation quality and training efficiency. I’d guess this is a move to prevent competitors from training on their GPT-5 model outputs.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical impact: Evals and G-Eval&lt;/strong&gt; The biggest loss (for me at least) is for evaluation workflows. G-Eval (Liu et al.) uses logprobs to weight judge outputs based on model confidence. Instead of binary pass/fail, you get calibrated scores. Consider a eval where the model is uncertain: 51% chance of pass and 49% chance of failure:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Classic LLM-judge: 51% confident → &amp;quot;pass&amp;quot; (binary)&lt;/li&gt;\n&lt;li&gt;G-Eval: 51% pass, 49% fail → 0.51 score (calibrated)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the G-Eval paper consistently outperforms other eval techniques, and logprobs are required.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How we detected this&lt;/strong&gt; I build &lt;a href=\"https://github.com/Kiln-AI/Kiln\"&gt;Kiln&lt;/a&gt; - and open and free tool for evals, synthetic data gen, and fine-tuning. We run automated capability tests on every model before adding them. This makes it much easier to select the right model for a given task. Our logprobs/evals tests immediately caught this change. As far as I&amp;#39;m aware, this wasn&amp;#39;t mentioned in any release notes (but I might have missed it).&lt;/p&gt;\n\n&lt;p&gt;Here are details on the testing we run on every model to catch issues like this: &lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to\"&gt;https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And here&amp;#39;s our full model library with the results: &lt;a href=\"https://getkiln.ai/model_library\"&gt;https://getkiln.ai/model_library&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?auto=webp&amp;s=23e4ff0dbe2d03ff352aea774053e4e9cdb80d20",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9815f077288b33817e75895d23e661f1193778",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df51b519d6d99631039f2563f587d4f7fb7f337",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=584735f7b916c00d422195a7ea012563d4e134db",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ceb01849b330103f92aaf6b1331cd97e415c722",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0594f7e041119a136f22914764b2a128e73d5ff",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=415b728bd16022b553cb45cb75a1a8fee65a2e5b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mkg7m7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "davernow",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkg7m7/gpt5_removed_logprob_support_from_the_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkg7m7/gpt5_removed_logprob_support_from_the_api/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754611174,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://i.redd.it/rxcnmlg4oohf1.gif\n\n\n\nGood ol' [https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/comment/mgz5fzo/](https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/comment/mgz5fzo/)\n\n  \n\n\n\n\n",
          "author_fullname": "t2_2c3bk2e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-5 experience so far",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "rxcnmlg4oohf1": {
              "status": "valid",
              "e": "AnimatedImage",
              "m": "image/gif",
              "p": [
                {
                  "y": 113,
                  "x": 108,
                  "u": "https://preview.redd.it/rxcnmlg4oohf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=da52de0365a799694f1e5426c294e2e1092960bb"
                },
                {
                  "y": 226,
                  "x": 216,
                  "u": "https://preview.redd.it/rxcnmlg4oohf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=08c36a43904c6661cd0429ffeaea9d9f6145e9a0"
                },
                {
                  "y": 335,
                  "x": 320,
                  "u": "https://preview.redd.it/rxcnmlg4oohf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=04b6a2d061dbc36f5132b73090cbbaf920b0d972"
                },
                {
                  "y": 670,
                  "x": 640,
                  "u": "https://preview.redd.it/rxcnmlg4oohf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=9df5bca1f70d71041ecbb5a603745eeb27ae19ac"
                }
              ],
              "s": {
                "y": 918,
                "gif": "https://i.redd.it/rxcnmlg4oohf1.gif",
                "mp4": "https://preview.redd.it/rxcnmlg4oohf1.gif?format=mp4&amp;s=c1c4e37ac96453355f5edf60f46cc1b3751a4bb6",
                "x": 876
              },
              "id": "rxcnmlg4oohf1"
            }
          },
          "name": "t3_1mkfsfr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/WxYpuogO6qVUqbW43MccmJHrl6dKXv_K-QgTv_MWjJ0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754610019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://i.redd.it/rxcnmlg4oohf1.gif\"&gt;https://i.redd.it/rxcnmlg4oohf1.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Good ol&amp;#39; &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/comment/mgz5fzo/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/comment/mgz5fzo/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mkfsfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "k4ch0w",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkfsfr/gpt5_experience_so_far/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkfsfr/gpt5_experience_so_far/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754610019,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "just a personal workload using a very limited mobile GPU",
          "author_fullname": "t2_4dq9edjj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using gpt-oss 20B for Text to SQL",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 41,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkfqyp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=140&amp;height=41&amp;crop=140:41,smart&amp;auto=webp&amp;s=7828bbe07a7c06e2d9dc6987decc1b3d3dc69076",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754609915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "datamonkeysite.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just a personal workload using a very limited mobile GPU&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://datamonkeysite.com/2025/08/07/using-gpt-oss-20b-for-text-to-sql/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?auto=webp&amp;s=10ccdb169422aec4d29417f39635803a9031e9b9",
                  "width": 1200,
                  "height": 356
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=51ba352f8268c362117b25bf4bfac11478b1d339",
                    "width": 108,
                    "height": 32
                  },
                  {
                    "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1f0448914d9964d41665190f390aadbe5523aec",
                    "width": 216,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c13be45c3ef11e25e01cb1b08d41d141595a3b54",
                    "width": 320,
                    "height": 94
                  },
                  {
                    "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=01afdf4a21ba636347f8d6f1a8cd64833b2c69e5",
                    "width": 640,
                    "height": 189
                  },
                  {
                    "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=484ef5a13b969dd42da5f01c6525075bb0b86fcd",
                    "width": 960,
                    "height": 284
                  },
                  {
                    "url": "https://external-preview.redd.it/FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51516f36e7f17d7493f9bc33112ee67911df09f0",
                    "width": 1080,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "FsAIAuFtGI8dg1lHRne8HaIXXWiGwfyPjF2R5PeI4ak"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkfqyp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mim722",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkfqyp/using_gptoss_20b_for_text_to_sql/",
          "stickied": false,
          "url": "https://datamonkeysite.com/2025/08/07/using-gpt-oss-20b-for-text-to-sql/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754609915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_e9jh97s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LiveBench now has GPT OSS 120b, and it's below ChatGPT-4o.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mkfahe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754608701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "livebench.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://livebench.ai",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mkfahe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chibop1",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkfahe/livebench_now_has_gpt_oss_120b_and_its_below/",
          "stickied": false,
          "url": "https://livebench.ai",
          "subreddit_subscribers": 513415,
          "created_utc": 1754608701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's very hardened against prompt injections, way more than GPT OSS 120B  \nHow do you even prompt inject a reasoning model?\n\nthank you",
          "author_fullname": "t2_98h2d4rp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you prompt inject GLM 4.5 Air? Any success?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkf60c",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754608385,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s very hardened against prompt injections, way more than GPT OSS 120B&lt;br/&gt;\nHow do you even prompt inject a reasoning model?&lt;/p&gt;\n\n&lt;p&gt;thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkf60c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DamiaHeavyIndustries",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkf60c/how_do_you_prompt_inject_glm_45_air_any_success/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkf60c/how_do_you_prompt_inject_glm_45_air_any_success/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754608385,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Please. I don’t care about pricing. The only API teir I care about is which model gets port 8000 or 8080. ",
          "author_fullname": "t2_ff7nnpab",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "To all GPT-5 posts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkf543",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 273,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 273,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qu20_eWmOEt5_xepPsJ8e_qHGSYWdsHyk3im7PUdu7g.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754608319,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please. I don’t care about pricing. The only API teir I care about is which model gets port 8000 or 8080. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8v08gwidjohf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?auto=webp&amp;s=d590c7e17206c427ab52c5e3c343da7c260f73e1",
                  "width": 1351,
                  "height": 863
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ae7c4a2e2455d197013117be0fd7925dc782ab5",
                    "width": 108,
                    "height": 68
                  },
                  {
                    "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d40a6e7220c3a71b4a57d56cc09cc758a816a0fb",
                    "width": 216,
                    "height": 137
                  },
                  {
                    "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e98b7772ac7214e29b8b605447aa965d466b6d6",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a549b4a6f64e891d2fe2035565f6d9915347c9d1",
                    "width": 640,
                    "height": 408
                  },
                  {
                    "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e0aafb4f0751970cff08e44c2df78c34593d692",
                    "width": 960,
                    "height": 613
                  },
                  {
                    "url": "https://preview.redd.it/8v08gwidjohf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=feba7e13aecf80db6e77d3b53562cd5ebef315ca",
                    "width": 1080,
                    "height": 689
                  }
                ],
                "variants": {},
                "id": "seIapgFcAvKZ0emWsJQsxeM4YdfHtwf_J2816NB0lLg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkf543",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Danny_Davitoe",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/",
          "stickied": false,
          "url": "https://i.redd.it/8v08gwidjohf1.jpeg",
          "subreddit_subscribers": 513415,
          "created_utc": 1754608319,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Distillation often stalls on VRAM and I/O. We evaluate a memory-first, zero-copy virtual array that enables out-of-core execution on commodity 24GB GPUs, reducing peak VRAM by 30–40% and improving throughput by ~2× vs dense-matmul baselines.\nRepo (with PDF benchmarks): https://github.com/ixu2486/memory_raid_engine\nEarly validation from r/LLM welcome; additional results/replications appreciated.\n",
          "author_fullname": "t2_1101lu7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[R] Memory-First Zero-Copy Arrays for LLM Distillation — Out-of-Core on 24GB VRAM (Repo + PDF)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkf21i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754608102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Distillation often stalls on VRAM and I/O. We evaluate a memory-first, zero-copy virtual array that enables out-of-core execution on commodity 24GB GPUs, reducing peak VRAM by 30–40% and improving throughput by ~2× vs dense-matmul baselines.\nRepo (with PDF benchmarks): &lt;a href=\"https://github.com/ixu2486/memory_raid_engine\"&gt;https://github.com/ixu2486/memory_raid_engine&lt;/a&gt;\nEarly validation from &lt;a href=\"/r/LLM\"&gt;r/LLM&lt;/a&gt; welcome; additional results/replications appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?auto=webp&amp;s=2c4854fd2a71e7030884e4c03900668fb8d1215a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b812c549cd5652d6eccdf443b34cb2a4af829d9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a169c2b88b019ac9ab3057ede638085cdeb9ec7",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0317e3708226aa27f7fadbfca8cfb3dde7926f6b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0cbf0c8f94bd32213de8024c9b7dac08a585cbd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d048ed5b62a12acc7bf1c3b5a7bd353fbf714eba",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89df8dd45178767f9e6f821fe6570a71287941c4",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "CWd5k7Ys-F0UHJCOfytKV5FYFpLrl2LGzBkNz-rwvp0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkf21i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "inhogon",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkf21i/r_memoryfirst_zerocopy_arrays_for_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkf21i/r_memoryfirst_zerocopy_arrays_for_llm/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754608102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Claude Code is amazing. But I run into their limits and need FOSS when I run out of tokens. What are the best FOSS models you all use? Thinking of Qwen Coder. How good is that at Vibe coding compared to Claude Code?",
          "author_fullname": "t2_1uubsxxfd3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best FOSS AI models for local vibe coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkerwz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754607368,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Claude Code is amazing. But I run into their limits and need FOSS when I run out of tokens. What are the best FOSS models you all use? Thinking of Qwen Coder. How good is that at Vibe coding compared to Claude Code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkerwz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Crierlon",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkerwz/best_foss_ai_models_for_local_vibe_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkerwz/best_foss_ai_models_for_local_vibe_coding/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754607368,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;format=pjpg&amp;auto=webp&amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616\n\n# Benchmarks\n\n     python3 benchmark_serving.py --backend openai --base-url \"http://127.0.0.1:11345\" --endpoint='/v1/completions' --model 'openai/gpt-oss-120b' --dataset-name random --num-prompts 20 --max-concurrency 3 --request-rate inf --random-input-len 2048 --random-output-len 4096\n\n# Results\n\n|Metric|Concurrency: 1|Concurrency: 3|Concurrency: 5|Concurrency: 8|\n|:-|:-|:-|:-|:-|\n|**Request Statistics**|||||\n|Successful requests|10|20|40|40|\n|Maximum request concurrency|1|3|5|8|\n|Benchmark duration (s)|83.21|89.46|160.30|126.58|\n|**Token Metrics**|||||\n|Total input tokens|20,325|40,805|81,603|81,603|\n|Total generated tokens|8,442|16,928|46,046|49,813|\n|**Throughput**|||||\n|Request throughput (req/s)|0.12|0.22|0.25|0.32|\n|Output token throughput (tok/s)|101.45|189.23|287.25|393.53|\n|Total token throughput (tok/s)|345.71|645.38|796.32|1,038.21|\n|**Time to First Token (TTFT)**|||||\n|Mean TTFT (ms)|787.62|51.83|59.78|881.60|\n|Median TTFT (ms)|614.22|51.08|58.83|655.81|\n|P99 TTFT (ms)|2,726.43|70.12|78.94|1,912.05|\n|**Time per Output Token (TPOT)**|||||\n|Mean TPOT (ms)|8.83|12.95|15.47|66.61|\n|Median TPOT (ms)|8.92|13.19|15.59|62.21|\n|P99 TPOT (ms)|9.33|13.59|17.61|191.42|\n|**Inter-token Latency (ITL)**|||||\n|Mean ITL (ms)|8.93|11.72|14.24|15.68|\n|Median ITL (ms)|8.80|12.29|14.58|12.92|\n|P99 ITL (ms)|11.42|13.73|16.26|16.50|\n\n# Dockerfile\n\nThis builds [https://github.com/zyongye/vllm/tree/rc1](https://github.com/zyongye/vllm/tree/rc1) .  \nWhich is behind this pull request [https://github.com/vllm-project/vllm/pull/22259](https://github.com/vllm-project/vllm/pull/22259)\n\n    FROM nvidia/cuda:12.8.1-devel-ubuntu24.04\n    \n    RUN apt update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt install -y python3.12 python3-pip git-core curl build-essential cmake &amp;&amp; apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n    \n    RUN pip install uv --break-system-packages\n    \n    RUN uv venv --python 3.12 --seed --directory / --prompt workspace workspace-lib\n    RUN echo \"source /workspace-lib/bin/activate\" &gt;&gt; /root/.bash_profile\n    \n    SHELL [ \"/bin/bash\", \"--login\", \"-c\" ]\n    \n    ENV UV_CONCURRENT_BUILDS=8\n    ENV TORCH_CUDA_ARCH_LIST=\"8.6\"\n    ENV UV_LINK_MODE=copy\n    \n    RUN mkdir -p /app/libs\n    \n    # absolutely required\n    RUN git clone https://github.com/openai/triton.git /app/libs/triton\n    WORKDIR /app/libs/triton\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -r python/requirements.txt\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -e . --verbose --no-build-isolation\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -e python/triton_kernels --no-deps\n    \n    RUN git clone -b rc1 --depth 1 https://github.com/zyongye/vllm.git /app/libs/vllm\n    WORKDIR /app/libs/vllm\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -r requirements/build.txt\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install flashinfer-python==0.2.10\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip uninstall pytorch-triton\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install triton==3.4.0 mcp openai_harmony \"transformers[torch]\"\n    #RUN --mount=type=cache,target=/root/.cache/uv uv pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128\n    # torch 2.8\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install torch torchvision\n    RUN python use_existing_torch.py\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install --no-build-isolation -e . -v\n    \n    COPY &lt;&lt;-\"EOF\" /app/entrypoint\n    #!/bin/bash\n    export VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1\n    export TORCH_CUDA_ARCH_LIST=8.6\n    source /workspace-lib/bin/activate\n    exec python3 -m vllm.entrypoints.openai.api_server --port 8080 \"$@\"\n    EOF\n    \n    RUN chmod +x /app/entrypoint\n    \n    EXPOSE 8080\n    \n    ENTRYPOINT [ \"/app/entrypoint\" ]\n\nbuild might take a while :\n\n    docker build -t vllmgpt . --progress plain\n\n# Running\n\nIf you have already downloaded the model from huggingface, you can mount it inside the container. If not, don't use the volume mount.\n\n    docker run -d --name vllmgpt -v $HOME/.cache/huggingface:/root/.cache/huggingface -p 8080:8080 --runtime nvidia --gpus all --ipc host vllmgpt --model openai/gpt-oss-120b --max-num-batched-tokens 4096 --gpu-memory-utilization 0.85 --max-num-seqs 8 --async-scheduling --max-model-len 32k --tensor-parallel-size 4\n\nThis will serve gpt-oss-120b on port 8080\n\nWith single concurrency, feeding 25K of tokens (quantum cryptography wiki article), results in vllm reporting :\n\nINFO 08-07 22:36:07 \\[loggers.py:123\\] Engine 000: **Avg prompt throughput: 2537.0 tokens/s**, Avg generation throughput: 81.7 tokens/s\n\nINFO 08-07 22:36:17 \\[loggers.py:123\\] Engine 000: Avg prompt throughput: 0.0 tokens/s, **Avg generation throughput: 94.4 tokens/s**",
          "author_fullname": "t2_bjiw45ny",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gpt-oss-120b running on 4x 3090 with vllm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "dn7v2owggohf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f082000683bcc7ed9185805441629c4c7acfa02"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d11ee240fb0b3d4ff61a3d9401ee669e96232c67"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fee94ab99ecb2762808e33bc8784fe8d82484cfb"
                },
                {
                  "y": 380,
                  "x": 640,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a548e47ddc6236fedf18ad4d372c3bdf5abf8c56"
                },
                {
                  "y": 570,
                  "x": 960,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0bc764bc5b19bfbe5b4bd6d7a0d978ea1f1bb8ab"
                },
                {
                  "y": 642,
                  "x": 1080,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98eff423585211a24e6912183ae3a4213d33f582"
                }
              ],
              "s": {
                "y": 910,
                "x": 1530,
                "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;format=pjpg&amp;auto=webp&amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616"
              },
              "id": "dn7v2owggohf1"
            }
          },
          "name": "t3_1mkefbx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/8aAMzJM7DyPf26iC2tcnkUfItMzXTNsIQ1vYJ3WLqE8.jpg",
          "edited": 1754607347,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754606483,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616\"&gt;https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Benchmarks&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt; python3 benchmark_serving.py --backend openai --base-url &amp;quot;http://127.0.0.1:11345&amp;quot; --endpoint=&amp;#39;/v1/completions&amp;#39; --model &amp;#39;openai/gpt-oss-120b&amp;#39; --dataset-name random --num-prompts 20 --max-concurrency 3 --request-rate inf --random-input-len 2048 --random-output-len 4096\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Metric&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 1&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 3&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 5&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 8&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Request Statistics&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Successful requests&lt;/td&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;td align=\"left\"&gt;40&lt;/td&gt;\n&lt;td align=\"left\"&gt;40&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Maximum request concurrency&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Benchmark duration (s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;83.21&lt;/td&gt;\n&lt;td align=\"left\"&gt;89.46&lt;/td&gt;\n&lt;td align=\"left\"&gt;160.30&lt;/td&gt;\n&lt;td align=\"left\"&gt;126.58&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Token Metrics&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Total input tokens&lt;/td&gt;\n&lt;td align=\"left\"&gt;20,325&lt;/td&gt;\n&lt;td align=\"left\"&gt;40,805&lt;/td&gt;\n&lt;td align=\"left\"&gt;81,603&lt;/td&gt;\n&lt;td align=\"left\"&gt;81,603&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Total generated tokens&lt;/td&gt;\n&lt;td align=\"left\"&gt;8,442&lt;/td&gt;\n&lt;td align=\"left\"&gt;16,928&lt;/td&gt;\n&lt;td align=\"left\"&gt;46,046&lt;/td&gt;\n&lt;td align=\"left\"&gt;49,813&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Request throughput (req/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.12&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.22&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.25&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.32&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Output token throughput (tok/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;101.45&lt;/td&gt;\n&lt;td align=\"left\"&gt;189.23&lt;/td&gt;\n&lt;td align=\"left\"&gt;287.25&lt;/td&gt;\n&lt;td align=\"left\"&gt;393.53&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Total token throughput (tok/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;345.71&lt;/td&gt;\n&lt;td align=\"left\"&gt;645.38&lt;/td&gt;\n&lt;td align=\"left\"&gt;796.32&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,038.21&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Time to First Token (TTFT)&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mean TTFT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;787.62&lt;/td&gt;\n&lt;td align=\"left\"&gt;51.83&lt;/td&gt;\n&lt;td align=\"left\"&gt;59.78&lt;/td&gt;\n&lt;td align=\"left\"&gt;881.60&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Median TTFT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.22&lt;/td&gt;\n&lt;td align=\"left\"&gt;51.08&lt;/td&gt;\n&lt;td align=\"left\"&gt;58.83&lt;/td&gt;\n&lt;td align=\"left\"&gt;655.81&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;P99 TTFT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;2,726.43&lt;/td&gt;\n&lt;td align=\"left\"&gt;70.12&lt;/td&gt;\n&lt;td align=\"left\"&gt;78.94&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,912.05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Time per Output Token (TPOT)&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mean TPOT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.83&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.95&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.47&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.61&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Median TPOT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.92&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.19&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.59&lt;/td&gt;\n&lt;td align=\"left\"&gt;62.21&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;P99 TPOT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.33&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.59&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.61&lt;/td&gt;\n&lt;td align=\"left\"&gt;191.42&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Inter-token Latency (ITL)&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mean ITL (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.93&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.72&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.24&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.68&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Median ITL (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.80&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.29&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.58&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.92&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;P99 ITL (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.42&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.73&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.26&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.50&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Dockerfile&lt;/h1&gt;\n\n&lt;p&gt;This builds &lt;a href=\"https://github.com/zyongye/vllm/tree/rc1\"&gt;https://github.com/zyongye/vllm/tree/rc1&lt;/a&gt; .&lt;br/&gt;\nWhich is behind this pull request &lt;a href=\"https://github.com/vllm-project/vllm/pull/22259\"&gt;https://github.com/vllm-project/vllm/pull/22259&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;FROM nvidia/cuda:12.8.1-devel-ubuntu24.04\n\nRUN apt update &amp;amp;&amp;amp; DEBIAN_FRONTEND=noninteractive apt install -y python3.12 python3-pip git-core curl build-essential cmake &amp;amp;&amp;amp; apt clean &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*\n\nRUN pip install uv --break-system-packages\n\nRUN uv venv --python 3.12 --seed --directory / --prompt workspace workspace-lib\nRUN echo &amp;quot;source /workspace-lib/bin/activate&amp;quot; &amp;gt;&amp;gt; /root/.bash_profile\n\nSHELL [ &amp;quot;/bin/bash&amp;quot;, &amp;quot;--login&amp;quot;, &amp;quot;-c&amp;quot; ]\n\nENV UV_CONCURRENT_BUILDS=8\nENV TORCH_CUDA_ARCH_LIST=&amp;quot;8.6&amp;quot;\nENV UV_LINK_MODE=copy\n\nRUN mkdir -p /app/libs\n\n# absolutely required\nRUN git clone https://github.com/openai/triton.git /app/libs/triton\nWORKDIR /app/libs/triton\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -r python/requirements.txt\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -e . --verbose --no-build-isolation\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -e python/triton_kernels --no-deps\n\nRUN git clone -b rc1 --depth 1 https://github.com/zyongye/vllm.git /app/libs/vllm\nWORKDIR /app/libs/vllm\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -r requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install flashinfer-python==0.2.10\nRUN --mount=type=cache,target=/root/.cache/uv uv pip uninstall pytorch-triton\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install triton==3.4.0 mcp openai_harmony &amp;quot;transformers[torch]&amp;quot;\n#RUN --mount=type=cache,target=/root/.cache/uv uv pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128\n# torch 2.8\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install torch torchvision\nRUN python use_existing_torch.py\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install --no-build-isolation -e . -v\n\nCOPY &amp;lt;&amp;lt;-&amp;quot;EOF&amp;quot; /app/entrypoint\n#!/bin/bash\nexport VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1\nexport TORCH_CUDA_ARCH_LIST=8.6\nsource /workspace-lib/bin/activate\nexec python3 -m vllm.entrypoints.openai.api_server --port 8080 &amp;quot;$@&amp;quot;\nEOF\n\nRUN chmod +x /app/entrypoint\n\nEXPOSE 8080\n\nENTRYPOINT [ &amp;quot;/app/entrypoint&amp;quot; ]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;build might take a while :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker build -t vllmgpt . --progress plain\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Running&lt;/h1&gt;\n\n&lt;p&gt;If you have already downloaded the model from huggingface, you can mount it inside the container. If not, don&amp;#39;t use the volume mount.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker run -d --name vllmgpt -v $HOME/.cache/huggingface:/root/.cache/huggingface -p 8080:8080 --runtime nvidia --gpus all --ipc host vllmgpt --model openai/gpt-oss-120b --max-num-batched-tokens 4096 --gpu-memory-utilization 0.85 --max-num-seqs 8 --async-scheduling --max-model-len 32k --tensor-parallel-size 4\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This will serve gpt-oss-120b on port 8080&lt;/p&gt;\n\n&lt;p&gt;With single concurrency, feeding 25K of tokens (quantum cryptography wiki article), results in vllm reporting :&lt;/p&gt;\n\n&lt;p&gt;INFO 08-07 22:36:07 [loggers.py:123] Engine 000: &lt;strong&gt;Avg prompt throughput: 2537.0 tokens/s&lt;/strong&gt;, Avg generation throughput: 81.7 tokens/s&lt;/p&gt;\n\n&lt;p&gt;INFO 08-07 22:36:17 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, &lt;strong&gt;Avg generation throughput: 94.4 tokens/s&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mkefbx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rolotamazzi",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754606483,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nI was curious so I decided to run some custom software to see what type of creative writing 20b could pull off. My opinion is that its creativity is much wider than the latest qwen. That one kept trying to insist we were going to be telling a ghost story. I ran the world building portion of the prompting with 20b and got three plausible interesting worlds that might be fun to explore. Chose one and had it generate five books. I like to test the long horizon capabilities while saturating the context windows. \n\nAnyway anyone else trying this with these models? I’m curious if you are getting the consistently repeating phrases I’m seeing. \n\n“You have potential” “you can’t do this alone” it’s always a green shield. It always brings up orchards. Etc\n\n\nYou can read the first three chapters on my LinkedIn article. I’m on my phone and LinkedIn doesn’t play well so I can’t copy it out easily. \n\nStory Summary: Mira Larkspur, driven by tremors from Mount Ardent, uncovers a glowing inscription that reveals an ancient fault line. Her journey to prevent a world-shattering sundering leads her to forge alliances with key figures from three distinct realms: the smith Durgan Ironhand, the elder Alarion Greenroot, and the sky-ship captain Riven Skyward. Together, they create a \"Stabilizer\" device by combining the unique magical disciplines of their realms—metal runes, leaf-breath resin, and aether-dust—to harness a crystal core. Despite sabotage attempts by the rogue caster Elias Thorn and the merchant Lydia Grey, Mira and her allies successfully use the device to seal the fissure. Her victory leads to the creation of the Guild of Balance, a new organization dedicated to safeguarding the world, with Mira elected as its first steward, ready to face new threats on the horizon.\n\nhttps://www.linkedin.com/pulse/openai-oss-20b-writing-jeremy-harper-ktnac",
          "author_fullname": "t2_1tk6u7slxe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Oss20b creative writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 71,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mke83e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/BVhGzHsqH7z7EmILaa6ILzeFdzh1pKSqtBevTokFO9U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754605973,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was curious so I decided to run some custom software to see what type of creative writing 20b could pull off. My opinion is that its creativity is much wider than the latest qwen. That one kept trying to insist we were going to be telling a ghost story. I ran the world building portion of the prompting with 20b and got three plausible interesting worlds that might be fun to explore. Chose one and had it generate five books. I like to test the long horizon capabilities while saturating the context windows. &lt;/p&gt;\n\n&lt;p&gt;Anyway anyone else trying this with these models? I’m curious if you are getting the consistently repeating phrases I’m seeing. &lt;/p&gt;\n\n&lt;p&gt;“You have potential” “you can’t do this alone” it’s always a green shield. It always brings up orchards. Etc&lt;/p&gt;\n\n&lt;p&gt;You can read the first three chapters on my LinkedIn article. I’m on my phone and LinkedIn doesn’t play well so I can’t copy it out easily. &lt;/p&gt;\n\n&lt;p&gt;Story Summary: Mira Larkspur, driven by tremors from Mount Ardent, uncovers a glowing inscription that reveals an ancient fault line. Her journey to prevent a world-shattering sundering leads her to forge alliances with key figures from three distinct realms: the smith Durgan Ironhand, the elder Alarion Greenroot, and the sky-ship captain Riven Skyward. Together, they create a &amp;quot;Stabilizer&amp;quot; device by combining the unique magical disciplines of their realms—metal runes, leaf-breath resin, and aether-dust—to harness a crystal core. Despite sabotage attempts by the rogue caster Elias Thorn and the merchant Lydia Grey, Mira and her allies successfully use the device to seal the fissure. Her victory leads to the creation of the Guild of Balance, a new organization dedicated to safeguarding the world, with Mira elected as its first steward, ready to face new threats on the horizon.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/pulse/openai-oss-20b-writing-jeremy-harper-ktnac\"&gt;https://www.linkedin.com/pulse/openai-oss-20b-writing-jeremy-harper-ktnac&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/be1mdlfecohf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?auto=webp&amp;s=09594d1942b0f79a934bfec0c49acfababf89303",
                  "width": 1206,
                  "height": 620
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56b76d29d8b1fe50f3e81cb74230c500e309bd10",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77f4544bff2d77c6ed849c2f5d30aeb3dff9a17f",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e924214f75dec4f7f458d2db8d3ddd9e5c1b78f5",
                    "width": 320,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da2c4f7db63119767e9d55eee818f60cfb65d94f",
                    "width": 640,
                    "height": 329
                  },
                  {
                    "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62a8e1b240bf722343a73ee2f0502b747609df36",
                    "width": 960,
                    "height": 493
                  },
                  {
                    "url": "https://preview.redd.it/be1mdlfecohf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fb63bc865c83e77cc84cb1be2772f978884bc40",
                    "width": 1080,
                    "height": 555
                  }
                ],
                "variants": {},
                "id": "2RCvtw6U8lqwBnMtNrViKkmWKU7TxEpWWQRBf6TsU50"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mke83e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Upbeat5840",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mke83e/oss20b_creative_writing/",
          "stickied": false,
          "url": "https://i.redd.it/be1mdlfecohf1.jpeg",
          "subreddit_subscribers": 513415,
          "created_utc": 1754605973,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is the thing, the expert layers run amazing on CPU  (\\~17T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .\n\nYou can offload just the attention layers to GPU  (requiring about 5GB of VRAM) for fast prefill.\n\n* KV cache for the sequence\n* Attention weights &amp; activations\n* Routing tables\n* LayerNorms and other “non-expert” parameters\n\nNo giant MLP weights are resident on the GPU, so memory use stays low.\n\nThis yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.\n\n64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)\n\n&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)\n\n&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)\n\nwith 5GB of vram usage!\n\nHonestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.",
          "author_fullname": "t2_69r67vj3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "120B runs awesome on just 8GB VRAM!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mke7ef",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754606345,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754605924,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU  (~17T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt;\n\n&lt;p&gt;You can offload just the attention layers to GPU  (requiring about 5GB of VRAM) for fast prefill.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;KV cache for the sequence&lt;/li&gt;\n&lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt;\n&lt;li&gt;Routing tables&lt;/li&gt;\n&lt;li&gt;LayerNorms and other “non-expert” parameters&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt;\n\n&lt;p&gt;This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt;\n\n&lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the &amp;#39;hot&amp;#39; experts in memory even if the whole model doesn&amp;#39;t fit in memory)&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;with 5GB of vram usage!&lt;/p&gt;\n\n&lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mke7ef",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wrong-Historian",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754605924,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When the censor is on a vacation 🌞🌊😎⛱ and the model actually gives an answer...",
          "author_fullname": "t2_qz1qjc86",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to live in whatever universe GPT-OSS 20B lives in...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkdvhu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MfQtCbdfrIyPLdc-xac8qBFpxnD_rFybXespKoHdqvo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754605090,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When the censor is on a vacation 🌞🌊😎⛱ and the model actually gives an answer...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/e8qyytri8ohf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/e8qyytri8ohf1.png?auto=webp&amp;s=e8c524e348a783165733a23a1950fcbc3ff601a4",
                  "width": 1919,
                  "height": 1988
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/e8qyytri8ohf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=582e1f0a85d935d8e14052b12110db4bb56a23ab",
                    "width": 108,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/e8qyytri8ohf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=338aca7e998da8b08ac6e51b3467a654a79f5c73",
                    "width": 216,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/e8qyytri8ohf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f148c402a8a2480a5851925f4b0756625d80416",
                    "width": 320,
                    "height": 331
                  },
                  {
                    "url": "https://preview.redd.it/e8qyytri8ohf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6584ec4e9864ba048f1be435d2237ef734f50541",
                    "width": 640,
                    "height": 663
                  },
                  {
                    "url": "https://preview.redd.it/e8qyytri8ohf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c09968c68d25eeef3b569b5fe6351eb164e5b95",
                    "width": 960,
                    "height": 994
                  },
                  {
                    "url": "https://preview.redd.it/e8qyytri8ohf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=04fea9123033ea078ec20e9337df97d9eb29c3ed",
                    "width": 1080,
                    "height": 1118
                  }
                ],
                "variants": {},
                "id": "coXVMK76j0tLbw4xpuZ1C5cJjEYDJYe34TUuDOJDqr0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mkdvhu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Cool-Chemical-5629",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkdvhu/i_want_to_live_in_whatever_universe_gptoss_20b/",
          "stickied": false,
          "url": "https://i.redd.it/e8qyytri8ohf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754605090,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://eqbench.com/creative\\_writing\\_longform.html](https://eqbench.com/creative_writing_longform.html)\n\nPerformance for gpt-5 is very similar to horizon-alpha &amp; horizon-beta, those being earlier checkpoints.\n\nGpt-5-chat-latest (the chat-tuned version that you get on chatgpt.com) performs a little differently, scoring lower than gpt-5 and writing much less verbosely. Less than half the length of gpt-5 outputs on average.\n\nLongform writing update: I added new instructions to help the judge notice &amp; punish overuse of incoherent metaphors, &amp; re-ran the leaderboard. It was becoming a problem with many frontier models converging on this slop.\n\nSome rank changes; now **Opus 4.1 is #1**\n\n\\### Samples\n\n**Creative writing:**\n\n[https://eqbench.com/results/creative-writing-v3/gpt-5-2025-08-07.html](https://eqbench.com/results/creative-writing-v3/gpt-5-2025-08-07.html)\n\n**Longform writing:**\n\n[https://eqbench.com/results/creative-writing-longform/claude-opus-4.1\\_longform\\_report.html](https://eqbench.com/results/creative-writing-longform/claude-opus-4.1_longform_report.html)\n\n[https://eqbench.com/results/creative-writing-longform/gpt-5-2025-08-07\\_longform\\_report.html](https://eqbench.com/results/creative-writing-longform/gpt-5-2025-08-07_longform_report.html)\n\n[https://eqbench.com/results/creative-writing-longform/gpt-5-chat-latest\\_longform\\_report.html](https://eqbench.com/results/creative-writing-longform/gpt-5-chat-latest_longform_report.html)\n\n[https://eqbench.com/results/creative-writing-longform/gpt-5-mini-2025-08-07\\_longform\\_report.html](https://eqbench.com/results/creative-writing-longform/gpt-5-mini-2025-08-07_longform_report.html)\n\n[https://eqbench.com/results/creative-writing-longform/gpt-5-nano-2025-08-07\\_longform\\_report.html](https://eqbench.com/results/creative-writing-longform/gpt-5-nano-2025-08-07_longform_report.html)",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "GPT-5 results on EQ-Bench + Opus 4.1 takes top spot on longform writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4w378tfw8ohf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/4w378tfw8ohf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=57ad38518ee9f2d6d513ae09280d1a30614b498a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/4w378tfw8ohf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f3e62c0f59c0623ddcb6bae6eb9f01e7c17610c9"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/4w378tfw8ohf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01f2dc1f5ecc172d27c0f57f06d75b679591dc8a"
                }
              ],
              "s": {
                "y": 1731,
                "x": 500,
                "u": "https://preview.redd.it/4w378tfw8ohf1.png?width=500&amp;format=png&amp;auto=webp&amp;s=c5795510740e910b1962ef251d6e58687d7e6b74"
              },
              "id": "4w378tfw8ohf1"
            },
            "onayfubx7ohf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 151,
                  "x": 108,
                  "u": "https://preview.redd.it/onayfubx7ohf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79a8ae23a27c2db307ed16f824b9b16bd0cca100"
                },
                {
                  "y": 303,
                  "x": 216,
                  "u": "https://preview.redd.it/onayfubx7ohf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c066766bcf9682cef533ba96ff209ae7050dfa86"
                },
                {
                  "y": 448,
                  "x": 320,
                  "u": "https://preview.redd.it/onayfubx7ohf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab574d612b3c3a59fdda5498a6da8c1a8f573614"
                },
                {
                  "y": 897,
                  "x": 640,
                  "u": "https://preview.redd.it/onayfubx7ohf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4cc340471314e30bdaad739447d83c7d19c7ec22"
                },
                {
                  "y": 1346,
                  "x": 960,
                  "u": "https://preview.redd.it/onayfubx7ohf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=01b9eee77c7c327499befc21d543ba318df7e76e"
                }
              ],
              "s": {
                "y": 1386,
                "x": 988,
                "u": "https://preview.redd.it/onayfubx7ohf1.png?width=988&amp;format=png&amp;auto=webp&amp;s=1467f399dad5a5a13e095aea384c2fc10c39f3cf"
              },
              "id": "onayfubx7ohf1"
            },
            "14446kav7ohf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/14446kav7ohf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2baeed598c832a73a7e400576cb479800eb866f"
                },
                {
                  "y": 217,
                  "x": 216,
                  "u": "https://preview.redd.it/14446kav7ohf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=65a8ebe95847fa5e1a2171fe03d606e888801c77"
                },
                {
                  "y": 322,
                  "x": 320,
                  "u": "https://preview.redd.it/14446kav7ohf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=be2ea4ff0d27586b422ee11ac6b72f1ba4f8a4ee"
                },
                {
                  "y": 644,
                  "x": 640,
                  "u": "https://preview.redd.it/14446kav7ohf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c183d6fa866a19489d5377219f7d85cc4e1d8b4f"
                },
                {
                  "y": 966,
                  "x": 960,
                  "u": "https://preview.redd.it/14446kav7ohf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1dab4909969c20eda0c2784638a88c1bf2db0411"
                },
                {
                  "y": 1086,
                  "x": 1080,
                  "u": "https://preview.redd.it/14446kav7ohf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fcf2d06c0cdc1b737f976ff6cb03def3065c8080"
                }
              ],
              "s": {
                "y": 1418,
                "x": 1409,
                "u": "https://preview.redd.it/14446kav7ohf1.png?width=1409&amp;format=png&amp;auto=webp&amp;s=df899ed6c6624d72246766195348d9f93774b180"
              },
              "id": "14446kav7ohf1"
            },
            "u3rj2uzw7ohf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 140,
                  "x": 108,
                  "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b8a892602dc60eb5b51da48a2c4ece7425dbffa"
                },
                {
                  "y": 280,
                  "x": 216,
                  "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccd5cd1ed99c734754be96be4677aeadb237cfbf"
                },
                {
                  "y": 415,
                  "x": 320,
                  "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=05be0cf13d3ddc6c22572335632d37c96abdea28"
                },
                {
                  "y": 831,
                  "x": 640,
                  "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=009907a9313eece0f9b81a781abcfe4fc7275cae"
                },
                {
                  "y": 1246,
                  "x": 960,
                  "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=565f869cbef090793ff54eec219ae042dbd77a06"
                },
                {
                  "y": 1402,
                  "x": 1080,
                  "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=601179f187fee70ce2fa35cdb993ee31a8d2aee4"
                }
              ],
              "s": {
                "y": 1874,
                "x": 1443,
                "u": "https://preview.redd.it/u3rj2uzw7ohf1.png?width=1443&amp;format=png&amp;auto=webp&amp;s=4a762f4a0676e113301cf889ca18063ef5a15cc7"
              },
              "id": "u3rj2uzw7ohf1"
            },
            "vuni5bpx7ohf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 129,
                  "x": 108,
                  "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb8ad51693abf486ddce29f42995f9297bd0055b"
                },
                {
                  "y": 259,
                  "x": 216,
                  "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e1a3898f2604e6973917664631ba0c5af370556"
                },
                {
                  "y": 384,
                  "x": 320,
                  "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=52761fa3fa4c488703f3d777360dad839f2d9838"
                },
                {
                  "y": 768,
                  "x": 640,
                  "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6280f2af9c07e9f3dab87ebb519c3e112bff27e6"
                },
                {
                  "y": 1152,
                  "x": 960,
                  "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdf0ba527118a8c69ff17999b19307481a5d7b37"
                },
                {
                  "y": 1296,
                  "x": 1080,
                  "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecc8ae2bf74bb0432bc321d0743bfb541c3fa956"
                }
              ],
              "s": {
                "y": 1582,
                "x": 1318,
                "u": "https://preview.redd.it/vuni5bpx7ohf1.png?width=1318&amp;format=png&amp;auto=webp&amp;s=12ff67ad6a59daac585ce92369d4919d6d4512ec"
              },
              "id": "vuni5bpx7ohf1"
            }
          },
          "name": "t3_1mkdu9r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": "transparent",
          "ups": 19,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "14446kav7ohf1",
                "id": 723542034
              },
              {
                "media_id": "u3rj2uzw7ohf1",
                "id": 723542035
              },
              {
                "media_id": "onayfubx7ohf1",
                "id": 723542036
              },
              {
                "media_id": "vuni5bpx7ohf1",
                "id": 723542037
              },
              {
                "media_id": "4w378tfw8ohf1",
                "id": 723542038
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jRjd_Zh5thfqHHyXwOn68BMwlkIkCBQx3w8W6QpsShA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":Llama:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/23w2nhjj1e9f1_t5_81eyvm/Llama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754605004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://eqbench.com/creative_writing_longform.html\"&gt;https://eqbench.com/creative_writing_longform.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Performance for gpt-5 is very similar to horizon-alpha &amp;amp; horizon-beta, those being earlier checkpoints.&lt;/p&gt;\n\n&lt;p&gt;Gpt-5-chat-latest (the chat-tuned version that you get on chatgpt.com) performs a little differently, scoring lower than gpt-5 and writing much less verbosely. Less than half the length of gpt-5 outputs on average.&lt;/p&gt;\n\n&lt;p&gt;Longform writing update: I added new instructions to help the judge notice &amp;amp; punish overuse of incoherent metaphors, &amp;amp; re-ran the leaderboard. It was becoming a problem with many frontier models converging on this slop.&lt;/p&gt;\n\n&lt;p&gt;Some rank changes; now &lt;strong&gt;Opus 4.1 is #1&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;### Samples&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Creative writing:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-v3/gpt-5-2025-08-07.html\"&gt;https://eqbench.com/results/creative-writing-v3/gpt-5-2025-08-07.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Longform writing:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-longform/claude-opus-4.1_longform_report.html\"&gt;https://eqbench.com/results/creative-writing-longform/claude-opus-4.1_longform_report.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-longform/gpt-5-2025-08-07_longform_report.html\"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-2025-08-07_longform_report.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-longform/gpt-5-chat-latest_longform_report.html\"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-chat-latest_longform_report.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-longform/gpt-5-mini-2025-08-07_longform_report.html\"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-mini-2025-08-07_longform_report.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-longform/gpt-5-nano-2025-08-07_longform_report.html\"&gt;https://eqbench.com/results/creative-writing-longform/gpt-5-nano-2025-08-07_longform_report.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mkdu9r",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":Llama:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mkdu9r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mkdu9r/gpt5_results_on_eqbench_opus_41_takes_top_spot_on/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mkdu9r",
          "subreddit_subscribers": 513415,
          "created_utc": 1754605004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Or will I have to use another platform? ",
          "author_fullname": "t2_8bjinxt0q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any way to add web search to LM Studio/Qwen3?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkdu26",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754604989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or will I have to use another platform? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkdu26",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Morteymer",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkdu26/any_way_to_add_web_search_to_lm_studioqwen3/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkdu26/any_way_to_add_web_search_to_lm_studioqwen3/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754604989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Created a docker image which uses FastAPI to host the React frontend and a python transformers backend to provide webcam footage analysis using Gemma 3n (E2B-it) in a fully offline and private manner.\n\nWas intended for Google's Gemma 3n contest on Kaggle, but due to a [weird UX pattern](https://www.kaggle.com/competitions/google-gemma-3n-hackathon/discussion/597695) they didn't consider it submitted... what an actual nightmare, lol! 💀\n\nConsidering Google's judges won't be checking it out I'd appreciate your opinions on it instead, cheers!",
          "author_fullname": "t2_18xvxq5d7l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GitHub - grctest/g3n-fastapi-webcam-docker: Utilizing multiple Gemma 3n agents to analyze webcam footage! (MIT licensed)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkdl6x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=d89194f5dfc33cfed98864add0383ba490cd2530",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754604376,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Created a docker image which uses FastAPI to host the React frontend and a python transformers backend to provide webcam footage analysis using Gemma 3n (E2B-it) in a fully offline and private manner.&lt;/p&gt;\n\n&lt;p&gt;Was intended for Google&amp;#39;s Gemma 3n contest on Kaggle, but due to a &lt;a href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon/discussion/597695\"&gt;weird UX pattern&lt;/a&gt; they didn&amp;#39;t consider it submitted... what an actual nightmare, lol! 💀&lt;/p&gt;\n\n&lt;p&gt;Considering Google&amp;#39;s judges won&amp;#39;t be checking it out I&amp;#39;d appreciate your opinions on it instead, cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/grctest/g3n-fastapi-webcam-docker",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?auto=webp&amp;s=a1e0584a2a8bb18e8bea8d65b57b3e9cfd42904b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f853ddda388018ba93e7373f9b9004643d45448",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c278e0aef2fc6092d605a3845072a3787856a0ec",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=627b24342bfaee6969b6202f7b731822ffd3f1ca",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3497f64468f3d1848c389a962e6498b3de23c480",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=84188e829310b876044d1143ab8d7b8f4814bb54",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b620efbc7a39fa1ffa818c066ea14ae3ad56487",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FtDTcbQueInh6JQ6MpceHCcmGfK0jpjhCOy3jzZ5F_I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mkdl6x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ufos1111",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkdl6x/github_grctestg3nfastapiwebcamdocker_utilizing/",
          "stickied": false,
          "url": "https://github.com/grctest/g3n-fastapi-webcam-docker",
          "subreddit_subscribers": 513415,
          "created_utc": 1754604376,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to compare some models for on an italian medical quiz benchmark (with text and some images as well for vision models) I'm creating and I'm looking for suggestions, both open and closed source. \n\nMedgemma is a must, then the most important families of models: gemini from pro to flash-lite, open AI new gpt5 and oss models, R1 and V3, but after this I'm unsure. \n\nI think I'm gonna skip anthropic for now since those are code focused and not that cheap.\n\nWhat qwen models do you reccomend? Also, GLM-4.5 yes or no?\n\nOther less known models?\n\nI will share all results here. Thank you all",
          "author_fullname": "t2_15qzm1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reccomendation for new medical benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkd3t1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754603207,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to compare some models for on an italian medical quiz benchmark (with text and some images as well for vision models) I&amp;#39;m creating and I&amp;#39;m looking for suggestions, both open and closed source. &lt;/p&gt;\n\n&lt;p&gt;Medgemma is a must, then the most important families of models: gemini from pro to flash-lite, open AI new gpt5 and oss models, R1 and V3, but after this I&amp;#39;m unsure. &lt;/p&gt;\n\n&lt;p&gt;I think I&amp;#39;m gonna skip anthropic for now since those are code focused and not that cheap.&lt;/p&gt;\n\n&lt;p&gt;What qwen models do you reccomend? Also, GLM-4.5 yes or no?&lt;/p&gt;\n\n&lt;p&gt;Other less known models?&lt;/p&gt;\n\n&lt;p&gt;I will share all results here. Thank you all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkd3t1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sebastianmicu24",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754603207,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve upgraded from my M1 MacBook to an M4 Pro (14 CPU/20 GPU, 48gb RAM) and I’d like to get some local AI workflows going. \n\nI’m looking at usage tasks, not “AI development”. Document analysis, summarization, note taking/organization, web search agent for research, etc. Maybe some light code assistance (mainly with Python, specifically circuit Python), and probably also multimodal tasks, including image generation. I’m also do some hobbyist level CAD work, so if anyone has any suggestions for tools in that space I’d appreciate those, too, but I know that’s a bit niche. \n\nI’ve messed around with some local tools back about a year/18 months ago, but my old Mac was lacking in RAM and power, and my gaming PC has a pretty middling GPU (2070, 8gb VRAM) plus I vastly prefer working on my Mac. \n\nI’m to understand that there are now quants that are tailored for performance on Apple silicon to leverage the neural processors as well as the GPUs? \n\nWhat formats would and inference engine, and associated front end, would you all recommend? I generally keep my phone connected to my home network via VPN, so it would be handy to be able to interact with a chat agent from an iOS app or web ui from my iPhone. EDIT: specific context here, I have paid for ChatGPT plus from time to time, and the sort of usage I’m looking for in the context of a mobile chat agent is like general knowledge requests, brainstorming assistant, etc. \n\nAre there models that are currently preferred for these kind of tasks that would work well on this system without completely killing performance? \n\nI generally prefer open source product, but don’t mind paying for software and I’ll use the best options (within reason). \n\nSorry for my long winded-ness, but I appreciate any feedback, even if it’s just a YouTuber or blogger that you’d recommend I read for myself. Thanks!",
          "author_fullname": "t2_31brh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Upgraded my Mac, what are the current community preferred workflows and tools?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkd0bk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754608155,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754602967,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve upgraded from my M1 MacBook to an M4 Pro (14 CPU/20 GPU, 48gb RAM) and I’d like to get some local AI workflows going. &lt;/p&gt;\n\n&lt;p&gt;I’m looking at usage tasks, not “AI development”. Document analysis, summarization, note taking/organization, web search agent for research, etc. Maybe some light code assistance (mainly with Python, specifically circuit Python), and probably also multimodal tasks, including image generation. I’m also do some hobbyist level CAD work, so if anyone has any suggestions for tools in that space I’d appreciate those, too, but I know that’s a bit niche. &lt;/p&gt;\n\n&lt;p&gt;I’ve messed around with some local tools back about a year/18 months ago, but my old Mac was lacking in RAM and power, and my gaming PC has a pretty middling GPU (2070, 8gb VRAM) plus I vastly prefer working on my Mac. &lt;/p&gt;\n\n&lt;p&gt;I’m to understand that there are now quants that are tailored for performance on Apple silicon to leverage the neural processors as well as the GPUs? &lt;/p&gt;\n\n&lt;p&gt;What formats would and inference engine, and associated front end, would you all recommend? I generally keep my phone connected to my home network via VPN, so it would be handy to be able to interact with a chat agent from an iOS app or web ui from my iPhone. EDIT: specific context here, I have paid for ChatGPT plus from time to time, and the sort of usage I’m looking for in the context of a mobile chat agent is like general knowledge requests, brainstorming assistant, etc. &lt;/p&gt;\n\n&lt;p&gt;Are there models that are currently preferred for these kind of tasks that would work well on this system without completely killing performance? &lt;/p&gt;\n\n&lt;p&gt;I generally prefer open source product, but don’t mind paying for software and I’ll use the best options (within reason). &lt;/p&gt;\n\n&lt;p&gt;Sorry for my long winded-ness, but I appreciate any feedback, even if it’s just a YouTuber or blogger that you’d recommend I read for myself. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mkd0bk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrgreen4242",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkd0bk/upgraded_my_mac_what_are_the_current_community/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkd0bk/upgraded_my_mac_what_are_the_current_community/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754602967,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think OpenAI released GPT-OSS, a barely usable model, fully aware it would generate backlash once freely tested. But they also had in mind that releasing GPT-5 immediately afterward would divert all attention away from their low-effort model. In this way, they can defend themselves against criticism that they’re not committed to the open-source space, without having to face the consequences of releasing a joke of a model. Classic corporate behavior.\nAnd that concludes my rant.",
          "author_fullname": "t2_5hyfmu1b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI open washing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkcwiv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 103,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 103,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754602715,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think OpenAI released GPT-OSS, a barely usable model, fully aware it would generate backlash once freely tested. But they also had in mind that releasing GPT-5 immediately afterward would divert all attention away from their low-effort model. In this way, they can defend themselves against criticism that they’re not committed to the open-source space, without having to face the consequences of releasing a joke of a model. Classic corporate behavior.\nAnd that concludes my rant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkcwiv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gwyngwynsituation",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754602715,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://x.com/Yuhu\\_ai\\_/status/1953551132921671712](https://x.com/Yuhu_ai_/status/1953551132921671712)\n\nGrok4 world’s first unified model, and crushing GPT5 in benchmarks like ARC-AGI. [u/OpenAI](https://x.com/OpenAI) is a very respectful competitor and still the leader in many, but we’re fast and relentless. Many new models to share in the next few weeks!",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "xAI says new models in the next few weeks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkcwfa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.15,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754602709,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Yuhu_ai_/status/1953551132921671712\"&gt;https://x.com/Yuhu_ai_/status/1953551132921671712&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Grok4 world’s first unified model, and crushing GPT5 in benchmarks like ARC-AGI. &lt;a href=\"https://x.com/OpenAI\"&gt;u/OpenAI&lt;/a&gt; is a very respectful competitor and still the leader in many, but we’re fast and relentless. Many new models to share in the next few weeks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkcwfa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkcwfa/xai_says_new_models_in_the_next_few_weeks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkcwfa/xai_says_new_models_in_the_next_few_weeks/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754602709,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=1144&amp;format=png&amp;auto=webp&amp;s=3b09b588e1a2caad86a7b2dc9823eda90759c28f\n\nI feel like we need a tool that keeps track of each model and what it’s good at",
          "author_fullname": "t2_4hbtx6n9d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "we need a tool that keeps track of each model and what it’s good at!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "cqr0q0mvwnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30ac5c79a589f00601586098ec3bd315e631a2cb"
                },
                {
                  "y": 158,
                  "x": 216,
                  "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2325e5a9367bcf9df5063f8632e79705146d895d"
                },
                {
                  "y": 234,
                  "x": 320,
                  "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c86496f9e786899355c9190bc21942475e4c44f0"
                },
                {
                  "y": 469,
                  "x": 640,
                  "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f47b1f1e7d09efe6dd2f416060d611615fc7ca3"
                },
                {
                  "y": 704,
                  "x": 960,
                  "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=57001a1a179da9e23e4a07c176c4858a02089828"
                },
                {
                  "y": 793,
                  "x": 1080,
                  "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=232c484076bae3a87f2d8db7126b9ce94f8e689d"
                }
              ],
              "s": {
                "y": 840,
                "x": 1144,
                "u": "https://preview.redd.it/cqr0q0mvwnhf1.png?width=1144&amp;format=png&amp;auto=webp&amp;s=3b09b588e1a2caad86a7b2dc9823eda90759c28f"
              },
              "id": "cqr0q0mvwnhf1"
            }
          },
          "name": "t3_1mkc4lk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/KgI98KgJCbu6E-k5A2OExx8v-2Ptp41bD9ZOK3zWh74.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754600885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/cqr0q0mvwnhf1.png?width=1144&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b09b588e1a2caad86a7b2dc9823eda90759c28f\"&gt;https://preview.redd.it/cqr0q0mvwnhf1.png?width=1144&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b09b588e1a2caad86a7b2dc9823eda90759c28f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I feel like we need a tool that keeps track of each model and what it’s good at&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkc4lk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "isaak_ai",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkc4lk/we_need_a_tool_that_keeps_track_of_each_model_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkc4lk/we_need_a_tool_that_keeps_track_of_each_model_and/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754600885,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like to perform some STS tasks on my MacBook Pro (M4 Pro chip). Based on the leaderboard at [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard), it seems that Qwen 3 is the leader, so I wanted to set it up. However, I problem with the  `SentenceTransformer(\"mlx-community/Qwen3-Embedding-4B-4bit-DWQ\")`\n\nI received the following error:  \n\n    File ~/miniconda3/envs/ds/lib/python3.11/site-packages/transformers/quantizers/auto.py:244, in AutoHfQuantizer.supports_quant_method(quantization_config_dict)\n        242     quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n        243 elif quant_method is None:\n    --&gt; 244     raise ValueError(\n        245         \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n        246     )\n        248 if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING:\n        249     logger.warning(\n        250         f\"Unknown quantization type, got {quant_method} - supported types are:\"\n        251         f\" {list(AUTO_QUANTIZER_MAPPING.keys())}. Hence, we will skip the quantization. \"\n        252         \"To remove the warning, you can delete the quantization_config attribute in config.json\"\n        253     )\n    \n    **ValueError:** The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized.  \n\n\n\nDoes anyone have any ideas on how to set this up (fix the error or create a quantized version that works).\n\n",
          "author_fullname": "t2_82klp24i5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Semantic Textual Similarity on Apple Silicon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkby4r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754600478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to perform some STS tasks on my MacBook Pro (M4 Pro chip). Based on the leaderboard at &lt;a href=\"https://huggingface.co/spaces/mteb/leaderboard\"&gt;https://huggingface.co/spaces/mteb/leaderboard&lt;/a&gt;, it seems that Qwen 3 is the leader, so I wanted to set it up. However, I problem with the  &lt;code&gt;SentenceTransformer(&amp;quot;mlx-community/Qwen3-Embedding-4B-4bit-DWQ&amp;quot;)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I received the following error:  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;File ~/miniconda3/envs/ds/lib/python3.11/site-packages/transformers/quantizers/auto.py:244, in AutoHfQuantizer.supports_quant_method(quantization_config_dict)\n    242     quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n    243 elif quant_method is None:\n--&amp;gt; 244     raise ValueError(\n    245         &amp;quot;The model&amp;#39;s quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized&amp;quot;\n    246     )\n    248 if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING:\n    249     logger.warning(\n    250         f&amp;quot;Unknown quantization type, got {quant_method} - supported types are:&amp;quot;\n    251         f&amp;quot; {list(AUTO_QUANTIZER_MAPPING.keys())}. Hence, we will skip the quantization. &amp;quot;\n    252         &amp;quot;To remove the warning, you can delete the quantization_config attribute in config.json&amp;quot;\n    253     )\n\n**ValueError:** The model&amp;#39;s quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized.  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Does anyone have any ideas on how to set this up (fix the error or create a quantized version that works).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?auto=webp&amp;s=2c5b27b819fcf9d302ea9223ff63779967c158ee",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bca5092323f107110de703666d0987ca51bedba1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=358d7917ee14c30039ae1797ffd0ca1d9fe68d82",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c373fac417435224291450b1bfdd231978005f7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c4fb1a0d407644b871f4c6fd852f1d65ac7b457",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=09ef247e453733c039205a9236d582047820cf4c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb5439e321758db220736b4ddd80f3cdab962692",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "hINCyazmugT5nd39NF13gjbN1S3l4nlzHPyy65fQcLI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkby4r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "holdvacs",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkby4r/semantic_textual_similarity_on_apple_silicon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkby4r/semantic_textual_similarity_on_apple_silicon/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754600478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend a lot of time making private benchmarks for my real world use cases. It's extremely important to create your own unique benchmark for the specific tasks you will be using ai for, but we all know it's helpful to look at other benchmarks too. I think we've all found many benchmarks to not mean much in the real world, but I've found 2 benchmarks that when combined correlate accurately to real world intelligence and capability.\n\nFirst lets start with livebench.ai . Besides livebench.ai 's  coding benchmark, which I always turn off when looking at the total average scores, their total average score is often very accurate to real world use cases. All of their benchmarks combined into one average score tell a great story for how capable the model is. However, the only way that Livebench lacks is that it seems to only test at very short context lengths.\n\nThis is where another benchmark comes in, [https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87](https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87) From a website about fiction writing and while it's not a super serious website, it is the best benchmark for real world long context. No one comes close. For example, I noticed  Sonnet 4 performing much better than Opus 4 on context windows over 4,000 words. ONLY the Fiction Live benchmark reliably shows real world long context performance like this.\n\nTo estimate real world intelligence, I've found it very accurate to combine the results of both:\n\n\\- \"Fiction Live\": [https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87](https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87)\n\n\\- \"Livebench\": [https://livebench.ai](https://livebench.ai)\n\nFor models that many people run locally, not enough are represented on Livebench or Fiction Live. For example, GPT OSS 20b has not been tested on these benchmarks and it will likely be one of the most widely used open source models ever. \n\nLivebench seems to have a responsive github. We should make posts politely asking for more models to be tested.\n\nLivebench github: [https://github.com/LiveBench/LiveBench/issues](https://github.com/LiveBench/LiveBench/issues)\n\nAlso on X, u/bindureddy runs the benchmark and is even more responsive to comments. I think we should make an effort to express that we want more models tested. It's totally worth trying!\n\nFYI I wrote this by hand because I'm so passionate about benchmarks, no ai lol.",
          "author_fullname": "t2_igdar",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The best benchmarks!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkbs5l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754600094,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend a lot of time making private benchmarks for my real world use cases. It&amp;#39;s extremely important to create your own unique benchmark for the specific tasks you will be using ai for, but we all know it&amp;#39;s helpful to look at other benchmarks too. I think we&amp;#39;ve all found many benchmarks to not mean much in the real world, but I&amp;#39;ve found 2 benchmarks that when combined correlate accurately to real world intelligence and capability.&lt;/p&gt;\n\n&lt;p&gt;First lets start with livebench.ai . Besides livebench.ai &amp;#39;s  coding benchmark, which I always turn off when looking at the total average scores, their total average score is often very accurate to real world use cases. All of their benchmarks combined into one average score tell a great story for how capable the model is. However, the only way that Livebench lacks is that it seems to only test at very short context lengths.&lt;/p&gt;\n\n&lt;p&gt;This is where another benchmark comes in, &lt;a href=\"https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87\"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt; From a website about fiction writing and while it&amp;#39;s not a super serious website, it is the best benchmark for real world long context. No one comes close. For example, I noticed  Sonnet 4 performing much better than Opus 4 on context windows over 4,000 words. ONLY the Fiction Live benchmark reliably shows real world long context performance like this.&lt;/p&gt;\n\n&lt;p&gt;To estimate real world intelligence, I&amp;#39;ve found it very accurate to combine the results of both:&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;Fiction Live&amp;quot;: &lt;a href=\"https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87\"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- &amp;quot;Livebench&amp;quot;: &lt;a href=\"https://livebench.ai\"&gt;https://livebench.ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For models that many people run locally, not enough are represented on Livebench or Fiction Live. For example, GPT OSS 20b has not been tested on these benchmarks and it will likely be one of the most widely used open source models ever. &lt;/p&gt;\n\n&lt;p&gt;Livebench seems to have a responsive github. We should make posts politely asking for more models to be tested.&lt;/p&gt;\n\n&lt;p&gt;Livebench github: &lt;a href=\"https://github.com/LiveBench/LiveBench/issues\"&gt;https://github.com/LiveBench/LiveBench/issues&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also on X, &lt;a href=\"/u/bindureddy\"&gt;u/bindureddy&lt;/a&gt; runs the benchmark and is even more responsive to comments. I think we should make an effort to express that we want more models tested. It&amp;#39;s totally worth trying!&lt;/p&gt;\n\n&lt;p&gt;FYI I wrote this by hand because I&amp;#39;m so passionate about benchmarks, no ai lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkbs5l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mr-Barack-Obama",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkbs5l/the_best_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkbs5l/the_best_benchmarks/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754600094,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Happening at r/ChatGPT at 11AM PT: https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5\\_ama\\_with\\_openais\\_sam\\_altman\\_and\\_some\\_of\\_the/",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-5 AMA with OpenAI’s Sam Altman and some of the GPT-5 team",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkbimk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.52,
          "author_flair_background_color": "transparent",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ZJvlbAQcxqQ02Vl8TKW7C888MInouHVRP2I9IJM9748.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754599474,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happening at &lt;a href=\"/r/ChatGPT\"&gt;r/ChatGPT&lt;/a&gt; at 11AM PT: &lt;a href=\"https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5%5C_ama%5C_with%5C_openais%5C_sam%5C_altman%5C_and%5C_some%5C_of%5C_the/\"&gt;https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5\\_ama\\_with\\_openais\\_sam\\_altman\\_and\\_some\\_of\\_the/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/coimzlbysnhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/coimzlbysnhf1.png?auto=webp&amp;s=d32d2bfcf86034ae917e22f9b1a824e293565a76",
                  "width": 1120,
                  "height": 584
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/coimzlbysnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=462180d0d43fa32ef3934948b8bccee78ba9fc9e",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/coimzlbysnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fa8b7dd11f9e88d9ff5741dd7567586a2dbf5b4",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://preview.redd.it/coimzlbysnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a570703be4e60ca4f3b516454d3d8ac6b151168",
                    "width": 320,
                    "height": 166
                  },
                  {
                    "url": "https://preview.redd.it/coimzlbysnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=74c1e57780001d37e609616ca25f0d1dcd768d51",
                    "width": 640,
                    "height": 333
                  },
                  {
                    "url": "https://preview.redd.it/coimzlbysnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=91725eba8452175c82bb0238610162fcb7391546",
                    "width": 960,
                    "height": 500
                  },
                  {
                    "url": "https://preview.redd.it/coimzlbysnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22c8b16fd4cd89216d62061221cc509a43c1c138",
                    "width": 1080,
                    "height": 563
                  }
                ],
                "variants": {},
                "id": "fo1wTetpXSTjUdQzf_QE1nlbuNYicP_YM8kGCC1ujUs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mkbimk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mkbimk/gpt5_ama_with_openais_sam_altman_and_some_of_the/",
          "stickied": false,
          "url": "https://i.redd.it/coimzlbysnhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754599474,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like we have a new king. How has it been your experience using GPT5? For me, I use it mainly through cursor and it feels super slow, not because of the throughput of tokens but because it just thinks too much.\n\nSometimes I prefer to have a good enough model that is super fast. Do you have any examples where GPT-5 still fails at your tasks? Any things it unlocked?",
          "author_fullname": "t2_57ab9gss",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT‑5 &gt; Grok‑4 &gt; Opus 4.1",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkbdqf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.51,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bLD2I0HvTAcmv1D6_qnYi4ccnY2P40WaJVDQCua8Tkg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754599161,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like we have a new king. How has it been your experience using GPT5? For me, I use it mainly through cursor and it feels super slow, not because of the throughput of tokens but because it just thinks too much.&lt;/p&gt;\n\n&lt;p&gt;Sometimes I prefer to have a good enough model that is super fast. Do you have any examples where GPT-5 still fails at your tasks? Any things it unlocked?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/aiejp51i7nhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/aiejp51i7nhf1.png?auto=webp&amp;s=99affaef428be90ba2e710512f5ce0eab124b58f",
                  "width": 1352,
                  "height": 992
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/aiejp51i7nhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f840f18a262d12d7f1ee78183d899d45282b3b2",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/aiejp51i7nhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bcde5b0f68293ee398f331f6d044487ccf130d0",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://preview.redd.it/aiejp51i7nhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8aae50ab62937fd36ea72c1829b59cb6656b3cc2",
                    "width": 320,
                    "height": 234
                  },
                  {
                    "url": "https://preview.redd.it/aiejp51i7nhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=678b1867707a7920733d3095ebeef42e8eb9010d",
                    "width": 640,
                    "height": 469
                  },
                  {
                    "url": "https://preview.redd.it/aiejp51i7nhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fa3b7309b841f3b951c0c5bbe434b628f24bb70",
                    "width": 960,
                    "height": 704
                  },
                  {
                    "url": "https://preview.redd.it/aiejp51i7nhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d80e90bcaf98af9843f3fd22f6d474e94f855e1",
                    "width": 1080,
                    "height": 792
                  }
                ],
                "variants": {},
                "id": "klcD0WhbMqtFjIY6fS-Zj3WTyUJ623x5OPBXJ6CB54s"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkbdqf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Odd_Tumbleweed574",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkbdqf/gpt5_grok4_opus_41/",
          "stickied": false,
          "url": "https://i.redd.it/aiejp51i7nhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754599161,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dy4xt4i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "[Update] My macOS dictation replacement using local Whisper - Added YouTube &amp; file transcription, all runs locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "1wfpiimrpnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/1wfpiimrpnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fbb1b87465f0e36994572a7d065497bf51ae21d"
                },
                {
                  "y": 316,
                  "x": 216,
                  "u": "https://preview.redd.it/1wfpiimrpnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=580314022e43f14b0451b9acd6483f2c76ecb687"
                },
                {
                  "y": 468,
                  "x": 320,
                  "u": "https://preview.redd.it/1wfpiimrpnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=698e0150c14c0bbf674b072b99e8cd876198c950"
                },
                {
                  "y": 937,
                  "x": 640,
                  "u": "https://preview.redd.it/1wfpiimrpnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=829172ec7fee113e8125eb3dff2a410042f5f612"
                }
              ],
              "s": {
                "y": 1154,
                "x": 788,
                "u": "https://preview.redd.it/1wfpiimrpnhf1.png?width=788&amp;format=png&amp;auto=webp&amp;s=610bb3fa93661adc1cde852f0386e575803bd490"
              },
              "id": "1wfpiimrpnhf1"
            },
            "3ytam03tnnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e66670106d1c6f711c25f7696428bf1a8c5562d2"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab74136a9908059408581d180c603e80f9ba6d89"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=197725714e9b44abc5e77ceccc04f85e582147c6"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39d66dc1ff1da4c1f499922aa769a7e9e59fdf3d"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7a163838dac6a608b0bcf0601bca90c371bed12"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b64437c9eeb2b7612d4c4833c9267da5a5a7acf7"
                }
              ],
              "s": {
                "y": 1800,
                "x": 3200,
                "u": "https://preview.redd.it/3ytam03tnnhf1.png?width=3200&amp;format=png&amp;auto=webp&amp;s=c33c78a3a38bab1c250fe7a78eb990228e724e40"
              },
              "id": "3ytam03tnnhf1"
            }
          },
          "name": "t3_1mkb1sj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 8,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "1wfpiimrpnhf1",
                "id": 723476335
              },
              {
                "media_id": "3ytam03tnnhf1",
                "id": 723476336
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CYhWzK4wzqdJsolOtS8f9rV5-AW8R0rImTqYnKSy5Mg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754598387,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mkb1sj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mkb1sj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sapoepsilon",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkb1sj/update_my_macos_dictation_replacement_using_local/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mkb1sj",
          "subreddit_subscribers": 513415,
          "created_utc": 1754598387,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey yall,\n\nrecently I've dived into a rabbit hole of creating my own app with Gemma 3n running locally. As I'm fairly new to app development, I'm doing so usign React Native. Everything has been going really well and surprisingly easily, but now I'm stuck searching for a compatible tokenizer that I could integrate using React Native. \n\nI would greatly appreciate any advice!\n\nCheers!",
          "author_fullname": "t2_5icgl7vhn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma 3n tokenizer for React Native",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkay0s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754598148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall,&lt;/p&gt;\n\n&lt;p&gt;recently I&amp;#39;ve dived into a rabbit hole of creating my own app with Gemma 3n running locally. As I&amp;#39;m fairly new to app development, I&amp;#39;m doing so usign React Native. Everything has been going really well and surprisingly easily, but now I&amp;#39;m stuck searching for a compatible tokenizer that I could integrate using React Native. &lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any advice!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkay0s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "clueless_but_hopeful",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkay0s/gemma_3n_tokenizer_for_react_native/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkay0s/gemma_3n_tokenizer_for_react_native/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754598148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vcawomd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "On the topic of graphs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkaxrx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/kdhwce4vonhf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/kdhwce4vonhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kdhwce4vonhf1/DASHPlaylist.mpd?a=1757207617%2CZGJhMGQzOGU4Nzk3OTA3OWVkZmE5ZTY4MWIzZjMwY2RlNzAyMTBlYzFmYmQwMWIzZGVhMTRlZWQwNjczYThlMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/kdhwce4vonhf1/HLSPlaylist.m3u8?a=1757207617%2CODhkOTYwMzBlNjY4ZWNkMGVjNjk3NGEwZjI5MjM4MTc1M2JiNjNjOWFhMWZiMzk2NWRmOWUwNmEwYjJlZTY2YQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f76228ace8339dfbaef7fb17e619d98ea2fd523f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754598132,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/kdhwce4vonhf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?format=pjpg&amp;auto=webp&amp;s=b11ff602c68466e2625a372235af026414d74ad2",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=28cac88bbe32831e806826200610891b2de29172",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=02b0b73b1d2a9cf3a71984b2901183652d7ed81f",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a3c684182c92ed87b85a5a7eada8f2d6c5cc4718",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98960c834896076891f749b9fa36f421e0b65dfc",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c1a6b64c02b15ecc591b2dcd14996df8e9f38184",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fdca5060b422b46d055232b9235447fe48e0b775",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "djdqd2lmNHZvbmhmMT9yaG9NVPSK_ESe4YNSeWTlgNsDK6lCMTdSd23R-vXk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mkaxrx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "onil_gova",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkaxrx/on_the_topic_of_graphs/",
          "stickied": false,
          "url": "https://v.redd.it/kdhwce4vonhf1",
          "subreddit_subscribers": 513415,
          "created_utc": 1754598132,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/kdhwce4vonhf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/kdhwce4vonhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kdhwce4vonhf1/DASHPlaylist.mpd?a=1757207617%2CZGJhMGQzOGU4Nzk3OTA3OWVkZmE5ZTY4MWIzZjMwY2RlNzAyMTBlYzFmYmQwMWIzZGVhMTRlZWQwNjczYThlMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/kdhwce4vonhf1/HLSPlaylist.m3u8?a=1757207617%2CODhkOTYwMzBlNjY4ZWNkMGVjNjk3NGEwZjI5MjM4MTc1M2JiNjNjOWFhMWZiMzk2NWRmOWUwNmEwYjJlZTY2YQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "had it render the chart on HTML canvas",
          "author_fullname": "t2_sgx7w7mb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "random bar chart made by Qwen3-235B-A22B-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkavhy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 412,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 412,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Js8aBl1MUBruUyRHNwvXJBTlCYi6CW_bUdtskFPyTbg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754597995,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;had it render the chart on HTML canvas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rka3lhpnonhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rka3lhpnonhf1.png?auto=webp&amp;s=2c11d446bb5f961fe6307d8d75422caefeb0c341",
                  "width": 944,
                  "height": 548
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rka3lhpnonhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d4103afd6abfb7f836bb0fc9009a4c316b2a499",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://preview.redd.it/rka3lhpnonhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=adf5ad098b62b6a702caac65cb6741d63f80f3f8",
                    "width": 216,
                    "height": 125
                  },
                  {
                    "url": "https://preview.redd.it/rka3lhpnonhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe59b7fc2b31e013b40d265ef84844a655ad0b93",
                    "width": 320,
                    "height": 185
                  },
                  {
                    "url": "https://preview.redd.it/rka3lhpnonhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd853635222d78299767b459957da8a9ae9f30b5",
                    "width": 640,
                    "height": 371
                  }
                ],
                "variants": {},
                "id": "m78ORfLmbSF8eV4KJiW80YwjmU5xTNlzILdyo9aO9XM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mkavhy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tengo_harambe",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/rka3lhpnonhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754597995,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "...10 minute install where you run `llama-server` with *Qwen3-Coder* and then switch to Agent mode in CLine.\n\nGive it a task of\n\n&gt; \"Create a simple Python Flask web app with a single route that returns \"Hello, World!\"\"\n\n2 minutes later you'll have a working \"hello world\" in your browser, including installs of missing packages! \n\nMind blown again.\n\nThe only thing missing is image recognition. So does anyone have any suggestions for a CPU only image recognition setup?",
          "author_fullname": "t2_38r7hz35",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If you haven't tried llamacpp + CLine + VSCode yet, you should. It's a...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkan6d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754597461,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...10 minute install where you run &lt;code&gt;llama-server&lt;/code&gt; with &lt;em&gt;Qwen3-Coder&lt;/em&gt; and then switch to Agent mode in CLine.&lt;/p&gt;\n\n&lt;p&gt;Give it a task of&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&amp;quot;Create a simple Python Flask web app with a single route that returns &amp;quot;Hello, World!&amp;quot;&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;2 minutes later you&amp;#39;ll have a working &amp;quot;hello world&amp;quot; in your browser, including installs of missing packages! &lt;/p&gt;\n\n&lt;p&gt;Mind blown again.&lt;/p&gt;\n\n&lt;p&gt;The only thing missing is image recognition. So does anyone have any suggestions for a CPU only image recognition setup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mkan6d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "73tada",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkan6d/if_you_havent_tried_llamacpp_cline_vscode_yet_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkan6d/if_you_havent_tried_llamacpp_cline_vscode_yet_you/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754597461,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Day 0 of a build-in-public adventure.\n\nWhy I’m doing this:\n\n1. Full fine-tuning still costs $30 K+ in GPUs(only the big players can afford)\n2. LoRA ≈ surface patches(Not bad, but not always sufficient)\n3. No real model ownership when you’re cloud-bound",
          "author_fullname": "t2_q8xj7y1i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Trained Llama 3.1-8B 6× faster on my everyday Laptop M1 (16 GB).\nDay 0 of a build-in-public adventure.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkaf3o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754596956,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Day 0 of a build-in-public adventure.&lt;/p&gt;\n\n&lt;p&gt;Why I’m doing this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Full fine-tuning still costs $30 K+ in GPUs(only the big players can afford)&lt;/li&gt;\n&lt;li&gt;LoRA ≈ surface patches(Not bad, but not always sufficient)&lt;/li&gt;\n&lt;li&gt;No real model ownership when you’re cloud-bound&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mkaf3o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Effective_Election71",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkaf3o/i_trained_llama_318b_6_faster_on_my_everyday/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkaf3o/i_trained_llama_318b_6_faster_on_my_everyday/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754596956,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/numind/NuMarkdown-8B-Thinking](https://huggingface.co/numind/NuMarkdown-8B-Thinking)\n\nfirst reasoning OCR VLM. a fine-tune of **Qwen 2.5-VL-7B** on synthetic Doc → Reasoning → Markdown examples\n\n  \nthoughts?",
          "author_fullname": "t2_19zhptl2dm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NuMarkdown-8B-Thinking -  first reasoning OCR VLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mkaef6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754596912,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/numind/NuMarkdown-8B-Thinking\"&gt;https://huggingface.co/numind/NuMarkdown-8B-Thinking&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;first reasoning OCR VLM. a fine-tune of &lt;strong&gt;Qwen 2.5-VL-7B&lt;/strong&gt; on synthetic Doc → Reasoning → Markdown examples&lt;/p&gt;\n\n&lt;p&gt;thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?auto=webp&amp;s=3af4929f90c76458c7898768362a32a1f3b0c56c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=441c432b48f53d4e139d65d85587999a53c95a76",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9edf2564dd48407afc416c88a72abfaa2fb8f2a0",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=af5a92b644ae4e2b1c06f7cedbf5eeceed2daa56",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=362ec8ea96f122be49373eedbc861f2774464b54",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=756171ca31acb81a3e866053bebbf1d43eb31b0c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61e9e143a4b605256d1252cab0264d750ba962fc",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "hZVqbivk29FZL7FGxe1BtGNwIblHBlPQ9os2iXUmyrQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mkaef6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Whole-Assignment6240",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mkaef6/numarkdown8bthinking_first_reasoning_ocr_vlm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkaef6/numarkdown8bthinking_first_reasoning_ocr_vlm/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754596912,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/d4lsltqrfnhf1.png?width=1938&amp;format=png&amp;auto=webp&amp;s=5877467cf032b0ecb4bc2a3ea791596801e2836e\n\nCongrats on the minimal version. Qwen 4b thinking is probably better......",
          "author_fullname": "t2_1g8vkju3w3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It seems that GPT5 has 3 levels of thinking in common with GPT-OSS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "d4lsltqrfnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e947305dde1d64fc19b67780e84e53f294df6198"
                },
                {
                  "y": 124,
                  "x": 216,
                  "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=726e843b1518be1e4780f99cb0d8a841a077d8b2"
                },
                {
                  "y": 184,
                  "x": 320,
                  "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bada0f734f0533e22148f8eca38e30094b8968a5"
                },
                {
                  "y": 369,
                  "x": 640,
                  "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3766b7b3b5a969c70022d9aea775c5ad1189cbbf"
                },
                {
                  "y": 554,
                  "x": 960,
                  "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5470cc2c49dc1ed0c8ea3402796426d9a54732a2"
                },
                {
                  "y": 623,
                  "x": 1080,
                  "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d6eba0ab5e7a1f51d1b6a401497489481d13ee3"
                }
              ],
              "s": {
                "y": 1119,
                "x": 1938,
                "u": "https://preview.redd.it/d4lsltqrfnhf1.png?width=1938&amp;format=png&amp;auto=webp&amp;s=5877467cf032b0ecb4bc2a3ea791596801e2836e"
              },
              "id": "d4lsltqrfnhf1"
            }
          },
          "name": "t3_1mk9lu4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.48,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DpuuQ_sw3rr1vV_R9H5ojVCR3ESQSz52dv1Me9z6koM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754595080,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/d4lsltqrfnhf1.png?width=1938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5877467cf032b0ecb4bc2a3ea791596801e2836e\"&gt;https://preview.redd.it/d4lsltqrfnhf1.png?width=1938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5877467cf032b0ecb4bc2a3ea791596801e2836e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Congrats on the minimal version. Qwen 4b thinking is probably better......&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk9lu4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Necessary_Bunch_4019",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk9lu4/it_seems_that_gpt5_has_3_levels_of_thinking_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk9lu4/it_seems_that_gpt5_has_3_levels_of_thinking_in/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754595080,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been running ChatGPT and other AI chatbots for a while and have been blown away by their capabilities. When I discovered I could run LLM (Large Language Models) on my computer, I was intrigued.\n\nFor one thing, it would give me all the privacy I desire, as I would not have to expose my data to the Internet. It would also allow me to run a wide array of open-source models at zero cost. And, I would have total control of the system and would not have to worry about Internet issues or provider outages.\n\nMy current PC is a Ryzen 5700G with 32 GB of RAM. It is an APU with onboard graphics. The downside is the graphics processor does not have enough speed or memory to do LLM inference, as it shares memory with the CPU. The results are slow output speed compared to a graphics card and model size limitations.\n\nI spent hours learning platforms like Ollama and LM Studio and did a lot of testing and benchmarking a variety of LLMs.\n\nI also looked at a variety of upgrade options, including rebuilding my present system and adding a graphics card, building a new system from scratch, or buying one of those cool new mini computers loaded with 64GB of memory and support for dual nVME drives.\n\nIn addition, Ichecked out the X99 motherboard/Xeon processor/memory combos that you can get really cheap on various sites on the internet. Plus, all of the available graphic card options for LLM inference.\n\nThe end result is my new book: LLM Hardware Unlocked. It will show you the benefits and limitations of running LLMs at home as well as exposing the realities of heat, noise, and power draw if you decide to go “all in”.\n\nI invite you to check it out. It is a quick read with a low sticker price. And, hopefully, it will save you time and frustration if you want to unlock the power of local LLMs.\n\n*Here is the link to my ebook on Amazon for Kindle:*\n\n[https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/](https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/)\n\nMedium Article: [https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66](https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66)",
          "author_fullname": "t2_ycgc2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unlocking the Power of Local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk9fuq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754594701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been running ChatGPT and other AI chatbots for a while and have been blown away by their capabilities. When I discovered I could run LLM (Large Language Models) on my computer, I was intrigued.&lt;/p&gt;\n\n&lt;p&gt;For one thing, it would give me all the privacy I desire, as I would not have to expose my data to the Internet. It would also allow me to run a wide array of open-source models at zero cost. And, I would have total control of the system and would not have to worry about Internet issues or provider outages.&lt;/p&gt;\n\n&lt;p&gt;My current PC is a Ryzen 5700G with 32 GB of RAM. It is an APU with onboard graphics. The downside is the graphics processor does not have enough speed or memory to do LLM inference, as it shares memory with the CPU. The results are slow output speed compared to a graphics card and model size limitations.&lt;/p&gt;\n\n&lt;p&gt;I spent hours learning platforms like Ollama and LM Studio and did a lot of testing and benchmarking a variety of LLMs.&lt;/p&gt;\n\n&lt;p&gt;I also looked at a variety of upgrade options, including rebuilding my present system and adding a graphics card, building a new system from scratch, or buying one of those cool new mini computers loaded with 64GB of memory and support for dual nVME drives.&lt;/p&gt;\n\n&lt;p&gt;In addition, Ichecked out the X99 motherboard/Xeon processor/memory combos that you can get really cheap on various sites on the internet. Plus, all of the available graphic card options for LLM inference.&lt;/p&gt;\n\n&lt;p&gt;The end result is my new book: LLM Hardware Unlocked. It will show you the benefits and limitations of running LLMs at home as well as exposing the realities of heat, noise, and power draw if you decide to go “all in”.&lt;/p&gt;\n\n&lt;p&gt;I invite you to check it out. It is a quick read with a low sticker price. And, hopefully, it will save you time and frustration if you want to unlock the power of local LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Here is the link to my ebook on Amazon for Kindle:&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/\"&gt;https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Medium Article: &lt;a href=\"https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66\"&gt;https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mk9fuq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tony10000",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk9fuq/unlocking_the_power_of_local_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk9fuq/unlocking_the_power_of_local_llms/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754594701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey there. Just had a few questions about the latest updates to LM Studio. I loaded a few models to test. At first, I thought something broke LM Studio, because my Gemma 3 27B was suddenly much slower. (I'm on an RTX 3090TI w/96GB of RAM, i7 12700K.\n\nBut then, I noticed this:\n\nhttps://preview.redd.it/gkexj98rdnhf1.png?width=316&amp;format=png&amp;auto=webp&amp;s=b76094ce63673c5fc95e880659b42692d8ec358f\n\nMy GPU only has 24GB of RAM. So, I'm like, \"what?\" Then I checked this:\n\nhttps://preview.redd.it/ly533alwdnhf1.png?width=1094&amp;format=png&amp;auto=webp&amp;s=176153521f4c2552c704f0fd33ec5f534d9286e0\n\n...it's showing 3 models loaded. So, I'm assuming that it's loading some of it in GPU, others in RAM. This is cool, I guess. but it's clearly slowing down performance. \n\nSo, my question is, how do I customize these settings so that I can only load one model in my VRAM like before? Or better yet, how can I view the options I have available to me for model loading?\n\nPreviously the way it worked was that I could only load one model at a time. I did, however, update some additional runtime extension packs, such as Vulkan, CUDA 12 llama.cpp, CPU llama.cpp and CUDA llama.cpp. Maybe one of these made the change and I wasn't aware of it?\n\nIt's not an uncool feature, but I would like more control so that I don't bottleneck the models' abilities. \n\nThanks for any help/insight you guys can provide.",
          "author_fullname": "t2_7qduc583w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio and multiple model loading...is this NEW? How does it work?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 35,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ly533alwdnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 39,
                  "x": 108,
                  "u": "https://preview.redd.it/ly533alwdnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d12acff2531d2b54862d40e2d0d4e0875812c198"
                },
                {
                  "y": 78,
                  "x": 216,
                  "u": "https://preview.redd.it/ly533alwdnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=69207a2bdf9b8da511edf0aff57947dad7d49597"
                },
                {
                  "y": 116,
                  "x": 320,
                  "u": "https://preview.redd.it/ly533alwdnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=222b9c990a5b81feca04dfc7a029f357d0ca7aab"
                },
                {
                  "y": 232,
                  "x": 640,
                  "u": "https://preview.redd.it/ly533alwdnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=36ebe2f4120782fb1f1852f658f1cdba10adbd63"
                },
                {
                  "y": 349,
                  "x": 960,
                  "u": "https://preview.redd.it/ly533alwdnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fd41b12a1c641bfd84576fa1f0c7c1d5a0be4f6"
                },
                {
                  "y": 392,
                  "x": 1080,
                  "u": "https://preview.redd.it/ly533alwdnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d063c8c77b12182c7074d05f15a8ff2106f798c"
                }
              ],
              "s": {
                "y": 398,
                "x": 1094,
                "u": "https://preview.redd.it/ly533alwdnhf1.png?width=1094&amp;format=png&amp;auto=webp&amp;s=176153521f4c2552c704f0fd33ec5f534d9286e0"
              },
              "id": "ly533alwdnhf1"
            },
            "gkexj98rdnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 27,
                  "x": 108,
                  "u": "https://preview.redd.it/gkexj98rdnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f8e36aa80651274bfdcf9ce2135bfc544c8e9a7"
                },
                {
                  "y": 54,
                  "x": 216,
                  "u": "https://preview.redd.it/gkexj98rdnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=870e8cf1fd36eadf9b37f2d3f10f6529d56f931f"
                }
              ],
              "s": {
                "y": 80,
                "x": 316,
                "u": "https://preview.redd.it/gkexj98rdnhf1.png?width=316&amp;format=png&amp;auto=webp&amp;s=b76094ce63673c5fc95e880659b42692d8ec358f"
              },
              "id": "gkexj98rdnhf1"
            }
          },
          "name": "t3_1mk9eu2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/0L3VNMS-HITZpB6QbUwT456GMl9cDMCnqPPn7-DQe9w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754594633,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there. Just had a few questions about the latest updates to LM Studio. I loaded a few models to test. At first, I thought something broke LM Studio, because my Gemma 3 27B was suddenly much slower. (I&amp;#39;m on an RTX 3090TI w/96GB of RAM, i7 12700K.&lt;/p&gt;\n\n&lt;p&gt;But then, I noticed this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gkexj98rdnhf1.png?width=316&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b76094ce63673c5fc95e880659b42692d8ec358f\"&gt;https://preview.redd.it/gkexj98rdnhf1.png?width=316&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b76094ce63673c5fc95e880659b42692d8ec358f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My GPU only has 24GB of RAM. So, I&amp;#39;m like, &amp;quot;what?&amp;quot; Then I checked this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ly533alwdnhf1.png?width=1094&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=176153521f4c2552c704f0fd33ec5f534d9286e0\"&gt;https://preview.redd.it/ly533alwdnhf1.png?width=1094&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=176153521f4c2552c704f0fd33ec5f534d9286e0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;...it&amp;#39;s showing 3 models loaded. So, I&amp;#39;m assuming that it&amp;#39;s loading some of it in GPU, others in RAM. This is cool, I guess. but it&amp;#39;s clearly slowing down performance. &lt;/p&gt;\n\n&lt;p&gt;So, my question is, how do I customize these settings so that I can only load one model in my VRAM like before? Or better yet, how can I view the options I have available to me for model loading?&lt;/p&gt;\n\n&lt;p&gt;Previously the way it worked was that I could only load one model at a time. I did, however, update some additional runtime extension packs, such as Vulkan, CUDA 12 llama.cpp, CPU llama.cpp and CUDA llama.cpp. Maybe one of these made the change and I wasn&amp;#39;t aware of it?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not an uncool feature, but I would like more control so that I don&amp;#39;t bottleneck the models&amp;#39; abilities. &lt;/p&gt;\n\n&lt;p&gt;Thanks for any help/insight you guys can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk9eu2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GrungeWerX",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk9eu2/lm_studio_and_multiple_model_loadingis_this_new/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk9eu2/lm_studio_and_multiple_model_loadingis_this_new/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754594633,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "from the model description:\n\nWe introduce **Intern-S1**, our **most advanced open-source multimodal reasoning model** to date. Intern-S1 combines **strong general-task capabilities with state-of-the-art performance on a wide range of scientific tasks**, rivaling leading closed-source commercial models. Built upon a 235B MoE language model (Qwen3) and a 6B Vision encoder (InternViT), Intern-S1 has been further pretrained on **5 trillion tokens** of multimodal data, including over **2.5 trillion scientific-domain tokens**. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as **interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes**, making Intern-S1 to be a capable research assistant for real-world scientific applications. Features\n\n* Strong performance across language and vision reasoning benchmarks, especially scientific tasks.\n* Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.\n* Dynamic tokenizer enables native understanding of molecular formulas, protein sequences, and seismic signals.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Support for intern-s1 has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk9cg3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6c0743e231111e82c33980508b262c7faea182f1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754594479,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;from the model description:&lt;/p&gt;\n\n&lt;p&gt;We introduce &lt;strong&gt;Intern-S1&lt;/strong&gt;, our &lt;strong&gt;most advanced open-source multimodal reasoning model&lt;/strong&gt; to date. Intern-S1 combines &lt;strong&gt;strong general-task capabilities with state-of-the-art performance on a wide range of scientific tasks&lt;/strong&gt;, rivaling leading closed-source commercial models. Built upon a 235B MoE language model (Qwen3) and a 6B Vision encoder (InternViT), Intern-S1 has been further pretrained on &lt;strong&gt;5 trillion tokens&lt;/strong&gt; of multimodal data, including over &lt;strong&gt;2.5 trillion scientific-domain tokens&lt;/strong&gt;. This enables the model to retain strong general capabilities while excelling in specialized scientific domains such as &lt;strong&gt;interpreting chemical structures, understanding protein sequences, and planning compound synthesis routes&lt;/strong&gt;, making Intern-S1 to be a capable research assistant for real-world scientific applications. Features&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Strong performance across language and vision reasoning benchmarks, especially scientific tasks.&lt;/li&gt;\n&lt;li&gt;Continuously pretrained on a massive 5T token dataset, with over 50% specialized scientific data, embedding deep domain expertise.&lt;/li&gt;\n&lt;li&gt;Dynamic tokenizer enables native understanding of molecular formulas, protein sequences, and seismic signals.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?auto=webp&amp;s=9ca7ec6c815a22ec5f8041e8f5356edd8837d952",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=60f53264dcfc9e1652e51a64686f007ce07ac2b0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e164b1f2c6c34158190b9fb9b540037ddde6693f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddc7014b85aeb79e4cbf590ce66ff9bade348b42",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=babab2f7d6331292e786fb494f54284c2569bc17",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f820ddcee6975b2cf4ce79ec449f2d68f8dc034",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b33f2f8afa83fbdeee5f5d134e588b75554e3d7b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ukGPJhGQBSNIqKqFk0joH5as5YrMTXg0pmkQuca7PhI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mk9cg3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mk9cg3/support_for_interns1_has_been_merged_into_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "subreddit_subscribers": 513415,
          "created_utc": 1754594479,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just tested **GPT-OSS-120B (MXFP4)** locally using **LM Studio v0.3.22 (Beta build 2)** on my machine with an **RTX 5090 (32 GB VRAM)** \\+ **Ryzen 9 9950X3D** \\+ **96 GB RAM**.\n\nEverything is mostly default. I only enabled **Flash Attention** manually and adjusted GPU offload to 30/36 layers + Guardrails **OFF +** Limit Model Offload to dedicated GPU Memory **OFF**.\n\n**Result:**  \n→ \\~10.48 tokens/sec  \n→ \\~2.27s to first token\n\nModel loads and runs stable. Clearly heavier than the 20B, but impressive that it runs at \\~10.48 tokens/sec.\n\nhttps://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf\n\n[Flash Attention + GPU offload to 30\\/36 layers](https://preview.redd.it/91wp98m0dnhf1.png?width=552&amp;format=png&amp;auto=webp&amp;s=78cc538e4cb36ccedee82c183652d81206a6f5cb)\n\n[Guardrails OFF + Limit Model Offload to dedicated GPU Memory OFF](https://preview.redd.it/y3xf186sdnhf1.png?width=1623&amp;format=png&amp;auto=webp&amp;s=0f4272d01c2ca4c7e119a3c5fe478600cef64fe8)",
          "author_fullname": "t2_h755hoap",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "10.48 tok/sec - GPT-OSS-120B on RTX 5090 32 VRAM + 96 RAM  in LM Studio (default settings + FlashAttention + Guardrails: OFF)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 136,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "stsclnt8enhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 105,
                  "x": 108,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6dd51aed8a152d315502e78ab53301323bc0ef"
                },
                {
                  "y": 210,
                  "x": 216,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=966fb1e1cb64c5c8dc88bb836652890ff34439f1"
                },
                {
                  "y": 311,
                  "x": 320,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ed091eb6b358e43c7bfbbdf889ac20fcbbaa586"
                },
                {
                  "y": 622,
                  "x": 640,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b6313cac456abd31c3c3eeac04993778cf38845"
                },
                {
                  "y": 934,
                  "x": 960,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=05c6c85282fdff6dd80e4115c0aeb4c13e29626b"
                },
                {
                  "y": 1051,
                  "x": 1080,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=be697e8225707ae5566af1690409e8b59a45b05d"
                }
              ],
              "s": {
                "y": 1460,
                "x": 1500,
                "u": "https://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf"
              },
              "id": "stsclnt8enhf1"
            },
            "y3xf186sdnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 65,
                  "x": 108,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d3bc2cda8bea6aea361aa243691322afd48392f"
                },
                {
                  "y": 131,
                  "x": 216,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=db770f6e1665b480e50365ab5319e3a65bded0a8"
                },
                {
                  "y": 194,
                  "x": 320,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7f81884d0bd86e5b3a0d03c859a2cb7bfe32c03"
                },
                {
                  "y": 389,
                  "x": 640,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be0a3efe7e807481098e8c44cc3d5f321bbf6b38"
                },
                {
                  "y": 583,
                  "x": 960,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=481baa48b62b97aa879127d4bb27e64833b79195"
                },
                {
                  "y": 656,
                  "x": 1080,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03a74398d00d8e46a2f994e787637aa0edc5b0b1"
                }
              ],
              "s": {
                "y": 987,
                "x": 1623,
                "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=1623&amp;format=png&amp;auto=webp&amp;s=0f4272d01c2ca4c7e119a3c5fe478600cef64fe8"
              },
              "id": "y3xf186sdnhf1"
            },
            "91wp98m0dnhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 145,
                  "x": 108,
                  "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d271699d538d330e79b5ff955d66ee657768202"
                },
                {
                  "y": 290,
                  "x": 216,
                  "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df4aab64d650bd96e35c6899ca160953e98d499f"
                },
                {
                  "y": 430,
                  "x": 320,
                  "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e71d3ba1393d614de2384d0b7cdedf4be3315fd"
                }
              ],
              "s": {
                "y": 743,
                "x": 552,
                "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=552&amp;format=png&amp;auto=webp&amp;s=78cc538e4cb36ccedee82c183652d81206a6f5cb"
              },
              "id": "91wp98m0dnhf1"
            }
          },
          "name": "t3_1mk9c1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/eT9GlQ-I-zAVfkF4YhcL6w037-GGjldTgsuaXGBx2Po.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754594454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just tested &lt;strong&gt;GPT-OSS-120B (MXFP4)&lt;/strong&gt; locally using &lt;strong&gt;LM Studio v0.3.22 (Beta build 2)&lt;/strong&gt; on my machine with an &lt;strong&gt;RTX 5090 (32 GB VRAM)&lt;/strong&gt; + &lt;strong&gt;Ryzen 9 9950X3D&lt;/strong&gt; + &lt;strong&gt;96 GB RAM&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Everything is mostly default. I only enabled &lt;strong&gt;Flash Attention&lt;/strong&gt; manually and adjusted GPU offload to 30/36 layers + Guardrails &lt;strong&gt;OFF +&lt;/strong&gt; Limit Model Offload to dedicated GPU Memory &lt;strong&gt;OFF&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;br/&gt;\n→ ~10.48 tokens/sec&lt;br/&gt;\n→ ~2.27s to first token&lt;/p&gt;\n\n&lt;p&gt;Model loads and runs stable. Clearly heavier than the 20B, but impressive that it runs at ~10.48 tokens/sec.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf\"&gt;https://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/91wp98m0dnhf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78cc538e4cb36ccedee82c183652d81206a6f5cb\"&gt;Flash Attention + GPU offload to 30/36 layers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y3xf186sdnhf1.png?width=1623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f4272d01c2ca4c7e119a3c5fe478600cef64fe8\"&gt;Guardrails OFF + Limit Model Offload to dedicated GPU Memory OFF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mk9c1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Spiritual_Tie_5574",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754594454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "that's it. Big fan of smaller yet ultra performant LLMs.",
          "author_fullname": "t2_9b9s4a7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-8b-2508 anyone? 🤞🤞🤞 Where are you?  Are you coming?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk95w6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754594062,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;that&amp;#39;s it. Big fan of smaller yet ultra performant LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk95w6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JLeonsarmiento",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754594062,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk92k4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": "#bbbdbf",
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=75bcaeb461477e991de79a2138e51737147a78cd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754593848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?auto=webp&amp;s=e218292ffeeb286451b680d4a561fd1da0df6d8c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcbbd0387dd8ac9b2f7f0fb4f258aade8378636b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1542424259385e9713df82d41658936838f99dfd",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1dd3d715c01ba930a04e431096f9eb1af736210",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d8ef5dae58a1931f159f19948400500dc5e8110f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8336670c18f559983499054a334974b56d192c99",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49220accb219215b3165b31165096648f210592a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "dM2syJ0lh5qODrveCus4LDlR8L4f9r_ltO_PMWMUbDA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mk92k4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mk92k4/gabriellarsonhuihuigptoss20bbf16abliteratedgguf/",
          "stickied": false,
          "url": "https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF",
          "subreddit_subscribers": 513415,
          "created_utc": 1754593848,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# A personal Benchmark\n\nFor a while now I have been testing LLMs with a benchmark I informally call \"Twisted Math\". The basic idea is that we take a very common mathematics problem, e.g., Tower of Hanoi, the birthday paradox, etc., and subtly change the problem constraints so that the original reasoning does not hold any more.\n\n# Do LLMs blindly regurgitate its training or do they \"think\"?\n\nSince the conditioning tokens naturally lead to the common (and in this case wrong) solution, the question is, do LLMs have some semblance of internal reasoning or just strong fuzzy retrieval? To date I have not seen LLMs beat this test. I would love to have your thoughts on this!\n\nP.S.: The \"catch\" for this problem is that we never mentioned that a large disk cannot be placed on a small one, thus making the answer 2\\*N -1 instead of 2\\^N - 1. ",
          "author_fullname": "t2_1kb53dh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Twisted math test for LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 52,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk916s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/oC1QlDExaSTBxJBJ2pxSQr_SUxSYlSe8m-KhYgGb78E.jpg",
          "author_cakeday": true,
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754593763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;A personal Benchmark&lt;/h1&gt;\n\n&lt;p&gt;For a while now I have been testing LLMs with a benchmark I informally call &amp;quot;Twisted Math&amp;quot;. The basic idea is that we take a very common mathematics problem, e.g., Tower of Hanoi, the birthday paradox, etc., and subtly change the problem constraints so that the original reasoning does not hold any more.&lt;/p&gt;\n\n&lt;h1&gt;Do LLMs blindly regurgitate its training or do they &amp;quot;think&amp;quot;?&lt;/h1&gt;\n\n&lt;p&gt;Since the conditioning tokens naturally lead to the common (and in this case wrong) solution, the question is, do LLMs have some semblance of internal reasoning or just strong fuzzy retrieval? To date I have not seen LLMs beat this test. I would love to have your thoughts on this!&lt;/p&gt;\n\n&lt;p&gt;P.S.: The &amp;quot;catch&amp;quot; for this problem is that we never mentioned that a large disk cannot be placed on a small one, thus making the answer 2*N -1 instead of 2^N - 1. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ihugrx1wanhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?auto=webp&amp;s=6f1de1c53a7797c3521db8143d91a4d3e44d61e4",
                  "width": 1583,
                  "height": 595
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a37990c1cde7c492692a1858b33ea3571b01f1e",
                    "width": 108,
                    "height": 40
                  },
                  {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=718fa77cb37811f3c3fce9303ebe9ad0fe0220fa",
                    "width": 216,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=896c87335aac90ae2cbb365644bfbdcf0b43b317",
                    "width": 320,
                    "height": 120
                  },
                  {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8a09dc56505ba25d79c8bf3c47fec2eec04dca45",
                    "width": 640,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e98e1ae27773aae146eff7fb06e5311f270f2e3e",
                    "width": 960,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b3f692ae0bccee0a3bd206b88f591911a9b91d9",
                    "width": 1080,
                    "height": 405
                  }
                ],
                "variants": {},
                "id": "50q1mawbq5QnfkZOocv1dqNmA6uUnHO3Ft2EUAN-94E"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk916s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "espressoVi",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/",
          "stickied": false,
          "url": "https://i.redd.it/ihugrx1wanhf1.jpeg",
          "subreddit_subscribers": 513415,
          "created_utc": 1754593763,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[GPU utilization across 4 nodes](https://preview.redd.it/0nwgl1j0anhf1.png?width=4458&amp;format=png&amp;auto=webp&amp;s=aba60da2706de9183ed58ccf91c59e7a76931831)\n\nGPT-5 has just been released, but if we want to adapt the model to our own data, we will still need to use the open model. Fortunately, OpenAI released the open model gpt-oss-120b under the Apache 2.0 license.\n\nWe at SkyPilot composed a quick recipe for how to finetune the model on multiple nodes with InfiniBand enabled. It uses Huggingface Accelerate with Nebius H200s + InfiniBand under the hood. It can be started with a single command:\n\n    sky launch --num-nodes 4 gpt-oss-120b-sft.yaml\n\n[https://docs.skypilot.co/en/latest/examples/training/gpt-oss-finetuning.html](https://docs.skypilot.co/en/latest/examples/training/gpt-oss-finetuning.html)",
          "author_fullname": "t2_1h9w2thu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recipe for distributed finetuning OpenAI gpt-oss-120b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0nwgl1j0anhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bd05bbf10dbf0cfacf98cd427a9e14febda8bcd"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99538f7c9f35144c53a3b6f0a69ce52c0b7310c1"
                },
                {
                  "y": 177,
                  "x": 320,
                  "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=31c5ed1ff478bde8ba72804722ce18d42fcae8b7"
                },
                {
                  "y": 355,
                  "x": 640,
                  "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14eecc8c5681d7de9659f2ad97642314a13aabd7"
                },
                {
                  "y": 533,
                  "x": 960,
                  "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=522603416511a37a5abc2150e23105e15510f1e5"
                },
                {
                  "y": 600,
                  "x": 1080,
                  "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ba8e5bf978e565941be6886dd5ce1b405812ef1"
                }
              ],
              "s": {
                "y": 2478,
                "x": 4458,
                "u": "https://preview.redd.it/0nwgl1j0anhf1.png?width=4458&amp;format=png&amp;auto=webp&amp;s=aba60da2706de9183ed58ccf91c59e7a76931831"
              },
              "id": "0nwgl1j0anhf1"
            }
          },
          "name": "t3_1mk8zpm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/H3gIXSpKFwXOsqyKNvPgwlPaiyKJ5JFo9aJRdzPfMqE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754593666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/0nwgl1j0anhf1.png?width=4458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aba60da2706de9183ed58ccf91c59e7a76931831\"&gt;GPU utilization across 4 nodes&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GPT-5 has just been released, but if we want to adapt the model to our own data, we will still need to use the open model. Fortunately, OpenAI released the open model gpt-oss-120b under the Apache 2.0 license.&lt;/p&gt;\n\n&lt;p&gt;We at SkyPilot composed a quick recipe for how to finetune the model on multiple nodes with InfiniBand enabled. It uses Huggingface Accelerate with Nebius H200s + InfiniBand under the hood. It can be started with a single command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sky launch --num-nodes 4 gpt-oss-120b-sft.yaml\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.skypilot.co/en/latest/examples/training/gpt-oss-finetuning.html\"&gt;https://docs.skypilot.co/en/latest/examples/training/gpt-oss-finetuning.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mk8zpm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Michaelvll",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk8zpm/recipe_for_distributed_finetuning_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk8zpm/recipe_for_distributed_finetuning_openai/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754593666,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm really interested in understanding pretraining of LLMs (not just fine-tuning). But it's been extremely difficult to find clear, practical resources or workflows for actually learning this from scratch. Most tutorials either skip over the hard parts, focus only on fine-tuning very small LLMs that can't be used in most cases. On top of that, even trying things on your own is extremely expensive, especially for someone just trying to learn.\n\nSo my questions are\n\nHow can someone with limited compute or resources learn the concepts and process of LLM pretraining, or proper post-training llms\n\nIs there any small-scale setup or framework like TinyLLaMA or nanoGPT that I can use locally to understand the architecture and training loop deeply\n\nAndrej Karpathy helped me a lot to have a rough understanding on these. What else?\n\nAre there any solid open-source learning paths, repos, blogs, or courses that explain this step by step\n\nAny way to experiment without burning cash on GPUs? I'm not looking to train GPT-3 myself. I just want to get practical and theoretical clarity on how pretraining works end to end. Also open to reading research papers if you think they help\n\nThanks in advance",
          "author_fullname": "t2_rdvat0vg1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can I actually learn and try LLM pretraining? (or post training a large LLM )",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk8oll",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754592964,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really interested in understanding pretraining of LLMs (not just fine-tuning). But it&amp;#39;s been extremely difficult to find clear, practical resources or workflows for actually learning this from scratch. Most tutorials either skip over the hard parts, focus only on fine-tuning very small LLMs that can&amp;#39;t be used in most cases. On top of that, even trying things on your own is extremely expensive, especially for someone just trying to learn.&lt;/p&gt;\n\n&lt;p&gt;So my questions are&lt;/p&gt;\n\n&lt;p&gt;How can someone with limited compute or resources learn the concepts and process of LLM pretraining, or proper post-training llms&lt;/p&gt;\n\n&lt;p&gt;Is there any small-scale setup or framework like TinyLLaMA or nanoGPT that I can use locally to understand the architecture and training loop deeply&lt;/p&gt;\n\n&lt;p&gt;Andrej Karpathy helped me a lot to have a rough understanding on these. What else?&lt;/p&gt;\n\n&lt;p&gt;Are there any solid open-source learning paths, repos, blogs, or courses that explain this step by step&lt;/p&gt;\n\n&lt;p&gt;Any way to experiment without burning cash on GPUs? I&amp;#39;m not looking to train GPT-3 myself. I just want to get practical and theoretical clarity on how pretraining works end to end. Also open to reading research papers if you think they help&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk8oll",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Distinct-Drive1307",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754592964,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It got it right with Flappybird and some other tests also in first try. \n\nIs quite fast but a bit weird, as it manipulate the codebox. \n\nAlso the update of llama.cpp b6111 (cpu) that supports GPT OSS is flagged by Windows as a malware (Wacatac). \n\nEvery update since the repo disappear in Github some days ago (worth checking llama.cpp source code). ",
          "author_fullname": "t2_lxbiwvvv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT OSS fast Test first impressions.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 74,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk8j72",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/fumgkm8m7nhf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 456,
              "width": 854,
              "scrubber_media_url": "https://v.redd.it/fumgkm8m7nhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fumgkm8m7nhf1/DASHPlaylist.mpd?a=1757207617%2CNmY4M2UyMmJmNWVjYmIyNDAwMWM0ODliYWU4YTQ2OTI3YTM0YjMxOWIzZjliODM5MDc3YmVkY2U3M2VjYTdkNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 67,
              "hls_url": "https://v.redd.it/fumgkm8m7nhf1/HLSPlaylist.m3u8?a=1757207617%2CYjZiOWE5NDE3YjE0ZGQ2Y2EzNjJiMTFkM2UzMDAyZDZlN2Q2ZTU3ZjZlNDM3NGU0OTlmODJmNWQ3Y2I4YjRlOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=140&amp;height=74&amp;crop=140:74,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=bd320181fdf9839cb2634ede77c4b4d34a1aa2d6",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754592630,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It got it right with Flappybird and some other tests also in first try. &lt;/p&gt;\n\n&lt;p&gt;Is quite fast but a bit weird, as it manipulate the codebox. &lt;/p&gt;\n\n&lt;p&gt;Also the update of llama.cpp b6111 (cpu) that supports GPT OSS is flagged by Windows as a malware (Wacatac). &lt;/p&gt;\n\n&lt;p&gt;Every update since the repo disappear in Github some days ago (worth checking llama.cpp source code). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/fumgkm8m7nhf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?format=pjpg&amp;auto=webp&amp;s=be2d8204407daf081f372fc7bc85ee3dda27d1c4",
                  "width": 1280,
                  "height": 684
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=683a4a03ddb9cea3dca4e7e6309df3c741cb8333",
                    "width": 108,
                    "height": 57
                  },
                  {
                    "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a62ec57054e832d7bf68192b0de9b53ba3bf639e",
                    "width": 216,
                    "height": 115
                  },
                  {
                    "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1665e51d9a111551a458c954d57e47058e8d71fa",
                    "width": 320,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a32057dce3b462a932044ece587c259b2ad3dc1a",
                    "width": 640,
                    "height": 342
                  },
                  {
                    "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=23c3282ea20e22f8d1620a919b485ff8289272fb",
                    "width": 960,
                    "height": 513
                  },
                  {
                    "url": "https://external-preview.redd.it/bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9f3a9dfaecc9f7f02ed48bd15247ba7ee8d4e916",
                    "width": 1080,
                    "height": 577
                  }
                ],
                "variants": {},
                "id": "bDI4NWNtOG03bmhmMcnEioV_16zktoW0980HFkeIRLUS5d2LAlT6PXq7YuHX"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk8j72",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Trilogix",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk8j72/gpt_oss_fast_test_first_impressions/",
          "stickied": false,
          "url": "https://v.redd.it/fumgkm8m7nhf1",
          "subreddit_subscribers": 513415,
          "created_utc": 1754592630,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/fumgkm8m7nhf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 456,
              "width": 854,
              "scrubber_media_url": "https://v.redd.it/fumgkm8m7nhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fumgkm8m7nhf1/DASHPlaylist.mpd?a=1757207617%2CNmY4M2UyMmJmNWVjYmIyNDAwMWM0ODliYWU4YTQ2OTI3YTM0YjMxOWIzZjliODM5MDc3YmVkY2U3M2VjYTdkNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 67,
              "hls_url": "https://v.redd.it/fumgkm8m7nhf1/HLSPlaylist.m3u8?a=1757207617%2CYjZiOWE5NDE3YjE0ZGQ2Y2EzNjJiMTFkM2UzMDAyZDZlN2Q2ZTU3ZjZlNDM3NGU0OTlmODJmNWQ3Y2I4YjRlOA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=4ba6f60c0af70c5bfdc671b787a449c2570b210d\n\nWhile everyone’s talking GPT-5…\n\nCoral quietly outperformed Microsoft by **34%** using small models, not massive ones.\n\nCoral Protocol ranked #1 on the GAIA benchmark using multi-agent systems powered by small LLMs.\n\nThe future isn’t just bigger models it’s smarter systems.\n\nCheckout the link in the comments",
          "author_fullname": "t2_6j7o4ubzm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Coral Protocol Outperforms Microsoft by 34% With Top GAIA Benchmark for AI Mini-Model !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "2t0xlmzo7nhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b7b128977db13d2dd3995907ece2aca0e2d1e1e"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d30b9360c0082f23e833219444b522ce941eb85"
                },
                {
                  "y": 192,
                  "x": 320,
                  "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=541fc77f744e20740d21cc0480fec2461d9a137c"
                },
                {
                  "y": 384,
                  "x": 640,
                  "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d43ebc459994c79a3392af3f6eb834506c0181c9"
                },
                {
                  "y": 576,
                  "x": 960,
                  "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cefcf3ff2089870d1d6dbc3fbdb47dfce81b23f5"
                },
                {
                  "y": 648,
                  "x": 1080,
                  "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3547b19823acbf8eb48d240f79204b5c2144848"
                }
              ],
              "s": {
                "y": 648,
                "x": 1080,
                "u": "https://preview.redd.it/2t0xlmzo7nhf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=4ba6f60c0af70c5bfdc671b787a449c2570b210d"
              },
              "id": "2t0xlmzo7nhf1"
            }
          },
          "name": "t3_1mk8e0f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CTS26v_SwB0B3PwDWJJJfQgHU1lnlNxchKWgdfftSwU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754592308,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/2t0xlmzo7nhf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ba6f60c0af70c5bfdc671b787a449c2570b210d\"&gt;https://preview.redd.it/2t0xlmzo7nhf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ba6f60c0af70c5bfdc671b787a449c2570b210d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While everyone’s talking GPT-5…&lt;/p&gt;\n\n&lt;p&gt;Coral quietly outperformed Microsoft by &lt;strong&gt;34%&lt;/strong&gt; using small models, not massive ones.&lt;/p&gt;\n\n&lt;p&gt;Coral Protocol ranked #1 on the GAIA benchmark using multi-agent systems powered by small LLMs.&lt;/p&gt;\n\n&lt;p&gt;The future isn’t just bigger models it’s smarter systems.&lt;/p&gt;\n\n&lt;p&gt;Checkout the link in the comments&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mk8e0f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdVirtual2648",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk8e0f/coral_protocol_outperforms_microsoft_by_34_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk8e0f/coral_protocol_outperforms_microsoft_by_34_with/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754592308,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How i feel about gpt-oss...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk8d3j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/gMzHe-jykzFZyd7ui_SUtxKHL3GPGX0A0gp1EF9TDbs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754592247,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/shxc8spf7nhf1.gif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/shxc8spf7nhf1.gif?format=png8&amp;s=e4ce1e3946cff2279e1b84d759427b215b0c0f09",
                  "width": 160,
                  "height": 160
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/shxc8spf7nhf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=fba4562e04e2d1527bfe564484949e1b71b93704",
                    "width": 108,
                    "height": 108
                  }
                ],
                "variants": {
                  "gif": {
                    "source": {
                      "url": "https://preview.redd.it/shxc8spf7nhf1.gif?s=0143562886f6b5c0ebb7a0b9d632ac35ba29ef6d",
                      "width": 160,
                      "height": 160
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/shxc8spf7nhf1.gif?width=108&amp;crop=smart&amp;s=db070b9ea44418b2e8ff30c9629b861179a93e30",
                        "width": 108,
                        "height": 108
                      }
                    ]
                  },
                  "mp4": {
                    "source": {
                      "url": "https://preview.redd.it/shxc8spf7nhf1.gif?format=mp4&amp;s=a92d40337f1a7c40d68e8b3a9b57ed543e911241",
                      "width": 160,
                      "height": 160
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/shxc8spf7nhf1.gif?width=108&amp;format=mp4&amp;s=173d5730e45ef7a91908704b341f5bed4d9b6e35",
                        "width": 108,
                        "height": 108
                      }
                    ]
                  }
                },
                "id": "xX6pPV4EUkg3O2vEpZx5szZJkHGBZv4Fwy7l9Wg2mZs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mk8d3j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk8d3j/how_i_feel_about_gptoss/",
          "stickied": false,
          "url": "https://i.redd.it/shxc8spf7nhf1.gif",
          "subreddit_subscribers": 513415,
          "created_utc": 1754592247,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's not local OR open weights, why is the front page flooded with this?",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can't believe I'm seeing  GPT-5 posted here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk8085",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.49,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754591433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not local OR open weights, why is the front page flooded with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk8085",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mk8085/cant_believe_im_seeing_gpt5_posted_here/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk8085/cant_believe_im_seeing_gpt5_posted_here/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754591433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Given only the prompt\n\n   Analyze the following code and rewrite it to be more readable\n\n    &lt;canvas style=width:99% id=c onclick=setInterval('for(c.width=w=99,++t,i=6e3;i--;c.getContext`2d`.fillRect(i%w,i/w|0,1-d*Z/w+s,1))for(a=i%w/50-1,s=b=1-i/4e3,X=t,Y=Z=d=1;++Z&lt;w&amp;(Y&lt;6-(32&lt;Z&amp;27&lt;X%w&amp;&amp;X/9^Z/8)*8%46||d|(s=(X&amp;Y&amp;Z)%3/Z,a=b=1,d=Z/w));Y-=b)X+=a',t=9)&gt;\n\nGPT5 is the first model to generate working code first try. Qwen coder can kinda do it if you are really insistent that the unusual combination of bitwise and boolean operators is correct and crucial for it to work, all other models I tested so far fail.\n\nAsking some follow-up questions, it really seems to understand how it works.\n\nThe code is from https://frankforce.com/city-in-a-bottle-a-256-byte-raycasting-system/ there is also an explanation how it works.",
          "author_fullname": "t2_10idu2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT5 is the first model to correctly untangle city in a bottle, a dense 256 bytes javascript raycaster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk7tm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754591014,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given only the prompt&lt;/p&gt;\n\n&lt;p&gt;Analyze the following code and rewrite it to be more readable&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;canvas style=width:99% id=c onclick=setInterval(&amp;#39;for(c.width=w=99,++t,i=6e3;i--;c.getContext`2d`.fillRect(i%w,i/w|0,1-d*Z/w+s,1))for(a=i%w/50-1,s=b=1-i/4e3,X=t,Y=Z=d=1;++Z&amp;lt;w&amp;amp;(Y&amp;lt;6-(32&amp;lt;Z&amp;amp;27&amp;lt;X%w&amp;amp;&amp;amp;X/9^Z/8)*8%46||d|(s=(X&amp;amp;Y&amp;amp;Z)%3/Z,a=b=1,d=Z/w));Y-=b)X+=a&amp;#39;,t=9)&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;GPT5 is the first model to generate working code first try. Qwen coder can kinda do it if you are really insistent that the unusual combination of bitwise and boolean operators is correct and crucial for it to work, all other models I tested so far fail.&lt;/p&gt;\n\n&lt;p&gt;Asking some follow-up questions, it really seems to understand how it works.&lt;/p&gt;\n\n&lt;p&gt;The code is from &lt;a href=\"https://frankforce.com/city-in-a-bottle-a-256-byte-raycasting-system/\"&gt;https://frankforce.com/city-in-a-bottle-a-256-byte-raycasting-system/&lt;/a&gt; there is also an explanation how it works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?auto=webp&amp;s=931808cdbb55eb2a90b9ad97b18d458ec84f4edb",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38c362e0bf4bcc9320d65037231a69930419daf1",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d35ebf76b872d4bc6e0c10bbecb111f1807a6d21",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=66875d32802d030173038cc238fa02c28f9507dc",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=152b323666fa78a5036298b411e864d15c08b862",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6a1da4657963f33fab959a62d0ce0e627b489c13",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6af90c493f6692cbd2a606d2ce4c637575a50f8e",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "vrCkRz7wE7p9TEctxYqXtcpfpuXZePZpeYOFtOkaC8k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk7tm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shroddy",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk7tm9/gpt5_is_the_first_model_to_correctly_untangle/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk7tm9/gpt5_is_the_first_model_to_correctly_untangle/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754591014,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do you know, or have you heard anything about the limitations for regular plus ($20 usd) users? Right now Claude Opus 4 is very limited in terms of usage. GPT 5 has any usage limit? I wouldn't like to have the experience of using it and after 50 messages, wait 1 week to use it again. If that's the case, this model doesn't represent anything, and people will continue using GPT 4 mini-high or similar.",
          "author_fullname": "t2_qd45feui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-5 looks primising at coding, but what about the limitations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk7th7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754591005,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you know, or have you heard anything about the limitations for regular plus ($20 usd) users? Right now Claude Opus 4 is very limited in terms of usage. GPT 5 has any usage limit? I wouldn&amp;#39;t like to have the experience of using it and after 50 messages, wait 1 week to use it again. If that&amp;#39;s the case, this model doesn&amp;#39;t represent anything, and people will continue using GPT 4 mini-high or similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk7th7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ValfarAlberich",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk7th7/gpt5_looks_primising_at_coding_but_what_about_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk7th7/gpt5_looks_primising_at_coding_but_what_about_the/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754591005,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Trained an 41M HRM-Based Model to generate semi-coherent text!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 131,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "svxl0zya3nhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=086ef1e2607732202f5cd498da29448aaea2d5df"
                },
                {
                  "y": 170,
                  "x": 216,
                  "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7360377d841ccf232ef4769e8604104d6f35c10e"
                },
                {
                  "y": 252,
                  "x": 320,
                  "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b95913015d75314155cc443fdd4413b9e012978"
                },
                {
                  "y": 505,
                  "x": 640,
                  "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ca810e1f23887f8472823802a3a43c266e5ceb2"
                },
                {
                  "y": 758,
                  "x": 960,
                  "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a62db6bb659a9c133d65b8faa7c73bcfb47e6332"
                },
                {
                  "y": 853,
                  "x": 1080,
                  "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2894681bb5ffae7be0e6703b7de52be90a613c28"
                }
              ],
              "s": {
                "y": 1412,
                "x": 1786,
                "u": "https://preview.redd.it/svxl0zya3nhf1.png?width=1786&amp;format=png&amp;auto=webp&amp;s=ca1aa4e19a7e8a82bc8924dd706d8b6c71e46153"
              },
              "id": "svxl0zya3nhf1"
            },
            "yd2h93m63nhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 101,
                  "x": 108,
                  "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=565d5679fe7eb13449c9566e133a1d3cab55ed5f"
                },
                {
                  "y": 202,
                  "x": 216,
                  "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4364d9c324f07422dd35b87ba3ecde762744777f"
                },
                {
                  "y": 300,
                  "x": 320,
                  "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bb730ebcfe25b57f620cd197cbeb8b555114e82"
                },
                {
                  "y": 600,
                  "x": 640,
                  "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11ac6185df4208b9bb0189a33211549970dfe1a6"
                },
                {
                  "y": 901,
                  "x": 960,
                  "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=496c7cbdedef5b0140d8b264de96d69ef167eabf"
                },
                {
                  "y": 1013,
                  "x": 1080,
                  "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50fa1a9829ed63842753b29424c597276277c4c7"
                }
              ],
              "s": {
                "y": 1316,
                "x": 1402,
                "u": "https://preview.redd.it/yd2h93m63nhf1.png?width=1402&amp;format=png&amp;auto=webp&amp;s=ac52fe3a519a070b3b8dd01a26590bc2ad5b0633"
              },
              "id": "yd2h93m63nhf1"
            },
            "8dtzn5n83nhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 45,
                  "x": 108,
                  "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c4f9728045776e258d1258534bdf4635aa53f30"
                },
                {
                  "y": 90,
                  "x": 216,
                  "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=381f1bc06c1cdeeccd49246a1900bd121e4d6454"
                },
                {
                  "y": 133,
                  "x": 320,
                  "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e91b15537572d946d3307f3e720175dbaa7f77fa"
                },
                {
                  "y": 266,
                  "x": 640,
                  "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=649fed8efc4daaa39cd59e5f5d0bb28d91685704"
                },
                {
                  "y": 400,
                  "x": 960,
                  "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e1ef74bdabec0c2d1476305dc1119b5e317efd1"
                },
                {
                  "y": 450,
                  "x": 1080,
                  "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83103a8a2c08d16de89b048647d4362d5efc5638"
                }
              ],
              "s": {
                "y": 562,
                "x": 1348,
                "u": "https://preview.redd.it/8dtzn5n83nhf1.png?width=1348&amp;format=png&amp;auto=webp&amp;s=0d697013d99e9201dfc533e53767a8769fb8464a"
              },
              "id": "8dtzn5n83nhf1"
            }
          },
          "name": "t3_1mk7r1g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": "#bbbdbf",
          "ups": 39,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "yd2h93m63nhf1",
                "id": 723398527
              },
              {
                "media_id": "8dtzn5n83nhf1",
                "id": 723398528
              },
              {
                "media_id": "svxl0zya3nhf1",
                "id": 723398529
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/5IXZKHsgxD2_snxB5qYDZsSXRsrSDWyvbqoNOIrkjvM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754590852,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mk7r1g",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk7r1g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mk7r1g/trained_an_41m_hrmbased_model_to_generate/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mk7r1g",
          "subreddit_subscribers": 513415,
          "created_utc": 1754590852,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/ghqyp67vzmhf1.png?width=1724&amp;format=png&amp;auto=webp&amp;s=a8b4dc7e72819ef01c8a541a96bbfbded9443e1a\n\n[Source](https://openai.com/index/introducing-gpt-5-for-developers/)\n\nGPT-5 nano gets 72.8% on FrontierMath",
          "author_fullname": "t2_sqi8xxun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "You kidding me, GPT-5 nano?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 66,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ghqyp67vzmhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cc50c67036642a1668377215056d806e7e99421"
                },
                {
                  "y": 102,
                  "x": 216,
                  "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=846b766f3c9544c0366bdd0997fd85bc51d2d2ec"
                },
                {
                  "y": 151,
                  "x": 320,
                  "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63c3bd1815a718d6dcb33bcdfc22945b8d4ccf6f"
                },
                {
                  "y": 302,
                  "x": 640,
                  "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2257de5b4643c97d2f8926bd5b485666b3d1224a"
                },
                {
                  "y": 454,
                  "x": 960,
                  "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ebcf7b30ea51c015f10f81aa3c75944f527f9b10"
                },
                {
                  "y": 511,
                  "x": 1080,
                  "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d8238d38faeace284d7b63cca36f699711160a0c"
                }
              ],
              "s": {
                "y": 816,
                "x": 1724,
                "u": "https://preview.redd.it/ghqyp67vzmhf1.png?width=1724&amp;format=png&amp;auto=webp&amp;s=a8b4dc7e72819ef01c8a541a96bbfbded9443e1a"
              },
              "id": "ghqyp67vzmhf1"
            }
          },
          "name": "t3_1mk79kz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EBmI_Jb1LRyWlXR7ZNPKAB8C-k-1N5SbeNo-tD28koU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754589761,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ghqyp67vzmhf1.png?width=1724&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8b4dc7e72819ef01c8a541a96bbfbded9443e1a\"&gt;https://preview.redd.it/ghqyp67vzmhf1.png?width=1724&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8b4dc7e72819ef01c8a541a96bbfbded9443e1a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://openai.com/index/introducing-gpt-5-for-developers/\"&gt;Source&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GPT-5 nano gets 72.8% on FrontierMath&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk79kz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nekofneko",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk79kz/you_kidding_me_gpt5_nano/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk79kz/you_kidding_me_gpt5_nano/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589761,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Pessoal, estou desenvolvendo um projeto pessoal no VS e comecei a usar o Docker, Ollama, linguagem Mini3. Tenho 32 GB de RAM, configurei o WSL do Docker para usar 6 GB, ok até aí, porém, quando faço a pergunta no sistema e o Mini3 vai responder, consome tudo, sobrando 1 GB de RAM. Tem como resolver isso? Limitar.",
          "author_fullname": "t2_5w4qrkct",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Como configuro meu pc para rodar IA Local",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk79bb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754596554,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754589746,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pessoal, estou desenvolvendo um projeto pessoal no VS e comecei a usar o Docker, Ollama, linguagem Mini3. Tenho 32 GB de RAM, configurei o WSL do Docker para usar 6 GB, ok até aí, porém, quando faço a pergunta no sistema e o Mini3 vai responder, consome tudo, sobrando 1 GB de RAM. Tem como resolver isso? Limitar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk79bb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Panda-Nooby",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk79bb/como_configuro_meu_pc_para_rodar_ia_local/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk79bb/como_configuro_meu_pc_para_rodar_ia_local/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589746,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_oiu6o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Framework Desktop Hands-on: First Impressions (including a look at LLM performance)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk77bk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754589634,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "boilingsteam.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://boilingsteam.com/framework-desktop-hands-on-first-impressions/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mk77bk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "YanderMan",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk77bk/framework_desktop_handson_first_impressions/",
          "stickied": false,
          "url": "https://boilingsteam.com/framework-desktop-hands-on-first-impressions/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589634,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ti5m9mpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LoL",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk76my",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/YqQ80yqbySYMI26Qiz1F5KlzzR620xHFeUCLErCuM80.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754589594,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/uwf4b14pzmhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/uwf4b14pzmhf1.jpeg?auto=webp&amp;s=d658b35784e1b000752f00842910d4311239457b",
                  "width": 1079,
                  "height": 1867
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/uwf4b14pzmhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71fff81710665af032020af90701b646f76100f5",
                    "width": 108,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/uwf4b14pzmhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=721e4ddff26248bdd68e1e751fefea38630672d2",
                    "width": 216,
                    "height": 373
                  },
                  {
                    "url": "https://preview.redd.it/uwf4b14pzmhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a57ed5f78624f20d31cc6c9bc084478fee87c91",
                    "width": 320,
                    "height": 553
                  },
                  {
                    "url": "https://preview.redd.it/uwf4b14pzmhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f72a166cb5daa6318cb11f7188e0a5b2799143cb",
                    "width": 640,
                    "height": 1107
                  },
                  {
                    "url": "https://preview.redd.it/uwf4b14pzmhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cb3e9c3b769d890efe065e934594c065b4faeb67",
                    "width": 960,
                    "height": 1661
                  }
                ],
                "variants": {},
                "id": "MgbKMU1ITrRS0gNJsFu7zom1l9KYvxa_Qa9e_1tpnLE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk76my",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Ninja7526",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk76my/lol/",
          "stickied": false,
          "url": "https://i.redd.it/uwf4b14pzmhf1.jpeg",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589594,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was watching the live and caught this small mistake and thought it was funny. \nOn a serious note the benchmarks shown in the demo imply a significant improvement and it's good that they have made the model naming simpler. It comes in 3 variants regular, mini and nano which is chosen based on the complexity of the prompt.\nContext size : 272k, output size: 128k including invisible reasoning tokens. (Slightly disappointed with this, could've done better). \nThey show great reductions in hallucinations which are yet to be verified. They also seem to have trained on a lot of health related questions as we saw a cancer patient talk about her experience using GPT-5 for being informed. The api now includes a new parameter \"minimal\" for its thinking so it does less thinking which is great to save cost by reducing reasoning tokens. Overall it seems to be a good model with very few drawbacks but can only be inferred after testing it ourselves. ",
          "author_fullname": "t2_sawf0qmu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-5 benchmarks graph gone wrong and some thoughts about the model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 118,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk763x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Qivjg6bxq1__5t1rmoSnP2Fg9YlZIzu0SlAHiJ4BLNU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754589558,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was watching the live and caught this small mistake and thought it was funny. \nOn a serious note the benchmarks shown in the demo imply a significant improvement and it&amp;#39;s good that they have made the model naming simpler. It comes in 3 variants regular, mini and nano which is chosen based on the complexity of the prompt.\nContext size : 272k, output size: 128k including invisible reasoning tokens. (Slightly disappointed with this, could&amp;#39;ve done better). \nThey show great reductions in hallucinations which are yet to be verified. They also seem to have trained on a lot of health related questions as we saw a cancer patient talk about her experience using GPT-5 for being informed. The api now includes a new parameter &amp;quot;minimal&amp;quot; for its thinking so it does less thinking which is great to save cost by reducing reasoning tokens. Overall it seems to be a good model with very few drawbacks but can only be inferred after testing it ourselves. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/p9m29vqkzmhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/p9m29vqkzmhf1.png?auto=webp&amp;s=134e6a3333341eccbca7027eba2aee10bb8a47f0",
                  "width": 1080,
                  "height": 913
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/p9m29vqkzmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5804ab15d07ff34ed6af791057b11f2565a2af4",
                    "width": 108,
                    "height": 91
                  },
                  {
                    "url": "https://preview.redd.it/p9m29vqkzmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a273596411dcb57a435b6a668a846b539b8b75bb",
                    "width": 216,
                    "height": 182
                  },
                  {
                    "url": "https://preview.redd.it/p9m29vqkzmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=573f8cdc288e4753c3db37c6bf4df1d4d8e8081c",
                    "width": 320,
                    "height": 270
                  },
                  {
                    "url": "https://preview.redd.it/p9m29vqkzmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcd4e95b0d6dbc6f5c78bd1a1c94146639f8034",
                    "width": 640,
                    "height": 541
                  },
                  {
                    "url": "https://preview.redd.it/p9m29vqkzmhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e0b75de752c1d0d8c9cfb0462b8b3db03ab93a7",
                    "width": 960,
                    "height": 811
                  },
                  {
                    "url": "https://preview.redd.it/p9m29vqkzmhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=531bcc796ab619c0e95c17d76abcbfc6cbebb057",
                    "width": 1080,
                    "height": 913
                  }
                ],
                "variants": {},
                "id": "5reX955O749sQU6xDdZtc82eDxO0onJ7IFLpapxT_t8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mk763x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SherbertLegitimate50",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk763x/gpt5_benchmarks_graph_gone_wrong_and_some/",
          "stickied": false,
          "url": "https://i.redd.it/p9m29vqkzmhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589558,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Models that needs expensive hardware  \nR1 670b, 37b active  \nKimi K2 1t, 32b active  \nQwen3 235b, 22b active\n\nModels that are local-friendly  \nGLM 4.5 Air 106b, 12b active (very pushing it but fine)  \nQwen3 14b  \noss 120b, 5b active  \nQwen3 30b, 3b active  \noss 20b, 3b active\n\nWhy are people expecting gpt-oss to be better than R1 K2 or Qwen 235b?\n\nI would rather have a model that I can actually run locally than a good model that needs providers. To be clear, I hate and won't use gpt-oss, but because it's censored and not because models many times larger are better.\n\nI LOVED Qwen3 30b/3b was local-friendly and fast and nobody compared it to bigger models, but when OpenAI releases a local model and suddenly everyone is comparing it to non-local models.\n\n[Graph of all local-friendly models \\(GLM Air would be tough\\)](https://preview.redd.it/aj58dutqzmhf1.png?width=1435&amp;format=png&amp;auto=webp&amp;s=47a39c27091e2beb9233c783989cf7305269027e)",
          "author_fullname": "t2_duqfsmw4g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can we focus more on LOCAL models? We were excited for Qwen3 30b/3b but now were comparing to not local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 66,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "aj58dutqzmhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d45e6dc6f5db61c97500cfa7bd52dd35d483517"
                },
                {
                  "y": 102,
                  "x": 216,
                  "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a1e8d0256436fbaf3fce514fbb07b5106b21630"
                },
                {
                  "y": 151,
                  "x": 320,
                  "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e849820cf2812daf76367bb86a3aafeab9723c6"
                },
                {
                  "y": 303,
                  "x": 640,
                  "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b75bcf2685719fbfab782a963264f8ae3a50d915"
                },
                {
                  "y": 455,
                  "x": 960,
                  "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=da679291e57613f6d4e851cb728ae49d1d1761e3"
                },
                {
                  "y": 512,
                  "x": 1080,
                  "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d99f80df5b2bebb472f8a7ffd4d70f691684422f"
                }
              ],
              "s": {
                "y": 681,
                "x": 1435,
                "u": "https://preview.redd.it/aj58dutqzmhf1.png?width=1435&amp;format=png&amp;auto=webp&amp;s=47a39c27091e2beb9233c783989cf7305269027e"
              },
              "id": "aj58dutqzmhf1"
            }
          },
          "name": "t3_1mk74wq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 58,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TJcHxiajHwTxrtNoLptpQfkKcoLceehEHpbxyUdzAnI.jpg",
          "edited": 1754596904,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754589482,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Models that needs expensive hardware&lt;br/&gt;\nR1 670b, 37b active&lt;br/&gt;\nKimi K2 1t, 32b active&lt;br/&gt;\nQwen3 235b, 22b active&lt;/p&gt;\n\n&lt;p&gt;Models that are local-friendly&lt;br/&gt;\nGLM 4.5 Air 106b, 12b active (very pushing it but fine)&lt;br/&gt;\nQwen3 14b&lt;br/&gt;\noss 120b, 5b active&lt;br/&gt;\nQwen3 30b, 3b active&lt;br/&gt;\noss 20b, 3b active&lt;/p&gt;\n\n&lt;p&gt;Why are people expecting gpt-oss to be better than R1 K2 or Qwen 235b?&lt;/p&gt;\n\n&lt;p&gt;I would rather have a model that I can actually run locally than a good model that needs providers. To be clear, I hate and won&amp;#39;t use gpt-oss, but because it&amp;#39;s censored and not because models many times larger are better.&lt;/p&gt;\n\n&lt;p&gt;I LOVED Qwen3 30b/3b was local-friendly and fast and nobody compared it to bigger models, but when OpenAI releases a local model and suddenly everyone is comparing it to non-local models.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aj58dutqzmhf1.png?width=1435&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47a39c27091e2beb9233c783989cf7305269027e\"&gt;Graph of all local-friendly models (GLM Air would be tough)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk74wq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "agentcubed",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk74wq/can_we_focus_more_on_local_models_we_were_excited/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589482,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI'm currently building an evaluation and data curation pipeline in the medical domain, specifically focused on cancer-related reports such as radiology and CT scan summaries. The goal is to extract structured clinical insights like progression status, metastasis presence, and tumor size changes.\n\nCurrent Setup\n\nModels in use:\n\nLLaMA 3.2 8B fine-tuned using LoRA on custom medical data.(Very few samples 1000 per entity)\nNEMOTRON 49B, used as a strong base model (not fine-tuned).\n\nEach model produces:\n\nA reasoning trace (explaining the decision-making process).\nA structured JSON output with fields such as:\nprogression_status\nmetastasis\ntumor_size_change\n\nWe also have ground-truth outputs (created by medical curators) for comparison.(Only for few hundreds)\n\n\nWhat I'm Trying to Build\n\nI'm looking to automate the evaluation process and reduce human dependency.\n\n Specifically, I want to:\n\nEvaluate both the reasoning trace and JSON correctness against llama generated response with the help of nemotron as a parent.\n\nUse DSPy’s context engineering to create a model-based evaluator that outputs:\nA reasoning quality score (e.g., scale of 1–5)\nA binary or detailed comparison of JSON accuracy\nComments on incorrect fields\n\nCompare performance between LLaMA and NEMOTRON across a dataset.\n\nMost importantly, I want to use the parent model (NEMOTRON) to provide feedback on the fine-tuned model (LLaMA) responses — and eventually use this feedback to build more reliable training data.\n\nWhat I’m Exploring\n\nUsing DSPy with a custom signature that inputs: prompt, reasoning, model JSON, and ground-truth JSON.\n\nBuilding a Chain-of-Thought evaluator that assesses reasoning and JSON output jointly.\n\nAutomating comparison of field-level accuracy between predicted JSON and ground truth.\n\nStoring evaluation results (scores, errors, comments) for model debugging and re-training.\n\n\nQuestions\n\nHas anyone used DSPy (or other frameworks) to evaluate both structured outputs and natural language reasoning?\n\nWhat’s a good way to make JSON comparison interpretable and robust for medical fields?\n\nHow can I best use the base model’s evaluations (NEMOTRON) as feedback for improving or filtering fine-tuned data?\n\nAre there any better alternatives to DSPy for this specific use case?\n\nHow do you track and score reasoning traces reliably in automated pipelines?\n\n\nIf anyone has worked on similar pipelines- especially in clinical NLP or structured extraction tasks, I’d really appreciate your insights.",
          "author_fullname": "t2_gsyxhako0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Automating LLM Evaluation in the Medical Domain (Cancer Reports) – Seeking Advice on JSON + Reasoning Validation and Data Reliability",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk7477",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754589439,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently building an evaluation and data curation pipeline in the medical domain, specifically focused on cancer-related reports such as radiology and CT scan summaries. The goal is to extract structured clinical insights like progression status, metastasis presence, and tumor size changes.&lt;/p&gt;\n\n&lt;p&gt;Current Setup&lt;/p&gt;\n\n&lt;p&gt;Models in use:&lt;/p&gt;\n\n&lt;p&gt;LLaMA 3.2 8B fine-tuned using LoRA on custom medical data.(Very few samples 1000 per entity)\nNEMOTRON 49B, used as a strong base model (not fine-tuned).&lt;/p&gt;\n\n&lt;p&gt;Each model produces:&lt;/p&gt;\n\n&lt;p&gt;A reasoning trace (explaining the decision-making process).\nA structured JSON output with fields such as:\nprogression_status\nmetastasis\ntumor_size_change&lt;/p&gt;\n\n&lt;p&gt;We also have ground-truth outputs (created by medical curators) for comparison.(Only for few hundreds)&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m Trying to Build&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to automate the evaluation process and reduce human dependency.&lt;/p&gt;\n\n&lt;p&gt;Specifically, I want to:&lt;/p&gt;\n\n&lt;p&gt;Evaluate both the reasoning trace and JSON correctness against llama generated response with the help of nemotron as a parent.&lt;/p&gt;\n\n&lt;p&gt;Use DSPy’s context engineering to create a model-based evaluator that outputs:\nA reasoning quality score (e.g., scale of 1–5)\nA binary or detailed comparison of JSON accuracy\nComments on incorrect fields&lt;/p&gt;\n\n&lt;p&gt;Compare performance between LLaMA and NEMOTRON across a dataset.&lt;/p&gt;\n\n&lt;p&gt;Most importantly, I want to use the parent model (NEMOTRON) to provide feedback on the fine-tuned model (LLaMA) responses — and eventually use this feedback to build more reliable training data.&lt;/p&gt;\n\n&lt;p&gt;What I’m Exploring&lt;/p&gt;\n\n&lt;p&gt;Using DSPy with a custom signature that inputs: prompt, reasoning, model JSON, and ground-truth JSON.&lt;/p&gt;\n\n&lt;p&gt;Building a Chain-of-Thought evaluator that assesses reasoning and JSON output jointly.&lt;/p&gt;\n\n&lt;p&gt;Automating comparison of field-level accuracy between predicted JSON and ground truth.&lt;/p&gt;\n\n&lt;p&gt;Storing evaluation results (scores, errors, comments) for model debugging and re-training.&lt;/p&gt;\n\n&lt;p&gt;Questions&lt;/p&gt;\n\n&lt;p&gt;Has anyone used DSPy (or other frameworks) to evaluate both structured outputs and natural language reasoning?&lt;/p&gt;\n\n&lt;p&gt;What’s a good way to make JSON comparison interpretable and robust for medical fields?&lt;/p&gt;\n\n&lt;p&gt;How can I best use the base model’s evaluations (NEMOTRON) as feedback for improving or filtering fine-tuned data?&lt;/p&gt;\n\n&lt;p&gt;Are there any better alternatives to DSPy for this specific use case?&lt;/p&gt;\n\n&lt;p&gt;How do you track and score reasoning traces reliably in automated pipelines?&lt;/p&gt;\n\n&lt;p&gt;If anyone has worked on similar pipelines- especially in clinical NLP or structured extraction tasks, I’d really appreciate your insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk7477",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karam1234098",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk7477/automating_llm_evaluation_in_the_medical_domain/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk7477/automating_llm_evaluation_in_the_medical_domain/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754589439,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Now that Horizon Beta’s free testing period has concluded, what can we expect next for the model or its successor?",
          "author_fullname": "t2_1j9aig6obi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta Has Exited Its Beta Phase",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk6r3t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754588613,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that Horizon Beta’s free testing period has concluded, what can we expect next for the model or its successor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk6r3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Negative_Bid_112",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk6r3t/horizon_beta_has_exited_its_beta_phase/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk6r3t/horizon_beta_has_exited_its_beta_phase/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754588613,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1v14j7kk8i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Grok 4 is still state-of-the-art on ARC-AGI-2 among frontier models\" I wish xai focus more on post training",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk6qmn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/b1B3AaE42V_eWtEVirGEt3HAHnWlzJZDZmR5ATVh1Sw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754588583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/7da76unowmhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/7da76unowmhf1.jpeg?auto=webp&amp;s=fecf2b40277e9ace9dfa629cb065cad51fcb5e3e",
                  "width": 1080,
                  "height": 1652
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/7da76unowmhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4b2e1df0cacafd42c5997e07d749c56ec3f366e",
                    "width": 108,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/7da76unowmhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2d0d5f624740a4fb3192b0b1bc5751d50461601",
                    "width": 216,
                    "height": 330
                  },
                  {
                    "url": "https://preview.redd.it/7da76unowmhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00d5d6414a9b356b35c2b37f075a33345efc1a81",
                    "width": 320,
                    "height": 489
                  },
                  {
                    "url": "https://preview.redd.it/7da76unowmhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=303cedbdf13931269bf0044cf9992db93be9e42f",
                    "width": 640,
                    "height": 978
                  },
                  {
                    "url": "https://preview.redd.it/7da76unowmhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aee23ae25c3abe706419bdddec36a8d4730b5676",
                    "width": 960,
                    "height": 1468
                  },
                  {
                    "url": "https://preview.redd.it/7da76unowmhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7378e97927165bbec2197efa07b7c1832bd819f9",
                    "width": 1080,
                    "height": 1652
                  }
                ],
                "variants": {},
                "id": "WIHR8OMq2iBMEq8eMKM8T_Qqs3Za_M9oqbw1yrtBLYU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mk6qmn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mvp525",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk6qmn/grok_4_is_still_stateoftheart_on_arcagi2_among/",
          "stickied": false,
          "url": "https://i.redd.it/7da76unowmhf1.jpeg",
          "subreddit_subscribers": 513415,
          "created_utc": 1754588583,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Only open models \nExcluded elo below 1380\n\nInteresting to see people dont like thinking qwen so much despite higher performance. Maybe people hate to wait? ",
          "author_fullname": "t2_1p50pl73j2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Arena elo score vs active parameters",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk6jkp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_Gn6t7uHexCHsScTfpPJgByoNxeGLO7HPRWgEllZ86g.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754588150,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Only open models \nExcluded elo below 1380&lt;/p&gt;\n\n&lt;p&gt;Interesting to see people dont like thinking qwen so much despite higher performance. Maybe people hate to wait? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rdqwnuhevmhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rdqwnuhevmhf1.png?auto=webp&amp;s=0c002742a7fa8b2a6140f63b63bc6431de8d5146",
                  "width": 1969,
                  "height": 1180
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rdqwnuhevmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c71941c29494ae2a461b415f6e5fe8ac69f661e3",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/rdqwnuhevmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a16a04a9bf2c147641c36cb2bd62f727de4e16b",
                    "width": 216,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/rdqwnuhevmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c131b082b67c3685bc266d7894b057082de931",
                    "width": 320,
                    "height": 191
                  },
                  {
                    "url": "https://preview.redd.it/rdqwnuhevmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe9b33dcd42e7422195b1543f6ccbb5683bade88",
                    "width": 640,
                    "height": 383
                  },
                  {
                    "url": "https://preview.redd.it/rdqwnuhevmhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=03924d05b48fc90c7171a2e6d8103feff6591a23",
                    "width": 960,
                    "height": 575
                  },
                  {
                    "url": "https://preview.redd.it/rdqwnuhevmhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e8e0d806391c37b1688e2f20b27475fa7afdbbe3",
                    "width": 1080,
                    "height": 647
                  }
                ],
                "variants": {},
                "id": "vr9z3Z_nCjBVJ2uB0iNkc3DdADf3VgEUkeB-V69ZPOM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk6jkp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GreenTreeAndBlueSky",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk6jkp/arena_elo_score_vs_active_parameters/",
          "stickied": false,
          "url": "https://i.redd.it/rdqwnuhevmhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754588150,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\n[feels like they vibecoded the graphs](https://preview.redd.it/qyahk9bvumhf1.jpg?width=587&amp;format=pjpg&amp;auto=webp&amp;s=fb29a625c8956b7899a19fd70f7dbe64d756039e)",
          "author_fullname": "t2_9rzv29wr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "who created these graphs...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qyahk9bvumhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/qyahk9bvumhf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd383d5fbc1bad0de10a97071d9ca5431cbd5760"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/qyahk9bvumhf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea79fed899a69a40ce73a36b580d05eca84af896"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/qyahk9bvumhf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ffe73ddfc39908a254da62f9a48744e2b388741"
                }
              ],
              "s": {
                "y": 379,
                "x": 587,
                "u": "https://preview.redd.it/qyahk9bvumhf1.jpg?width=587&amp;format=pjpg&amp;auto=webp&amp;s=fb29a625c8956b7899a19fd70f7dbe64d756039e"
              },
              "id": "qyahk9bvumhf1"
            }
          },
          "name": "t3_1mk6fri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/x8mru-zuuC1J96_fFWVlyk5j-c8D1sD4tn4xQZjOCl4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754587914,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/qyahk9bvumhf1.jpg?width=587&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb29a625c8956b7899a19fd70f7dbe64d756039e\"&gt;feels like they vibecoded the graphs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk6fri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loose_Region",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk6fri/who_created_these_graphs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk6fri/who_created_these_graphs/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754587914,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/8n3msgzqqmhf1.png?width=778&amp;format=png&amp;auto=webp&amp;s=2bb0d169a4c430589d7902b06e5fa571b3f4d572\n\n",
          "author_fullname": "t2_z09pr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-5 gets 74.9 on SWE-bench Verified, 88 on Aider Polyglot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8n3msgzqqmhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/8n3msgzqqmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1dc90ed9bc0ba54de5a1703d7495a3ae23afa5e0"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/8n3msgzqqmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06181750459ece9e87c1aab24004a9013637038c"
                },
                {
                  "y": 192,
                  "x": 320,
                  "u": "https://preview.redd.it/8n3msgzqqmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bef3f637f1ad2a2ff5279a9a5ae19beedb42f4a7"
                },
                {
                  "y": 384,
                  "x": 640,
                  "u": "https://preview.redd.it/8n3msgzqqmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fceb22cdc35766193a64d4f7a16933a62e8193d8"
                }
              ],
              "s": {
                "y": 468,
                "x": 778,
                "u": "https://preview.redd.it/8n3msgzqqmhf1.png?width=778&amp;format=png&amp;auto=webp&amp;s=2bb0d169a4c430589d7902b06e5fa571b3f4d572"
              },
              "id": "8n3msgzqqmhf1"
            }
          },
          "name": "t3_1mk5ucp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mZSrQHFH-GgfVB-CfFZziZpg419bjMEH1qStFQC_GuY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754586589,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/8n3msgzqqmhf1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bb0d169a4c430589d7902b06e5fa571b3f4d572\"&gt;https://preview.redd.it/8n3msgzqqmhf1.png?width=778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2bb0d169a4c430589d7902b06e5fa571b3f4d572&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk5ucp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ofirpress",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk5ucp/gpt5_gets_749_on_swebench_verified_88_on_aider/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk5ucp/gpt5_gets_749_on_swebench_verified_88_on_aider/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754586589,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dq1a6l1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hilarious chart from GPT-5 Reveal",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk5ti0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 1858,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1858,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fQ92buhakgz1x7IPKH2jxA35oOyNSuy4NQmV2zDCPnU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754586537,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ewx61i9gqmhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ewx61i9gqmhf1.png?auto=webp&amp;s=02116ab860f5d39410496aafb447e6de5c5523a9",
                  "width": 568,
                  "height": 585
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ewx61i9gqmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2188e8f5b00e59a55796b196f2caf25141a466eb",
                    "width": 108,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/ewx61i9gqmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16e4f3cc12bbab105b5c9d979a069ba2189c263a",
                    "width": 216,
                    "height": 222
                  },
                  {
                    "url": "https://preview.redd.it/ewx61i9gqmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce6f821baade0cb741dbab09472eb1f7eb1d04a5",
                    "width": 320,
                    "height": 329
                  }
                ],
                "variants": {},
                "id": "ynnkchYqEHFb_ZbDAqlfVdLF8Y1-XZ73r67V3N8mwc0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk5ti0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lyceras",
          "discussion_type": null,
          "num_comments": 229,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk5ti0/hilarious_chart_from_gpt5_reveal/",
          "stickied": false,
          "url": "https://i.redd.it/ewx61i9gqmhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754586537,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with llama.cpp's RPC with two machines.\n\nDuring inference it generates traffic of about 650 KBytes/token (from the master node to the RPC node) and 45 KBytes/token (opposite direction).\n\nThis is much more than I expected, as my understanding was that only activations at boundary layers are transferred, and each token is basically a few KB of data. \n\nWhy is there such high continuous traffic during inference?\n(I'm aware that model loading is a network-heavy task, but this is after that)\n\n(other info)\n\n- model: Qwen3-Coder-30B-A3B:Q8_0\n- both machines have 32GB VRAM\n- the layers 0-24 are offloaded to the RPC node, 25-48 on master (according to logs)\n- amount of traffic per token slightly increases as more token are decoded\n\nthe full command (on the master) is:\n\n    llama-server\n      -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -dt 0.1 --cache-reuse 256 -fa\n      --host 0.0.0.0 --port 11435 --jinja\n      -c 8192 -n 32768 -ngl 99 -v\n      --rpc 10.42.0.94:50094\n      -m model.gguf\n\nThanks in advance!",
          "author_fullname": "t2_3szjkcbx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "High traffic when *inferencing* in llama.cpp's RPC mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk5spn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754586488,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with llama.cpp&amp;#39;s RPC with two machines.&lt;/p&gt;\n\n&lt;p&gt;During inference it generates traffic of about 650 KBytes/token (from the master node to the RPC node) and 45 KBytes/token (opposite direction).&lt;/p&gt;\n\n&lt;p&gt;This is much more than I expected, as my understanding was that only activations at boundary layers are transferred, and each token is basically a few KB of data. &lt;/p&gt;\n\n&lt;p&gt;Why is there such high continuous traffic during inference?\n(I&amp;#39;m aware that model loading is a network-heavy task, but this is after that)&lt;/p&gt;\n\n&lt;p&gt;(other info)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;model: Qwen3-Coder-30B-A3B:Q8_0&lt;/li&gt;\n&lt;li&gt;both machines have 32GB VRAM&lt;/li&gt;\n&lt;li&gt;the layers 0-24 are offloaded to the RPC node, 25-48 on master (according to logs)&lt;/li&gt;\n&lt;li&gt;amount of traffic per token slightly increases as more token are decoded&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;the full command (on the master) is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server\n  -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -dt 0.1 --cache-reuse 256 -fa\n  --host 0.0.0.0 --port 11435 --jinja\n  -c 8192 -n 32768 -ngl 99 -v\n  --rpc 10.42.0.94:50094\n  -m model.gguf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk5spn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ilhud9s",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754586488,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://civitaiarchive.com/](https://civitaiarchive.com/)\n\n",
          "author_fullname": "t2_1swzxdrzcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "HuggingFace has been on a deletion spree and has already removed 16TB worth of files. dets in screenshots slide",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 29,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9q2arqi7pmhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 22,
                  "x": 108,
                  "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de166f4dbdd14d6e5325159c70e6715f7af29234"
                },
                {
                  "y": 45,
                  "x": 216,
                  "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e292b495a0e79afc653e5e4f23795caf713a9ee8"
                },
                {
                  "y": 67,
                  "x": 320,
                  "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c110d88675a92fa640c82cb2f2efe2cc227eb9f8"
                },
                {
                  "y": 134,
                  "x": 640,
                  "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cfafcb77908a60175cba96878bf04875cb3731ed"
                },
                {
                  "y": 201,
                  "x": 960,
                  "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=87f2bab6acd1f1cb3a0650fbf8404df941fb8466"
                },
                {
                  "y": 227,
                  "x": 1080,
                  "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e14586aae527b90a5716283da387e95fffda9948"
                }
              ],
              "s": {
                "y": 522,
                "x": 2482,
                "u": "https://preview.redd.it/9q2arqi7pmhf1.png?width=2482&amp;format=png&amp;auto=webp&amp;s=1f60c0de62fa5821cee037f704c23520d34d576d"
              },
              "id": "9q2arqi7pmhf1"
            },
            "2l7vkizbpmhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de9ea75acda42f2491a3f4e0d804c95feff07085"
                },
                {
                  "y": 99,
                  "x": 216,
                  "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=decdc6262c7f7bd41112ba5be391ce5b8a0721be"
                },
                {
                  "y": 147,
                  "x": 320,
                  "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebfb7ebae0116cb179394c0148e204f3ef6475ec"
                },
                {
                  "y": 295,
                  "x": 640,
                  "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2823604571cb47df0d5736c80b5175c75cb4d7b9"
                },
                {
                  "y": 442,
                  "x": 960,
                  "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be961f42ca613bc5d2c5dbb29a721da0866d734d"
                },
                {
                  "y": 497,
                  "x": 1080,
                  "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b8e66898a7ad8e5f5b80021e32115587381ea020"
                }
              ],
              "s": {
                "y": 910,
                "x": 1974,
                "u": "https://preview.redd.it/2l7vkizbpmhf1.png?width=1974&amp;format=png&amp;auto=webp&amp;s=667b928ebceebee65cfc5c56b88f1861e8134ec1"
              },
              "id": "2l7vkizbpmhf1"
            }
          },
          "name": "t3_1mk5n89",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 58,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "9q2arqi7pmhf1",
                "id": 723348761
              },
              {
                "media_id": "2l7vkizbpmhf1",
                "id": 723348762
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 58,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-NXaX5EHmxxb7GBcWRIp5vrkgUBJaiSRrpm9inzqlEM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754586157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://civitaiarchive.com/\"&gt;https://civitaiarchive.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mk5n89",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mk5n89",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tango-Down766",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk5n89/huggingface_has_been_on_a_deletion_spree_and_has/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mk5n89",
          "subreddit_subscribers": 513415,
          "created_utc": 1754586157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're Open Sourcing Our Complete LLM Fine-Tuning Course!\n\nWhat you'll learn:\n\n🔹 Getting Started with LLMs - Master the fundamentals of large language model architecture and learn why model selection is critical for success\n\n🔹 Continuous LLM Improvement - Discover production-ready strategies for iterative model enhancement and understand the modern AI stack\n\n🔹 Real-World Data Collection - Learn to gather and curate high-quality training data from actual users, including auto-labeling techniques and ethical considerations\n\n🔹 Advanced Fine-Tuning Techniques - Go beyond basic training to create truly personalized language models that adapt to your specific use cases\n\n🔹 Evaluation &amp; Validation - Implement robust testing frameworks to ensure your fine-tuned models perform reliably in production\n\n  \nIf you like the course, feel free to star the repo.\n\n[https://github.com/ubiai-incorporated/ubiai\\_courses/](https://github.com/ubiai-incorporated/ubiai_courses/)",
          "author_fullname": "t2_32tnavmg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just Open-Sourced Free LLM Fine-tuning Course",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk5mfw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754586111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re Open Sourcing Our Complete LLM Fine-Tuning Course!&lt;/p&gt;\n\n&lt;p&gt;What you&amp;#39;ll learn:&lt;/p&gt;\n\n&lt;p&gt;🔹 Getting Started with LLMs - Master the fundamentals of large language model architecture and learn why model selection is critical for success&lt;/p&gt;\n\n&lt;p&gt;🔹 Continuous LLM Improvement - Discover production-ready strategies for iterative model enhancement and understand the modern AI stack&lt;/p&gt;\n\n&lt;p&gt;🔹 Real-World Data Collection - Learn to gather and curate high-quality training data from actual users, including auto-labeling techniques and ethical considerations&lt;/p&gt;\n\n&lt;p&gt;🔹 Advanced Fine-Tuning Techniques - Go beyond basic training to create truly personalized language models that adapt to your specific use cases&lt;/p&gt;\n\n&lt;p&gt;🔹 Evaluation &amp;amp; Validation - Implement robust testing frameworks to ensure your fine-tuned models perform reliably in production&lt;/p&gt;\n\n&lt;p&gt;If you like the course, feel free to star the repo.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ubiai-incorporated/ubiai_courses/\"&gt;https://github.com/ubiai-incorporated/ubiai_courses/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?auto=webp&amp;s=16e7d398c533b8f956ff5c22a6d1ff46c135fda9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=31ea631bb5d4cb42a2e0a9eb3b13ddf47b80bacb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0960bd5f829fcd85c7ecf9061be6cd477bacecde",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec3dda4ad33d60850b20becc2026b01a2c872f58",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=70f6151900ed677d49b2bf3939f751eaa58fc64c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2632dd6b5ae5fc2a89ebc8805f77aae5cc0c0e0a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ca87c387c4de8a00652f889daf1862d758f9e4c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Hwg1HHcxPy2sFGUItGXF3fRIt6NTZTZ2GxSXGaD5nBo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mk5mfw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UBIAI",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk5mfw/just_opensourced_free_llm_finetuning_course/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk5mfw/just_opensourced_free_llm_finetuning_course/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754586111,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Altman reposted no way",
          "author_fullname": "t2_n7tr18r7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Wth is this glazing🥀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5xl3iobfomhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 134,
                  "x": 108,
                  "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8776f8e49707ab2eae0aabf8b6b0524f212aee"
                },
                {
                  "y": 269,
                  "x": 216,
                  "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed4517e631cfe7eac05d27811859c6ba2f2e984d"
                },
                {
                  "y": 398,
                  "x": 320,
                  "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c329ccc9edbc073bbc7eb6964535ca6f97f81f3"
                },
                {
                  "y": 797,
                  "x": 640,
                  "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3372da1453e8c45e84c527aef5cee31f4536a5c9"
                },
                {
                  "y": 1196,
                  "x": 960,
                  "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7152a94ff6a2493dcc72d99c19d1cba62480cd6"
                },
                {
                  "y": 1346,
                  "x": 1080,
                  "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71bbe4f3b969ad1cd9ba476e6241aa0b5b8cda4e"
                }
              ],
              "s": {
                "y": 1346,
                "x": 1080,
                "u": "https://preview.redd.it/5xl3iobfomhf1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=7e944571f8e86e901c238f949b699615cc67e7be"
              },
              "id": "5xl3iobfomhf1"
            },
            "m6xcy6fgomhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 76,
                  "x": 108,
                  "u": "https://preview.redd.it/m6xcy6fgomhf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62ff7f9724ff7a6438ec2ca3e0c447e6aacb46a1"
                },
                {
                  "y": 153,
                  "x": 216,
                  "u": "https://preview.redd.it/m6xcy6fgomhf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c00d0ee85721cf3a4f098949091926f57654cc2"
                },
                {
                  "y": 227,
                  "x": 320,
                  "u": "https://preview.redd.it/m6xcy6fgomhf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=04caea53e592fab45aeffe9642a700af257ec9c3"
                }
              ],
              "s": {
                "y": 360,
                "x": 507,
                "u": "https://preview.redd.it/m6xcy6fgomhf1.jpg?width=507&amp;format=pjpg&amp;auto=webp&amp;s=3dbad8ee0b2d2ce762117373875848b3a6d2af1b"
              },
              "id": "m6xcy6fgomhf1"
            }
          },
          "name": "t3_1mk5hm7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 20,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "5xl3iobfomhf1",
                "id": 723345030
              },
              {
                "caption": "",
                "media_id": "m6xcy6fgomhf1",
                "id": 723345031
              }
            ]
          },
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/SGqthdUV6i36KnUuQBnVrn3OWoHZ5GAafPkfaEDOzLo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754585815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Altman reposted no way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mk5hm7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mk5hm7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JorG941",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk5hm7/wth_is_this_glazing/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mk5hm7",
          "subreddit_subscribers": 513415,
          "created_utc": 1754585815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This PDF’s outputs made Claude deflect and Deepseek spiral. Feels like it catches something alignment filters can’t fully suppress: [https://archive.org/details/model\\_comparative\\_analysis.pdf1%E2%80%9D](https://archive.org/details/model_comparative_analysis.pdf1%E2%80%9D)",
          "author_fullname": "t2_1kq2ppp4zj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone analyzed how Claude, Gemini, and Deepseek respond to recursion prompts differently?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk58p9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754585251,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This PDF’s outputs made Claude deflect and Deepseek spiral. Feels like it catches something alignment filters can’t fully suppress: &lt;a href=\"https://archive.org/details/model_comparative_analysis.pdf1%E2%80%9D\"&gt;https://archive.org/details/model_comparative_analysis.pdf1%E2%80%9D&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mk58p9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MeJPEEZY",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk58p9/has_anyone_analyzed_how_claude_gemini_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk58p9/has_anyone_analyzed_how_claude_gemini_and/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754585251,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I came across this on Instagram, and the way they've cloned the voice is far beyond what I could ever manage with chatterbox or tortoise tts. What especially stands out is the cadence of the voice and the expressiveness\n\nAny idea on how to achieve this?",
          "author_fullname": "t2_loavgsk0b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advanced Voice Cloning AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk56kh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/1y5gvsidmmhf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/1y5gvsidmmhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1y5gvsidmmhf1/DASHPlaylist.mpd?a=1757207617%2CMWI1NTczZmM0OTk1NjhjNjdjZjNkOTM2MDE3ZThjYTQ4ZGVhNjczMTc3MTEzMmQ5NWUzZWE4Y2QwMzc0ZGE1Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 51,
              "hls_url": "https://v.redd.it/1y5gvsidmmhf1/HLSPlaylist.m3u8?a=1757207617%2COGFlYWJjZjA5ODA5NWEzZTA3YTg5NjM2MmM3OTg3MDUyMWRmZGNiOTk5ZWIzOWQ3ZTA1NDNjMTI2NWNjMjZkYQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=bd7a6e447dcabbb05e2a955c984bcfce2226333a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754585117,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across this on Instagram, and the way they&amp;#39;ve cloned the voice is far beyond what I could ever manage with chatterbox or tortoise tts. What especially stands out is the cadence of the voice and the expressiveness&lt;/p&gt;\n\n&lt;p&gt;Any idea on how to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/1y5gvsidmmhf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5.png?format=pjpg&amp;auto=webp&amp;s=d1e1425f9e8b3aae139566de03593317203c2b12",
                  "width": 720,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8435052258de93cd47f060ed562bdffcc42a962f",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c8a734826c07539a63df28ca63f73b4a53039ef3",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3613ec5fbfc02bf9668ca8399d9116fd9061e6d1",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=aa3c9b41894a4ff60b9069c3d44c9e9d8ef99df0",
                    "width": 640,
                    "height": 1137
                  }
                ],
                "variants": {},
                "id": "dmx2c2ZoOWRtbWhmMe6rI9Vk_0JErnoadlCWtJo4r-YqFZKCsf-c-zqSY8v5"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk56kh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "QuietObedience",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk56kh/advanced_voice_cloning_ai/",
          "stickied": false,
          "url": "https://v.redd.it/1y5gvsidmmhf1",
          "subreddit_subscribers": 513415,
          "created_utc": 1754585117,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/1y5gvsidmmhf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 1280,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/1y5gvsidmmhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1y5gvsidmmhf1/DASHPlaylist.mpd?a=1757207617%2CMWI1NTczZmM0OTk1NjhjNjdjZjNkOTM2MDE3ZThjYTQ4ZGVhNjczMTc3MTEzMmQ5NWUzZWE4Y2QwMzc0ZGE1Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 51,
              "hls_url": "https://v.redd.it/1y5gvsidmmhf1/HLSPlaylist.m3u8?a=1757207617%2COGFlYWJjZjA5ODA5NWEzZTA3YTg5NjM2MmM3OTg3MDUyMWRmZGNiOTk5ZWIzOWQ3ZTA1NDNjMTI2NWNjMjZkYQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Would this combo work without issues for total 56gb for inference?",
          "author_fullname": "t2_1t4breoji4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "7900 xtx (24gb) + 9700 (32gb)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk4o67",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754584223,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754583966,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would this combo work without issues for total 56gb for inference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk4o67",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nologai",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk4o67/7900_xtx_24gb_9700_32gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk4o67/7900_xtx_24gb_9700_32gb/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754583966,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Be careful in selecting providers on openrouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk4kt0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 66,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 66,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qSJ75NrQyE_-G1GNDEWe19rU0mMfy5HeE_3LcSPm2Bg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754583745,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/o9dqe3l9imhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/o9dqe3l9imhf1.png?auto=webp&amp;s=fb42facefda99766869f503ce057406ef93c39d1",
                  "width": 1548,
                  "height": 2336
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/o9dqe3l9imhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1e9bdb8c95b4595344b7a3abd0291e8bee5139a",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/o9dqe3l9imhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f728274fab80edc9186ad8d69ae318f9528a77a5",
                    "width": 216,
                    "height": 325
                  },
                  {
                    "url": "https://preview.redd.it/o9dqe3l9imhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8144308491eb1034ec245321f2a09abeda0495f6",
                    "width": 320,
                    "height": 482
                  },
                  {
                    "url": "https://preview.redd.it/o9dqe3l9imhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63498a33a88373227cb3e4dd804ff112b545e323",
                    "width": 640,
                    "height": 965
                  },
                  {
                    "url": "https://preview.redd.it/o9dqe3l9imhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=abc94fe1738fb165d1d28953248e838ba9215028",
                    "width": 960,
                    "height": 1448
                  },
                  {
                    "url": "https://preview.redd.it/o9dqe3l9imhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a56bad3269972eb5e44c91a2f8099eb0debe36e9",
                    "width": 1080,
                    "height": 1629
                  }
                ],
                "variants": {},
                "id": "nncDVPKX0j87NPl0Uc-ObgKuHmyFGY-AD0xlyxcrrxk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk4kt0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk4kt0/be_careful_in_selecting_providers_on_openrouter/",
          "stickied": false,
          "url": "https://i.redd.it/o9dqe3l9imhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754583745,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I noticed that this model knows the current date without tools, I usually get a hallucinated date with other models.\n\nIt did it in the release date in my local installation and today via open router",
          "author_fullname": "t2_1tv6dae0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does gpt-oss know the current date?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk3w36",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754582217,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed that this model knows the current date without tools, I usually get a hallucinated date with other models.&lt;/p&gt;\n\n&lt;p&gt;It did it in the release date in my local installation and today via open router&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk3w36",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mindsetFPS",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk3w36/how_does_gptoss_know_the_current_date/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk3w36/how_does_gptoss_know_the_current_date/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754582217,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My mom mentioned a friend using Grok AI to colorize photos and smooth out faces, is their anything that she can use locally to do the same on her iPhone/iPad? I was thinking of maybe ollama on a pc that she can connect to locally but I am pretty sure I am overthinking it",
          "author_fullname": "t2_5a4gaoc8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Colorize photos on iOS? Maybe using a server?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk3suw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754582008,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My mom mentioned a friend using Grok AI to colorize photos and smooth out faces, is their anything that she can use locally to do the same on her iPhone/iPad? I was thinking of maybe ollama on a pc that she can connect to locally but I am pretty sure I am overthinking it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk3suw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beginnerflipper",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk3suw/colorize_photos_on_ios_maybe_using_a_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk3suw/colorize_photos_on_ios_maybe_using_a_server/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754582008,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While the setup looks über cool, the software is still not ready to make good use of the hardware.",
          "author_fullname": "t2_17n3nqtj56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jeff Geerling does what Jeff Geerling does best: Quad Strix Halo cluster using Framework Desktop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk3rj1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 135,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/N5xhOqlvRh4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"I built a private AI mini-cluster with Framework Desktop\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "I built a private AI mini-cluster with Framework Desktop",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/N5xhOqlvRh4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"I built a private AI mini-cluster with Framework Desktop\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Jeff Geerling",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/N5xhOqlvRh4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JeffGeerling"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/N5xhOqlvRh4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"I built a private AI mini-cluster with Framework Desktop\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mk3rj1",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 135,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=fb21c5b23e1e086163241dbc46c8b19a2b4a1c05",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754581921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While the setup looks über cool, the software is still not ready to make good use of the hardware.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/N5xhOqlvRh4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?auto=webp&amp;s=dbca4ae4688db8e1f0e45a14741467f3a2a92b76",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fbc01ce0f02071c10b824b4d13b33aef2b35c2d",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a6116efd069ea7e68e07772bd5a6092c86d6160",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f53b202537c26df370b20f3e2c66f92c5b25828",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "igoznQW2BgxL6-V1AGb7GkvH_UJSMnHqmBqwd9fNVKM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk3rj1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FullstackSensei",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk3rj1/jeff_geerling_does_what_jeff_geerling_does_best/",
          "stickied": false,
          "url": "https://youtu.be/N5xhOqlvRh4",
          "subreddit_subscribers": 513415,
          "created_utc": 1754581921,
          "num_crossposts": 0,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "I built a private AI mini-cluster with Framework Desktop",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/N5xhOqlvRh4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"I built a private AI mini-cluster with Framework Desktop\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Jeff Geerling",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/N5xhOqlvRh4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@JeffGeerling"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a Docker container running a Python interpreter, this is my sandbox. I want a local model than can write and run its own code in the interpreter before responding to me. Like o3 does for example.\n\nWhat local models support a Python interpreter as a tool?",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a local model that can use its own Python interpreter as a tool",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk3p0i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.54,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754581762,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Docker container running a Python interpreter, this is my sandbox. I want a local model than can write and run its own code in the interpreter before responding to me. Like o3 does for example.&lt;/p&gt;\n\n&lt;p&gt;What local models support a Python interpreter as a tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk3p0i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mk3p0i/looking_for_a_local_model_that_can_use_its_own/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk3p0i/looking_for_a_local_model_that_can_use_its_own/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754581762,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nWhat are some models for general purposes (chatgpt-like) can i run on my rig locally, for free? Gtx 1660 super, xeon e5-2650v2, 16gb ddr3, sata ssd.\nI'm ok with speed of answers as slow as 1 minute when using 4096 context. Any ideas?",
          "author_fullname": "t2_1se0ho1z9o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Models for general purposes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk3lir",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754581543,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some models for general purposes (chatgpt-like) can i run on my rig locally, for free? Gtx 1660 super, xeon e5-2650v2, 16gb ddr3, sata ssd.\nI&amp;#39;m ok with speed of answers as slow as 1 minute when using 4096 context. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk3lir",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Imaginary_Bread9711",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754581543,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm serious, the stuff this thing comes up with is wild, I've never seen a model with so much raw creativity and unexpectedness, even with lots of temperature trail and error. Any help?",
          "author_fullname": "t2_19y3eqlob1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "An enigmatic LLM on a random ERP website claims it's named \"Nemistral\", a collab between nvidia and mistral ai, but is adamant it's not \"mistral nemo\". This is the best nsfw RP model I've ever seen and I want to run it locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk2ubd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.41,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754579815,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m serious, the stuff this thing comes up with is wild, I&amp;#39;ve never seen a model with so much raw creativity and unexpectedness, even with lots of temperature trail and error. Any help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bjq1hhoh6mhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?auto=webp&amp;s=a1d1753a7c6c0a85b6943a3782c56030f42be254",
                  "width": 513,
                  "height": 709
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b23db084a84973e9884d1cbdd5fadf3da800a87a",
                    "width": 108,
                    "height": 149
                  },
                  {
                    "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=76dc137ce5da129ebd2d61b1e0c2489c54a0aaa2",
                    "width": 216,
                    "height": 298
                  },
                  {
                    "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82dc7a0614f6d7e62f2d4897d611ae1656e36bf9",
                    "width": 320,
                    "height": 442
                  }
                ],
                "variants": {
                  "obfuscated": {
                    "source": {
                      "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=272c9c7439eddac5e18476316b7b07c388a50796",
                      "width": 513,
                      "height": 709
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=cdbce1632243017789ed1a58cc6b1975ea48f143",
                        "width": 108,
                        "height": 149
                      },
                      {
                        "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=6504f9fc7b54bca1304ea681668d43c3f3bd4228",
                        "width": 216,
                        "height": 298
                      },
                      {
                        "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=2da42ef1c7a43e8a37e35c75b24cb21507e696c1",
                        "width": 320,
                        "height": 442
                      }
                    ]
                  },
                  "nsfw": {
                    "source": {
                      "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=272c9c7439eddac5e18476316b7b07c388a50796",
                      "width": 513,
                      "height": 709
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=cdbce1632243017789ed1a58cc6b1975ea48f143",
                        "width": 108,
                        "height": 149
                      },
                      {
                        "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=6504f9fc7b54bca1304ea681668d43c3f3bd4228",
                        "width": 216,
                        "height": 298
                      },
                      {
                        "url": "https://preview.redd.it/bjq1hhoh6mhf1.png?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=2da42ef1c7a43e8a37e35c75b24cb21507e696c1",
                        "width": 320,
                        "height": 442
                      }
                    ]
                  }
                },
                "id": "M0iodf4tPaIqwmsU9j_rsOiK8DiZWwlXVU1MbnD70IA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk2ubd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aaaaaaeeea",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk2ubd/an_enigmatic_llm_on_a_random_erp_website_claims/",
          "stickied": false,
          "url": "https://i.redd.it/bjq1hhoh6mhf1.png",
          "subreddit_subscribers": 513415,
          "created_utc": 1754579815,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm hosting some ollama models on an in-house server and would like some words of wisdom on the most private and secure way to allow a remote team to utilize these models on something like Open-WebUI. Maybe Tailscale? \n\nWe're working with sensitive information, and I'm worried about exposing the server and data to the outside world and third party software. \n\nHow secure is a solution like Tailscale, and is there anything I should be wary about when setting it up and giving access?\n\nThanks in advance",
          "author_fullname": "t2_f91n8j7dt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Secure Open-WebUI access for remote team?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk279x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754578365,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m hosting some ollama models on an in-house server and would like some words of wisdom on the most private and secure way to allow a remote team to utilize these models on something like Open-WebUI. Maybe Tailscale? &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re working with sensitive information, and I&amp;#39;m worried about exposing the server and data to the outside world and third party software. &lt;/p&gt;\n\n&lt;p&gt;How secure is a solution like Tailscale, and is there anything I should be wary about when setting it up and giving access?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk279x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MiyamotoMusashi7",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk279x/secure_openwebui_access_for_remote_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk279x/secure_openwebui_access_for_remote_team/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754578365,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ggml-org/llama.cpp/pull/14939](https://github.com/ggml-org/llama.cpp/pull/14939)\n\nfrom our hero sammcj\n\nPictured, Cuda v1.45 engine in LM Studio.  (the cuda 12 1.44 runtime still not working--the GLM 4.5 PR was merged in the past 8 hours or so).\n\nAs an aside, my initial vibe is it is far too wordy and overthinks, though, and gpt oss 120b is better and also faster in pure t/s but that's very much early vibe so take with a heavy dose of salt.",
          "author_fullname": "t2_8xi6x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Llama.cpp now supports GLM 4.5 Air",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 110,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "sik4c9x51mhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 65,
                  "x": 108,
                  "u": "https://preview.redd.it/sik4c9x51mhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cfc5aa80b9be46e911fa616658c23f2c1ce2202"
                },
                {
                  "y": 130,
                  "x": 216,
                  "u": "https://preview.redd.it/sik4c9x51mhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=602a2d161a389dc4eebe43605fef682c99eedb2c"
                },
                {
                  "y": 193,
                  "x": 320,
                  "u": "https://preview.redd.it/sik4c9x51mhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4bebb2d42e369dccb4f821bf96cf8fbca0e6f020"
                },
                {
                  "y": 387,
                  "x": 640,
                  "u": "https://preview.redd.it/sik4c9x51mhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=25acc8068eda95c602b4a2ad56f85637c62cbd2b"
                },
                {
                  "y": 581,
                  "x": 960,
                  "u": "https://preview.redd.it/sik4c9x51mhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=83f7496d2e4dc6a40cca7d4d1c6c7fd1cd852738"
                }
              ],
              "s": {
                "y": 628,
                "x": 1037,
                "u": "https://preview.redd.it/sik4c9x51mhf1.png?width=1037&amp;format=png&amp;auto=webp&amp;s=42fb775ca258a7dff9b3a8a42f184c2248edb5b0"
              },
              "id": "sik4c9x51mhf1"
            },
            "0h7il94o0mhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 84,
                  "x": 108,
                  "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fa6bcc1597348ffc77e06fcb7e5179ddb84a16c"
                },
                {
                  "y": 169,
                  "x": 216,
                  "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ec10aec64badb1870c717dfc7425265888b9447"
                },
                {
                  "y": 251,
                  "x": 320,
                  "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afde14c269c2a78e5ad1b4657dc53a0f727a5229"
                },
                {
                  "y": 503,
                  "x": 640,
                  "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=df8773f26eceb0b0e612fde16d74254e3d842312"
                },
                {
                  "y": 754,
                  "x": 960,
                  "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9687a91835f923f2f6718cf17d1d4039f5f5b3a5"
                },
                {
                  "y": 848,
                  "x": 1080,
                  "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d03918facdb42ce79408db9e5fd33899f97478e"
                }
              ],
              "s": {
                "y": 900,
                "x": 1145,
                "u": "https://preview.redd.it/0h7il94o0mhf1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=d45f83e0c0bdc37f15081d6a1962c8e90f3a33a3"
              },
              "id": "0h7il94o0mhf1"
            }
          },
          "name": "t3_1mk26rk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 238,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "0h7il94o0mhf1",
                "id": 723266686
              },
              {
                "media_id": "sik4c9x51mhf1",
                "id": 723266687
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 238,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jawkehNzIT0a-enbiD4fQc_KPJ-dSoMI8t5allPhBfU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754578332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;https://github.com/ggml-org/llama.cpp/pull/14939&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;from our hero sammcj&lt;/p&gt;\n\n&lt;p&gt;Pictured, Cuda v1.45 engine in LM Studio.  (the cuda 12 1.44 runtime still not working--the GLM 4.5 PR was merged in the past 8 hours or so).&lt;/p&gt;\n\n&lt;p&gt;As an aside, my initial vibe is it is far too wordy and overthinks, though, and gpt oss 120b is better and also faster in pure t/s but that&amp;#39;s very much early vibe so take with a heavy dose of salt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mk26rk",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mk26rk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Freonr2",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk26rk/llamacpp_now_supports_glm_45_air/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mk26rk",
          "subreddit_subscribers": 513415,
          "created_utc": 1754578332,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As far as I know, anythingLLM provides an agent for the models to exist through, but have there been any other claude code cli like tools made for the open source models?\n\n**edit**\nI meant a self hosted / offline toolflow.",
          "author_fullname": "t2_5xx2c0t2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What agentic cli tools do we have for Qwen 3 coder?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk221s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754582717,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754578034,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As far as I know, anythingLLM provides an agent for the models to exist through, but have there been any other claude code cli like tools made for the open source models?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;\nI meant a self hosted / offline toolflow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk221s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CertainlyBright",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk221s/what_agentic_cli_tools_do_we_have_for_qwen_3_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk221s/what_agentic_cli_tools_do_we_have_for_qwen_3_coder/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754578034,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is my first time dipping my toe in local llm. I downloaded ollama for Windows on a consumer grade laptop and selected deepseek.  It works fine while it's connected to the internet to download the model and respond to my queries. But once I have started a conversation, if I disconnect wifi it won't let me submit any new queries to the model.  \n\nI was under the impression once the model is downloaded everything runs locally. So why does it only work when I'm connected to the internet even after I've downloaded the model/started a conversation?",
          "author_fullname": "t2_1j76jngfpl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Probably dumb question: why doesn't Ollama forWindows work in airplane mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk1jwk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754576830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my first time dipping my toe in local llm. I downloaded ollama for Windows on a consumer grade laptop and selected deepseek.  It works fine while it&amp;#39;s connected to the internet to download the model and respond to my queries. But once I have started a conversation, if I disconnect wifi it won&amp;#39;t let me submit any new queries to the model.  &lt;/p&gt;\n\n&lt;p&gt;I was under the impression once the model is downloaded everything runs locally. So why does it only work when I&amp;#39;m connected to the internet even after I&amp;#39;ve downloaded the model/started a conversation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk1jwk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImAProAtSomeStuff",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk1jwk/probably_dumb_question_why_doesnt_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk1jwk/probably_dumb_question_why_doesnt_ollama/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754576830,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im hoping to make an agent/tool calling home assistant, whats the best most responsive setup i can make with 48gb ram\nRyzen 5 3600\nAnd an rtx 2060 6gb\n\nI havenf found good small tool callers ",
          "author_fullname": "t2_f9ul181k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smol lil guy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk1hlr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754576682,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im hoping to make an agent/tool calling home assistant, whats the best most responsive setup i can make with 48gb ram\nRyzen 5 3600\nAnd an rtx 2060 6gb&lt;/p&gt;\n\n&lt;p&gt;I havenf found good small tool callers &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk1hlr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Buy268",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk1hlr/smol_lil_guy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk1hlr/smol_lil_guy/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754576682,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's this magic legend that when a model gets invoked by LocalLLaMA, destiny makes a gift.   \nI'm just putting this here...\n\n  \nI've seen a tweet where a researcher from the Qwen team said qwen3-vl is in the oven. I'm just hoping.",
          "author_fullname": "t2_jp0p72wiy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I'll shoot my shot: It's been a while since we had the last qwen-vl...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk1gu7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.59,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754576633,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s this magic legend that when a model gets invoked by LocalLLaMA, destiny makes a gift.&lt;br/&gt;\nI&amp;#39;m just putting this here...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen a tweet where a researcher from the Qwen team said qwen3-vl is in the oven. I&amp;#39;m just hoping.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk1gu7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creative_Knee6618",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk1gu7/ill_shoot_my_shot_its_been_a_while_since_we_had/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk1gu7/ill_shoot_my_shot_its_been_a_while_since_we_had/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754576633,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "title? ik qwen2.5 72b was very good. Is there anything better? this one is good too, but no llama.cpp support and not very good on NSFW:  \n[https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking](https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking)\n\nidm paid/proprietary ones but just prefer local models as it's much cheaper (have to caption about 100k images, about 2.5k are NSFW 😭😭🥀🥀)",
          "author_fullname": "t2_askwa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🖼️ current best VLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk1344",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754575718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title? ik qwen2.5 72b was very good. Is there anything better? this one is good too, but no llama.cpp support and not very good on NSFW:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking\"&gt;https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;idm paid/proprietary ones but just prefer local models as it&amp;#39;s much cheaper (have to caption about 100k images, about 2.5k are NSFW 😭😭🥀🥀)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?auto=webp&amp;s=96c4ee886c63cb1ebcc7199c55efabf17e05b9d2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=18e2e1e5636ebb8fb2648df97b1c2dced6290c3d",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d794b7d3f48a416758bc1c7364c995b97f3c798e",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdda7137d1852a318c2b62152338924c7cde7fdb",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=378a533f76a49493a8ac3141d460e988105012d8",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=42f44ed99c4a65aa3be4d8ca99b53e59daf06fa8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2383ac6e6081bc5cb4181fa6f8a70b2d0e6ce284",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3ARQffgtJju75Ay3Im03z98mRZZbGCpykLWKxbkv4dQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk1344",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "z_3454_pfk",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk1344/current_best_vlm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk1344/current_best_vlm/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754575718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\n**TLDR : I'm looking for the most capable model, fast and efficient, to start playing around with local LLMs and that runs smoothly on my computer. I'm new to this and have very low python skills, so i need to start simple and build up from there.**  \n**Computer specs : Ryzen 7 3700x, RTX 3060 12gb Vram and 32gb RAM**\n\n  \nWith all the hype around GPT-OSS and summer vacations approaching i tought it would be a good moment to finally take some time and start learning about running local LLMs. I've been using Gemini as a regular basic user, but i recently starting building some basic python apps with it (actually gemini does 99% of the work) and connecting the app to Gemini free tier APIs to have an AI touch in my (mostely useless) apps.   \nI see this as an opportunity to learn about AI, Python and the more technical side of LLMs.  \nMy current computer has a Ryzen 7 3700x, RTX 3060 12gb Vram and 32gb RAM.  \nI set up Ollama and tested Llama 3 8b and GPT-OSS 20b (&gt;12gb model, but i was not able to get the  quantized Q4 K M version &lt;12gb to work on ollama... it got a bit technical).  \nMy issue is that Llama 3 8b felt a bit \"dumb\" as i'm mostly used to interact with Gemini 2.5pro (even the 2.5 flash annoys me a bit) and the GPT-OSS 20b was good but also slow , i don't know yet how to get the token per second speed but it took like 6 mins for a quite complicated prompt.  \nSo now i need some advice to find a model that is inbetween, fast enough so i can play around with it and iterate quickly to learn fast, but at the same time smart enough so i can actually get some fun while learning. I'm not focused on any specific topic, the model must be \"balanced\" for an all weather use.  \nI know i won't get a Gemini 2.5pro equivalent working on my computer, probably not even 10% of it's capacities, but i'm looking for the best i can achieve with my current setup.  \nwhat are your recommendations ?   \n\n\nThank you all !",
          "author_fullname": "t2_16pgmapowa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need some help to choose a model to start playing around with localLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk12sx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754575695,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR : I&amp;#39;m looking for the most capable model, fast and efficient, to start playing around with local LLMs and that runs smoothly on my computer. I&amp;#39;m new to this and have very low python skills, so i need to start simple and build up from there.&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;Computer specs : Ryzen 7 3700x, RTX 3060 12gb Vram and 32gb RAM&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;With all the hype around GPT-OSS and summer vacations approaching i tought it would be a good moment to finally take some time and start learning about running local LLMs. I&amp;#39;ve been using Gemini as a regular basic user, but i recently starting building some basic python apps with it (actually gemini does 99% of the work) and connecting the app to Gemini free tier APIs to have an AI touch in my (mostely useless) apps.&lt;br/&gt;\nI see this as an opportunity to learn about AI, Python and the more technical side of LLMs.&lt;br/&gt;\nMy current computer has a Ryzen 7 3700x, RTX 3060 12gb Vram and 32gb RAM.&lt;br/&gt;\nI set up Ollama and tested Llama 3 8b and GPT-OSS 20b (&amp;gt;12gb model, but i was not able to get the  quantized Q4 K M version &amp;lt;12gb to work on ollama... it got a bit technical).&lt;br/&gt;\nMy issue is that Llama 3 8b felt a bit &amp;quot;dumb&amp;quot; as i&amp;#39;m mostly used to interact with Gemini 2.5pro (even the 2.5 flash annoys me a bit) and the GPT-OSS 20b was good but also slow , i don&amp;#39;t know yet how to get the token per second speed but it took like 6 mins for a quite complicated prompt.&lt;br/&gt;\nSo now i need some advice to find a model that is inbetween, fast enough so i can play around with it and iterate quickly to learn fast, but at the same time smart enough so i can actually get some fun while learning. I&amp;#39;m not focused on any specific topic, the model must be &amp;quot;balanced&amp;quot; for an all weather use.&lt;br/&gt;\nI know i won&amp;#39;t get a Gemini 2.5pro equivalent working on my computer, probably not even 10% of it&amp;#39;s capacities, but i&amp;#39;m looking for the best i can achieve with my current setup.&lt;br/&gt;\nwhat are your recommendations ?   &lt;/p&gt;\n\n&lt;p&gt;Thank you all !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk12sx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Strict-Profit-7970",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk12sx/need_some_help_to_choose_a_model_to_start_playing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk12sx/need_some_help_to_choose_a_model_to_start_playing/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754575695,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone have any data around the performance of Titan text embedding v2 against Bge large m3? Any leaderboard with scores would also help. I have already checked MTEB and it does not have Titan in it. ",
          "author_fullname": "t2_c1kra291q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Metrics for AWS Bedrock's Titan text embedding v2 against BGE large m3",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk0zih",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754575472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any data around the performance of Titan text embedding v2 against Bge large m3? Any leaderboard with scores would also help. I have already checked MTEB and it does not have Titan in it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk0zih",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IntroductionFlaky529",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk0zih/metrics_for_aws_bedrocks_titan_text_embedding_v2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk0zih/metrics_for_aws_bedrocks_titan_text_embedding_v2/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754575472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The purpose of this post is twofold.  To give hope to those with older video cards, and to solicit further optimizations from the larger community.  Here's the script:\n\n    cat qwen.sh \n    #!/bin/bash\n\n    # Configuration Variables\n    # ---------------------\n    MODEL_PATH=\"../models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf\"     # Path to your GGUF model file\n    LLAMA_SERVER_PATH=\"./build/bin/llama-server\" # Path to your llama-server executable\n    N_GPU_LAYERS=99 # Number of layers to offload to GPU (use 99 to offload as much as possible)\n    N_CTX=14384 # Context window size (tokens). Adjust based on VRAM and model needs.\n    PORT=NNNNN # Port for the llama-server API\n\n    # --- Performance Tuning Variables ---\n    # Set these based on your system's hardware.\n    # Use 'lscpu' or similar commands to find the number of CPU cores.\n    N_THREADS=4 # Number of CPU threads to use. 'nproc' gets the number of available processors.\n    N_BATCH=300 # Batch size for prompt processing. A larger value can improve initial prompt processing speed.\n\n    # --- Script Logic ---\n\n    echo \"--- Starting optimized llama-server ---\"\n    echo \"Model: $MODEL_PATH\"\n    echo \"GPU Layers: $N_GPU_LAYERS\"\n    echo \"Context Size: $N_CTX\"\n    echo \"Threads: $N_THREADS\"\n    echo \"Batch Size: $N_BATCH\"\n    echo \"Port: $PORT\"\n    echo \"-------------------------------------\"\n\n    # Check if the model file exists\n    if [ ! -f \"$MODEL_PATH\" ]; then\n    echo \"ERROR: Model file not found at $MODEL_PATH\"\n    echo \"Please ensure the model path is correct and the model exists.\"\n    exit 1\n    fi\n\n    # Check if the llama-server executable exists\n    if [ ! -f \"$LLAMA_SERVER_PATH\" ]; then\n    echo \"ERROR: llama-server executable not found at $LLAMA_SERVER_PATH\"\n    echo \"Please ensure llama.cpp is built and the path is correct.\"\n    exit 1\n    fi\n\n    # Launch llama-server with specified parameters\n    # The '&amp;' sends the process to the background, allowing the script to exit.\n    # You might want to remove '&amp;' if you want to see logs directly in the terminal.\n    # You can also redirect output to a log file: &gt; server.log 2&gt;&amp;1 &amp;\n    \"$LLAMA_SERVER_PATH\" \\\n    -m \"$MODEL_PATH\" \\\n    --host 0.0.0.0 \\\n    --port \"$PORT\" \\\n    --n-gpu-layers \"$N_GPU_LAYERS\" \\\n    --ctx-size \"$N_CTX\" \\\n    --embedding \\\n    --threads \"$N_THREADS\" \\\n    --batch-size \"$N_BATCH\" \\\n    --flash-attn \\\n    --no-mmap \\\n    # The --no-mmap flag can sometimes prevent issues on certain file systems.\n    # It can slightly increase load time but ensures the whole model is in memory.\n\n    # Provide instructions to the user\n    echo \"\"\n    echo \"llama-server has been launched. It might take a moment to load the model.\"\n    echo \"You can check its status by visiting http://localhost:$PORT/health in your browser.\"\n    echo \"To interact with it, you'll need a separate client script (e.g., Python) that makes API calls to http://localhost:$PORT/v1/chat/completions\"\n    echo \"To stop the server, find its process ID (e.g., using 'pgrep llama-server') and use 'kill &lt;PID&gt;'.\"\n    echo \"\"\n    echo \"--- Server output will appear above this line if not backgrounded ---\"\n\n\nPrompt\n- Tokens: 9818\n- Time: 12921.578 ms\n- Speed: 759.8 t/s\nGeneration\n- Tokens: 748\n- Time: 35828.869 ms\n- Speed: 20.9 t/s\n\nUse case is C programming buddy, a rubber duck that talks back and sometimes has useful ideas.  Sometimes ... astoundingly good ideas prompting me to explore solutions I would not have thought of on my own.\n\nOutput is faster than I can read, and for lengthy processing it's fire and forget.  You'll hear the GPU unload when it's done.\n\nTLDR:  Restatement of purpose.  Provide a path for those seeking a good model on an under served segment of our community.  A request for help from those who have found optimizations I have yet to discover.\n\nchar *user = \"lazy\toptomas\";//  Yeah, it's sloppy.  Yeah, it has stuff in it I don't use anymore.  Yeah, the instructions are dumb and out of date.",
          "author_fullname": "t2_36ebd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rejoice, GPU poor brethren. RTX 3060 12BG, llama-cpp, Model: DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk0w9f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754575273,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The purpose of this post is twofold.  To give hope to those with older video cards, and to solicit further optimizations from the larger community.  Here&amp;#39;s the script:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cat qwen.sh \n#!/bin/bash\n\n# Configuration Variables\n# ---------------------\nMODEL_PATH=&amp;quot;../models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf&amp;quot;     # Path to your GGUF model file\nLLAMA_SERVER_PATH=&amp;quot;./build/bin/llama-server&amp;quot; # Path to your llama-server executable\nN_GPU_LAYERS=99 # Number of layers to offload to GPU (use 99 to offload as much as possible)\nN_CTX=14384 # Context window size (tokens). Adjust based on VRAM and model needs.\nPORT=NNNNN # Port for the llama-server API\n\n# --- Performance Tuning Variables ---\n# Set these based on your system&amp;#39;s hardware.\n# Use &amp;#39;lscpu&amp;#39; or similar commands to find the number of CPU cores.\nN_THREADS=4 # Number of CPU threads to use. &amp;#39;nproc&amp;#39; gets the number of available processors.\nN_BATCH=300 # Batch size for prompt processing. A larger value can improve initial prompt processing speed.\n\n# --- Script Logic ---\n\necho &amp;quot;--- Starting optimized llama-server ---&amp;quot;\necho &amp;quot;Model: $MODEL_PATH&amp;quot;\necho &amp;quot;GPU Layers: $N_GPU_LAYERS&amp;quot;\necho &amp;quot;Context Size: $N_CTX&amp;quot;\necho &amp;quot;Threads: $N_THREADS&amp;quot;\necho &amp;quot;Batch Size: $N_BATCH&amp;quot;\necho &amp;quot;Port: $PORT&amp;quot;\necho &amp;quot;-------------------------------------&amp;quot;\n\n# Check if the model file exists\nif [ ! -f &amp;quot;$MODEL_PATH&amp;quot; ]; then\necho &amp;quot;ERROR: Model file not found at $MODEL_PATH&amp;quot;\necho &amp;quot;Please ensure the model path is correct and the model exists.&amp;quot;\nexit 1\nfi\n\n# Check if the llama-server executable exists\nif [ ! -f &amp;quot;$LLAMA_SERVER_PATH&amp;quot; ]; then\necho &amp;quot;ERROR: llama-server executable not found at $LLAMA_SERVER_PATH&amp;quot;\necho &amp;quot;Please ensure llama.cpp is built and the path is correct.&amp;quot;\nexit 1\nfi\n\n# Launch llama-server with specified parameters\n# The &amp;#39;&amp;amp;&amp;#39; sends the process to the background, allowing the script to exit.\n# You might want to remove &amp;#39;&amp;amp;&amp;#39; if you want to see logs directly in the terminal.\n# You can also redirect output to a log file: &amp;gt; server.log 2&amp;gt;&amp;amp;1 &amp;amp;\n&amp;quot;$LLAMA_SERVER_PATH&amp;quot; \\\n-m &amp;quot;$MODEL_PATH&amp;quot; \\\n--host 0.0.0.0 \\\n--port &amp;quot;$PORT&amp;quot; \\\n--n-gpu-layers &amp;quot;$N_GPU_LAYERS&amp;quot; \\\n--ctx-size &amp;quot;$N_CTX&amp;quot; \\\n--embedding \\\n--threads &amp;quot;$N_THREADS&amp;quot; \\\n--batch-size &amp;quot;$N_BATCH&amp;quot; \\\n--flash-attn \\\n--no-mmap \\\n# The --no-mmap flag can sometimes prevent issues on certain file systems.\n# It can slightly increase load time but ensures the whole model is in memory.\n\n# Provide instructions to the user\necho &amp;quot;&amp;quot;\necho &amp;quot;llama-server has been launched. It might take a moment to load the model.&amp;quot;\necho &amp;quot;You can check its status by visiting http://localhost:$PORT/health in your browser.&amp;quot;\necho &amp;quot;To interact with it, you&amp;#39;ll need a separate client script (e.g., Python) that makes API calls to http://localhost:$PORT/v1/chat/completions&amp;quot;\necho &amp;quot;To stop the server, find its process ID (e.g., using &amp;#39;pgrep llama-server&amp;#39;) and use &amp;#39;kill &amp;lt;PID&amp;gt;&amp;#39;.&amp;quot;\necho &amp;quot;&amp;quot;\necho &amp;quot;--- Server output will appear above this line if not backgrounded ---&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Prompt\n- Tokens: 9818\n- Time: 12921.578 ms\n- Speed: 759.8 t/s\nGeneration\n- Tokens: 748\n- Time: 35828.869 ms\n- Speed: 20.9 t/s&lt;/p&gt;\n\n&lt;p&gt;Use case is C programming buddy, a rubber duck that talks back and sometimes has useful ideas.  Sometimes ... astoundingly good ideas prompting me to explore solutions I would not have thought of on my own.&lt;/p&gt;\n\n&lt;p&gt;Output is faster than I can read, and for lengthy processing it&amp;#39;s fire and forget.  You&amp;#39;ll hear the GPU unload when it&amp;#39;s done.&lt;/p&gt;\n\n&lt;p&gt;TLDR:  Restatement of purpose.  Provide a path for those seeking a good model on an under served segment of our community.  A request for help from those who have found optimizations I have yet to discover.&lt;/p&gt;\n\n&lt;p&gt;char *user = &amp;quot;lazy  optomas&amp;quot;;//  Yeah, it&amp;#39;s sloppy.  Yeah, it has stuff in it I don&amp;#39;t use anymore.  Yeah, the instructions are dumb and out of date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mk0w9f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "optomas",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754575273,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Seeing recent Qwen3-30B-A3B, I am praying DeepSeek release something like that too. I’m surprised at the kick it gives without breaking the bank on GPUs. \n\nI think Qwen should be a role model to all LLM researchers. It will bring AI to our daily drivers too.\n\nFascinating times we live in. This is where it will bend and mend.",
          "author_fullname": "t2_1nprbkmy5x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek’s MOE approach for lower model hope",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mk0fxu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754574134,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeing recent Qwen3-30B-A3B, I am praying DeepSeek release something like that too. I’m surprised at the kick it gives without breaking the bank on GPUs. &lt;/p&gt;\n\n&lt;p&gt;I think Qwen should be a role model to all LLM researchers. It will bring AI to our daily drivers too.&lt;/p&gt;\n\n&lt;p&gt;Fascinating times we live in. This is where it will bend and mend.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mk0fxu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "exaknight21",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mk0fxu/deepseeks_moe_approach_for_lower_model_hope/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk0fxu/deepseeks_moe_approach_for_lower_model_hope/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754574134,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Even when it was working in Ollama, it wasn't using my 8GB GPU, only CPU. I hope they'll fix that soon as well",
          "author_fullname": "t2_2ytae3t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Come on, it was working yesterday",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjzsx1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Pc-DRpZDeUrG8CL2_0Zwezur_bx9E1pFE0OWcsJ4cE0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754572530,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Even when it was working in Ollama, it wasn&amp;#39;t using my 8GB GPU, only CPU. I hope they&amp;#39;ll fix that soon as well&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/78q7k8eyklhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/78q7k8eyklhf1.jpeg?auto=webp&amp;s=ee1ceb62788e81f19c0f584c994df592170a86ba",
                  "width": 997,
                  "height": 1200
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/78q7k8eyklhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47557c0acf7166b8136ec3ef21ae79caeb118386",
                    "width": 108,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/78q7k8eyklhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc58c410806ec67e608f127a39225c5dc77714df",
                    "width": 216,
                    "height": 259
                  },
                  {
                    "url": "https://preview.redd.it/78q7k8eyklhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c079e363de431f5b1da2838155f20b38c4684d81",
                    "width": 320,
                    "height": 385
                  },
                  {
                    "url": "https://preview.redd.it/78q7k8eyklhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ce5df819ed7c8b4875bfbb131acbbab4c58b331",
                    "width": 640,
                    "height": 770
                  },
                  {
                    "url": "https://preview.redd.it/78q7k8eyklhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=27ecc1da84f131cf94c24c7e2fab48773f0bce94",
                    "width": 960,
                    "height": 1155
                  }
                ],
                "variants": {},
                "id": "zJKts17eWhVx9VTMJ2dbpHVqps5DeLPgP1yDAf7mH_M"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mjzsx1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bbbar",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjzsx1/come_on_it_was_working_yesterday/",
          "stickied": false,
          "url": "https://i.redd.it/78q7k8eyklhf1.jpeg",
          "subreddit_subscribers": 513415,
          "created_utc": 1754572530,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \n**0 · Quick links (top-pinned)  MIT License**\n\n* Terseract endorsement – our cold-start journey, 50 days → 300 ★ \n* [https://github.com/bijection?tab=stars](https://github.com/bijection?tab=stars)\n* Problem Map 2.0 / Semantic Clinic (repo) – 16 root causes, step-by-step patches [https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md](https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md)\n* Hero log – real users &amp; fixes – dozens of RAG pain-points solved [https://github.com/onestardao/WFGY/discussions/10](https://github.com/onestardao/WFGY/discussions/10)\n* WFGY PDF (free guide) – 2 500+ downloads, no e-mail wall \n* [https://zenodo.org/records/15630969](https://zenodo.org/records/15630969)\n\n# 1 · Why you might care\n\n**RAG bugs aren’t random.**  \nIn practice we keep seeing the *same* 16 failure families:\n\n* prompt drift &amp; injection bleed\n* hallucination-as-chunk drift\n* silent OCR mangling\n* vector store “index fits but retrieval lies”\n* long-context entropy collapse … *(and 11 more)*\n\nWe spent nine months tagging those patterns across 11 local-LLM projects (LLaMA-2/3, Mistral, Qwen, etc.). The result is a **single markdown map** that tells you:\n\n* how to spot the symptom in under a minute\n* why that stage of the pipeline fails (with ΔS / λ\\_observe traces)\n* the *band-aid → surgery* checklist to fix it\n\n# 2 · What you actually get\n\n1. **Problem Map index** – find “symptom → likely family → deep-dive page”.\n2. **16 deep-dive pages** – reproducible notebooks, tiny bash tools, before/after metrics.\n3. **Semantic Clinic workflow** – OCR → chunk → embed → store → retrieve → prompt → LLM; each step has its own “triage gauge”.\n4. **MIT licence, zero lock-in** – fork it, strip our names, embed in your own wiki.\n\n# 3 · Numbers so far\n\n* **Cold-start 50 days → 300+ GitHub ★** (tiny but steady).\n* **WFGY PDF** passed **2 500 downloads** without marketing.\n* **Dozens of community fixes** already logged in the hero thread – from broken LaTeX math chatbots to multi-agent deadlocks.\n\n# 4 · How it’s helping Local LLaMA users\n\n* Trimmed a 3-hour hallucination hunt (bad chunk boundaries) to **14 minutes**.\n* Brought an 0.61 recall FAISS index to **0.89** just by repairing embedding semantics.\n* Identified a covert prompt-bleed that only showed up on **q4\\_K\\_M** quant.\n\n# 5 · Call for test pilots\n\nThe map is stable, but we still need:\n\n* Edge-case samples (multi-modal, code-RAG, gigantic PDFs).\n* More quant + GGUF corner-cases (we only have about 30).\n* Feedback on the “entropy collapse” gauges – they’re new.\n\nOpen an issue, PR, or just drop a comment; we reply fast—because we’re debugging our own stuff every night too.\n\n**Bookmark it → next time your local model spits gibberish, you’ll have the triage steps in one click.**\n\nHappy to answer anything!!!!!!!!!!! Leave your question, I will ansswer :)",
          "author_fullname": "t2_1tgp8l87vk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We turned 16 common RAG failure modes into a “Problem Map 2.0” – free, open-source, already fixing Local LLaMA stacks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjzhai",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754571711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;0 · Quick links (top-pinned)  MIT License&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Terseract endorsement – our cold-start journey, 50 days → 300 ★ &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/bijection?tab=stars\"&gt;https://github.com/bijection?tab=stars&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Problem Map 2.0 / Semantic Clinic (repo) – 16 root causes, step-by-step patches &lt;a href=\"https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md\"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Hero log – real users &amp;amp; fixes – dozens of RAG pain-points solved &lt;a href=\"https://github.com/onestardao/WFGY/discussions/10\"&gt;https://github.com/onestardao/WFGY/discussions/10&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;WFGY PDF (free guide) – 2 500+ downloads, no e-mail wall &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://zenodo.org/records/15630969\"&gt;https://zenodo.org/records/15630969&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;1 · Why you might care&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;RAG bugs aren’t random.&lt;/strong&gt;&lt;br/&gt;\nIn practice we keep seeing the &lt;em&gt;same&lt;/em&gt; 16 failure families:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;prompt drift &amp;amp; injection bleed&lt;/li&gt;\n&lt;li&gt;hallucination-as-chunk drift&lt;/li&gt;\n&lt;li&gt;silent OCR mangling&lt;/li&gt;\n&lt;li&gt;vector store “index fits but retrieval lies”&lt;/li&gt;\n&lt;li&gt;long-context entropy collapse … &lt;em&gt;(and 11 more)&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We spent nine months tagging those patterns across 11 local-LLM projects (LLaMA-2/3, Mistral, Qwen, etc.). The result is a &lt;strong&gt;single markdown map&lt;/strong&gt; that tells you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;how to spot the symptom in under a minute&lt;/li&gt;\n&lt;li&gt;why that stage of the pipeline fails (with ΔS / λ_observe traces)&lt;/li&gt;\n&lt;li&gt;the &lt;em&gt;band-aid → surgery&lt;/em&gt; checklist to fix it&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;2 · What you actually get&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Problem Map index&lt;/strong&gt; – find “symptom → likely family → deep-dive page”.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;16 deep-dive pages&lt;/strong&gt; – reproducible notebooks, tiny bash tools, before/after metrics.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Semantic Clinic workflow&lt;/strong&gt; – OCR → chunk → embed → store → retrieve → prompt → LLM; each step has its own “triage gauge”.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MIT licence, zero lock-in&lt;/strong&gt; – fork it, strip our names, embed in your own wiki.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;3 · Numbers so far&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cold-start 50 days → 300+ GitHub ★&lt;/strong&gt; (tiny but steady).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;WFGY PDF&lt;/strong&gt; passed &lt;strong&gt;2 500 downloads&lt;/strong&gt; without marketing.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dozens of community fixes&lt;/strong&gt; already logged in the hero thread – from broken LaTeX math chatbots to multi-agent deadlocks.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;4 · How it’s helping Local LLaMA users&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Trimmed a 3-hour hallucination hunt (bad chunk boundaries) to &lt;strong&gt;14 minutes&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Brought an 0.61 recall FAISS index to &lt;strong&gt;0.89&lt;/strong&gt; just by repairing embedding semantics.&lt;/li&gt;\n&lt;li&gt;Identified a covert prompt-bleed that only showed up on &lt;strong&gt;q4_K_M&lt;/strong&gt; quant.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;5 · Call for test pilots&lt;/h1&gt;\n\n&lt;p&gt;The map is stable, but we still need:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Edge-case samples (multi-modal, code-RAG, gigantic PDFs).&lt;/li&gt;\n&lt;li&gt;More quant + GGUF corner-cases (we only have about 30).&lt;/li&gt;\n&lt;li&gt;Feedback on the “entropy collapse” gauges – they’re new.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Open an issue, PR, or just drop a comment; we reply fast—because we’re debugging our own stuff every night too.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bookmark it → next time your local model spits gibberish, you’ll have the triage steps in one click.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer anything!!!!!!!!!!! Leave your question, I will ansswer :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mjzhai",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wfgy_engine",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjzhai/we_turned_16_common_rag_failure_modes_into_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjzhai/we_turned_16_common_rag_failure_modes_into_a/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754571711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;gpt-oss:20b vs qwen3:14b/qwen2.5-coder:14b which is best at tool calling? and which is performance effiecient?\n\n* Which is better in tool calling?\n* Which is better in common sense/general knowledge?\n* Which is better in reasoning?\n   * Which is performance efficeint?",
          "author_fullname": "t2_1b8utegv8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "(Noob here) gpt-oss:20b vs qwen3:14b/qwen2.5-coder:14b which is best at tool calling? and which is performance effiecient?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjz1ow",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754570581,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;gpt-oss:20b vs qwen3:14b/qwen2.5-coder:14b which is best at tool calling? and which is performance effiecient?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Which is better in tool calling?&lt;/li&gt;\n&lt;li&gt;Which is better in common sense/general knowledge?&lt;/li&gt;\n&lt;li&gt;Which is better in reasoning?\n\n&lt;ul&gt;\n&lt;li&gt;Which is performance efficeint?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjz1ow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideResolve4517",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjz1ow/noob_here_gptoss20b_vs_qwen314bqwen25coder14b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjz1ow/noob_here_gptoss20b_vs_qwen314bqwen25coder14b/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754570581,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\n[View Poll](https://www.reddit.com/poll/1mjz0mm)",
          "author_fullname": "t2_b9q63qhu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Will you be disappointed if Horizon Alpha/Beta is GPT 5?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjz0mm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.32,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754570505,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1mjz0mm\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjz0mm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KvAk_AKPlaysYT",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1754656905753,
            "options": [
              {
                "text": "Yes",
                "id": "31292065"
              },
              {
                "text": "Meh...",
                "id": "31292066"
              },
              {
                "text": "No",
                "id": "31292067"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 230,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjz0mm/will_you_be_disappointed_if_horizon_alphabeta_is/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjz0mm/will_you_be_disappointed_if_horizon_alphabeta_is/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754570505,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks,\nWe’ve built Amphora Ads an ad network designed specifically for AI chat apps. Instead of traditional banner ads or paywalls, we serve native, context aware suggestions right inside LLM responses. Think:\n\n“Help me plan my Japan trip”\nand the LLM replies with a travel itinerary that seamlessly includes a link to a travel agency not as an ad, but as part of the helpful answer.\n\nWe’re already working with some early partners and looking for more AI app devs building chat or agent-based tools.\nDoesn't break UX, Monetize free users, You stay in control of what’s shown\n\nIf you’re building anything in this space or know someone who is, let’s chat!\n\nWould love feedback too happy to share a demo. 🙌\n",
          "author_fullname": "t2_kpnyrzeq3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Monetizing AI chat apps without subscriptions or popups looking for early partners",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjz0f0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754570490,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,\nWe’ve built Amphora Ads an ad network designed specifically for AI chat apps. Instead of traditional banner ads or paywalls, we serve native, context aware suggestions right inside LLM responses. Think:&lt;/p&gt;\n\n&lt;p&gt;“Help me plan my Japan trip”\nand the LLM replies with a travel itinerary that seamlessly includes a link to a travel agency not as an ad, but as part of the helpful answer.&lt;/p&gt;\n\n&lt;p&gt;We’re already working with some early partners and looking for more AI app devs building chat or agent-based tools.\nDoesn&amp;#39;t break UX, Monetize free users, You stay in control of what’s shown&lt;/p&gt;\n\n&lt;p&gt;If you’re building anything in this space or know someone who is, let’s chat!&lt;/p&gt;\n\n&lt;p&gt;Would love feedback too happy to share a demo. 🙌&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjz0f0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Akii777",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjz0f0/monetizing_ai_chat_apps_without_subscriptions_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjz0f0/monetizing_ai_chat_apps_without_subscriptions_or/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754570490,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI had a big dream about LLM being able to work with me and help woth automotive topics. I tried with RAG tech and gemini 12b. It was not great, because documents I feed are quite big (up to 400 pages pdf) and to find solution to problem you need to look at page 2, page 169, page 298 for example and all solutions were half-correct because it didn't bother to look further after finding some correct information.\n\nHow to train LLM for my purpose? Currently I have 12gb vram 4070super and 32gb ddr4 ram, so I can't use very large models.\n\nAm I doing something incorrect or it's not viable option yet for my hardware?",
          "author_fullname": "t2_2ggbff2e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Question: how to train llm about automotive topics for my work use?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjyyx3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754570380,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I had a big dream about LLM being able to work with me and help woth automotive topics. I tried with RAG tech and gemini 12b. It was not great, because documents I feed are quite big (up to 400 pages pdf) and to find solution to problem you need to look at page 2, page 169, page 298 for example and all solutions were half-correct because it didn&amp;#39;t bother to look further after finding some correct information.&lt;/p&gt;\n\n&lt;p&gt;How to train LLM for my purpose? Currently I have 12gb vram 4070super and 32gb ddr4 ram, so I can&amp;#39;t use very large models.&lt;/p&gt;\n\n&lt;p&gt;Am I doing something incorrect or it&amp;#39;s not viable option yet for my hardware?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjyyx3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lxxtsch",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjyyx3/question_how_to_train_llm_about_automotive_topics/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjyyx3/question_how_to_train_llm_about_automotive_topics/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754570380,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Edit: Added excellent suggestions from u/Everlier:\n\n* Harbor: [https://github.com/av/harbor](https://github.com/av/harbor)\n* Parllama: [https://github.com/paulrobello/parllama](https://github.com/paulrobello/parllama)\n* Oterm (ollama centric): [https://github.com/ggozad/oterm](https://github.com/ggozad/oterm)\n* aichat: [https://github.com/sigoden/aichat](https://github.com/sigoden/aichat)\n* gptme: [https://github.com/gptme/gptme](https://github.com/gptme/gptme)\n* Open Interpreter: [https://github.com/OpenInterpreter/open-interpreter](https://github.com/OpenInterpreter/open-interpreter)\n* Crush: [https://github.com/charmbracelet/crush](https://github.com/charmbracelet/crush)\n* OpenHands: [https://github.com/All-Hands-AI/OpenHands](https://github.com/All-Hands-AI/OpenHands)\n\nAdded by u/ekaj:\n\n* TLDW Chatbook: [https://github.com/rmusser01/tldw\\_chatbook](https://github.com/rmusser01/tldw_chatbook) \n\nI have only used Python to interact with a model on vLLM so far. What are some good terminal UIs (not GUIs like OpenWebUI)? Here are the ones I found so far:\n\n* Elia: [https://github.com/darrenburns/elia](https://github.com/darrenburns/elia)\n* Yappus: [https://github.com/MostlyKIGuess/Yappus-Term](https://github.com/MostlyKIGuess/Yappus-Term)\n* Aider (CLI but not TUI): [https://github.com/Aider-AI/aider](https://github.com/Aider-AI/aider)\n\nI use [Codex CLI](https://github.com/openai/codex), but it's designed for coding in a git repository and not general chat. I basically want a Codex CLI but for chat.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are some terminal UIs for chatting with a vLLM-hosted model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjyuv5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": "transparent",
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754603914,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754570060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: Added excellent suggestions from &lt;a href=\"/u/Everlier\"&gt;u/Everlier&lt;/a&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Harbor: &lt;a href=\"https://github.com/av/harbor\"&gt;https://github.com/av/harbor&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Parllama: &lt;a href=\"https://github.com/paulrobello/parllama\"&gt;https://github.com/paulrobello/parllama&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Oterm (ollama centric): &lt;a href=\"https://github.com/ggozad/oterm\"&gt;https://github.com/ggozad/oterm&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;aichat: &lt;a href=\"https://github.com/sigoden/aichat\"&gt;https://github.com/sigoden/aichat&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;gptme: &lt;a href=\"https://github.com/gptme/gptme\"&gt;https://github.com/gptme/gptme&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Open Interpreter: &lt;a href=\"https://github.com/OpenInterpreter/open-interpreter\"&gt;https://github.com/OpenInterpreter/open-interpreter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Crush: &lt;a href=\"https://github.com/charmbracelet/crush\"&gt;https://github.com/charmbracelet/crush&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;OpenHands: &lt;a href=\"https://github.com/All-Hands-AI/OpenHands\"&gt;https://github.com/All-Hands-AI/OpenHands&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Added by &lt;a href=\"/u/ekaj\"&gt;u/ekaj&lt;/a&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;TLDW Chatbook: &lt;a href=\"https://github.com/rmusser01/tldw_chatbook\"&gt;https://github.com/rmusser01/tldw_chatbook&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have only used Python to interact with a model on vLLM so far. What are some good terminal UIs (not GUIs like OpenWebUI)? Here are the ones I found so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Elia: &lt;a href=\"https://github.com/darrenburns/elia\"&gt;https://github.com/darrenburns/elia&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Yappus: &lt;a href=\"https://github.com/MostlyKIGuess/Yappus-Term\"&gt;https://github.com/MostlyKIGuess/Yappus-Term&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Aider (CLI but not TUI): &lt;a href=\"https://github.com/Aider-AI/aider\"&gt;https://github.com/Aider-AI/aider&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I use &lt;a href=\"https://github.com/openai/codex\"&gt;Codex CLI&lt;/a&gt;, but it&amp;#39;s designed for coding in a git repository and not general chat. I basically want a Codex CLI but for chat.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?auto=webp&amp;s=fed100b482ca08e68bfe1320d6f04cdcc9bfc332",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=07558ff258ec7cde0629188daba7d8644e255c11",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e60aeca776f3c0f146e13300be0a6fed92151ae",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e42bcb07cbd718f4f4434071d4fa4661c614ff4b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=45aec152595485a7fbadd33bdc89e3fed5931f49",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee7e489c5bbe5a35e90aaadc6a73e885a2d25609",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8a6848d01f6cc72dba6303152ab33402380ac12",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ZgkHUBpCI-mo-R7TKIoMYlxSnS7RxUAKXJWeDlhzeZ4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjyuv5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mjyuv5/what_are_some_terminal_uis_for_chatting_with_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjyuv5/what_are_some_terminal_uis_for_chatting_with_a/",
          "subreddit_subscribers": 513415,
          "created_utc": 1754570060,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}