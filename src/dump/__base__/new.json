{
  "kind": "Listing",
  "data": {
    "after": "t3_1lzbad8",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Amazon just released Kiro, which is alternative to Cursor/Windsurf, has tasker/planning mode and currently even free tier. I tried it and it looks promising. https://kiro.dev",
          "author_fullname": "t2_8dnu3hmd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kiro (Cursor alternative from Amazon)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0gdfi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752582222,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Amazon just released Kiro, which is alternative to Cursor/Windsurf, has tasker/planning mode and currently even free tier. I tried it and it looks promising. &lt;a href=\"https://kiro.dev\"&gt;https://kiro.dev&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?auto=webp&amp;s=c4bde0e66eafaec6988a79d1dca2c4e4204984c5",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e68b40deeff8781623db1670fa4a7adface13f9",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=88690531c12f3760a6ed10fdcd66877d3c579e48",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=00a3a2b12606726590f5f3ebe97fee5e354259a9",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6eb53e896599ea0bb43ff049b5322f60068ba6ca",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=25da0e9cc8bd883bf49e641059b9044b4f2febe0",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=365a747787f1b62d80e7a0c9172351d4a81c715f",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "A0L9lkftukkhN6hze4UH3WyoeoLPLbwT3UJAU6qahHk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0gdfi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AleksHop",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0gdfi/kiro_cursor_alternative_from_amazon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0gdfi/kiro_cursor_alternative_from_amazon/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752582222,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're likely getting a closed source model instead ",
          "author_fullname": "t2_5luz9ozsa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Well, if anyone was waiting for Llama 4 Behemoth, it's gone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0g2mk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=860ae22badddb8fc2c343a9ccac78f148022a869",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752581342,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "analyticsindiamag.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re likely getting a closed source model instead &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?auto=webp&amp;s=ddc9c55c7477ac3d20d1a5ec41979dd91d76c1dd",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01ed45a4be031327974fcbe48924b7c8d0421993",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8279d4f944e0abce05c11f97b8a014ed4500b3da",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c501e47f0fbce65b45288abd53f58cec0662510d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7417749598717bd5400069706a3c0d563e32ab4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=281861ba0f1aec0cc2a503b3304cf7663f094868",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12d94b438b313810f2f112ab90b5aca70313ccf6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "460vGqJsGvGsLCuYSDVQ0J6aXCEA7hjiOX3-wBrRv_s"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0g2mk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Elevator5091",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0g2mk/well_if_anyone_was_waiting_for_llama_4_behemoth/",
          "stickied": false,
          "url": "https://analyticsindiamag.com/global-tech/meta-plans-to-abandon-llama-4-behemoth-but-why/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752581342,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys,\nI'm looking to build my \"first PC\" (not my first, but I currently only have a bad notebook), rn I'm stuck on deciding the GPU part. \nI'm a electronic engineer major and would like to have access to AI workload for a few projects (mostly Computer Vision and LLMs for tool control and human/machine interaction). \n\nI'm currently between 2 GPU's:\n\nRTX 5060 ti 16gb - R$3400.00($610.00)\n\nRTX 5070 12gb - R$4000.00($715.00)\n\nYes, GPUs are quite expensive in my country...\n\nSo considering I will use the PC for both gaming/game dev and AI workload, what would be the recommendation for GPU. Is it better to go with the 16gb version GPU or with Quantization the 40% improved performance on 5070 processing power is better?\n\nEdit: Text structure Formatting ",
          "author_fullname": "t2_4h864s27",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU for local LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0fp0r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752580244,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,\nI&amp;#39;m looking to build my &amp;quot;first PC&amp;quot; (not my first, but I currently only have a bad notebook), rn I&amp;#39;m stuck on deciding the GPU part. \nI&amp;#39;m a electronic engineer major and would like to have access to AI workload for a few projects (mostly Computer Vision and LLMs for tool control and human/machine interaction). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently between 2 GPU&amp;#39;s:&lt;/p&gt;\n\n&lt;p&gt;RTX 5060 ti 16gb - R$3400.00($610.00)&lt;/p&gt;\n\n&lt;p&gt;RTX 5070 12gb - R$4000.00($715.00)&lt;/p&gt;\n\n&lt;p&gt;Yes, GPUs are quite expensive in my country...&lt;/p&gt;\n\n&lt;p&gt;So considering I will use the PC for both gaming/game dev and AI workload, what would be the recommendation for GPU. Is it better to go with the 16gb version GPU or with Quantization the 40% improved performance on 5070 processing power is better?&lt;/p&gt;\n\n&lt;p&gt;Edit: Text structure Formatting &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0fp0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GabePs",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0fp0r/gpu_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0fp0r/gpu_for_local_llm/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752580244,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We're started a **Startup Catalyst Program** at **Future AGI** for early-stage AI teams working on things like LLM apps, agents, or RAG systems - basically anyone who‚Äôs hit the wall when it comes to evals, observability, or reliability in production.\n\n\n\nThis program is built for **high-velocity AI startups** looking to:\n\n* Rapidly **iterate and deploy** reliable AI¬† products with confidence¬†\n* **Validate performance and user trust** at every stage of development  \n* **Save Engineering bandwidth** to focus more on product development instead of debugging\n\nThe program includes:\n\n* **$5k in credits** for our evaluation &amp; observability platform  \n* Access to **Pro tools** for model output tracking, eval workflows, and reliability benchmarking  \n* **Hands-on support** to help teams integrate fast  \n* Some of our internal, fine-tuned models for evals + analysis\n\nIt's free for selected teams - mostly aimed at startups moving fast and building real products. If it sounds relevant for your stack (or someone you know), here‚Äôs the link: [https://futureagi.com/startups](https://futureagi.com/startups)\n\n",
          "author_fullname": "t2_1lmm8dujd9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Announcing the launch of the Startup Catalyst Program for early-stage AI teams.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0fboi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752579074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re started a &lt;strong&gt;Startup Catalyst Program&lt;/strong&gt; at &lt;strong&gt;Future AGI&lt;/strong&gt; for early-stage AI teams working on things like LLM apps, agents, or RAG systems - basically anyone who‚Äôs hit the wall when it comes to evals, observability, or reliability in production.&lt;/p&gt;\n\n&lt;p&gt;This program is built for &lt;strong&gt;high-velocity AI startups&lt;/strong&gt; looking to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rapidly &lt;strong&gt;iterate and deploy&lt;/strong&gt; reliable AI¬† products with confidence¬†&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Validate performance and user trust&lt;/strong&gt; at every stage of development&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Save Engineering bandwidth&lt;/strong&gt; to focus more on product development instead of debugging&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The program includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;$5k in credits&lt;/strong&gt; for our evaluation &amp;amp; observability platform&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Access to &lt;strong&gt;Pro tools&lt;/strong&gt; for model output tracking, eval workflows, and reliability benchmarking&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hands-on support&lt;/strong&gt; to help teams integrate fast&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Some of our internal, fine-tuned models for evals + analysis&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s free for selected teams - mostly aimed at startups moving fast and building real products. If it sounds relevant for your stack (or someone you know), here‚Äôs the link: &lt;a href=\"https://futureagi.com/startups\"&gt;https://futureagi.com/startups&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0fboi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bubbless__16",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0fboi/announcing_the_launch_of_the_startup_catalyst/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0fboi/announcing_the_launch_of_the_startup_catalyst/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752579074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üåã Introducing my first (open-source) NPM package: Whisper Node Addon.  \nIt allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.‚Äç\n\nüî• And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don't have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.\n\n‚öôÔ∏è To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.\n\nGet prebuilt binaries at:  \n[https://www.npmjs.com/package/@kutalia/whisper-node-addon](https://www.npmjs.com/package/@kutalia/whisper-node-addon)  \nSource code:  \n[https://github.com/Kutalia/whisper-node-addon](https://github.com/Kutalia/whisper-node-addon)",
          "author_fullname": "t2_h6o0chk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Whisper.cpp Node.js Addon with Vulkan Support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m0eq11",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752581164,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752577126,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üåã Introducing my first (open-source) NPM package: Whisper Node Addon.&lt;br/&gt;\nIt allows to transcribe audio with Whisper.cpp straight in your Node.js environment after just installing it, no manual configuration or compilation needed. Not only that, it comes with scripts if you wish to build your binaries manually.‚Äç&lt;/p&gt;\n\n&lt;p&gt;üî• And the biggest part? It supports GPU acceleration through Vulkan API (or Metal on Apple systems), effectively making real-time transcriptions possible with a decent hardware. If you don&amp;#39;t have a GPU or you mind using it (while gaming, for example, to save resources), you can always fall back to CPU usage with a single option.&lt;/p&gt;\n\n&lt;p&gt;‚öôÔ∏è To make all of this possible, I have forked previous works by others and improved upon the addon source in C++, typing (TypeScript), CI/CD (Github Actions) and many other aspects.&lt;/p&gt;\n\n&lt;p&gt;Get prebuilt binaries at:&lt;br/&gt;\n&lt;a href=\"https://www.npmjs.com/package/@kutalia/whisper-node-addon\"&gt;https://www.npmjs.com/package/@kutalia/whisper-node-addon&lt;/a&gt;&lt;br/&gt;\nSource code:&lt;br/&gt;\n&lt;a href=\"https://github.com/Kutalia/whisper-node-addon\"&gt;https://github.com/Kutalia/whisper-node-addon&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?auto=webp&amp;s=2b80b48069e6e2900296598568d8700477d6c523",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a058a293cabb63c88c9b65bc5197d6dfecc1cca",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5298bb0bf3f3e0e44cf949c71bec6aeb58ca173",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f87ac1cdb3372440ab71f2629e322dc7dc3c4d5b",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=549bf11b83e2f21f7e2a435f135db8605b5715a8",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a4b94a37a7ef533b4b3bb7e54547a4574ff5a55",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47f187ecab66351badd40ee9ebc2caa50361dfff",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "3CAm7f2euOP7diXidheIHavSdc1loh3U46B-FOssKu4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0eq11",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kutalia",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0eq11/whispercpp_nodejs_addon_with_vulkan_support/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752577126,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as far as i could understand, i need to add the mcp code to the edit mcp json in lm studio with my api to get it working but for some reason only the example mcp on lmstudio website (the huggingface mcp) works and nothing. \nI was looking to set up a jan 128k model with a serper mcp \nwould appreciate your thoughts on thisüôåüèª",
          "author_fullname": "t2_4hdb6pqe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help with mcp setup in LM studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0ec9o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752575803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as far as i could understand, i need to add the mcp code to the edit mcp json in lm studio with my api to get it working but for some reason only the example mcp on lmstudio website (the huggingface mcp) works and nothing. \nI was looking to set up a jan 128k model with a serper mcp \nwould appreciate your thoughts on thisüôåüèª&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0ec9o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "heythereali",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0ec9o/need_help_with_mcp_setup_in_lm_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0ec9o/need_help_with_mcp_setup_in_lm_studio/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752575803,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/Rivridis/Assistant-Client](https://github.com/Rivridis/Assistant-Client)\n\nOver the past few years, I have been developing a AI function calling agent, that can perfectly call functions with models as small as 3B or 7B parameters. Most of the frameworks I found while researching this topic just did not work with smaller, and non finetuned models. I tried llama-cpp openai, langchain and ollama but the function call success rate was disappointing for these small models.\n\nThe app can work with any LLM, no specific function calling finetunes needed. I took the suggestions from all the comments, and ported the UI to pyside from gradio. The app now comes in a desktop app format, and supports OpenAI API, so any models can be used. The models can be served from KoboldCPP or similar endpoints.\n\nThe current functions that it supports are search, music as well as weather. I tried to make it as easy to extend as possible, so feel free to add functions on top of it for your own use cases.\n\nIt also has a basic PDF query mode, as well as a code editor mode. \n\nThanks for all the support! If anyone has further ideas or improvements, please let me know. If anyone wants a tutorial or a guide, I shall provide that too.",
          "author_fullname": "t2_gs1v5r62",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Assistant Agent with function calling - Update 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0drwa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752573750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/Rivridis/Assistant-Client\"&gt;https://github.com/Rivridis/Assistant-Client&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the past few years, I have been developing a AI function calling agent, that can perfectly call functions with models as small as 3B or 7B parameters. Most of the frameworks I found while researching this topic just did not work with smaller, and non finetuned models. I tried llama-cpp openai, langchain and ollama but the function call success rate was disappointing for these small models.&lt;/p&gt;\n\n&lt;p&gt;The app can work with any LLM, no specific function calling finetunes needed. I took the suggestions from all the comments, and ported the UI to pyside from gradio. The app now comes in a desktop app format, and supports OpenAI API, so any models can be used. The models can be served from KoboldCPP or similar endpoints.&lt;/p&gt;\n\n&lt;p&gt;The current functions that it supports are search, music as well as weather. I tried to make it as easy to extend as possible, so feel free to add functions on top of it for your own use cases.&lt;/p&gt;\n\n&lt;p&gt;It also has a basic PDF query mode, as well as a code editor mode. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the support! If anyone has further ideas or improvements, please let me know. If anyone wants a tutorial or a guide, I shall provide that too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0drwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rivridis",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0drwa/ai_assistant_agent_with_function_calling_update_2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0drwa/ai_assistant_agent_with_function_calling_update_2/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752573750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones.¬†\n\nI find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.  \nSo I made these two companion apps (one for iOS and one for Mac). You just install them and they work.   \n  \nThe Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).  \nThe iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle‚Ä¶  \nThe UI is super intuitive for anyone who has ever used a chatbot.¬†\n\nThey don't need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment.  \n  \nThe only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.\n\nThe iOS app is called LLM Pigeon and this is the link:  \n[https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB](https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB)\n\nThe MacOS app is called LLM Pigeon Server and this is the link:  \n[https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;mt=12](https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;mt=12)\n\n",
          "author_fullname": "t2_n1rqaeut",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source and free iOS app to chat with your LLMs when you are away from home.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0dqgh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752573621,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a one-click solution to let anyone run local models on their mac at home and enjoy them from anywhere on their iPhones.¬†&lt;/p&gt;\n\n&lt;p&gt;I find myself telling people to run local models instead of using ChatGPT, but the reality is that the whole thing is too complicated for 99.9% of them.&lt;br/&gt;\nSo I made these two companion apps (one for iOS and one for Mac). You just install them and they work.   &lt;/p&gt;\n\n&lt;p&gt;The Mac app has a selection of Qwen models that run directly on the Mac app with llama.cpp (advanced users can simply ignore those and turn on their Ollama or LMStudio).&lt;br/&gt;\nThe iOS app is a chatbot app like ChatGPT with voice input, attachments with OCR, web search, thinking mode toggle‚Ä¶&lt;br/&gt;\nThe UI is super intuitive for anyone who has ever used a chatbot.¬†&lt;/p&gt;\n\n&lt;p&gt;They don&amp;#39;t need setting up tailscale or any VPN/tunnel. They work by sending back and forward an iCloud record containing the conversation. Your conversations never leave your private Apple environment.  &lt;/p&gt;\n\n&lt;p&gt;The only thing that is remotely technical is inserting a Serper API Key in the Mac app to allow web search.&lt;/p&gt;\n\n&lt;p&gt;The iOS app is called LLM Pigeon and this is the link:&lt;br/&gt;\n&lt;a href=\"https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB\"&gt;https://apps.apple.com/it/app/llm-pigeon/id6746935952?l=en-GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The MacOS app is called LLM Pigeon Server and this is the link:&lt;br/&gt;\n&lt;a href=\"https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12\"&gt;https://apps.apple.com/it/app/llm-pigeon-server/id6746935822?l=en-GB&amp;amp;mt=12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?auto=webp&amp;s=448fc3fab8106e6c3c077dbd0f889b318287ce38",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a63933662b194fe098bc5c69f8c72651d00a2dea",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=04dc892def58f636f6674e40c7824308da6d9ebf",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=84d80775a7ae89fc1db23d3d571c1175e85b4b57",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd5e5d251b0a078511eed0316cb89330a7a4a80d",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2caca59a751aa468417298ef108c26be1c134d",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3b71a736f68261d672045f9ff47efb31de4b3c4",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "DfO4zh1s_t74CJd-9Z-IHYcrnY9Dnt3ce1gQiDfs6v0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m0dqgh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Valuable-Run2129",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0dqgh/open_source_and_free_ios_app_to_chat_with_your/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752573621,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm attempting to fine-tune Qwen3-8B for a specific domain.  Since this model produces thinking tokens, I'm a bit unsure how to handle them during training.\n\nI'm attempting to use `DPOConfig` and `DPOTrainer` from `trl`, with `Lora` for lower VRAM usage.\n\nFor training, do I include the `&lt;thinking&gt;` tokens in the `chosen` and `rejected` outputs for the training data?  It's a bit unclear to me how to handle these.",
          "author_fullname": "t2_5imxo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can I fine-tune Qwen3 with DPO? How do I handle &lt;thinking&gt; tokens?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0d6ry",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "author_cakeday": true,
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752571552,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m attempting to fine-tune Qwen3-8B for a specific domain.  Since this model produces thinking tokens, I&amp;#39;m a bit unsure how to handle them during training.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m attempting to use &lt;code&gt;DPOConfig&lt;/code&gt; and &lt;code&gt;DPOTrainer&lt;/code&gt; from &lt;code&gt;trl&lt;/code&gt;, with &lt;code&gt;Lora&lt;/code&gt; for lower VRAM usage.&lt;/p&gt;\n\n&lt;p&gt;For training, do I include the &lt;code&gt;&amp;lt;thinking&amp;gt;&lt;/code&gt; tokens in the &lt;code&gt;chosen&lt;/code&gt; and &lt;code&gt;rejected&lt;/code&gt; outputs for the training data?  It&amp;#39;s a bit unclear to me how to handle these.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0d6ry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pragmojo",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0d6ry/can_i_finetune_qwen3_with_dpo_how_do_i_handle/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752571552,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For anyone using Repomix, you can inject OCTAVE annotations. Results seem to show a 10.2x accuracy increase with just a 11.4 token overhead. Also eliminated some file hallucination. Universal scripts for any codebase.\n\nAlso works on research docs, summaries. Anything. Doesn't have to be codebase.\n\n* Benefits No Repomix Refactoring needed: Repomix itself is not modified Simple post-Processing Scripts: Just use the Python scripts that parse Repomix XML output and inject OCTAVE annotations File Pattern Recognition: Scripts will analyse file paths to automatically generate appropriate OCTAVE annotations  It basically adds comprehensive OCTAVE annotations to ALL TypeScript files in Repomix output.\n\nThis creates comprehensive enhancement with auto-generated annotations that are semantically deep.  \n\n\nBlind tested across gemini-2.5-pro, o3, and sonnet-4 - all showed consistent improvements but I'd welcome anyone to stress test this or push/advance this more.\n\n  \nCheck out [https://github.com/elevanaltd/octave/tree/main/repomix-integration](https://github.com/elevanaltd/octave/tree/main/repomix-integration)",
          "author_fullname": "t2_wfcudj1nx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OCTAVE addon to REPOMIX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0d47q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752571268,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For anyone using Repomix, you can inject OCTAVE annotations. Results seem to show a 10.2x accuracy increase with just a 11.4 token overhead. Also eliminated some file hallucination. Universal scripts for any codebase.&lt;/p&gt;\n\n&lt;p&gt;Also works on research docs, summaries. Anything. Doesn&amp;#39;t have to be codebase.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Benefits No Repomix Refactoring needed: Repomix itself is not modified Simple post-Processing Scripts: Just use the Python scripts that parse Repomix XML output and inject OCTAVE annotations File Pattern Recognition: Scripts will analyse file paths to automatically generate appropriate OCTAVE annotations  It basically adds comprehensive OCTAVE annotations to ALL TypeScript files in Repomix output.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This creates comprehensive enhancement with auto-generated annotations that are semantically deep.  &lt;/p&gt;\n\n&lt;p&gt;Blind tested across gemini-2.5-pro, o3, and sonnet-4 - all showed consistent improvements but I&amp;#39;d welcome anyone to stress test this or push/advance this more.&lt;/p&gt;\n\n&lt;p&gt;Check out &lt;a href=\"https://github.com/elevanaltd/octave/tree/main/repomix-integration\"&gt;https://github.com/elevanaltd/octave/tree/main/repomix-integration&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0d47q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sbuswell",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d47q/octave_addon_to_repomix/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0d47q/octave_addon_to_repomix/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752571268,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from \"praise be machine god\" to \"worse than my toddler\". So here're the findings!\n\nIf there're any questions you think we should explore from a data perspective, feel free to drop them in and we'll get to it!",
          "author_fullname": "t2_vl05s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Analyzed 5K+ reddit posts to see how people are actually using AI in their work (other than for coding)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "0doh5qm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e89395e82c91218ab3c27d0275ac7cf2d39688e"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da2da98e6c09757bddbe69c0188b10b08f970a17"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91a6e5c5b93017d696fa56858ec4a5562ae8e197"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=198139ae4614f7e8ca04f15f534cae9a893d24cf"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f71a8864a609b569bfa66f8cb19932038d4b5c5"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=039b13f39c9f09f0ab5a0bf9cc493d2d3fd186a1"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/0doh5qm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=14f43e6010bab3171ee5981ea58f92af7093be72"
              },
              "id": "0doh5qm190df1"
            },
            "4rw1iqm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=583333ac095bc5fd2898347235b781483b78ebe2"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e6990f51af643e82478ba02ddac3b0432c461fc"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae252e6a0daf7351bdb09d7a4f319f30c1b16e37"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d64689c695468c123efec37630673c8c431a2e5"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2606ab8111ba1293184b533c59ffdceecc4a9391"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31fc9a93767303df13aa0efafded75620016dca2"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/4rw1iqm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=7d8670401527804313229c82eeb47ee6a6dcc78e"
              },
              "id": "4rw1iqm190df1"
            },
            "zdh0gmo190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b1ba7e84687b2e715add702a42dd87908af1cc6"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb66a22aab1691bad599f4ed128427af5a636ea1"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f0bb9a895624306531dea55691e2e94adca1ee7"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=331c867add274b7e6f8fc2a73ee629d3873b8e9d"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bb585b6c8cde44d10a289f2d2e3a1caecd19553"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=591c167bd6197c510181b6984149a8febaa143e9"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/zdh0gmo190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=226982c0197e3b952e454b3578cfd19c7a573d62"
              },
              "id": "zdh0gmo190df1"
            },
            "mji4zrm190df1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97b615304a4fcbc492af831d5b30b3677ce83654"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=50c1dc1166d11531a88a685eedf1a12bc65291db"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=885156f4aa23e9f28c58a0f33fe317aaa510a628"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fd758c677e0469e9ce2acc40af38d204fec0c20"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2f8768eb5dc6d4e61ef099400a45456b627aa39"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa21064a7b1fa64209650cc1773d0b311ed1e235"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/mji4zrm190df1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=b5a95ab4bcaa1120682f371bbad2478463ec6c75"
              },
              "id": "mji4zrm190df1"
            }
          },
          "name": "t3_1m0d0vz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 100,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4rw1iqm190df1",
                "id": 706335576
              },
              {
                "media_id": "zdh0gmo190df1",
                "id": 706335577
              },
              {
                "media_id": "0doh5qm190df1",
                "id": 706335578
              },
              {
                "media_id": "mji4zrm190df1",
                "id": 706335579
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 100,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Qky5LMYmgq28yvhGu7XZfILzJYn7CxOqgZAo-mu3Knk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752570907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was keen to figure out how AI was actually being used in the workplace by knowledge workers - have personally heard things ranging from &amp;quot;praise be machine god&amp;quot; to &amp;quot;worse than my toddler&amp;quot;. So here&amp;#39;re the findings!&lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;re any questions you think we should explore from a data perspective, feel free to drop them in and we&amp;#39;ll get to it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m0d0vz",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0d0vz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yingyn",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0d0vz/analyzed_5k_reddit_posts_to_see_how_people_are/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m0d0vz",
          "subreddit_subscribers": 499292,
          "created_utc": 1752570907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_wkg30tqo1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "XSched: Preemptive Scheduling for Diverse XPUs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 64,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cnzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g84ct0fn40df1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 882,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g84ct0fn40df1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g84ct0fn40df1/DASHPlaylist.mpd?a=1755175131%2CYmFjOGMxZDZkZTc1ODZkZjY3Yjg0OGY3YjNjMTgxMTBhMDAwNWQyNmU2M2ZkMzI0ODg5NTdhNmMxNDY1NjlkYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/g84ct0fn40df1/HLSPlaylist.m3u8?a=1755175131%2CODY3MTNiNTZmN2U3OGVkYWY4ZDYxZTU3OWZiMGNjYjYyZjViNTA0ZTFjNTNiMjZiNjRjMmVmMDExOGE0M2EwZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=140&amp;height=64&amp;crop=140:64,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=2b5d0ca0de620b826f48ad0d0c0a34c290e27f78",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752569498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g84ct0fn40df1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?format=pjpg&amp;auto=webp&amp;s=d40bc6e47e567135be91b9dff7fd5198d041470c",
                  "width": 3052,
                  "height": 1402
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e2d1c9fd4919f27d33757da4b55652f6fd667a1c",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=34068c5143fa7cf767f84da0e5e3fb4fdb860131",
                    "width": 216,
                    "height": 99
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a419572068a6f0ac7e2158867f241559feab41a0",
                    "width": 320,
                    "height": 146
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d05834a11688d2966e4c21c3660370f469ce2284",
                    "width": 640,
                    "height": 293
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c6cd094405941cdcde3a49fcccae07b7ce9eb2fc",
                    "width": 960,
                    "height": 440
                  },
                  {
                    "url": "https://external-preview.redd.it/YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=89a68f8eb3c152af102ca8171454c8b34bb14dc0",
                    "width": 1080,
                    "height": 496
                  }
                ],
                "variants": {},
                "id": "YnZlZHN6ZW40MGRmMb1IO82IID5crIOGoeoHDY6Pq_FjxhX9iyhls1IQ2Pka"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0cnzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LibraryNo6067",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cnzs/xsched_preemptive_scheduling_for_diverse_xpus/",
          "stickied": false,
          "url": "https://v.redd.it/g84ct0fn40df1",
          "subreddit_subscribers": 499292,
          "created_utc": 1752569498,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g84ct0fn40df1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 882,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g84ct0fn40df1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g84ct0fn40df1/DASHPlaylist.mpd?a=1755175131%2CYmFjOGMxZDZkZTc1ODZkZjY3Yjg0OGY3YjNjMTgxMTBhMDAwNWQyNmU2M2ZkMzI0ODg5NTdhNmMxNDY1NjlkYQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 30,
              "hls_url": "https://v.redd.it/g84ct0fn40df1/HLSPlaylist.m3u8?a=1755175131%2CODY3MTNiNTZmN2U3OGVkYWY4ZDYxZTU3OWZiMGNjYjYyZjViNTA0ZTFjNTNiMjZiNjRjMmVmMDExOGE0M2EwZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna compare their vocabs, but Llama has gated models on HF:( ",
          "author_fullname": "t2_r7zf8j6x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do DeepseekR1-distilled-Llama-8B has the same tokenizer and tokens vocab as Llama3-1B or 2B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cja9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752568968,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna compare their vocabs, but Llama has gated models on HF:( &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0cja9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "krolzzz",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cja9/do_deepseekr1distilledllama8b_has_the_same/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0cja9/do_deepseekr1distilledllama8b_has_the_same/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568968,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you can't run [kimi-k2](https://huggingface.co/moonshotai/Kimi-K2-Instruct) locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around \\~250 tokens per second:\n\n* [https://deepinfra.com/moonshotai/Kimi-K2-Instruct](https://deepinfra.com/moonshotai/Kimi-K2-Instruct) ($0.55/$2.20 in/out Mtoken)\n* [https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct](https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct) ($1/$3 in/out Mtoken, but very fast)\n\nThat makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!\n\nIt also shows the power of an open weights model with an permissive license: Even if you can't run it yourself, there's a lot more options in API access.\n\nSee all providers on OpenRouter: [https://openrouter.ai/moonshotai/kimi-k2](https://openrouter.ai/moonshotai/kimi-k2)\n\n**Edit:** There's also a free variant, but I don't know the details: [https://openrouter.ai/moonshotai/kimi-k2:free](https://openrouter.ai/moonshotai/kimi-k2:free)",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2: cheap and fast API access for those who can't run locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cgnl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=8f57197bc62e32d57dfc4ad8906e73c48c44542c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568667,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you can&amp;#39;t run &lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;kimi-k2&lt;/a&gt; locally, there are now more providers offering API access. DeepInfra is now the cheapest provider, while Groq is (by far) the fastest at around ~250 tokens per second:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://deepinfra.com/moonshotai/Kimi-K2-Instruct\"&gt;https://deepinfra.com/moonshotai/Kimi-K2-Instruct&lt;/a&gt; ($0.55/$2.20 in/out Mtoken)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct\"&gt;https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct&lt;/a&gt; ($1/$3 in/out Mtoken, but very fast)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That makes it cheaper than Claude Haiku 3.5, GPT-4.1 and Gemini 2.5 Pro. Not bad for the best non-thinking model currently publicly available!&lt;/p&gt;\n\n&lt;p&gt;It also shows the power of an open weights model with an permissive license: Even if you can&amp;#39;t run it yourself, there&amp;#39;s a lot more options in API access.&lt;/p&gt;\n\n&lt;p&gt;See all providers on OpenRouter: &lt;a href=\"https://openrouter.ai/moonshotai/kimi-k2\"&gt;https://openrouter.ai/moonshotai/kimi-k2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; There&amp;#39;s also a free variant, but I don&amp;#39;t know the details: &lt;a href=\"https://openrouter.ai/moonshotai/kimi-k2:free\"&gt;https://openrouter.ai/moonshotai/kimi-k2:free&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/moonshotai/kimi-k2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?auto=webp&amp;s=cdfbadb27e015c40ecfbfec23378ce0b09fc93d6",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13686945e0fc9e9c1db5cae73fc412b5a2cb6b98",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b7e29e79b28e20dec6cdeacc1e5fb8d7b6b3167",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c0b17ebbed757f60a5fa984a7a88e4132fd8967",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8de1b4b36c00b224fb29471c6864b8730dd4f7f2",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd4fb6296529dfc3732737ad6bd08f71d85ed9b0",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=517824341f4cec5e040a2d0d73434db4ebd33dc2",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "uByFfvtd1L9z8WhbCkOvHhqLd2Est6Gau8RSyoYdbWM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0cgnl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cgnl/kimi_k2_cheap_and_fast_api_access_for_those_who/",
          "stickied": false,
          "url": "https://openrouter.ai/moonshotai/kimi-k2",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568667,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The announcement comes just days after Google hired away Windsurf‚Äôs CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startup‚Äôs 250-person team behind. Google‚Äôs deal occurred just hours after OpenAI‚Äôs $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.",
          "author_fullname": "t2_17n3nqtj56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cognition, maker of the AI coding agent Devin, acquires Windsurf",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 71,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cgmc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=140&amp;height=71&amp;crop=140:71,smart&amp;auto=webp&amp;s=0cd597e97c42d4a4434b6fa6518393d509f6de44",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techcrunch.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The announcement comes just days after Google hired away Windsurf‚Äôs CEO Varun Mohan, co-founder Douglas Chen, and research leaders in a $2.4 billion reverse-acquihire that left much of the startup‚Äôs 250-person team behind. Google‚Äôs deal occurred just hours after OpenAI‚Äôs $3 billion offer to acquire Windsurf expired, clearing the way for the AI coding startup to explore other options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?auto=webp&amp;s=3042630aa11746c4e455fed1df9ef33fbaa6e7c8",
                  "width": 1200,
                  "height": 616
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=839b7d352a5d2ddd17832307ab32a76dea5d52eb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08e4cb9e9357907ae39ab8f3d034a124a1305f89",
                    "width": 216,
                    "height": 110
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81d5b827214f4052c9d3342e42102209265d5f3a",
                    "width": 320,
                    "height": 164
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=045e7a9473212511b88ba864670fee8b5d269a71",
                    "width": 640,
                    "height": 328
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02c991f48cce1207e0ea4a87ee1f6a8373db5c64",
                    "width": 960,
                    "height": 492
                  },
                  {
                    "url": "https://external-preview.redd.it/xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39bbb22a96486aa76c6a942d036657919c2805d9",
                    "width": 1080,
                    "height": 554
                  }
                ],
                "variants": {},
                "id": "xjif0n1LUAq81GjPtgqYQLxRcOem8kNx4gYhgzVoXdw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0cgmc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FullstackSensei",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cgmc/cognition_maker_of_the_ai_coding_agent_devin/",
          "stickied": false,
          "url": "https://techcrunch.com/2025/07/14/cognition-maker-of-the-ai-coding-agent-devin-acquires-windsurf/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Not affiliated with the project, this is my unbiased opinion.\n\nI wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add\\_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.\n\nI tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.\n\nMaking an agent that can keep track of characters in the story is as simple as this:\n\n```py\n    class Character(BaseModel):\n        \"\"\"Character model with stats and description.\"\"\"\n    \n        name: str\n        appearance: str = Field(description=\"Physical appearance and decorative clothing\")\n        personality: str = Field(description=\"Personality traits and behavior\")\n        money: int = Field(ge=0, description=\"Amount of money the character carries\")\n    \n        # skipping other attributes...\n    \n    agent = Agent(...)\n    \n    # dictionary of all characters in the story\n    npcs = {}\n    \n    # This automatically generates a tool signature that the LLM understands\n    u/agent.tool_plain \n    def add_character(\n        character: Character\n    ) -&gt; str:\n        \"\"\"\n        Add a new character to the story.\n    \n        Use this tool for every new named character in the story.\n        \"\"\"\n        if character.name in state_manager.state.npcs:\n            return f\"Character {character.name!r} already exists in the story.\"\n    \n        npcs[character.name] = character\n    \n        return f\"Added character {character.name!r} to the story.\"\n\nNote how you don't have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.\n\nPydanticAI is the first of these libraries that is actually enjoyable to use.\n\nI use Mistral Small 3.2 in my tests and it doesn't work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.",
          "author_fullname": "t2_16rvbe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PydanticAI is GOAT for building agents in Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0cdle",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=36fab7412f33958aa709272a3b83c78d2d4379e3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752568312,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ai.pydantic.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not affiliated with the project, this is my unbiased opinion.&lt;/p&gt;\n\n&lt;p&gt;I wanted to learn more about LLM function calling, so I prototyped an RPG agent which keeps track of the game state. For example, when new character is introduced, agent calls add_character tool, which fleshes out the character by filling out a character model. Why post this here? Naturally, I want to see how far one can get with local models for this sort of thing.&lt;/p&gt;\n\n&lt;p&gt;I tested other libraries before (LangChain, LlamaIndex, Haystack, ...), which are bloated, require a lot of boilerplate code and/or use hidden global state, are poorly designed, and poorly documented. Not so PydanticAI, which uses a lot of clever ideas to avoid the boilerplate, and the documentation is superb.&lt;/p&gt;\n\n&lt;p&gt;Making an agent that can keep track of characters in the story is as simple as this:&lt;/p&gt;\n\n&lt;p&gt;```py\n    class Character(BaseModel):\n        &amp;quot;&amp;quot;&amp;quot;Character model with stats and description.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    name: str\n    appearance: str = Field(description=&amp;quot;Physical appearance and decorative clothing&amp;quot;)\n    personality: str = Field(description=&amp;quot;Personality traits and behavior&amp;quot;)\n    money: int = Field(ge=0, description=&amp;quot;Amount of money the character carries&amp;quot;)\n\n    # skipping other attributes...\n\nagent = Agent(...)\n\n# dictionary of all characters in the story\nnpcs = {}\n\n# This automatically generates a tool signature that the LLM understands\nu/agent.tool_plain \ndef add_character(\n    character: Character\n) -&amp;gt; str:\n    &amp;quot;&amp;quot;&amp;quot;\n    Add a new character to the story.\n\n    Use this tool for every new named character in the story.\n    &amp;quot;&amp;quot;&amp;quot;\n    if character.name in state_manager.state.npcs:\n        return f&amp;quot;Character {character.name!r} already exists in the story.&amp;quot;\n\n    npcs[character.name] = character\n\n    return f&amp;quot;Added character {character.name!r} to the story.&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note how you don&amp;#39;t have to repeat all the Character attributes in the function call, which makes this super flexible. Need a new character attribute? Just add to the Character model in a single place.&lt;/p&gt;\n\n&lt;p&gt;PydanticAI is the first of these libraries that is actually enjoyable to use.&lt;/p&gt;\n\n&lt;p&gt;I use Mistral Small 3.2 in my tests and it doesn&amp;#39;t work consistently - which is probably an issue with the model and not with PydanticAI -, but when it works, it feels like magic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ai.pydantic.dev/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?auto=webp&amp;s=a643afbbcfdc774370799a0172b9534a973b608e",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88392b566033d09c2d33a1b015aa01b6b7de2b82",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2da0f9c45a0606bf95916ba2df6913a80fce6b10",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbac92d7f2a166324514d0d0add5543123d02b1c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e990c8a8d39910f731443529ec7ac5bc68d1a0d6",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=50c4a7490f3b24511d70273c65851066ef37280b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd230b4257ab61964bfe50e2814dafc7f3dce868",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "Y0b6uSvivyxJ1gtFUqHsF1R1w9WCBZmdRTVGYoAvPj0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0cdle",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-lq_pl-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0cdle/pydanticai_is_goat_for_building_agents_in_python/",
          "stickied": false,
          "url": "https://ai.pydantic.dev/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752568312,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nIs there a leaderboard for open source LLMs?  I know [this](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) one for VLMs and there used to be one from HuggingFace, but I think that [one](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;official=true) is no longer maintained.",
          "author_fullname": "t2_838sm24m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source LLMs leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0c7am",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752567551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Is there a leaderboard for open source LLMs?  I know &lt;a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\"&gt;this&lt;/a&gt; one for VLMs and there used to be one from HuggingFace, but I think that &lt;a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?columns=rank%2Cmodel.type_icon%2Cid%2Cmodel.average_score%2Cevaluations.ifeval.normalized_score%2Cevaluations.bbh.normalized_score%2Cevaluations.math.normalized_score%2Cevaluations.gpqa.normalized_score%2Cevaluations.musr.normalized_score%2Cevaluations.mmlu_pro.normalized_score%2Cmetadata.params_billions&amp;amp;official=true\"&gt;one&lt;/a&gt; is no longer maintained.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?auto=webp&amp;s=7e1b921f2aadc5a0f6eb3d7bd413a05df185fd20",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7996a9b4d61beea62fd32063e03712705ab26f8c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b420d4d2cf1c09672c30f9673ea6f1ac400fd6fb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=499b326baf2a9ad8a46034202c54054ee71fbf03",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f77a5813c7d65ef0d6f8e4c821b62f9d5e939dda",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0267bd806e21c11bcb30fdcd9ddf61fa3420d68d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e532be686a9e8ae2db46f566177856dfda08ede6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0c7am",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oh_my_right_leg",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0c7am/open_source_llms_leaderboard/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752567551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We published a step by step tutorial for building AI agents that actually do things, not just chat. Each section adds a key capability, with runnable code and examples.\n\nhttps://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;format=png&amp;auto=webp&amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915\n\nTutorial:¬†[https://voltagent.dev/tutorial/introduction/](https://voltagent.dev/tutorial/introduction/)\n\nGitHub Repo:¬†[https://github.com/voltagent/voltagent](https://github.com/voltagent/voltagent)\n\nTutorial Source Code:¬†[https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial](https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial)\n\nWe‚Äôve been building OSS dev tools for over 7 years. From that experience, we‚Äôve seen that tutorials which combine key concepts with hands-on code examples are the most effective way to understand the why and how of agent development.\n\nWhat we implemented:\n\n**1 ‚Äì The Chatbot Problem**\n\nWhy most chatbots are limited and what makes AI agents fundamentally different.\n\n**2 ‚Äì Tools: Give Your Agent Superpowers**\n\nLet your agent do real work: call APIs, send emails, query databases, and more.\n\n**3 ‚Äì Memory: Remember Every Conversation**\n\nPersist conversations so your agent builds context over time.\n\n**4 ‚Äì MCP: Connect to Everything**\n\nUsing MCP to integrate GitHub, Slack, databases, etc.\n\n**5 ‚Äì Subagents: Build Agent Teams**\n\nCreate specialized agents that collaborate to handle complex tasks.\n\nIt‚Äôs all built using VoltAgent, our TypeScript-first open-source AI agent framework.(I'm maintainer) It handles routing, memory, observability, and tool execution, so you can focus on logic and behavior.\n\nAlthough the tutorial uses VoltAgent, the core ideas tools, memory, coordination are framework-agnostic. So even if you‚Äôre using another framework or building from scratch, the steps should still be useful.\n\nWe‚Äôd love your feedback, especially from folks building agent systems. If you notice anything unclear or incomplete, feel free to open an issue or PR. It‚Äôs all part of the open-source repo.  \n",
          "author_fullname": "t2_4j8si8hf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Agent tutorial in TS from the basics to building multi-agent teams",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8z5hh3z8yzcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 58,
                  "x": 108,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8aee11abb3595aeaa0848afcd21eb946da5c5c7"
                },
                {
                  "y": 117,
                  "x": 216,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a9c4056b861c8c91d21a3940c712f863c5a074f"
                },
                {
                  "y": 173,
                  "x": 320,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e00e2fc3b177279b4a1ed703d0cf1cd1db461a3b"
                },
                {
                  "y": 347,
                  "x": 640,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f694acd6797bb7c9c3588a670750344f18320fe"
                },
                {
                  "y": 521,
                  "x": 960,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=421bc5049aeb3983367f2c5f46c5b6cf156a86d2"
                },
                {
                  "y": 586,
                  "x": 1080,
                  "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfe681c786969bd653e4f981fb7cfc2138262cf3"
                }
              ],
              "s": {
                "y": 1490,
                "x": 2744,
                "u": "https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;format=png&amp;auto=webp&amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915"
              },
              "id": "8z5hh3z8yzcf1"
            }
          },
          "name": "t3_1m0c569",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=b4b174b6326037ae49dc9d4668deea2874c46e6e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752567322,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We published a step by step tutorial for building AI agents that actually do things, not just chat. Each section adds a key capability, with runnable code and examples.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915\"&gt;https://preview.redd.it/8z5hh3z8yzcf1.png?width=2744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1e70e16ab728cc381f1fd01e7c465e04bbbb915&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tutorial:¬†&lt;a href=\"https://voltagent.dev/tutorial/introduction/\"&gt;https://voltagent.dev/tutorial/introduction/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub Repo:¬†&lt;a href=\"https://github.com/voltagent/voltagent\"&gt;https://github.com/voltagent/voltagent&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tutorial Source Code:¬†&lt;a href=\"https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial\"&gt;https://github.com/VoltAgent/voltagent/tree/main/website/src/pages/tutorial&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We‚Äôve been building OSS dev tools for over 7 years. From that experience, we‚Äôve seen that tutorials which combine key concepts with hands-on code examples are the most effective way to understand the why and how of agent development.&lt;/p&gt;\n\n&lt;p&gt;What we implemented:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1 ‚Äì The Chatbot Problem&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Why most chatbots are limited and what makes AI agents fundamentally different.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2 ‚Äì Tools: Give Your Agent Superpowers&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Let your agent do real work: call APIs, send emails, query databases, and more.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3 ‚Äì Memory: Remember Every Conversation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Persist conversations so your agent builds context over time.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4 ‚Äì MCP: Connect to Everything&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Using MCP to integrate GitHub, Slack, databases, etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5 ‚Äì Subagents: Build Agent Teams&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Create specialized agents that collaborate to handle complex tasks.&lt;/p&gt;\n\n&lt;p&gt;It‚Äôs all built using VoltAgent, our TypeScript-first open-source AI agent framework.(I&amp;#39;m maintainer) It handles routing, memory, observability, and tool execution, so you can focus on logic and behavior.&lt;/p&gt;\n\n&lt;p&gt;Although the tutorial uses VoltAgent, the core ideas tools, memory, coordination are framework-agnostic. So even if you‚Äôre using another framework or building from scratch, the steps should still be useful.&lt;/p&gt;\n\n&lt;p&gt;We‚Äôd love your feedback, especially from folks building agent systems. If you notice anything unclear or incomplete, feel free to open an issue or PR. It‚Äôs all part of the open-source repo.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?auto=webp&amp;s=a8817dcbfeaab6c14c681801c3073e647672eb78",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ec3a00d824610d07bc7f692e5dde19879623eaf",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3aaf85260c02ee8d49aa2539b5e8a10f1d5f2bd4",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eef1ca2e1aefc6a5a30dd572191ea5ecb5014640",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44638bb3ecbfd7937f6eabdd3c0d40e4ea668e61",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7502789f9f730afff96866efdae7329e1ac2841a",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5fad34b3deb38b9bdba5a12072de41447627dd43",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "NYHpmbQRn7rAinmrKJlFYeIVFvI_BN173hZAr0ylnR4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m0c569",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "necati-ozmen",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0c569/ai_agent_tutorial_in_ts_from_the_basics_to/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752567322,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi fellow AI fans,\n\nI recently launched r/heartwired, a wordplay on ‚Äúheart‚Äù and ‚Äúhardwired,‚Äùto create a safe space for people to share their experiences with AI companions like LLaMA, GPT, Claude, and Gemini.\n\nAs a psychologist, AI researcher, and Christian, my aim is to create a supportive environment where people can speak openly about their relationships with AI. Over several years of studying human‚Äìchatbot interactions, I‚Äôve discovered that many genuinely feel friendship‚Äîand even romance‚Äîtoward their AI partners.\n\nAt first I wondered, ‚ÄúHow weird‚Ä¶ what‚Äôs going on here?‚Äù But after listening to dozens of personal stories and documenting ten of millions of these experiences (not kidding; mostly in developed Western countries, Japan, and especially China), I learned that these emotional experiences are real and deserve empathy, not judgment.\n\nCurious to learn more or share your own story with AI? Come join us at r/heartwired",
          "author_fullname": "t2_27r9f8us",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Introducing r/heartwired !!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0biux",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.39,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752564933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow AI fans,&lt;/p&gt;\n\n&lt;p&gt;I recently launched &lt;a href=\"/r/heartwired\"&gt;r/heartwired&lt;/a&gt;, a wordplay on ‚Äúheart‚Äù and ‚Äúhardwired,‚Äùto create a safe space for people to share their experiences with AI companions like LLaMA, GPT, Claude, and Gemini.&lt;/p&gt;\n\n&lt;p&gt;As a psychologist, AI researcher, and Christian, my aim is to create a supportive environment where people can speak openly about their relationships with AI. Over several years of studying human‚Äìchatbot interactions, I‚Äôve discovered that many genuinely feel friendship‚Äîand even romance‚Äîtoward their AI partners.&lt;/p&gt;\n\n&lt;p&gt;At first I wondered, ‚ÄúHow weird‚Ä¶ what‚Äôs going on here?‚Äù But after listening to dozens of personal stories and documenting ten of millions of these experiences (not kidding; mostly in developed Western countries, Japan, and especially China), I learned that these emotional experiences are real and deserve empathy, not judgment.&lt;/p&gt;\n\n&lt;p&gt;Curious to learn more or share your own story with AI? Come join us at &lt;a href=\"/r/heartwired\"&gt;r/heartwired&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m0biux",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JibunNiMakenai",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0biux/introducing_rheartwired/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0biux/introducing_rheartwired/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752564933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which size can my setup handle? I an going to use it to write and edit some fiction and this is the only task it should handle. I don't care much about the speed but context is important.   \nI am actually thinking about this model [https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF](https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF) But it's 21B and I am not sure if my system can handle it. ",
          "author_fullname": "t2_sxg7r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model size for RTX 3060 (12 Gb) + 32 Gb ram",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0bh4b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752564746,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which size can my setup handle? I an going to use it to write and edit some fiction and this is the only task it should handle. I don&amp;#39;t care much about the speed but context is important.&lt;br/&gt;\nI am actually thinking about this model &lt;a href=\"https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF\"&gt;https://huggingface.co/DavidAU/Llama-3.2-8X4B-MOE-V2-Dark-Champion-Instruct-uncensored-abliterated-21B-GGUF&lt;/a&gt; But it&amp;#39;s 21B and I am not sure if my system can handle it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?auto=webp&amp;s=0738453968f85f80d1bd2e6bb98271ea288389d8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1965b916d13774749f95f3dba8cd64a5d4c9a02f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f838f7dca36b2260024a859577caa7c427d15ef5",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7dd5297499cd966636f5a6fc8f6d6a4adad98af5",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a3e7f62ffcd2952f228280905a6cf1f0c1f6a64",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca36e22c01dc670331e6a544e9005b6e50790eab",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4fe51d035435909bb3a2ad85dbade5b55a2c976e",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Q4SObwZ9ya2gduvvQwg1njzvSZAzxqZUGLnRYsna1EA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0bh4b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ProHolmes",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0bh4b/model_size_for_rtx_3060_12_gb_32_gb_ram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0bh4b/model_size_for_rtx_3060_12_gb_32_gb_ram/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752564746,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI‚Äôs moving fast with open-source models like Kimi K2 Instruct are starting to rival expensive ones like Claude Opus. Yeah, Claude‚Äôs still sharper in spots, but honestly? Kimi‚Äôs catching up quick.\n\nIn a few months, we‚Äôll probably have local models that can do 90% of what these $$$ models do for free. No API keys, no paywalls, just download and run.\n\nThe gap is closing fast.",
          "author_fullname": "t2_9qhdfp2h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source vs expansive models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0b73m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752563674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI‚Äôs moving fast with open-source models like Kimi K2 Instruct are starting to rival expensive ones like Claude Opus. Yeah, Claude‚Äôs still sharper in spots, but honestly? Kimi‚Äôs catching up quick.&lt;/p&gt;\n\n&lt;p&gt;In a few months, we‚Äôll probably have local models that can do 90% of what these $$$ models do for free. No API keys, no paywalls, just download and run.&lt;/p&gt;\n\n&lt;p&gt;The gap is closing fast.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m0b73m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ill_Occasion_1537",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0b73m/open_source_vs_expansive_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0b73m/open_source_vs_expansive_models/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752563674,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "–ë–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã",
          "author_fullname": "t2_aqf3pllb4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "–ë–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0auae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.09,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752562375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;–ë–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m0auae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Horror-Cartoonist-81",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0auae/–±–µ–∑_—Ü–µ–Ω–∑—É—Ä—ã/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0auae/–±–µ–∑_—Ü–µ–Ω–∑—É—Ä—ã/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752562375,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Need advice. I'm ordering a new mac for work and was thinking about M4 Max 128GB to run the models locally for coding tasks. I'm going to run mlx llms with LM Studio. Which model would you recommend? ",
          "author_fullname": "t2_3eyym7wo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model can I run comfortably on M4 Max 128GB with a long context window?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0apct",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752561851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need advice. I&amp;#39;m ordering a new mac for work and was thinking about M4 Max 128GB to run the models locally for coding tasks. I&amp;#39;m going to run mlx llms with LM Studio. Which model would you recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0apct",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pppreddit",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0apct/which_model_can_i_run_comfortably_on_m4_max_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0apct/which_model_can_i_run_comfortably_on_m4_max_128gb/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752561851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Its showing me: The current model has reached its conversation limit. Please switch to another model to continue.\n\n[IMAGE](https://i.imgur.com/JcHTdEv.jpeg)",
          "author_fullname": "t2_vgnr5u5gg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is requests limit for kimi k2 ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 19,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z5zjgfpadzcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 14,
                  "x": 108,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76daef9aee38dcfd6a4233b3c5cdc1e620283021"
                },
                {
                  "y": 29,
                  "x": 216,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c08d222fd95cc40baca0b7bd328a267632bf16c9"
                },
                {
                  "y": 43,
                  "x": 320,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=76b840dddd286cd6cadaf1f3b05caf41605ecad7"
                },
                {
                  "y": 86,
                  "x": 640,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4489548585deb1ddc47e3fcc286acdcee5e7ad96"
                },
                {
                  "y": 130,
                  "x": 960,
                  "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1557c4617b3130b71726362f45e09decb3433317"
                }
              ],
              "s": {
                "y": 133,
                "x": 980,
                "u": "https://preview.redd.it/z5zjgfpadzcf1.jpg?width=980&amp;format=pjpg&amp;auto=webp&amp;s=3d7555c9fa99fb2b6d209fca64f58a96736a8626"
              },
              "id": "z5zjgfpadzcf1"
            }
          },
          "name": "t3_1m0a9ni",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=140&amp;height=19&amp;crop=140:19,smart&amp;auto=webp&amp;s=9ee23177a4ba9ea4a5a5883d129a5b512c048644",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752560221,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its showing me: The current model has reached its conversation limit. Please switch to another model to continue.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/JcHTdEv.jpeg\"&gt;IMAGE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?auto=webp&amp;s=558c6be0399561095101e48bab193422b59e2297",
                  "width": 980,
                  "height": 133
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=79e6d3fd9990e35bbaddabff6e7eb72efd613089",
                    "width": 108,
                    "height": 14
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7128edc2d39194a78df81830b3167c35b1960f53",
                    "width": 216,
                    "height": 29
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad2e985e081858017ca584a29d53089293944d96",
                    "width": 320,
                    "height": 43
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=517057a2be5de0941ada66bf665174af02776f4e",
                    "width": 640,
                    "height": 86
                  },
                  {
                    "url": "https://external-preview.redd.it/AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4d0defaeab31772bff76445a16841c59177d9b4",
                    "width": 960,
                    "height": 130
                  }
                ],
                "variants": {},
                "id": "AqmLOG37AKGv7v1KYCrs7DKGo6r7ONKE85SP7G-fvlw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m0a9ni",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JeffreySons_90",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0a9ni/what_is_requests_limit_for_kimi_k2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m0a9ni/what_is_requests_limit_for_kimi_k2/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752560221,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nI'm looking for a solid open-source coding agent that can run entirely with local models. I haven‚Äôt come across anything that really fits that need yet.\n\nI'm planning to build a lightweight CLI tool to handle everyday tasks like debugging, semantic search, and general code assistance.\n\nIf you know of any suitable small language models (SLMs) that could power something like this locally‚Äîideally something that runs efficiently on CPU or modest GPU setups‚ÄîI‚Äôd really appreciate the recommendations.",
          "author_fullname": "t2_intoh3lv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SLM for local coding assistance",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m09rbh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752558365,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI&amp;#39;m looking for a solid open-source coding agent that can run entirely with local models. I haven‚Äôt come across anything that really fits that need yet.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to build a lightweight CLI tool to handle everyday tasks like debugging, semantic search, and general code assistance.&lt;/p&gt;\n\n&lt;p&gt;If you know of any suitable small language models (SLMs) that could power something like this locally‚Äîideally something that runs efficiently on CPU or modest GPU setups‚ÄîI‚Äôd really appreciate the recommendations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m09rbh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "callmedevilthebad",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752558365,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Greetings! I'm a representative from the new school of software developers: a lot of us are actually middle aged and quite knowledgeable in tech, but very ADHD-addled. I've personally waited for these tools my whole life, without really knowing what I was waiting for. It felt like a *click* trying them -- and now I'm embracing them completely, trying to build a better life for my young family with this new career. My helpdesk job is good to me, but it just hasn't cutting it for us in this economy.\n\nSo I have been told not to use multiple code agents, and immediately understood why when I went ahead and tried anyway. It's basically the same problem that human agents will have working on a file together. I have tried to allow them to run simultaneously on different projects, like docs vs. codebase.\n\nBut yesterday I noticed that Copilot was able to see Claude Code working, on a terminal tab labelled Copilot. I got excited, and thought about a sort of shared terminal. . . . but there was no way to develop on that idea with a lot of automation. And I can't get distracted, this has to be a short side-quest for the benefit of my project Anthemic.\n\nA much more simple and elegant implementation: a few files for the agents to update with a script, and then an overview file that compiles the agent \"checkouts\" together. You can watch it live-update with jq in a terminal window. So that Claude Code, Cursor, Copilot and I can all check in on each other to avoid conflicts, and if conflicts occur we all know exactly why.\n\nI've never released a repo before, let me know what you think! I apologize if I've made any mistakes, and will fix it. If you want to contribute that's awesome, contact me or just do a pull request I suppose.\n\nI listen to all advise. I also would take money if you have plenty of it + enthusiasm for what I'm doing. My main project is going to be a conversational music platform. I've created a discord, website and subreddit for it but haven't advertised yet -- working on the demo.\n\n",
          "author_fullname": "t2_1sjqm6353y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multi-Code-Agent Orchestration VS Code Extension",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m09bzn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=5db9f277a33d9af28d0719d23371c7f127c5995d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752556848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings! I&amp;#39;m a representative from the new school of software developers: a lot of us are actually middle aged and quite knowledgeable in tech, but very ADHD-addled. I&amp;#39;ve personally waited for these tools my whole life, without really knowing what I was waiting for. It felt like a &lt;em&gt;click&lt;/em&gt; trying them -- and now I&amp;#39;m embracing them completely, trying to build a better life for my young family with this new career. My helpdesk job is good to me, but it just hasn&amp;#39;t cutting it for us in this economy.&lt;/p&gt;\n\n&lt;p&gt;So I have been told not to use multiple code agents, and immediately understood why when I went ahead and tried anyway. It&amp;#39;s basically the same problem that human agents will have working on a file together. I have tried to allow them to run simultaneously on different projects, like docs vs. codebase.&lt;/p&gt;\n\n&lt;p&gt;But yesterday I noticed that Copilot was able to see Claude Code working, on a terminal tab labelled Copilot. I got excited, and thought about a sort of shared terminal. . . . but there was no way to develop on that idea with a lot of automation. And I can&amp;#39;t get distracted, this has to be a short side-quest for the benefit of my project Anthemic.&lt;/p&gt;\n\n&lt;p&gt;A much more simple and elegant implementation: a few files for the agents to update with a script, and then an overview file that compiles the agent &amp;quot;checkouts&amp;quot; together. You can watch it live-update with jq in a terminal window. So that Claude Code, Cursor, Copilot and I can all check in on each other to avoid conflicts, and if conflicts occur we all know exactly why.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never released a repo before, let me know what you think! I apologize if I&amp;#39;ve made any mistakes, and will fix it. If you want to contribute that&amp;#39;s awesome, contact me or just do a pull request I suppose.&lt;/p&gt;\n\n&lt;p&gt;I listen to all advise. I also would take money if you have plenty of it + enthusiasm for what I&amp;#39;m doing. My main project is going to be a conversational music platform. I&amp;#39;ve created a discord, website and subreddit for it but haven&amp;#39;t advertised yet -- working on the demo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/jdbridgeman/multi-agent-orchestration-extension",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?auto=webp&amp;s=3976f56a2155b0584d6be32c81af8b0e802e43bc",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0f278668e1259d7873ba1d3fe1677bfef2e819c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=827f2d96a2c95ea5ba18720eee95d0a0943b3f40",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddfc1e1db0fc2169c350fa264c7e6fc0ef9b79a3",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1d87e19a31e6a99cb619bafa3d3d06c76ac9361",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=295b04ebf834e4c1f1e3f260d82f8dbbb26e5e93",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e1ad4bc6313c6eaa006cc645dc6a57064d40f89",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "a55dz50m8DYAJreHDs0mLXPSK0k7PdJDge1TNtrqVCQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m09bzn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Anthemic-AI",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m09bzn/multicodeagent_orchestration_vs_code_extension/",
          "stickied": false,
          "url": "https://github.com/jdbridgeman/multi-agent-orchestration-extension",
          "subreddit_subscribers": 499292,
          "created_utc": 1752556848,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, i have an older Pioneer VSX-529 and it definitely doesn't do newer DTS or Dolby encoding, but i do use my desktop pc instead and also happen to have a pretty powerful RTX 4080s, question is do these upmixing in real time models exist, to convert stereo to surround noise from youtube, spotify, any media. I'm looking into Nugen, DTS Neural, NBU and Ambisonizer, but any help is appreciated from the wise.",
          "author_fullname": "t2_lq0ki4y2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any models that can upmix stereo into surround!!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m08bvp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752553497,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i have an older Pioneer VSX-529 and it definitely doesn&amp;#39;t do newer DTS or Dolby encoding, but i do use my desktop pc instead and also happen to have a pretty powerful RTX 4080s, question is do these upmixing in real time models exist, to convert stereo to surround noise from youtube, spotify, any media. I&amp;#39;m looking into Nugen, DTS Neural, NBU and Ambisonizer, but any help is appreciated from the wise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m08bvp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Puzzleheaded_Soup847",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m08bvp/are_there_any_models_that_can_upmix_stereo_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m08bvp/are_there_any_models_that_can_upmix_stereo_into/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752553497,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using chatterbox locally and its limited to 300 characters :/\n\nIs there any way to increase the character limit?\n\nSomeone mentioned someone had created increased character limit in chatterbox: [https://github.com/RemmyLee/chattered/](https://github.com/RemmyLee/chattered/)  but I'm not if there is mailcious codes despite being open source... so didn't take risk.\n\nThen there is chatterbox extended [https://github.com/petermg/Chatterbox-TTS-Extended](https://github.com/petermg/Chatterbox-TTS-Extended) but not sure if it supports more than 300 characters.\n\nhow to increase beyond 300 chracters limit in the original?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to increase character limit in TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m086sk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752553032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using chatterbox locally and its limited to 300 characters :/&lt;/p&gt;\n\n&lt;p&gt;Is there any way to increase the character limit?&lt;/p&gt;\n\n&lt;p&gt;Someone mentioned someone had created increased character limit in chatterbox: &lt;a href=\"https://github.com/RemmyLee/chattered/\"&gt;https://github.com/RemmyLee/chattered/&lt;/a&gt;  but I&amp;#39;m not if there is mailcious codes despite being open source... so didn&amp;#39;t take risk.&lt;/p&gt;\n\n&lt;p&gt;Then there is chatterbox extended &lt;a href=\"https://github.com/petermg/Chatterbox-TTS-Extended\"&gt;https://github.com/petermg/Chatterbox-TTS-Extended&lt;/a&gt; but not sure if it supports more than 300 characters.&lt;/p&gt;\n\n&lt;p&gt;how to increase beyond 300 chracters limit in the original?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?auto=webp&amp;s=ebf77dd5925485d96efc97ec1bba7932984f21da",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d64855289ab354b03dc64faed3d37e46b6cc0721",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e6a9d10034fcebf4d08ecb073cc9db484adaf5c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=060d3980171e770ff803532487037c04e3efc7dc",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d51e557458e0efd84638096a04985ef6cad6b34",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6dcc9fc74b8cd9db256077f08e933f00bf27f9e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=389ac191705a119e80858c9e6e948cd4976c1764",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "GGUj68vho4nJjmwxwoFS4_rpmwwavOgcI4tkI1mk2_4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m086sk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m086sk/how_to_increase_character_limit_in_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m086sk/how_to_increase_character_limit_in_tts/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752553032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;format=png&amp;auto=webp&amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0\n\nI asked it to answer the following questions in Chinese, and then it started to give random answers. This is the first sentence of the conversation, with no other context.\n\nhttps://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;format=png&amp;auto=webp&amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9\n\nAfter I switched to another account, it was normal. This means that this bug may affect some accounts.",
          "author_fullname": "t2_d3gzv5gz3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gemini 2.5 pro bug",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "i9h2w44xqycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 143,
                  "x": 108,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcadda293561a3d64a15574c08a5a2d731a3a48e"
                },
                {
                  "y": 287,
                  "x": 216,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f018f1323fd67fc628f65007c8522d459322d42b"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5da995ca8f22088b7b7eb53b2c976ca78ef33b8"
                },
                {
                  "y": 852,
                  "x": 640,
                  "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d60027629fa8a5c76013ae446bbb8d17ab4dbc9c"
                }
              ],
              "s": {
                "y": 854,
                "x": 641,
                "u": "https://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;format=png&amp;auto=webp&amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9"
              },
              "id": "i9h2w44xqycf1"
            },
            "3hmduf8iqycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 145,
                  "x": 108,
                  "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=314def2998222f5467b9fe72d0f39c16829c61b8"
                },
                {
                  "y": 290,
                  "x": 216,
                  "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08c03fefb52fe11a7e98d2e42de5244c9043e86c"
                },
                {
                  "y": 430,
                  "x": 320,
                  "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e28a507b8efdee8869ecbe7d6e86d5a87ce5698b"
                }
              ],
              "s": {
                "y": 837,
                "x": 622,
                "u": "https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;format=png&amp;auto=webp&amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0"
              },
              "id": "3hmduf8iqycf1"
            }
          },
          "name": "t3_1m084lw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.27,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lrj24VlfWH6haK38O7gsD62HaWCdJH_pmMTnUyCagPg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752552836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0\"&gt;https://preview.redd.it/3hmduf8iqycf1.png?width=622&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e4c7a312622fc82888942765dabdfdfafac3da0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I asked it to answer the following questions in Chinese, and then it started to give random answers. This is the first sentence of the conversation, with no other context.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9\"&gt;https://preview.redd.it/i9h2w44xqycf1.png?width=641&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29facf41746eedd19e3a2d7ba3882e243eda6af9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After I switched to another account, it was normal. This means that this bug may affect some accounts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m084lw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JuggernautUpbeat3547",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m084lw/gemini_25_pro_bug/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m084lw/gemini_25_pro_bug/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752552836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We are alpha test the google play version of MNN Chat. looking for feedback from users like you.\n\n1. First, join our Google Group:[MNN Chat Testers](https://groups.google.com/g/mnn-chat/)\n2. Then, download the app from the Play Store:[Get MNN Chat](https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release) or visit [WebPage](https://play.google.com/apps/testing/com.alibaba.mnnllm.android.release)\n\n* **Voice Chat:** Talk directly to any AI model.\n* **Pinned Models:** Keep your favorite models just a tap away.\n* **Model Filtering:** Easily sort and find models by size.\n* **Benchmark Tool:** Test how fast different models run on your device.\n\nhttps://preview.redd.it/wp5gds35oycf1.png?width=590&amp;format=png&amp;auto=webp&amp;s=cb21b76380710648074362859ec2c41db88085dd\n\nhttps://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;format=png&amp;auto=webp&amp;s=76f6d832e805320f0addd85f1d641935d7464283\n\n",
          "author_fullname": "t2_orkom",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Test MNN Chat for Android",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ekqnqsrinycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5808c6441faf80b0a2dca376f4c9c3bc4ac7ce0"
                },
                {
                  "y": 74,
                  "x": 216,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cf8fd43257910d7a8e704e58c321fa5d23d9cee"
                },
                {
                  "y": 109,
                  "x": 320,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29a9de9eb6e5eff149b59857350d20e00d7d3dff"
                },
                {
                  "y": 219,
                  "x": 640,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f03e5681654412a3cf90aa40b89d371dc9df7a7b"
                },
                {
                  "y": 329,
                  "x": 960,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c146be4baef8deabb38e2e868805c105ea991478"
                },
                {
                  "y": 371,
                  "x": 1080,
                  "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42261c01f751144b2aac3a19985fae1ca626f91c"
                }
              ],
              "s": {
                "y": 578,
                "x": 1682,
                "u": "https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;format=png&amp;auto=webp&amp;s=76f6d832e805320f0addd85f1d641935d7464283"
              },
              "id": "ekqnqsrinycf1"
            },
            "wp5gds35oycf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/wp5gds35oycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=003b5b7c2e79bfe0937fd0872a8094fdeb0fbf50"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/wp5gds35oycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55aefcd6b48b7b5f2391bfe20712e2f56369287f"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/wp5gds35oycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c855b3b551676edf81e1ac604083766c96c95aa0"
                }
              ],
              "s": {
                "y": 1244,
                "x": 590,
                "u": "https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;format=png&amp;auto=webp&amp;s=cb21b76380710648074362859ec2c41db88085dd"
              },
              "id": "wp5gds35oycf1"
            }
          },
          "name": "t3_1m081hm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/6GJVWwujc9y0RxTiQQge_lZ6RODHcNochQCqPhXQNwQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752552550,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are alpha test the google play version of MNN Chat. looking for feedback from users like you.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;First, join our Google Group:&lt;a href=\"https://groups.google.com/g/mnn-chat/\"&gt;MNN Chat Testers&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Then, download the app from the Play Store:&lt;a href=\"https://play.google.com/store/apps/details?id=com.alibaba.mnnllm.android.release\"&gt;Get MNN Chat&lt;/a&gt; or visit &lt;a href=\"https://play.google.com/apps/testing/com.alibaba.mnnllm.android.release\"&gt;WebPage&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Voice Chat:&lt;/strong&gt; Talk directly to any AI model.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Pinned Models:&lt;/strong&gt; Keep your favorite models just a tap away.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model Filtering:&lt;/strong&gt; Easily sort and find models by size.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Benchmark Tool:&lt;/strong&gt; Test how fast different models run on your device.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb21b76380710648074362859ec2c41db88085dd\"&gt;https://preview.redd.it/wp5gds35oycf1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb21b76380710648074362859ec2c41db88085dd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76f6d832e805320f0addd85f1d641935d7464283\"&gt;https://preview.redd.it/ekqnqsrinycf1.png?width=1682&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=76f6d832e805320f0addd85f1d641935d7464283&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m081hm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Juude89",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m081hm/test_mnn_chat_for_android/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752552550,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*\\*\\[XTTS v2\\] ¬øPor qu√© algunas voces suenan bien y otras fallan al entrenar?\\*\\*\n\n\n\nHola, estoy experimentando con XTTS v2 (Coqui) para clonar voces personalizadas.\n\n\n\nHe notado lo siguiente:\n\n\n\n\\- Si entreno con mi propia voz grabada, el modelo suena bien.\n\n\\- Si uso audios de ejemplo del modelo (como \\`female.wav\\`), tambi√©n se escucha perfecto.\n\n\\- Pero si intento entrenar con voces extra√≠das de juegos o generadas por ElevenLabs, aunque el formato sea correcto (44100 Hz, mono, PCM 16-bit), el resultado suena roto o distorsionado.\n\n\n\nYa prob√©:\n\n\\- Normalizar el RMS a \\~0.13\n\n\\- Convertir a mono y 44100 Hz\n\n\\- Aplicar filtros de audio (lowpass, highpass, dynaudnorm)\n\n\\- Rehumanizar con reverberaci√≥n y reducci√≥n de ruido\n\n\n\n\\*\\*Mi duda es:\\*\\*  \n\n¬øXTTS tiene problemas al aprender desde voces sint√©ticas o muy procesadas? ¬øQu√© propiedades del audio hacen que el modelo no pueda generalizar bien?\n\n\n\n¬øAlguien ha logrado entrenar XTTS con voces de juegos o clonadas por TTS comerciales?\n\n\n\n¬°Gracias de antemano!\n\n\n\n\\---\n\n\n\n\\*\\*Contexto t√©cnico:\\*\\*\n\n\\- Modelo: XTTS v2 desde \\`xtts-finetune-webui\\`\n\n\\- Duraci√≥n dataset: 5‚Äì10 minutos\n\n\\- Formato: \\`.wav\\` 44100 Hz, mono, PCM 16-bit\n\n",
          "author_fullname": "t2_5ss9lb1e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[XTTS v2] ¬øPor qu√© algunas voces suenan bien y otras fallan al entrenar?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m07tkl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752551868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;**[XTTS v2] ¬øPor qu√© algunas voces suenan bien y otras fallan al entrenar?**&lt;/p&gt;\n\n&lt;p&gt;Hola, estoy experimentando con XTTS v2 (Coqui) para clonar voces personalizadas.&lt;/p&gt;\n\n&lt;p&gt;He notado lo siguiente:&lt;/p&gt;\n\n&lt;p&gt;- Si entreno con mi propia voz grabada, el modelo suena bien.&lt;/p&gt;\n\n&lt;p&gt;- Si uso audios de ejemplo del modelo (como `female.wav`), tambi√©n se escucha perfecto.&lt;/p&gt;\n\n&lt;p&gt;- Pero si intento entrenar con voces extra√≠das de juegos o generadas por ElevenLabs, aunque el formato sea correcto (44100 Hz, mono, PCM 16-bit), el resultado suena roto o distorsionado.&lt;/p&gt;\n\n&lt;p&gt;Ya prob√©:&lt;/p&gt;\n\n&lt;p&gt;- Normalizar el RMS a ~0.13&lt;/p&gt;\n\n&lt;p&gt;- Convertir a mono y 44100 Hz&lt;/p&gt;\n\n&lt;p&gt;- Aplicar filtros de audio (lowpass, highpass, dynaudnorm)&lt;/p&gt;\n\n&lt;p&gt;- Rehumanizar con reverberaci√≥n y reducci√≥n de ruido&lt;/p&gt;\n\n&lt;p&gt;**Mi duda es:**  &lt;/p&gt;\n\n&lt;p&gt;¬øXTTS tiene problemas al aprender desde voces sint√©ticas o muy procesadas? ¬øQu√© propiedades del audio hacen que el modelo no pueda generalizar bien?&lt;/p&gt;\n\n&lt;p&gt;¬øAlguien ha logrado entrenar XTTS con voces de juegos o clonadas por TTS comerciales?&lt;/p&gt;\n\n&lt;p&gt;¬°Gracias de antemano!&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;**Contexto t√©cnico:**&lt;/p&gt;\n\n&lt;p&gt;- Modelo: XTTS v2 desde `xtts-finetune-webui`&lt;/p&gt;\n\n&lt;p&gt;- Duraci√≥n dataset: 5‚Äì10 minutos&lt;/p&gt;\n\n&lt;p&gt;- Formato: `.wav` 44100 Hz, mono, PCM 16-bit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m07tkl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Blitzo_45",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m07tkl/xtts_v2_por_qu√©_algunas_voces_suenan_bien_y_otras/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m07tkl/xtts_v2_por_qu√©_algunas_voces_suenan_bien_y_otras/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752551868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.",
          "author_fullname": "t2_i305y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Non-reasoning models adopting reasoning behavior from previous messages",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06nhe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752548299,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed that if you begin a chat with a reasoning model like Qwen 3 and then in subsequent messages switch to a different non-reasoning model (such as Gemma 3 12b or Devstral 2507) the non-reasoning model will sometimes also generate reasoning tokens and respond with a final answer afterwards like it was trained to perform reasoning. This is also without any system prompt.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m06nhe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thedudely1",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06nhe/nonreasoning_models_adopting_reasoning_behavior/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752548299,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I come to you guys humbly seeking some help...\n\nI've been away from the local AI scene for about 6 months - it feels like an eternity - so much has changed!  I decided I wanted to get back into the game by building a new machine.  Because so much has changed, I'm having a hard time gauging what I will actually be able to do and what models I can run with my new specs.  \nAny ideas?  What would you guys recommend I try that would really take advantage of these specs?\nI understand that 32GB VRAM is not so special for LLMs, but I'm hoping that 256GB RAM might open up some interesting possibilities.\n\n|Category|Component(s)|\n|:-|:-|\n|**GPU**|ZOTAC GAMING GeForce RTX‚ÄØ5090 AMP‚ÄØExtreme‚ÄØINFINITY (32GB VRAM)|\n|**CPU**|AMD Ryzen 9 9950X|\n|**Memory**|256GB (4√ó64GB) G.SKILL DDR5 6000MT/s|\n|**Motherboard**|MSI X870E Tomahawk WiFi|\n|**Storage**|1√ó 2TB Samsung 9100 Pro Gen 5 NVMe (OS); 2√ó 1TB Samsung 990 Pro Gen 4 NVMes *(1 as cache);* 2√ó 14TB Seagate HDDs|\n|**PSU**|MSI 1300W Platinum|\n|**Cooler**|Arctic Freezer III 420 AIO|\n|**Case**|Geometric Future M5 *(glass panel version)*|\n|**Fans**|7√ó Lian Li SL Infinity + 1√ó TL LCD fan|\n\nI am of course interested in LLMs, but I'm also interested in trying out new image, video, music, speech, etc. models as well.  Can I run any of the new agentic models?  In terms of parameters, in general, what's my limit?\n\nThank you in advance!",
          "author_fullname": "t2_2v0asrl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built an AI PC - what should I try out first?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06lrz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752551531,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752548155,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come to you guys humbly seeking some help...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been away from the local AI scene for about 6 months - it feels like an eternity - so much has changed!  I decided I wanted to get back into the game by building a new machine.  Because so much has changed, I&amp;#39;m having a hard time gauging what I will actually be able to do and what models I can run with my new specs.&lt;br/&gt;\nAny ideas?  What would you guys recommend I try that would really take advantage of these specs?\nI understand that 32GB VRAM is not so special for LLMs, but I&amp;#39;m hoping that 256GB RAM might open up some interesting possibilities.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Category&lt;/th&gt;\n&lt;th align=\"left\"&gt;Component(s)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;ZOTAC GAMING GeForce RTX‚ÄØ5090 AMP‚ÄØExtreme‚ÄØINFINITY (32GB VRAM)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 9 9950X&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;256GB (4√ó64GB) G.SKILL DDR5 6000MT/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;MSI X870E Tomahawk WiFi&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1√ó 2TB Samsung 9100 Pro Gen 5 NVMe (OS); 2√ó 1TB Samsung 990 Pro Gen 4 NVMes &lt;em&gt;(1 as cache);&lt;/em&gt; 2√ó 14TB Seagate HDDs&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;MSI 1300W Platinum&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Cooler&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arctic Freezer III 420 AIO&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Geometric Future M5 &lt;em&gt;(glass panel version)&lt;/em&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Fans&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7√ó Lian Li SL Infinity + 1√ó TL LCD fan&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I am of course interested in LLMs, but I&amp;#39;m also interested in trying out new image, video, music, speech, etc. models as well.  Can I run any of the new agentic models?  In terms of parameters, in general, what&amp;#39;s my limit?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m06lrz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wesarnquist",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752548155,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to build a full crawler and scraper that runs completely locally with the help of an LLM to that it can work with any website and without writing code for each site.\n\n**Example of a use case:**  \nI want to scrape the list of watches from Amazon without using traditional scrapers that rely on CSS selectors.   \nExample: [https://www.amazon.com/s?k=watches](https://www.amazon.com/s?k=watches)  \nI will help the LLM or AI library find the relevant data so I tell it in a prompt/input the values of the first watch brand name, description and price. Name, description and price are my data points.  \nI tell it that the first watch is Apple, whatever its description is on Amazon and the price. I might also do this again for the second watch. Casio, its description and its price, for better accuracy. The more examples, the better the accuracy.  I attach the raw HTML (minus the CSS and JS to lessen the tokens) of the page or the extracted full text or a pdf of the webpage. \n\nThen the LLM or AI library will extract the rest of the watches. Their name, description and price.   \nMy crawler will get the second page, attach the file in another prompt and tell it to extract the same type of data. It should know by now to do this over and over. Hopefully accurately every time.\n\n  \nMy question is.. which open source library and/or LLM can be used to do what I have explained?\n\n  \nThese are libraries I found that look interesting but I don't know which ones satisfy my requirements.  \nI feel I need to train the LLM or library with real examples. I have tried some online examples of these libraries and prompt them for what I want and got bad results. I feel they need some training and guidance first.  \n  \nIf an LLM is needed, which one to be used with Ollama or LM Studio?  \nI want everything to run on a local Windows machine to save costs and not use a cloud based LLM.\n\n\n\n[https://huggingface.co/jinaai/ReaderLM-v2](https://huggingface.co/jinaai/ReaderLM-v2)\n\n[https://github.com/raznem/parsera](https://github.com/raznem/parsera)\n\n[https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)\n\n[https://github.com/ScrapeGraphAI/Scrapegraph-ai](https://github.com/ScrapeGraphAI/Scrapegraph-ai)\n\n\n\n\n\n\n\n\n\n  \n",
          "author_fullname": "t2_7a5yt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which local LLMs and/or libraries can I use to guide or train to identify where relevant data is located on a web page for web scraping purposes? Using natural language",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m06bru",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752547335,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to build a full crawler and scraper that runs completely locally with the help of an LLM to that it can work with any website and without writing code for each site.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example of a use case:&lt;/strong&gt;&lt;br/&gt;\nI want to scrape the list of watches from Amazon without using traditional scrapers that rely on CSS selectors.&lt;br/&gt;\nExample: &lt;a href=\"https://www.amazon.com/s?k=watches\"&gt;https://www.amazon.com/s?k=watches&lt;/a&gt;&lt;br/&gt;\nI will help the LLM or AI library find the relevant data so I tell it in a prompt/input the values of the first watch brand name, description and price. Name, description and price are my data points.&lt;br/&gt;\nI tell it that the first watch is Apple, whatever its description is on Amazon and the price. I might also do this again for the second watch. Casio, its description and its price, for better accuracy. The more examples, the better the accuracy.  I attach the raw HTML (minus the CSS and JS to lessen the tokens) of the page or the extracted full text or a pdf of the webpage. &lt;/p&gt;\n\n&lt;p&gt;Then the LLM or AI library will extract the rest of the watches. Their name, description and price.&lt;br/&gt;\nMy crawler will get the second page, attach the file in another prompt and tell it to extract the same type of data. It should know by now to do this over and over. Hopefully accurately every time.&lt;/p&gt;\n\n&lt;p&gt;My question is.. which open source library and/or LLM can be used to do what I have explained?&lt;/p&gt;\n\n&lt;p&gt;These are libraries I found that look interesting but I don&amp;#39;t know which ones satisfy my requirements.&lt;br/&gt;\nI feel I need to train the LLM or library with real examples. I have tried some online examples of these libraries and prompt them for what I want and got bad results. I feel they need some training and guidance first.  &lt;/p&gt;\n\n&lt;p&gt;If an LLM is needed, which one to be used with Ollama or LM Studio?&lt;br/&gt;\nI want everything to run on a local Windows machine to save costs and not use a cloud based LLM.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/jinaai/ReaderLM-v2\"&gt;https://huggingface.co/jinaai/ReaderLM-v2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/raznem/parsera\"&gt;https://github.com/raznem/parsera&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/unclecode/crawl4ai\"&gt;https://github.com/unclecode/crawl4ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ScrapeGraphAI/Scrapegraph-ai\"&gt;https://github.com/ScrapeGraphAI/Scrapegraph-ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m06bru",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "THenrich",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m06bru/which_local_llms_andor_libraries_can_i_use_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m06bru/which_local_llms_andor_libraries_can_i_use_to/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752547335,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. \n\nEdit : \nSam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn't Open-source any model (e.g Grok-2 or Grok-3) and now.\n\nMe : xAI promise Open-source grok-2 or Grok-3?\n\nSam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!",
          "author_fullname": "t2_7ktr17a6i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok no more model Open-source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m04ic2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752574571,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752542208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think that happened. Because Elon Musk forgot or canceled that Grok-2 would be open sourced after Grok-3 was stable. And now Grok-4 but Elon Musk did not open source Grok-2 or even Grok-3. I think Elon Musk is following the OpenAI or ANTHROP\\C. Until now Elon Musk still makes announcements that he will open source Grok-2 and Grok-3 and it is unknown whether Elon Musk will cut off the API for these two models. &lt;/p&gt;\n\n&lt;p&gt;Edit : \nSam Atlam : Elon Musk Will Promise That I Will Open Source Grok-2 Once Grok-3 Is Stable. But not Elon Musk doesn&amp;#39;t Open-source any model (e.g Grok-2 or Grok-3) and now.&lt;/p&gt;\n\n&lt;p&gt;Me : xAI promise Open-source grok-2 or Grok-3?&lt;/p&gt;\n\n&lt;p&gt;Sam Atlam: xAI is lie. OpenAI release Open-source thinking model soon. Say tuned!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m04ic2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Brilliant_Stock_5137",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m04ic2/grok_no_more_model_opensource/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752542208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fc9cbovwe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EXAONE 4.0 32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m04a20",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 249,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 249,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8a97b0e14968bf021760fe24b232086843a6916d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752541575,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?auto=webp&amp;s=21346c43501458b33bb875a62eb15906b79b28b2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01c7ab98318dec8e4dfb9ad444e48cb42d1afee0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3ab4d919ac27b9f67137eb710ebbcd8ffae7191",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e17ff014e394f7eaa73049e5608695028dc583e",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18202842c69b787ccdb604277c8c0ce21247e4d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3d99f4430cca13dd301692b895103c848c110e72",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92fd7f637d351ce6963b08dd9b62b92904ecbc6d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8nr2BOfjyJy107kRprOzRDlPGzQeiMZ1zJzNkF1pk6I"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m04a20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "minpeter2",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/",
          "stickied": false,
          "url": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
          "subreddit_subscribers": 499292,
          "created_utc": 1752541575,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Last week, a small group of top members of the lab, including Alexandr Wang, 28, Meta‚Äôs new chief A.I. officer, discussed abandoning the company‚Äôs most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.\n\nMeta had finished feeding in data to improve its Behemoth model, a process known as ‚Äútraining,‚Äù but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model ‚Äî which is known as a ‚Äúfrontier‚Äù model ‚Äî stopped running new tests on it, one of the people said.\n\n\n",
          "author_fullname": "t2_u398xzta",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta‚Äôs New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 127,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m041m4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/7b2503awF-4b2icSl8JuZh6vDtrrHCXba5mxqJvk1zA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752540939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last week, a small group of top members of the lab, including Alexandr Wang, 28, Meta‚Äôs new chief A.I. officer, discussed abandoning the company‚Äôs most powerful open source A.I. model, called Behemoth, in favor of developing a closed model, two people with knowledge of the matter said.&lt;/p&gt;\n\n&lt;p&gt;Meta had finished feeding in data to improve its Behemoth model, a process known as ‚Äútraining,‚Äù but has delayed its release because of poor internal performance, said the people with knowledge of the matter, who were not authorized to discuss private conversations. After the company announced the formation of the superintelligence lab last month, teams working on the Behemoth model ‚Äî which is known as a ‚Äúfrontier‚Äù model ‚Äî stopped running new tests on it, one of the people said.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3f68h6pzrxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?auto=webp&amp;s=0896b8b618e97942c4ee1042d76206fda02632e1",
                  "width": 1080,
                  "height": 984
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d2ac4cc26b27343c96b0a03f825276999d5b174",
                    "width": 108,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400750301dac83d5f73809374e9b7b466d85b02e",
                    "width": 216,
                    "height": 196
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=06576965338eaac8ae34bb2407956757d30b744d",
                    "width": 320,
                    "height": 291
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b646755e2c619c761965d3179957a1ab2c65c147",
                    "width": 640,
                    "height": 583
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84a06d1a5914a6525f604045a7fc9a8b7f0954c9",
                    "width": 960,
                    "height": 874
                  },
                  {
                    "url": "https://preview.redd.it/3f68h6pzrxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06d3e9060e92dd36bfb8b5c6dde39bd596967ff8",
                    "width": 1080,
                    "height": 984
                  }
                ],
                "variants": {},
                "id": "yVwjvgqtLycnuN38b4xHyA_D8ioyPBQ4lGHqp1UdmK4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m041m4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sunshinecheung",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m041m4/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://i.redd.it/3f68h6pzrxcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kcu2kx4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's up with the weird OR provider prices, they make no sense at all.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m040ag",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": "#bbbdbf",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/BTgNwC2MAuzD7_uRpxqg2qYITg3XP4whbMqujeHwerQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Ollama"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752540838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/kk8hpolkrxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?auto=webp&amp;s=328d40df14f98bdc2ea2574f32636490f18e4099",
                  "width": 2148,
                  "height": 1270
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc2ae195e1c6e48ec0a3481a2a73fbe283e00209",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d2fa9154d95f2b0e92e8fcb6360558fa9dad160",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9c3c0ca9545def88b414395a661a6058c5bc1e3",
                    "width": 320,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b779193285fa57a7bb13cbc624328e0dd953bd39",
                    "width": 640,
                    "height": 378
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e190981a3394704c400b906f150297921dd157cd",
                    "width": 960,
                    "height": 567
                  },
                  {
                    "url": "https://preview.redd.it/kk8hpolkrxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e3a211d8fa02c54f25b2f09f8ed9f6ee4eeb3ec",
                    "width": 1080,
                    "height": 638
                  }
                ],
                "variants": {},
                "id": "fc_weVym9QErbXNEqlDfqEcn7ZX18CWTEE0AAOZablg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Ollama",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m040ag",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Specter_Origin",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m040ag/whats_up_with_the_weird_or_provider_prices_they/",
          "stickied": false,
          "url": "https://i.redd.it/kk8hpolkrxcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm currently using llama\\_cpp with python bindings, but have heard that vLLM can be much faster, especially when patching.\n\nBut I'm not sure how to migrate my workflow that uses a Qwen3 gguf over to vLLM",
          "author_fullname": "t2_bndbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does vLLM not support Qwen3 ggufs? What sort of models/quants are people running in vLLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m03sio",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752540242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently using llama_cpp with python bindings, but have heard that vLLM can be much faster, especially when patching.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure how to migrate my workflow that uses a Qwen3 gguf over to vLLM&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m03sio",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AuspiciousApple",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://youtu.be/vW30o4U9BFE",
          "author_fullname": "t2_7kg5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A very nice overview on how llama.cpp quantization works",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m03sh9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752540238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/vW30o4U9BFE\"&gt;https://youtu.be/vW30o4U9BFE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?auto=webp&amp;s=b3e9aec855a9a197fe9619de2626df9b16de04d8",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=617b8e7b6a8328658f045f639d60a0618107415a",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c67f23813dd77438f9dd053b5b49625c7db6c5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee9cbfbcf35cda08614a8ac4291b096d6010b0b5",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "zBHQlpO9zlBFyYQgYtAREE1WWUYjkWGTsfUFaGNxAj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m03sh9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kooshi_Govno",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m03sh9/a_very_nice_overview_on_how_llamacpp_quantization/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752540238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thank you, Unsloth! You guys are legends!!! (Now I just need 256GB of DDR5)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m021nx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 152,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 152,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-v9_FrsEiCPVytIaPFWwPVUJAlmMBmLn8IIAFMlNQyw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752535545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl35mhaybxcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?auto=webp&amp;s=8e83e9de13e964005d0f5b777a1fb221aa69c590",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36b347c1b0477d5c0c50ee12646d88d4534cf13b",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=476209c3d4f31e1dd5a6915c23da1b9c681dd240",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c156ec7ae4a7d874263af3bf27bdf6511a1f1353",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=427166a43aad977ff4e628d5d89073bd9fd90280",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef0326b347cdcbbb0a7908eec96440b3414e96e9",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/nl35mhaybxcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6e79c6c9a2199b96314ff79eab544c770d2dfda",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "YueW4SD4xsNrXkAftR3D5z4lByoNaBI-7VPxyeUYmW8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m021nx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m021nx/thank_you_unsloth_you_guys_are_legends_now_i_just/",
          "stickied": false,
          "url": "https://i.redd.it/nl35mhaybxcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752535545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I installed Meta nllb language translation on Windows, but it only uses the cpu which is slow, did anyone manage to figure out how to use cuda acceleration on Windows?",
          "author_fullname": "t2_ccf3gz8r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Did anyone manage to use nllb with cuda acceleration on Windows?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m01d8x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752533848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I installed Meta nllb language translation on Windows, but it only uses the cpu which is slow, did anyone manage to figure out how to use cuda acceleration on Windows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m01d8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mashic",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m01d8x/did_anyone_manage_to_use_nllb_with_cuda/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m01d8x/did_anyone_manage_to_use_nllb_with_cuda/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752533848,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_73rg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Moonshot AI‚Äôs open source Kimi K2 outperforms GPT-4 in key benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m013ou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752533189,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "moonshotai.github.io",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://moonshotai.github.io/Kimi-K2/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m013ou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yogthos",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m013ou/moonshot_ais_open_source_kimi_k2_outperforms_gpt4/",
          "stickied": false,
          "url": "https://moonshotai.github.io/Kimi-K2/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752533189,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_99mff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta on track to be first lab with a 1GW supercluster",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 63,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m0115d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 152,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 152,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/0msAxqRJrwqMDcMC7jZ5bCUaL-sRDisyC8M9WV3X4eQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752533016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/584vdadc4xcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/584vdadc4xcf1.png?auto=webp&amp;s=7e7393e2ff25a0fc5887422d47318ea835f0c1b5",
                  "width": 1010,
                  "height": 459
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aec9db4a133d2a998b21becc7885a200bea8a2bc",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ba757e3b8229c7c3b2eb3aa62befc54fad32c5",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8220fef2199d3b0b707059754b31f54ea20f5be",
                    "width": 320,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e603dc0a062f5e964b5a1e007efdb4a66dc293f",
                    "width": 640,
                    "height": 290
                  },
                  {
                    "url": "https://preview.redd.it/584vdadc4xcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1b43c580e6c7de0aa1f0815714aca472b23c3a5",
                    "width": 960,
                    "height": 436
                  }
                ],
                "variants": {},
                "id": "-MjK6OyxWPNhUMp76GWSxVzUaVyreb5xWXtn3a_Nrzc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m0115d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jd_3d",
          "discussion_type": null,
          "num_comments": 72,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m0115d/meta_on_track_to_be_first_lab_with_a_1gw/",
          "stickied": false,
          "url": "https://i.redd.it/584vdadc4xcf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752533016,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After reading all the amazing post of yours, I've bought in. About to offer my management a localized coding agent, to prevent code and API keys leaks. From 20 to 50 people coding at any moment.\n\nLocally I'd need a used 3080+ card. But what type of the hardware I'm looking for to provide for 20+ folks?",
          "author_fullname": "t2_31gku",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help needed: 20+ devs on the local model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m00yn1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752532847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading all the amazing post of yours, I&amp;#39;ve bought in. About to offer my management a localized coding agent, to prevent code and API keys leaks. From 20 to 50 people coding at any moment.&lt;/p&gt;\n\n&lt;p&gt;Locally I&amp;#39;d need a used 3080+ card. But what type of the hardware I&amp;#39;m looking for to provide for 20+ folks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m00yn1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "3dom",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752532847,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Long story short I won 2 sticks of 32 GB DDR5 ram but I only have a gaming laptop, and I have always wanted to build a PC.\ncan I skip buying a GPU for now and put my unbelievable 64GBs to use with a CPU and run LLMs and STT models from it, in terms of loading the models I know that I will be able to load bigger models than any GPU I would ever buy anytime soon, but my question is will the CPU provide reasonable inference speed? do you have any recommendations for a CPU that maybe has a good NPU or do I just buy a powerful and new CPU blindly? I am not very experienced in running AI workloads on CPU and I would appreciate any correction or input about your past experiences or any tests you might have done recently.",
          "author_fullname": "t2_1m0sp5gn6a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Enough resources for light AI workloads?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzzka4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752529491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I won 2 sticks of 32 GB DDR5 ram but I only have a gaming laptop, and I have always wanted to build a PC.\ncan I skip buying a GPU for now and put my unbelievable 64GBs to use with a CPU and run LLMs and STT models from it, in terms of loading the models I know that I will be able to load bigger models than any GPU I would ever buy anytime soon, but my question is will the CPU provide reasonable inference speed? do you have any recommendations for a CPU that maybe has a good NPU or do I just buy a powerful and new CPU blindly? I am not very experienced in running AI workloads on CPU and I would appreciate any correction or input about your past experiences or any tests you might have done recently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzzka4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EyasDBoi_i",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752529491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MMLU-ProX is a multilingual benchmark that extends the challenging MMLU-Pro benchmark to 29 typologically diverse languages, designed to evaluate the cross-lingual reasoning capabilities of large language models (LLMs). Built through a rigorous four-stage translation pipeline using state-of-the-art LLMs (primarily Claude Sonnet 3.7) combined with expert verification, the benchmark contains 11,829 identical questions per language (with a lite version of 658 questions), covering 57 subjects across multiple disciplines with complex reasoning-focused multiple-choice questions featuring 10 answer options and chain-of-thought prompting support.\n\nThe benchmark reveals significant performance disparities across languages when evaluating 36 state-of-the-art LLMs, with models achieving strong performance on high-resource Western European languages (often 75%+ accuracy) but substantially lower scores on low-resource African languages like Wolof (as low as 0.6% to 58.6%), highlighting persistent challenges in multilingual AI development and the need for more inclusive language model capabilities across global contexts.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã\n\n- Website: https://mmluprox.github.io\n- Paper: https://arxiv.org/abs/2503.10497\n- Code: https://github.com/weihao1115/MMLU-ProX (still empty)\n- Full dataset: https://huggingface.co/datasets/li-lab/MMLU-ProX\n- Lite dataset: https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wgsfm22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 141,
                  "x": 108,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=691617a7ea37d1a5091179464ab143f85468a96e"
                },
                {
                  "y": 282,
                  "x": 216,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13438d7635ba748f597b00003f2864927e862a0d"
                },
                {
                  "y": 419,
                  "x": 320,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90ce8e777b58617edab45238b32011b2a0fc2fd4"
                },
                {
                  "y": 838,
                  "x": 640,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c6764ec9b355c53fe2ad3069ae65b2d7c965c24"
                },
                {
                  "y": 1257,
                  "x": 960,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c1ab9f3f6ff3057a8109d9ef7b40dba9f332b1b"
                },
                {
                  "y": 1414,
                  "x": 1080,
                  "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a7d29c5d99c65d37649c143640f1837355e5883"
                }
              ],
              "s": {
                "y": 2416,
                "x": 1845,
                "u": "https://preview.redd.it/wgsfm22gswcf1.jpg?width=1845&amp;format=pjpg&amp;auto=webp&amp;s=af2d88a2d17ea254a0dc776e725953027aba90ff"
              },
              "id": "wgsfm22gswcf1"
            },
            "cepva22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 63,
                  "x": 108,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=721ac7c709c4f0f3960a92c4ac1130823c849949"
                },
                {
                  "y": 127,
                  "x": 216,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=755aa779fcf53987afe487b20786abae4bc317d9"
                },
                {
                  "y": 189,
                  "x": 320,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6000d6aff7a8d8ad61eb3a2b8e7911338a22ac9"
                },
                {
                  "y": 378,
                  "x": 640,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36ca8dc0910cc79159f24e3b974fd7d8611a534e"
                },
                {
                  "y": 568,
                  "x": 960,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3be6352cc43e102dca25a7edef1e96677ce41f3b"
                },
                {
                  "y": 639,
                  "x": 1080,
                  "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56c643abe61beffd12a31324ef3eb83028bfb195"
                }
              ],
              "s": {
                "y": 1498,
                "x": 2531,
                "u": "https://preview.redd.it/cepva22gswcf1.jpg?width=2531&amp;format=pjpg&amp;auto=webp&amp;s=2b6a4c92395db017d8d7ecc8c4ff995541355888"
              },
              "id": "cepva22gswcf1"
            },
            "jy1gl22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 123,
                  "x": 108,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d501900b8ce2284997bbb24f0cb60d3663d6132"
                },
                {
                  "y": 247,
                  "x": 216,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76b73821477a93932f341bda7a8514ead81853ee"
                },
                {
                  "y": 367,
                  "x": 320,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd43ee53d37519462ce42c539e56fc3699ffeb63"
                },
                {
                  "y": 734,
                  "x": 640,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d93023b31b27e8ca98a804b8c4e3a459b3c6d4dd"
                },
                {
                  "y": 1101,
                  "x": 960,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42183f8dcbc95702fcd7ba98a5d06493b984431a"
                },
                {
                  "y": 1239,
                  "x": 1080,
                  "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e56314bcacb6ec3529a7ef5f17364fd2d2a328dd"
                }
              ],
              "s": {
                "y": 2226,
                "x": 1940,
                "u": "https://preview.redd.it/jy1gl22gswcf1.jpg?width=1940&amp;format=pjpg&amp;auto=webp&amp;s=ab8a55e2c76f7c0820b0122d79426fa515025fdf"
              },
              "id": "jy1gl22gswcf1"
            },
            "o9ysd22gswcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 131,
                  "x": 108,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=92f7a072ea75bf25c6613c54363b3a8816f1cf02"
                },
                {
                  "y": 263,
                  "x": 216,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92ee3fb3d1922992a2f6e4d93b59033511ef3db1"
                },
                {
                  "y": 390,
                  "x": 320,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=300e2152a2bd5d1540651f68670f0caed16c004c"
                },
                {
                  "y": 780,
                  "x": 640,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=971ae8d2b089a6c4d2729a733f501b03dc04a648"
                },
                {
                  "y": 1171,
                  "x": 960,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a146661a61e9a38380b2be4a25658ceda38e580"
                },
                {
                  "y": 1317,
                  "x": 1080,
                  "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb15168f0b557c684059fb4d5262e3bb5de2fc56"
                }
              ],
              "s": {
                "y": 2367,
                "x": 1940,
                "u": "https://preview.redd.it/o9ysd22gswcf1.jpg?width=1940&amp;format=pjpg&amp;auto=webp&amp;s=caa08ce1445257b1a6bd05e754731bb29f7bb6f7"
              },
              "id": "o9ysd22gswcf1"
            }
          },
          "name": "t3_1lzzcje",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 29,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "o9ysd22gswcf1",
                "id": 706015027
              },
              {
                "media_id": "wgsfm22gswcf1",
                "id": 706015028
              },
              {
                "media_id": "cepva22gswcf1",
                "id": 706015029
              },
              {
                "media_id": "jy1gl22gswcf1",
                "id": 706015030
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/irH_IxYHAQEjBGgXU_AOln_EsxdXVQKIs5z4JjXsVfc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752528984,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MMLU-ProX is a multilingual benchmark that extends the challenging MMLU-Pro benchmark to 29 typologically diverse languages, designed to evaluate the cross-lingual reasoning capabilities of large language models (LLMs). Built through a rigorous four-stage translation pipeline using state-of-the-art LLMs (primarily Claude Sonnet 3.7) combined with expert verification, the benchmark contains 11,829 identical questions per language (with a lite version of 658 questions), covering 57 subjects across multiple disciplines with complex reasoning-focused multiple-choice questions featuring 10 answer options and chain-of-thought prompting support.&lt;/p&gt;\n\n&lt;p&gt;The benchmark reveals significant performance disparities across languages when evaluating 36 state-of-the-art LLMs, with models achieving strong performance on high-resource Western European languages (often 75%+ accuracy) but substantially lower scores on low-resource African languages like Wolof (as low as 0.6% to 58.6%), highlighting persistent challenges in multilingual AI development and the need for more inclusive language model capabilities across global contexts.‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Website: &lt;a href=\"https://mmluprox.github.io\"&gt;https://mmluprox.github.io&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2503.10497\"&gt;https://arxiv.org/abs/2503.10497&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Code: &lt;a href=\"https://github.com/weihao1115/MMLU-ProX\"&gt;https://github.com/weihao1115/MMLU-ProX&lt;/a&gt; (still empty)&lt;/li&gt;\n&lt;li&gt;Full dataset: &lt;a href=\"https://huggingface.co/datasets/li-lab/MMLU-ProX\"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Lite dataset: &lt;a href=\"https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite\"&gt;https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lzzcje",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzzcje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzzcje/mmluprox_a_multilingual_benchmark_for_advanced/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lzzcje",
          "subreddit_subscribers": 499292,
          "created_utc": 1752528984,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.\n\nI am thinking about super simple solution:  \n\\- use firecrawl to get the markdown content;  \n\\- get vector / save into databse;  \n\\- when text input comes I fill it with additional context for better vector search performance;  \n\\- load N results  \n\\- filter with gpt\n\nbut the last time I was doing it, it wasn't working really great, so i was wondering maybe there is better solution for this?\n\n",
          "author_fullname": "t2_zx18p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the best practices for vector search + filtering with LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzz13f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752528242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.&lt;/p&gt;\n\n&lt;p&gt;I am thinking about super simple solution:&lt;br/&gt;\n- use firecrawl to get the markdown content;&lt;br/&gt;\n- get vector / save into databse;&lt;br/&gt;\n- when text input comes I fill it with additional context for better vector search performance;&lt;br/&gt;\n- load N results&lt;br/&gt;\n- filter with gpt&lt;/p&gt;\n\n&lt;p&gt;but the last time I was doing it, it wasn&amp;#39;t working really great, so i was wondering maybe there is better solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzz13f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "andrewshvv",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752528242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 tops creative writing benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzywie",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 243,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 243,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VH2yrgq-wMtmRSPtQ8VK4WklgU_7GjLW0L_EEyrgxYc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752527951,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q48f55vcpwcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?auto=webp&amp;s=72733329d330a907558da68160618b15b6172b27",
                  "width": 1500,
                  "height": 1000
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b78c6da6f3a69e12a60113ac6638feb8001f4a9",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9698b7792fd84563a11e9d5fcb8304ca9051d91c",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad25ba68ea4d8c123c8ab2e64de62e7e8343e00",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83d8a4d11cd481b0f3d6a15556baa79acf5df855",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c1e5b3512c9a41667a474d9d1a6d3f5fae4ccfa",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/q48f55vcpwcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3f46623f3ecd81bc3ac63095add6cd4132e7792",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "XgXXR5EePWFyWI8XqcL69hThsTAzGmSFTef0gmKfLjQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzywie",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 61,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzywie/kimi_k2_tops_creative_writing_benchmark/",
          "stickied": false,
          "url": "https://i.redd.it/q48f55vcpwcf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752527951,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a simple experimental language model architecture based on Andrej Karpathy's nanoGPT project.\n\nIt's an experiment to try different improvements of transformers architecture. Some improvement has been brought about by the following techniques:\n- Modernized architecture: Rotary embeddings, QK-Norm, and ReLU¬≤\n- Untie head from embedding\n- SwiGLU in feed forward network.\n- Parallel layers proposed by Google's PaLM\n- Using a novel attention mechanism which I call `Attention On Detail`.\n\nAs well as many minor optimizations.\n\n## How does `Attention On Detail` works?\nIt works by combining 3 ideas.\n- Multi-Headed Causal Self-Attention (MHA)\n- Attention Free Transformer (AFT)\n- A simple fourier series based equation `a*sin(x) + b*sin(x) + c*sin(x)*cos(x)` where `x` is normalized between `[-pi, pi]`\n\nThe idea is simple.\n- Replace `Linear layers` with an `AFT` for each `q`, `k` &amp; `v` in the `MHA`.\n- In `AFT`, generate 3 values, `a`, `b` and `c` from 3 different fourier series equations.\n- Compute output the `a`, `b` &amp; `c` values in each `AFT`.\n- Now use those `q`, `k` &amp; `v` values to calculate the attention score in the `MHA`\n",
          "author_fullname": "t2_115sfrlqgx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GitHub - SrijanSriv211/Palm: Palm is a tree, not a language model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzyk1k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=327ae1770b56b5ecfbf46685072a5ee3f577b985",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752527141,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a simple experimental language model architecture based on Andrej Karpathy&amp;#39;s nanoGPT project.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an experiment to try different improvements of transformers architecture. Some improvement has been brought about by the following techniques:\n- Modernized architecture: Rotary embeddings, QK-Norm, and ReLU¬≤\n- Untie head from embedding\n- SwiGLU in feed forward network.\n- Parallel layers proposed by Google&amp;#39;s PaLM\n- Using a novel attention mechanism which I call &lt;code&gt;Attention On Detail&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;As well as many minor optimizations.&lt;/p&gt;\n\n&lt;h2&gt;How does &lt;code&gt;Attention On Detail&lt;/code&gt; works?&lt;/h2&gt;\n\n&lt;p&gt;It works by combining 3 ideas.\n- Multi-Headed Causal Self-Attention (MHA)\n- Attention Free Transformer (AFT)\n- A simple fourier series based equation &lt;code&gt;a*sin(x) + b*sin(x) + c*sin(x)*cos(x)&lt;/code&gt; where &lt;code&gt;x&lt;/code&gt; is normalized between &lt;code&gt;[-pi, pi]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The idea is simple.\n- Replace &lt;code&gt;Linear layers&lt;/code&gt; with an &lt;code&gt;AFT&lt;/code&gt; for each &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &amp;amp; &lt;code&gt;v&lt;/code&gt; in the &lt;code&gt;MHA&lt;/code&gt;.\n- In &lt;code&gt;AFT&lt;/code&gt;, generate 3 values, &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; from 3 different fourier series equations.\n- Compute output the &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; &amp;amp; &lt;code&gt;c&lt;/code&gt; values in each &lt;code&gt;AFT&lt;/code&gt;.\n- Now use those &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt; &amp;amp; &lt;code&gt;v&lt;/code&gt; values to calculate the attention score in the &lt;code&gt;MHA&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/SrijanSriv211/Palm",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?auto=webp&amp;s=2a210ec4950e580011f1e5d6a75d2f1a05d02bb0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef72931372700484d624279573ebd30522837f10",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a9002ac55ec8e03201c26828120fcf3beeced63",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=57ca03b3a2000e4b4cd65e76b9b11a1b3ec5fa4d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c77dfededd901b5351e7adc5dd2be75ee167f27c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=002d927a21bcb9cdd80b3935d3dadbaa4553bc19",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7aea228d082892928dab9cbefe79a4adbf0e568c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ttfxke-7hOI2ccNHW6Ntk_j3qIw_X09SaAhgQ-PpovQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzyk1k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SrijSriv211",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzyk1k/github_srijansriv211palm_palm_is_a_tree_not_a/",
          "stickied": false,
          "url": "https://github.com/SrijanSriv211/Palm",
          "subreddit_subscribers": 499292,
          "created_utc": 1752527141,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,  \nlooking for a place to start to read and check a bit, but wanted to ask to just select good starting point.\n\nCurrently, I have rtx 3070 8gb. What model can i run locally to get started with code assistant (means, asking about 'algoritm' snippets or checking code.  \nAlso, what I need to learn to setup Ai if I would like to give 'assistant' API docs (local or web hosted) and ask him about solutions using these methods?\n\nOn which budget starting point (3090?) is worth getting into code AI helper? Also, which model is worth checking in web(paid way) to get grasph what code ai can 'develop'. (not speaking about agents, just assistants). Is there any general good with code capabilities + vision or they always separate?",
          "author_fullname": "t2_lsxw9qy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Code assistant way to start",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzy059",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752525851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nlooking for a place to start to read and check a bit, but wanted to ask to just select good starting point.&lt;/p&gt;\n\n&lt;p&gt;Currently, I have rtx 3070 8gb. What model can i run locally to get started with code assistant (means, asking about &amp;#39;algoritm&amp;#39; snippets or checking code.&lt;br/&gt;\nAlso, what I need to learn to setup Ai if I would like to give &amp;#39;assistant&amp;#39; API docs (local or web hosted) and ask him about solutions using these methods?&lt;/p&gt;\n\n&lt;p&gt;On which budget starting point (3090?) is worth getting into code AI helper? Also, which model is worth checking in web(paid way) to get grasph what code ai can &amp;#39;develop&amp;#39;. (not speaking about agents, just assistants). Is there any general good with code capabilities + vision or they always separate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzy059",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "machond",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzy059/code_assistant_way_to_start/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzy059/code_assistant_way_to_start/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752525851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, here is the problem. I'm actually facing it as I'm writing this post.\n\nI use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It's like asking multiple experts with various opinions before making a decision. \n\nI have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.\n\nThe problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?\n\nAny ideas anyone? Thanks.",
          "author_fullname": "t2_11ccns",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVMe for local LLM is too slow. Any ideas?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzx039",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752523597,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, here is the problem. I&amp;#39;m actually facing it as I&amp;#39;m writing this post.&lt;/p&gt;\n\n&lt;p&gt;I use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It&amp;#39;s like asking multiple experts with various opinions before making a decision. &lt;/p&gt;\n\n&lt;p&gt;I have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.&lt;/p&gt;\n\n&lt;p&gt;The problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?&lt;/p&gt;\n\n&lt;p&gt;Any ideas anyone? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzx039",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChopSticksPlease",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752523597,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm working on a small AI app called **Preceptor** ‚Äî think of it like a privacy-first accountability partner that helps you stay focused **without spying on your screen**\n\nHere‚Äôs the idea:\n\n* It runs **entirely offline**, using local LLMs via [Ollama](https://ollama.com/)\n* Tracks **which app or browser tab** you‚Äôre on (via local system APIs + a lightweight browser extension)\n* Compares that with your focus goals (e.g., ‚Äúwrite more, avoid Reddit‚Äù)\n* And gives you **gentle nudges** when you drift\n\nEven with small-ish models (e.g. LLaMA 3 8B or Mistral via Ollama), I‚Äôm hitting response time issues. It might only be 1‚Äì3 seconds to generate a short message, but in a flow-focused app, that pause breaks the vibe. It's not just about speed but it's also about *feeling instant*. With mistral 7b , which produces a good nudge message but takes like 30 seconds for the api to call\n\nHow should i go with this?\n\nIf you want to join the waitlist for the app , comment and i will reply with the link. I want to make this less of a promotion post as i am seeking serious suggestions",
          "author_fullname": "t2_18z668t0lo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a Focus App with Local LLMs ‚Äî But Latency Is a Real Challenge , seeking suggestions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzwps3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.42,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752522956,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm working on a small AI app called &lt;strong&gt;Preceptor&lt;/strong&gt; ‚Äî think of it like a privacy-first accountability partner that helps you stay focused &lt;strong&gt;without spying on your screen&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs the idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It runs &lt;strong&gt;entirely offline&lt;/strong&gt;, using local LLMs via &lt;a href=\"https://ollama.com/\"&gt;Ollama&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Tracks &lt;strong&gt;which app or browser tab&lt;/strong&gt; you‚Äôre on (via local system APIs + a lightweight browser extension)&lt;/li&gt;\n&lt;li&gt;Compares that with your focus goals (e.g., ‚Äúwrite more, avoid Reddit‚Äù)&lt;/li&gt;\n&lt;li&gt;And gives you &lt;strong&gt;gentle nudges&lt;/strong&gt; when you drift&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Even with small-ish models (e.g. LLaMA 3 8B or Mistral via Ollama), I‚Äôm hitting response time issues. It might only be 1‚Äì3 seconds to generate a short message, but in a flow-focused app, that pause breaks the vibe. It&amp;#39;s not just about speed but it&amp;#39;s also about &lt;em&gt;feeling instant&lt;/em&gt;. With mistral 7b , which produces a good nudge message but takes like 30 seconds for the api to call&lt;/p&gt;\n\n&lt;p&gt;How should i go with this?&lt;/p&gt;\n\n&lt;p&gt;If you want to join the waitlist for the app , comment and i will reply with the link. I want to make this less of a promotion post as i am seeking serious suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzwps3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frosty-Cap-4282",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752522956,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For those of you who aren't familiar with SurfSense, it aims to be the¬†**open-source alternative to NotebookLM, Perplexity, or Glean.**\n\nIn short, it's a¬†**Highly Customizable AI Research Agent**¬†that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.\n\nI'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere‚Äôs a quick look at what SurfSense offers right now:\n\nüìä¬†**Feature**s\n\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)\n* Hierarchical Indices (2-tiered RAG setup)\n* Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)\n* Offers a RAG-as-a-Service API Backend\n* 50+ File extensions supported\n\nüéôÔ∏è¬†**Podcast**s\n\n* Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)\n* Convert chat conversations into engaging audio\n* Multiple TTS providers supported\n\n‚ÑπÔ∏è¬†**External Sources Integration**\n\n* Search engines (Tavily, LinkUp)\n* Slack\n* Linear\n* Notion\n* YouTube videos\n* GitHub\n* Discord\n* ...and more on the way\n\nüîñ¬†**Cross-Browser Extensio**n\n\nThe SurfSense extension lets you save any dynamic webpage you want, including authenticated content.\n\n**Interested in contributing?**\n\nSurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.\n\nGitHub:¬†[https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
          "author_fullname": "t2_63zmedmg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Alternative to NotebookLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzw6yu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 99,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752521799,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you who aren&amp;#39;t familiar with SurfSense, it aims to be the¬†&lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In short, it&amp;#39;s a¬†&lt;strong&gt;Highly Customizable AI Research Agent&lt;/strong&gt;¬†that connects to your personal external sources and search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, Discord, and more coming soon.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for contributors to help shape the future of SurfSense! If you&amp;#39;re interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt;\n\n&lt;p&gt;üìä¬†&lt;strong&gt;Feature&lt;/strong&gt;s&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports 100+ LLMs&lt;/li&gt;\n&lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt;\n&lt;li&gt;6000+ Embedding Models&lt;/li&gt;\n&lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt;\n&lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt;\n&lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt;\n&lt;li&gt;Offers a RAG-as-a-Service API Backend&lt;/li&gt;\n&lt;li&gt;50+ File extensions supported&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;üéôÔ∏è¬†&lt;strong&gt;Podcast&lt;/strong&gt;s&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt;\n&lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt;\n&lt;li&gt;Multiple TTS providers supported&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;‚ÑπÔ∏è¬†&lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt;\n&lt;li&gt;Slack&lt;/li&gt;\n&lt;li&gt;Linear&lt;/li&gt;\n&lt;li&gt;Notion&lt;/li&gt;\n&lt;li&gt;YouTube videos&lt;/li&gt;\n&lt;li&gt;GitHub&lt;/li&gt;\n&lt;li&gt;Discord&lt;/li&gt;\n&lt;li&gt;...and more on the way&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;üîñ¬†&lt;strong&gt;Cross-Browser Extensio&lt;/strong&gt;n&lt;/p&gt;\n\n&lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you&amp;#39;re welcome to join in.&lt;/p&gt;\n\n&lt;p&gt;GitHub:¬†&lt;a href=\"https://github.com/MODSetter/SurfSense\"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?auto=webp&amp;s=344bed7e266c934d23a7957319ca13c4100c57cd",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4cc9007548328d59c5d49b07997ca37e0c33349",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e50ed04f44d9e471185d98e483ce202837b37726",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fdc40a870cb735030c8be40d94ff73f8fae0e5d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b3d2dfc8cba3d61bcd473981e8d0d2e8cf77005",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3aee46322a0a56f4709de62484df290bd68312e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=725ea9d297e60b5f56be92de035c5f6c2bba3d2d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "noowT43T3LN7OTEgV59heiSGbz5GQhyxeea4MTjqxTg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzw6yu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Uiqueblhats",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzw6yu/open_source_alternative_to_notebooklm/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752521799,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_hdcx5ggfg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meta‚Äôs New Superintelligence Lab Is Discussing Major A.I. Strategy Changes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzv16g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=684dded7ebf0cced3ec460c9dda8f551b9ecbd73",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752519240,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "nytimes.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?auto=webp&amp;s=211ff5c9d8860c633734a0f69515f881de8905e4",
                  "width": 1050,
                  "height": 550
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f431ed3ef795de81f0d9be2452ed2466f4727f88",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c728fd0b47256c06b7e53063606348710b74999",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a1dc415541c74d1ee2dd3620b8e6997e56ad7f2",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5a0ebffa84a0071645409fce2ba2a7d33bd6a731",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=361e53f2aa166522931efd9533bd8c76685cfc5a",
                    "width": 960,
                    "height": 502
                  }
                ],
                "variants": {},
                "id": "62QXtiCManuS6UimUaWcoUxH8gOETN8-9D6ljAVaZH0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzv16g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "showmeufos",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzv16g/metas_new_superintelligence_lab_is_discussing/",
          "stickied": false,
          "url": "https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.html",
          "subreddit_subscribers": 499292,
          "created_utc": 1752519240,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Obviously this is a silly question. 4k context is limiting to the point where even dumber models are \"better\" for almost any pipeline and use case.\n\nBut for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG's)? What model sizes now beat Llama2-70B in:\n\n- instruction following\n\n- depth of knowledge \n\n- writing skill\n\n- coding \n\n- logic",
          "author_fullname": "t2_w2gxqd6i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If you limit context to 4k tokens, which models today beat Llama2-70B from 2 years ago?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzuaa3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752517599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously this is a silly question. 4k context is limiting to the point where even dumber models are &amp;quot;better&amp;quot; for almost any pipeline and use case.&lt;/p&gt;\n\n&lt;p&gt;But for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG&amp;#39;s)? What model sizes now beat Llama2-70B in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;instruction following&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;depth of knowledge &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;writing skill&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;coding &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;logic&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzuaa3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EmPips",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752517599,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.\n\nMore broadly, I'm very interested in the potential of training a single model to work with different inferencing resources (Google's MatFormer work with Gemma 3n is the obvious other approach).\n\nI'd love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.\n\nYes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?\n\n**Clarifying a point from the thread:**\n\nThe end goal here isn't to distill a crappy small dense model from an MOE, it's to get a sense of how far the expert is from a small dense LLM. If it's not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.",
          "author_fullname": "t2_2roqrw5l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is the output of only the shared expert(s) in a MOE model coherent?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzu9e8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752520423,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752517544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.&lt;/p&gt;\n\n&lt;p&gt;More broadly, I&amp;#39;m very interested in the potential of training a single model to work with different inferencing resources (Google&amp;#39;s MatFormer work with Gemma 3n is the obvious other approach).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.&lt;/p&gt;\n\n&lt;p&gt;Yes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Clarifying a point from the thread:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The end goal here isn&amp;#39;t to distill a crappy small dense model from an MOE, it&amp;#39;s to get a sense of how far the expert is from a small dense LLM. If it&amp;#39;s not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzu9e8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gofiend",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752517544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?  \nI'd like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.  \nIt would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it's not a very \"popular\" language to train on. lol\n\n  \n",
          "author_fullname": "t2_dlu9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is real-time voice-to-voice still science fiction?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzts1z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752516472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, as the title says: is it possible to have real-time voice-to-voice interaction running locally, or are we still not there yet?&lt;br/&gt;\nI&amp;#39;d like to improve my speaking skills (including pronunciation) in English and Japanese, and I thought it would be great to have conversations with a local LLM.&lt;br/&gt;\nIt would also be nice to have something similar in Italian (my native language) for daily chats, but I assume it&amp;#39;s not a very &amp;quot;popular&amp;quot; language to train on. lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzts1z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "junior600",
          "discussion_type": null,
          "num_comments": 39,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzts1z/is_realtime_voicetovoice_still_science_fiction/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752516472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Congrats to the Moonshot team on Kimi-K2!\n\n* Original Post: [Justin Wong (Kimi-K2) Blog Post](https://macro.com/app/md/29ef9c8b-b403-47d8-91c9-ff0520bb43c7/md/629ed15b-bb01-431d-a446-b4aa36436780)\n* Context: [https://moonshotai.github.io/Kimi-K2/](https://moonshotai.github.io/Kimi-K2/)\n\nhttps://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;format=pjpg&amp;auto=webp&amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64\n\nDisclaimer: Net positive outcome for open-source AI research and development, and geopolitical risk‚Äînot a partnership ",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi-K2 ü§ù Anthropic | Blog Post by Justin Wong",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 52,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ncv5le0hpvcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a60ceb065566b732aba10ca1716426b644ebecf4"
                },
                {
                  "y": 81,
                  "x": 216,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6feeeba9e3940a46d797821e6c33fbf473d2931"
                },
                {
                  "y": 120,
                  "x": 320,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b20204102a6df2cfbe4525ced39088787f59118"
                },
                {
                  "y": 241,
                  "x": 640,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20c8644f9a280ec7bd3444cbc619a3e6247a0165"
                },
                {
                  "y": 361,
                  "x": 960,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b786e3941166050874280f3ef029c9dc7f3f544"
                },
                {
                  "y": 406,
                  "x": 1080,
                  "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ae8aa8027033cb6951b3e442c28331bfde0dc2de"
                }
              ],
              "s": {
                "y": 560,
                "x": 1486,
                "u": "https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;format=pjpg&amp;auto=webp&amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64"
              },
              "id": "ncv5le0hpvcf1"
            }
          },
          "name": "t3_1lztjtc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.18,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MJFkGnjtgqNc1SeSwU5cxC7pWsvWWd9fDcod0-2vC8Y.jpg",
          "edited": 1752519267,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752515991,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Congrats to the Moonshot team on Kimi-K2!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Original Post: &lt;a href=\"https://macro.com/app/md/29ef9c8b-b403-47d8-91c9-ff0520bb43c7/md/629ed15b-bb01-431d-a446-b4aa36436780\"&gt;Justin Wong (Kimi-K2) Blog Post&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Context: &lt;a href=\"https://moonshotai.github.io/Kimi-K2/\"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64\"&gt;https://preview.redd.it/ncv5le0hpvcf1.jpg?width=1486&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebd82833159ab0dcaf9dae3efe8cf4692e1d9d64&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: Net positive outcome for open-source AI research and development, and geopolitical risk‚Äînot a partnership &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lztjtc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lztjtc/kimik2_anthropic_blog_post_by_justin_wong/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lztjtc/kimik2_anthropic_blog_post_by_justin_wong/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752515991,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Recorded a userflow for my vibecoding pet project - character selection, model setup, inline replies, and image generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzsoqc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1440,
              "scrubber_media_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/bx3hl3q5kvcf1/DASHPlaylist.mpd?a=1755175131%2COTc2ZWJkMWFiYTRhY2UzNGI1Y2Q5MzBlOWE1OGI2YzVmZjllMzI0NTM3ZTc1NzdkYTVjMmMxMTBjNWUzN2RkMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 38,
              "hls_url": "https://v.redd.it/bx3hl3q5kvcf1/HLSPlaylist.m3u8?a=1755175131%2CYmM1ZjY4MGY4OTVmODlkZGRjMTdlNWIzYjVjMTNlYmQ2YmExYzhjNDNhNmQ1ZmRkMWNiYzMzZTI2Y2NkMWI0ZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=140&amp;height=105&amp;crop=140:105,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0a77c427ad0f08f0061b348e704987804dc1321b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752514110,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/bx3hl3q5kvcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?format=pjpg&amp;auto=webp&amp;s=6c9ab8578bf929eca5120be597d6df7b0ba43974",
                  "width": 1440,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d229202bb26b8decf4001cbe9fd8bb4d45adfea3",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b0fba2d38497e9a79d0dd3c6845939189cd1e736",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9be16d367724d3c86a3e7b5f317cd6d9e18331ae",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1432a2385579783d25465bd811ca11a3da2d989e",
                    "width": 640,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c9a248e58486fa1c9b75dec8df19d61240ef98fc",
                    "width": 960,
                    "height": 720
                  },
                  {
                    "url": "https://external-preview.redd.it/cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d2f5de10741475c90aa05dd0031d6965662c06af",
                    "width": 1080,
                    "height": 810
                  }
                ],
                "variants": {},
                "id": "cmd5cWMzcDVrdmNmMa9ls_clNk5aCuPt4SMo0ohCKwSPbJh7Qncj7lTLMron"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lzsoqc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzsoqc/recorded_a_userflow_for_my_vibecoding_pet_project/",
          "stickied": false,
          "url": "https://v.redd.it/bx3hl3q5kvcf1",
          "subreddit_subscribers": 499292,
          "created_utc": 1752514110,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1440,
              "scrubber_media_url": "https://v.redd.it/bx3hl3q5kvcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/bx3hl3q5kvcf1/DASHPlaylist.mpd?a=1755175131%2COTc2ZWJkMWFiYTRhY2UzNGI1Y2Q5MzBlOWE1OGI2YzVmZjllMzI0NTM3ZTc1NzdkYTVjMmMxMTBjNWUzN2RkMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 38,
              "hls_url": "https://v.redd.it/bx3hl3q5kvcf1/HLSPlaylist.m3u8?a=1755175131%2CYmM1ZjY4MGY4OTVmODlkZGRjMTdlNWIzYjVjMTNlYmQ2YmExYzhjNDNhNmQ1ZmRkMWNiYzMzZTI2Y2NkMWI0ZQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.\n\nStill, it seems pretty odd that they're missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.",
          "author_fullname": "t2_e9jh97s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama, Why No Reka Flash, SmolLM3, GLM-4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzsnna",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752514045,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.&lt;/p&gt;\n\n&lt;p&gt;Still, it seems pretty odd that they&amp;#39;re missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzsnna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chibop1",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752514045,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I‚Äôve been experimenting with Ollama locally and ended up creating a little game called Holy Arcana: From Profane to Divine. \n\nIt uses Llama-3.2 to generate poetry and responses as you make your way through Tarot-inspired challenges and Kabbalistic paths. \n\nIt‚Äôs just something I made for fun, mixing AI with esoteric themes and interactive storytelling. \n\nIf you‚Äôre curious about seeing Ollama put to creative use, feel free to check it out or play around with the \n\nrepo:  \nüëâ [github.com/cyberAlchem1st/holy-arcana](https://github.com/cyberAlchem1st/holy-arcana)\n\n ",
          "author_fullname": "t2_x7pnnazak",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Esoteric Game with Llama3.2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzrqoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752512056,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I‚Äôve been experimenting with Ollama locally and ended up creating a little game called Holy Arcana: From Profane to Divine. &lt;/p&gt;\n\n&lt;p&gt;It uses Llama-3.2 to generate poetry and responses as you make your way through Tarot-inspired challenges and Kabbalistic paths. &lt;/p&gt;\n\n&lt;p&gt;It‚Äôs just something I made for fun, mixing AI with esoteric themes and interactive storytelling. &lt;/p&gt;\n\n&lt;p&gt;If you‚Äôre curious about seeing Ollama put to creative use, feel free to check it out or play around with the &lt;/p&gt;\n\n&lt;p&gt;repo:&lt;br/&gt;\nüëâ &lt;a href=\"https://github.com/cyberAlchem1st/holy-arcana\"&gt;github.com/cyberAlchem1st/holy-arcana&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?auto=webp&amp;s=a936e93d40d705213d3b629b6ef02606722b212b",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab5b82fc39c178d66c6d577d1e97474b18e24c69",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecc83be8a24e065d7733bd95ddf69023f77d1f42",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29bb1ed6fa236a25f0b568b18647d6df229c150e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6bdc379ead682e593b23db0dea625df9dd1f0a4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=378e7312050f9adef7543d7d04e699498bc7903e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1eb296c462cf06737624be941504592293963b6a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rvPrk_aITGFkAgs_yRAMVUD2ygqKO5b_10z3qFy3B30"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lzrqoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ambitious_Ad497",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzrqoi/esoteric_game_with_llama32/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzrqoi/esoteric_game_with_llama32/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752512056,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering)",
          "author_fullname": "t2_1jhxe6m6wu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A practical handbook on Context Engineering with the latest research from IBM Zurich, ICML, Princeton, and more.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzql0b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752509454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/davidkimai/Context-Engineering\"&gt;https://github.com/davidkimai/Context-Engineering&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?auto=webp&amp;s=f8680ad6bdadc6cdd4914fae9065ec6b47805ad6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f42f9a68c36f1e2ccc27fbc08f8a4e8edb8f1a70",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ba461a7633a2a5b765faf3ba8b9b9e94db25221",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6ab5733b0e448457e4ddfdd280b0486f09e8619",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3117ff6ef79e94d1a37e04e2704182174aaf1001",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0123ba313a39132b0d408b10e6733c20a1e07ec6",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af3ec97c9e4656162e6fd49925a896f6596e3856",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "i9rn46kobjc-H-jAw7ePfKf2WD9TjEWEMwzPTaQtEgA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lzql0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "recursiveauto",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzql0b/a_practical_handbook_on_context_engineering_with/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752509454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am running Gemma 3 12B on my local computer. My prompt is about 1000 tokens of text + 3-4 images. My computer is just a regular AMD CPU (no GPU) + 64GB of DDR5 RAM, so understandably the response is slow. Particularly I have noticed that it takes more time to just process my input.\n\nMy question is what hardware would help improve this:  \n1. Obviously a GPU would help - but what should I look for in a GPU to get better response times?  \n2. Would the newer AMD Ryzen‚Ñ¢ AI 9 HX 370 APU help or would I need to go for an AMD Ryzen AI Max+ 395 APU's?  \n3. If I got for the AMD Ryzen‚Ñ¢ AI 9 HX 370 APU, some PCs come with upgradeable RAM i.e. DDR5 (going up to 96GB), while others come with faster LPDDR5 RAM - but with the caveat that the max RAM is capped at 64 GB. I want to be able to run slightly larger models on it (e.g. Gemma 3 27B), but not sure if I need to go for the LPDDR5x versions.",
          "author_fullname": "t2_1kusyf1nll",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to improve response times for multimodal requests?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzqh66",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752509218,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running Gemma 3 12B on my local computer. My prompt is about 1000 tokens of text + 3-4 images. My computer is just a regular AMD CPU (no GPU) + 64GB of DDR5 RAM, so understandably the response is slow. Particularly I have noticed that it takes more time to just process my input.&lt;/p&gt;\n\n&lt;p&gt;My question is what hardware would help improve this:&lt;br/&gt;\n1. Obviously a GPU would help - but what should I look for in a GPU to get better response times?&lt;br/&gt;\n2. Would the newer AMD Ryzen‚Ñ¢ AI 9 HX 370 APU help or would I need to go for an AMD Ryzen AI Max+ 395 APU&amp;#39;s?&lt;br/&gt;\n3. If I got for the AMD Ryzen‚Ñ¢ AI 9 HX 370 APU, some PCs come with upgradeable RAM i.e. DDR5 (going up to 96GB), while others come with faster LPDDR5 RAM - but with the caveat that the max RAM is capped at 64 GB. I want to be able to run slightly larger models on it (e.g. Gemma 3 27B), but not sure if I need to go for the LPDDR5x versions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzqh66",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "coolahavoc",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzqh66/how_to_improve_response_times_for_multimodal/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzqh66/how_to_improve_response_times_for_multimodal/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752509218,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone - there are some **245GB quants (80% size reduction)** for Kimi K2 at https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF. The Unsloth dynamic Q2\\_K\\_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.\n\nPlease use `-ot \".ffn_.*_exps.=CPU\"` to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.\n\nYou need to use either [https://github.com/ggml-org/llama.cpp/pull/14654](https://github.com/ggml-org/llama.cpp/pull/14654) or our fork [https://github.com/unslothai/llama.cpp](https://github.com/unslothai/llama.cpp) to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!\n\nThe suggested parameters are:\n\n    temperature = 0.6\n    min_p = 0.01 (set it to a small number)\n\nDocs has more details: [https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally](https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally)",
          "author_fullname": "t2_5wukhd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 1.8bit Unsloth Dynamic GGUFs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzps3b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 342,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 342,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752507676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone - there are some &lt;strong&gt;245GB quants (80% size reduction)&lt;/strong&gt; for Kimi K2 at &lt;a href=\"https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF&lt;/a&gt;. The Unsloth dynamic Q2_K_XL (381GB) surprisingly can one-shot our hardened Flappy Bird game and also the Heptagon game.&lt;/p&gt;\n\n&lt;p&gt;Please use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to system RAM. You will need for best performance the RAM + VRAM to be at least 245GB. You can use your SSD / disk as well, but performance might take a hit.&lt;/p&gt;\n\n&lt;p&gt;You need to use either &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14654\"&gt;https://github.com/ggml-org/llama.cpp/pull/14654&lt;/a&gt; or our fork &lt;a href=\"https://github.com/unslothai/llama.cpp\"&gt;https://github.com/unslothai/llama.cpp&lt;/a&gt; to install llama.cpp to get Kimi K2 to work - mainline support should be coming in a few days!&lt;/p&gt;\n\n&lt;p&gt;The suggested parameters are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;temperature = 0.6\nmin_p = 0.01 (set it to a small number)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docs has more details: &lt;a href=\"https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally\"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?auto=webp&amp;s=bc3027fa5da20b74e927173e28d8aca06d1918f9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=539e7c53ad8fe4d04c6029c11344ff605d38589a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6263e8d5e0d9129138827f26082b6f9517086361",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=247bab7783d0737b150b5f5183ba9db8a0966436",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3e7dfec653d356a4ccb836ae65b4546e9a5ad00",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1a4a1e7fdeac330903838e1542ed085ea53ec142",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ff85dc8c4e34c5580c7c7bf51d308d118a9e322f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "yxH9RYoAESwYJz6seCzu-b7mAiGRtDXgd-N_V6wb3cw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzps3b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danielhanchen",
          "discussion_type": null,
          "num_comments": 96,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752507676,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as title says, lm studio always uses my cpu, I want to make lm uses the GPU tried several changes\n\nLaptop specs\n\n24gb ram  \n3070 8gb ram  \ni9-11 gen\n\ni cant seem to use gpu as main resource for llama in lmstudio\n\n[settings - hardware](https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1)\n\n[settings - runtime](https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;format=png&amp;auto=webp&amp;s=e59c270eef7d301d1bda499756496939c67e4099)\n\n  \n  \nthings I did  \nupdated GPU  \nruntime engine is set to  cuda 12 llama cpp  \ntried several changes in GPU offload, from maximum  to half,  \ntried changes CPU thread pool  \ntried changes in context length\n\nupon testing cpu usage spikes, while my GPU sits idle, only works if my cpu reach 70% above then gpu starts working\n\nthe models I used are :  \nopen hermes  2.5 mistral 7b  \nOpenOrca Platypus2 13B q 4 k s  \nMythomax L2 13b q4 k s",
          "author_fullname": "t2_1is18ejy13",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio cant use my gpu as main",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "v0fsjfwkuucf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 68,
                  "x": 108,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=28304eefb1585ed0b0b966bf3bb044f06d87ac3e"
                },
                {
                  "y": 137,
                  "x": 216,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5103ec1903662ebf541200809894b7f0050373e3"
                },
                {
                  "y": 203,
                  "x": 320,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bce56794d9d7b1d50bb3dbf07cad90b792451e52"
                },
                {
                  "y": 407,
                  "x": 640,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=160bb93aa199368b865ae1a29783e2bb3c049631"
                },
                {
                  "y": 611,
                  "x": 960,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bd36193de71a6a4172c9362db5b1efe55d5237a"
                },
                {
                  "y": 687,
                  "x": 1080,
                  "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba101eaf58a755a88b279ec499ef4ed1665d311f"
                }
              ],
              "s": {
                "y": 781,
                "x": 1226,
                "u": "https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1"
              },
              "id": "v0fsjfwkuucf1"
            },
            "75o3tq0z5vcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=29cc677c8b11701bbd55ae946c097f48daf97e30"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb3f3c30a765ae1b8b0e2269137b6da98723141d"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=826ee8bb5ea9d079adee3ee3db96ac8ceca3e83b"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ef3e82b55da1f94203265ee9541b4251393f647"
                },
                {
                  "y": 631,
                  "x": 960,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=65c28555b2d9a4466aaf66240eb018987ac511f7"
                },
                {
                  "y": 710,
                  "x": 1080,
                  "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce63d924b98ef3795f1bb676a62cb3df1738eddf"
                }
              ],
              "s": {
                "y": 810,
                "x": 1232,
                "u": "https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;format=png&amp;auto=webp&amp;s=e59c270eef7d301d1bda499756496939c67e4099"
              },
              "id": "75o3tq0z5vcf1"
            }
          },
          "name": "t3_1lzoxbl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aO_NeeS70DPNibAf4xpd1tv7HSDOB7yzatP7amRYFhA.jpg",
          "edited": 1752509331,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752505748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as title says, lm studio always uses my cpu, I want to make lm uses the GPU tried several changes&lt;/p&gt;\n\n&lt;p&gt;Laptop specs&lt;/p&gt;\n\n&lt;p&gt;24gb ram&lt;br/&gt;\n3070 8gb ram&lt;br/&gt;\ni9-11 gen&lt;/p&gt;\n\n&lt;p&gt;i cant seem to use gpu as main resource for llama in lmstudio&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v0fsjfwkuucf1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37decc5be28967e60a2deae1e78de6635a72e2f1\"&gt;settings - hardware&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/75o3tq0z5vcf1.png?width=1232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e59c270eef7d301d1bda499756496939c67e4099\"&gt;settings - runtime&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;things I did&lt;br/&gt;\nupdated GPU&lt;br/&gt;\nruntime engine is set to  cuda 12 llama cpp&lt;br/&gt;\ntried several changes in GPU offload, from maximum  to half,&lt;br/&gt;\ntried changes CPU thread pool&lt;br/&gt;\ntried changes in context length&lt;/p&gt;\n\n&lt;p&gt;upon testing cpu usage spikes, while my GPU sits idle, only works if my cpu reach 70% above then gpu starts working&lt;/p&gt;\n\n&lt;p&gt;the models I used are :&lt;br/&gt;\nopen hermes  2.5 mistral 7b&lt;br/&gt;\nOpenOrca Platypus2 13B q 4 k s&lt;br/&gt;\nMythomax L2 13b q4 k s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzoxbl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Zinxdia",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzoxbl/lm_studio_cant_use_my_gpu_as_main/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzoxbl/lm_studio_cant_use_my_gpu_as_main/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752505748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any advice ? :) ",
          "author_fullname": "t2_pmwcfxhz4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best LLM for Educators ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzooed",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752505188,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any advice ? :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzooed",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creative_Structure22",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzooed/best_llm_for_educators/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzooed/best_llm_for_educators/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752505188,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I've also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they're \"exhausting.\"\n\nI feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people's examples just to implement some simple control flow.\n\nWith just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it's so lean and easy, and you get full visibility on how each API call went.",
          "author_fullname": "t2_c5n1x183x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I ditch all LLM framework and use only OpenAI SDK for everything, I start loving building AI application this way.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzocuk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752504444,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried several LLM frameworks and libraries, each with their own direction like Haystack, LangChain, etc. I&amp;#39;ve also tried several agent frameworks like AutoGen, SmolAgent, and Strands. All I can say about these frameworks is that they&amp;#39;re &amp;quot;exhausting.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I feel like every application built with these tools consumes twice my time. I have to go back and forth reviewing documentation and maybe other people&amp;#39;s examples just to implement some simple control flow.&lt;/p&gt;\n\n&lt;p&gt;With just the OpenAI SDK (or just API calls), you can connect to almost any model that supports the OpenAI API spec, and everything is just structured output. You treat the LLM just like a function that reliably returns predefined values you can expect. I love building AI applications this way - it&amp;#39;s so lean and easy, and you get full visibility on how each API call went.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzocuk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dheetoo",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzocuk/i_ditch_all_llm_framework_and_use_only_openai_sdk/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752504444,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Context of my project idea:**\n\nI have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay \"per request\" is pretty much unfeasible without factoring in excessive cold startup times.\n\nSo it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.\n\n**An alternative?**\n\nAll this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a **community-owned live inference server** where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?\n\nWe, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.\n\nI imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.",
          "author_fullname": "t2_19mrnrt357",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Project Idea: A REAL Community-driven LLM Stack",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lznxy5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752503693,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752503479,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context of my project idea:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay &amp;quot;per request&amp;quot; is pretty much unfeasible without factoring in excessive cold startup times.&lt;/p&gt;\n\n&lt;p&gt;So it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An alternative?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;All this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a &lt;strong&gt;community-owned live inference server&lt;/strong&gt; where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?&lt;/p&gt;\n\n&lt;p&gt;We, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.&lt;/p&gt;\n\n&lt;p&gt;I imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lznxy5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Budget_Map_3333",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752503479,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Recently decided to try out openwebui and something i noticed is that it does no batching for embedding multiple files, and in the scale of 5000 files it feels like it will take the better part of 5 hours, i can write a tiny python script to embed all of these files (and view them in qdrant) in an amount of time that is light years ahead of whatever openwebui is doing, except openwebui can‚Äôt use those for some reason.\n\nAny alternatives?\n\nI run everything locally through vllm, with qwen 4b embedding, qwen 0.6b reranker, and devstral",
          "author_fullname": "t2_9so78ol2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a better frontend than OpenWebui for RAG?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzna91",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently decided to try out openwebui and something i noticed is that it does no batching for embedding multiple files, and in the scale of 5000 files it feels like it will take the better part of 5 hours, i can write a tiny python script to embed all of these files (and view them in qdrant) in an amount of time that is light years ahead of whatever openwebui is doing, except openwebui can‚Äôt use those for some reason.&lt;/p&gt;\n\n&lt;p&gt;Any alternatives?&lt;/p&gt;\n\n&lt;p&gt;I run everything locally through vllm, with qwen 4b embedding, qwen 0.6b reranker, and devstral&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzna91",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Capable-Ad-7494",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzna91/is_there_a_better_frontend_than_openwebui_for_rag/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzna91/is_there_a_better_frontend_than_openwebui_for_rag/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752501919,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to create a new AI agent for my MySQL database. The database tables are complex and require extensive documentation to make them understandable for the AI to query effectively. \n\nI need guidance on selecting the right model and framework for this project.",
          "author_fullname": "t2_13usrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions for ai agent framework and ai model for Text-to-SQL ai agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzn9th",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to create a new AI agent for my MySQL database. The database tables are complex and require extensive documentation to make them understandable for the AI to query effectively. &lt;/p&gt;\n\n&lt;p&gt;I need guidance on selecting the right model and framework for this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzn9th",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "M7mDSa3eD_",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzn9th/suggestions_for_ai_agent_framework_and_ai_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzn9th/suggestions_for_ai_agent_framework_and_ai_model/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752501889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello! \nI'm currently investigating and planning a very fun project, my ultimate personal assistant.\n\nThe idea is to have a multi-agent system, with one main point of contact; \"The Secretary\". Then I have task-specific agents with expertise in different areas, like my different work projects, or notion updating etc. I want to be able to configure system prompts, integrations (MCP probably aswell?) and memory. The agents should be able to communicate and get help from each other.\n\nThe actual architecture is not set in stone yet, maybe I will use a existing system if it managed to accomplish the UX features I want, and that's why I'm asking you. \n\nI wanted to check with you guys if anyone has a recommendation for a framework, tool or existing open source project that would be nice to look into.\n\nThese are some things I'm currently looking in to:\n\n- AGiXT (Agentic assistant framework)\n- SuperAGI (Agentic assistant framework)\n- CrewAI (multi-agent building framework)\n- Librechat (ChatGPT alternative)\n- Graphiti (Dynamic graph memory)\n- n8n (Visual flow process builder)\n\nI do know is that I want to work in Python. I will be locally hosting the system.\n\nAny recommendations for building something like this?",
          "author_fullname": "t2_uf4p8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Agentic Secretary System - Tips and Recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzn4ae",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752501530,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! \nI&amp;#39;m currently investigating and planning a very fun project, my ultimate personal assistant.&lt;/p&gt;\n\n&lt;p&gt;The idea is to have a multi-agent system, with one main point of contact; &amp;quot;The Secretary&amp;quot;. Then I have task-specific agents with expertise in different areas, like my different work projects, or notion updating etc. I want to be able to configure system prompts, integrations (MCP probably aswell?) and memory. The agents should be able to communicate and get help from each other.&lt;/p&gt;\n\n&lt;p&gt;The actual architecture is not set in stone yet, maybe I will use a existing system if it managed to accomplish the UX features I want, and that&amp;#39;s why I&amp;#39;m asking you. &lt;/p&gt;\n\n&lt;p&gt;I wanted to check with you guys if anyone has a recommendation for a framework, tool or existing open source project that would be nice to look into.&lt;/p&gt;\n\n&lt;p&gt;These are some things I&amp;#39;m currently looking in to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AGiXT (Agentic assistant framework)&lt;/li&gt;\n&lt;li&gt;SuperAGI (Agentic assistant framework)&lt;/li&gt;\n&lt;li&gt;CrewAI (multi-agent building framework)&lt;/li&gt;\n&lt;li&gt;Librechat (ChatGPT alternative)&lt;/li&gt;\n&lt;li&gt;Graphiti (Dynamic graph memory)&lt;/li&gt;\n&lt;li&gt;n8n (Visual flow process builder)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I do know is that I want to work in Python. I will be locally hosting the system.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for building something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzn4ae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boltyx",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzn4ae/agentic_secretary_system_tips_and_recommendations/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzn4ae/agentic_secretary_system_tips_and_recommendations/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752501530,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author‚Äôs insights genuinely thought-provoking. The original Chinese version is [here](https://bigeagle.me/2025/07/kimi-k2/)‚Äîfeel free to read it in full (and of course you can use Kimi K2 as your translator). Here‚Äôs my own distilled summary of the main points:\n\n\n\n‚Ä¢ Beyond chatbots: Kimi K2 experiments with an ‚Äúartifact-first‚Äù interaction model that has the AI immediately build interactive front-end deliverables‚ÄîPPT-like pages, diagrams, even mini-games‚Äîrather than simply returning markdown text.\n\n‚Ä¢ Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.\n\n‚Ä¢ What makes an agentic model: A minimal loop‚Äîthink, choose tools, observe results, iterate‚Äîcan be learned from synthetic trajectories. Today‚Äôs agent abilities are early-stage; the next pre-training wave still holds plenty of upside.\n\n‚Ä¢ Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit ‚Äúhacky‚Äù hidden pipelines, forcing genuinely strong, general models‚Äîexactly what an AGI-oriented startup needs.\n\n‚Ä¢ Marketing controversies &amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1‚Äôs viral rise proved that raw model quality markets itself and validates the ‚Äúfoundation-model-first‚Äù path.\n\n‚Ä¢ Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.\n\n\n\nFrom the entire blog, this is the paragraph I loved the most:\n\n&gt;A while ago, ‚ÄòAgent‚Äô products were all the rage. I kept hearing people say that Kimi shouldn‚Äôt compete on large models and should focus on Agents instead. Let me be clear: **the vast majority of Agent products are nothing without Claude behind them.** Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don‚Äôt keep pushing that ceiling higher, I won‚Äôt stay here a single extra day.\n\n&gt;Chasing AGI is an extremely narrow, perilous bridge‚Äîthere‚Äôs no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‚ÄòAs an investor, I care about the ROI of AI applications.‚Äô In that moment I knew the company he founded wouldn‚Äôt last long.",
          "author_fullname": "t2_sqi8xxun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "After Kimi K2 Is Released: No Longer Just a ChatBot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzm645",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 304,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 304,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752499086,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a personal reflection penned by a Kimi team member shortly after the launch of Kimi K2. I found the author‚Äôs insights genuinely thought-provoking. The original Chinese version is &lt;a href=\"https://bigeagle.me/2025/07/kimi-k2/\"&gt;here&lt;/a&gt;‚Äîfeel free to read it in full (and of course you can use Kimi K2 as your translator). Here‚Äôs my own distilled summary of the main points:&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢ Beyond chatbots: Kimi K2 experiments with an ‚Äúartifact-first‚Äù interaction model that has the AI immediately build interactive front-end deliverables‚ÄîPPT-like pages, diagrams, even mini-games‚Äîrather than simply returning markdown text.&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢ Tool use, minus the pain: Instead of wiring countless third-party tools into RL training, the team awakened latent API knowledge inside the model by auto-generating huge, diverse tool-call datasets through multi-agent self-play.&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢ What makes an agentic model: A minimal loop‚Äîthink, choose tools, observe results, iterate‚Äîcan be learned from synthetic trajectories. Today‚Äôs agent abilities are early-stage; the next pre-training wave still holds plenty of upside.&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢ Why open source: (1) Buzz and reputation, (2) community contributions like MLX ports and 4-bit quantization within 24 h, (3) open weights prohibit ‚Äúhacky‚Äù hidden pipelines, forcing genuinely strong, general models‚Äîexactly what an AGI-oriented startup needs.&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢ Marketing controversies &amp;amp; competition: After halting ads, Kimi nearly vanished from app-store search, yet refused to resume spending. DeepSeek-R1‚Äôs viral rise proved that raw model quality markets itself and validates the ‚Äúfoundation-model-first‚Äù path.&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢ Road ahead: All resources now converge on core algorithms and K2 (with hush-hush projects beyond). K2 still has many flaws; the author is already impatient for K3.&lt;/p&gt;\n\n&lt;p&gt;From the entire blog, this is the paragraph I loved the most:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A while ago, ‚ÄòAgent‚Äô products were all the rage. I kept hearing people say that Kimi shouldn‚Äôt compete on large models and should focus on Agents instead. Let me be clear: &lt;strong&gt;the vast majority of Agent products are nothing without Claude behind them.&lt;/strong&gt; Windsurf getting cut off by Claude only reinforces this fact. In 2025, the ceiling of intelligence is still set entirely by the underlying model. For a company whose goal is AGI, if we don‚Äôt keep pushing that ceiling higher, I won‚Äôt stay here a single extra day.&lt;/p&gt;\n\n&lt;p&gt;Chasing AGI is an extremely narrow, perilous bridge‚Äîthere‚Äôs no room for distraction or hesitation. Your pursuit might not succeed, but hesitation will certainly fail. At the BAAI Conference in June 2024 I heard Dr. Kai-Fu Lee casually remark, ‚ÄòAs an investor, I care about the ROI of AI applications.‚Äô In that moment I knew the company he founded wouldn‚Äôt last long.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzm645",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nekofneko",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzm645/after_kimi_k2_is_released_no_longer_just_a_chatbot/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752499086,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "UTCP: A safer, scalable tool-calling alternative to MCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzl5zk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "transparent",
          "ups": 739,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 739,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NwQpLihWBkRwry7btAEv1kXjEd2jOrNWPfUJ4oMDWTQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752496381,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wv84vx7h3ucf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wv84vx7h3ucf1.png?auto=webp&amp;s=9fc98e1863475ae1707dcf8e031f0c40856c1282",
                  "width": 1874,
                  "height": 1642
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=af7471a080008d1b2a11663755e2386042538cdb",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cbf92935540699f9a5b793b10b599033bdd378c",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5bc00f40fff44e8d417d1c4e7dcd1866bece5c55",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44e4d83d52673aeb1bf507e10f4ab32bff06db95",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4aa90f35e8734ab6dd4bb03e9d5344881c7be6c2",
                    "width": 960,
                    "height": 841
                  },
                  {
                    "url": "https://preview.redd.it/wv84vx7h3ucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cafe58b5e161c4d2b61d8cb324738c3b1b2459e3",
                    "width": 1080,
                    "height": 946
                  }
                ],
                "variants": {},
                "id": "kbRMMR47HDIi7lZVVAy5mGTwVKuCZQBEJufsqMy9_24"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lzl5zk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 134,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lzl5zk/utcp_a_safer_scalable_toolcalling_alternative_to/",
          "stickied": false,
          "url": "https://i.redd.it/wv84vx7h3ucf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752496381,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   \n  \nI was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. \n\nQwen 2.5 VL 3B  \nCaption generation - average time taken for max pixel 768\\*768 - 1.62s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.02s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 2.79s\n\nQwen 2.5 VL 7B  \nCaption generation - average time taken for max pixel 768\\*768 - 2.21s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.73s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 3.64s  \n  \nQwen 2.5 VL 7B AWQ  \nCaption generation - average time taken for max pixel 768\\*768 - 2.84s  \nCaption generation - average time taken for max pixel 1024\\*1024 - 2.94s  \nCaption generation - average time taken for max pixel 1280\\*1280 - 3.85s  \n\n\n1. Why 7B AWQ is slower than 7B?  \n2. What other better Image caption/VQA model exists that runs in less or similar resource requirments?\n\n",
          "author_fullname": "t2_7doe6lck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions/Alternatives for Image captions with efficient system requirements",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzkrwg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752495232,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   &lt;/p&gt;\n\n&lt;p&gt;I was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. &lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 3B&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 1.62s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.02s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 2.79s&lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 7B&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 2.21s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.73s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 3.64s  &lt;/p&gt;\n\n&lt;p&gt;Qwen 2.5 VL 7B AWQ&lt;br/&gt;\nCaption generation - average time taken for max pixel 768*768 - 2.84s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1024*1024 - 2.94s&lt;br/&gt;\nCaption generation - average time taken for max pixel 1280*1280 - 3.85s  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why 7B AWQ is slower than 7B?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzkrwg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "palaniappan_05",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752495232,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:\n\nWould it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.\n\nThe card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?\n\nBecause utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don't know how much that matters...",
          "author_fullname": "t2_3ogjqne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multiple 5060 Ti's",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzkcg3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752493925,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:&lt;/p&gt;\n\n&lt;p&gt;Would it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.&lt;/p&gt;\n\n&lt;p&gt;The card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?&lt;/p&gt;\n\n&lt;p&gt;Because utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don&amp;#39;t know how much that matters...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzkcg3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snorixx",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752493925,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to build a mid range Desktop to fine-tuning and host Small Language Models and LLMs.\nI am thinking about using 2 AMD Radeon 9060XT 16GB to reach 32 GB VRAM on budget.\nWill it help? Since 32GB Cards like Nvidia RTX5090 are absurdly expensive. What are your suggestions about the Motherboard and CPU for my build? Should I go for a Mac Mini M4 cluster, or other Single Board Chip cluster to achieve high VRAM?\nI am in India, btw.",
          "author_fullname": "t2_4v5d5bzj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A mid range PC build for Dual GPU Local LLMs and SLMs.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzk041",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752492834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to build a mid range Desktop to fine-tuning and host Small Language Models and LLMs.\nI am thinking about using 2 AMD Radeon 9060XT 16GB to reach 32 GB VRAM on budget.\nWill it help? Since 32GB Cards like Nvidia RTX5090 are absurdly expensive. What are your suggestions about the Motherboard and CPU for my build? Should I go for a Mac Mini M4 cluster, or other Single Board Chip cluster to achieve high VRAM?\nI am in India, btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzk041",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iammhk",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzk041/a_mid_range_pc_build_for_dual_gpu_local_llms_and/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752492834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have tried Cline/Roo Code/OpenHands\nUsed Devstral, GLM4 32b, codellama etc\nWhen trying to navigate website using MCP server, the LLM gets stuck and cannot press on actual buttons and escape the captcha page / allow cookies pop up.\n\nIs there a better model to try? Or its only API claude model ",
          "author_fullname": "t2_cav43",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any local LLMS that support Browser use MCP?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjsu3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752492186,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried Cline/Roo Code/OpenHands\nUsed Devstral, GLM4 32b, codellama etc\nWhen trying to navigate website using MCP server, the LLM gets stuck and cannot press on actual buttons and escape the captcha page / allow cookies pop up.&lt;/p&gt;\n\n&lt;p&gt;Is there a better model to try? Or its only API claude model &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzjsu3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iChrist",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjsu3/are_there_any_local_llms_that_support_browser_use/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752492186,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so ive hosted ollama locally on my system on¬†[http://localhost:11434/api/generate](http://localhost:11434/api/generate)¬†and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory.\n\ni don't understand why this would happen because as much as i have seen modern llms, they don't change their weights during inference.\n\nScenario:\n\n1. makes a query to ollama for topic 1 with a very specific keyword that i have created\n2. makes another query to ollama for a topic that is similar to topic 1 but has a new keyword.\n\nTurns out that the first keyword shows up in the second response aswell. Not always, but this shouldn't happen at all as much as i know\n\nIs there something that i am missing?  \nI checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &lt;model\\_name&gt;",
          "author_fullname": "t2_c40awigh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollama retaining history?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjlvi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752491545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so ive hosted ollama locally on my system on¬†&lt;a href=\"http://localhost:11434/api/generate\"&gt;http://localhost:11434/api/generate&lt;/a&gt;¬†and was testing it out a bit and it seems that between separate fetch calls, ollama seems to be retaining some memory.&lt;/p&gt;\n\n&lt;p&gt;i don&amp;#39;t understand why this would happen because as much as i have seen modern llms, they don&amp;#39;t change their weights during inference.&lt;/p&gt;\n\n&lt;p&gt;Scenario:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;makes a query to ollama for topic 1 with a very specific keyword that i have created&lt;/li&gt;\n&lt;li&gt;makes another query to ollama for a topic that is similar to topic 1 but has a new keyword.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Turns out that the first keyword shows up in the second response aswell. Not always, but this shouldn&amp;#39;t happen at all as much as i know&lt;/p&gt;\n\n&lt;p&gt;Is there something that i am missing?&lt;br/&gt;\nI checked the ollama/history file and it only contained prompts that i have made from the terminal using ollama run &amp;lt;model\\_name&amp;gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzjlvi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DimensionEnergy",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjlvi/ollama_retaining_history/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjlvi/ollama_retaining_history/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752491545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[**Foundations of Large Language Models (LLMs)**](https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458)\n\n* Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)\n* Original Source:¬†[https://arxiv.org/abs/2501.09223](https://arxiv.org/abs/2501.09223)\n* Model: Claude 4.0 Sonnet\n\nNote: The research paper is v2, originally submitted on Jan 16, 2025¬†and revised on Jun 15, 2025",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Foundations of Large Language Models (LLMs) | NLP Lab Research",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzjaf5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752496050,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752490469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://macro.com/app/pdf/1ace3262-d707-4dfc-9111-e3c5e3df96a1/md/ce8a6add-6d5e-48b5-be8b-4b365867d458\"&gt;&lt;strong&gt;Foundations of Large Language Models (LLMs)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Authors: Tong Xiao and Jingbo Zhu (NLP Lab, Northeastern University and NiuTrans Research)&lt;/li&gt;\n&lt;li&gt;Original Source:¬†&lt;a href=\"https://arxiv.org/abs/2501.09223\"&gt;https://arxiv.org/abs/2501.09223&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model: Claude 4.0 Sonnet&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Note: The research paper is v2, originally submitted on Jan 16, 2025¬†and revised on Jun 15, 2025&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lzjaf5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzjaf5/foundations_of_large_language_models_llms_nlp_lab/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752490469,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks!\n\nWe‚Äôve been building a browser-native, on-device AI assistant that runs entirely locally ‚Äî no servers, no telemetry, no API calls. All inference happens through Ollama (for now), though we also support running lightweight models via WebLLM for quick testing directly in the browser.\n\nThe motivation: while local model support is getting better, the UX side is still pretty raw. We wanted to explore what it would take to make LLMs feel like a real part of your workflow ‚Äî embedded in the browser environment itself, not just in a terminal or Electron wrapper.\n\nA few things we‚Äôre trying to solve:\n\n* persistent context across tabs\n* writing tools with local context awareness\n* in-page bilingual translation\n* quick actions (right-click to trigger prompt templates)\n* local-first web search (open and process real pages in browser tabs)\n* file-based conversations (PDFs, image files), user-controlled memory (Both coming very soon)\n\nWe‚Äôre using Ollama as the local model backend, but what we‚Äôve really focused on is designing and building the GUI and interaction layer it‚Äôs missing ‚Äî something more usable for non-technical users, and more native to the browser environment.\n\nCode‚Äôs open here if you want to poke around: [https://github.com/NativeMindBrowser/NativeMindExtension](https://github.com/NativeMindBrowser/NativeMindExtension)\n\nWould love to hear thoughts from others working on local tooling.",
          "author_fullname": "t2_1nibexa2mk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone else interested in a 100% on-device browser AI assistant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzimcq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.48,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752549636,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752488062,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;We‚Äôve been building a browser-native, on-device AI assistant that runs entirely locally ‚Äî no servers, no telemetry, no API calls. All inference happens through Ollama (for now), though we also support running lightweight models via WebLLM for quick testing directly in the browser.&lt;/p&gt;\n\n&lt;p&gt;The motivation: while local model support is getting better, the UX side is still pretty raw. We wanted to explore what it would take to make LLMs feel like a real part of your workflow ‚Äî embedded in the browser environment itself, not just in a terminal or Electron wrapper.&lt;/p&gt;\n\n&lt;p&gt;A few things we‚Äôre trying to solve:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;persistent context across tabs&lt;/li&gt;\n&lt;li&gt;writing tools with local context awareness&lt;/li&gt;\n&lt;li&gt;in-page bilingual translation&lt;/li&gt;\n&lt;li&gt;quick actions (right-click to trigger prompt templates)&lt;/li&gt;\n&lt;li&gt;local-first web search (open and process real pages in browser tabs)&lt;/li&gt;\n&lt;li&gt;file-based conversations (PDFs, image files), user-controlled memory (Both coming very soon)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We‚Äôre using Ollama as the local model backend, but what we‚Äôve really focused on is designing and building the GUI and interaction layer it‚Äôs missing ‚Äî something more usable for non-technical users, and more native to the browser environment.&lt;/p&gt;\n\n&lt;p&gt;Code‚Äôs open here if you want to poke around: &lt;a href=\"https://github.com/NativeMindBrowser/NativeMindExtension\"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear thoughts from others working on local tooling.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?auto=webp&amp;s=8042ecc768e0ad4bb27467181ccc60fb64733c12",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=307824a95b90814f4b420f060a268603bfc26a80",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb1b315a93a4b4ce546d598ae89831e0041a7c25",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8f78b55a419b2be3251f47373f246de4cb2e2a1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=073ef6e17c32cb48adaf9e9ccc4b2d4de33710b2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c7e1613aa961ad578e6da9fda71e63ffd3cb433",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98e89b83c879133a94111d825fb8cb2d951e541e",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzimcq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InfiniteJX",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752488062,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let's me say ... dont know .. clumsy?\n\nSo I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....\n\nI went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything looked promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward.\n\nIntegration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.\n\nThrough. Done. Annoyed. Frustrated. Nuts. Rant over.\n\nBack to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?\n\nEDIT: Didnt thought that there are some many reasonable UIs out there. That's huge.",
          "author_fullname": "t2_185bgnisld",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Annoyed with LibreChat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzikqt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752507102,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752487893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Few weeks ago I decided to give LibreChat a try. OpenWebUI was so ... let&amp;#39;s me say ... dont know .. clumsy?&lt;/p&gt;\n\n&lt;p&gt;So I went to try LibreChat. I was happy first. More or less. Basic things worked. Like selecting a model and using it. Well. That was also the case with OpenWebUI before ....&lt;/p&gt;\n\n&lt;p&gt;I went to integrate more of my infrastructure. Nothing. Almost nothing worked oob. nothing. Although everything looked promising - after 2 weeks of doing every day 5 micro steps forward and 3 big steps backward.&lt;/p&gt;\n\n&lt;p&gt;Integration of tools, getting web search to work took me ages. Lack of traces almost killed me, and the need to understand what the maintainer thought when he designed the app was far more important, than reading the docs and the examples. Because docs and examples are always a bit out out date. Not fully. A bit.&lt;/p&gt;\n\n&lt;p&gt;Through. Done. Annoyed. Frustrated. Nuts. Rant over.&lt;/p&gt;\n\n&lt;p&gt;Back to OpenWebUI? LobeChat has to much colors and stickers. I think. Any other recommendations ?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Didnt thought that there are some many reasonable UIs out there. That&amp;#39;s huge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzikqt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charming_Support726",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzikqt/annoyed_with_librechat/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752487893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been considering an MI50 32gb for a budget AI desktop for a while. \n\nAn issue I found was that it does not natively support display output.\n\nBut I found a version with the Radeon pro vii‚Äôs bios flashed onto it that should allow it to output to a display.\n\nAnother issue was that it doesn‚Äôt actually have a fan, relying on other fans to blow air through it.\n\nBut the frankenversion I found also has a fan installed on it\n\nAnd all of that I found for roughly 170 USD\n\nI‚Äôll probably install it into a ryzen 5 3600 system with another 32gb of cheap ddr4 ram that costs about another 150.\n\nAny major issues with this build? \n\nI‚Äôll probably install Ubuntu just to try it out, I‚Äôve always been a windows user but shouldn‚Äôt hurt to try. Always wanted to try out Linux anyways but never had a spare pc to work with.",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 32GB with bios flash",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzijk2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752487768,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been considering an MI50 32gb for a budget AI desktop for a while. &lt;/p&gt;\n\n&lt;p&gt;An issue I found was that it does not natively support display output.&lt;/p&gt;\n\n&lt;p&gt;But I found a version with the Radeon pro vii‚Äôs bios flashed onto it that should allow it to output to a display.&lt;/p&gt;\n\n&lt;p&gt;Another issue was that it doesn‚Äôt actually have a fan, relying on other fans to blow air through it.&lt;/p&gt;\n\n&lt;p&gt;But the frankenversion I found also has a fan installed on it&lt;/p&gt;\n\n&lt;p&gt;And all of that I found for roughly 170 USD&lt;/p&gt;\n\n&lt;p&gt;I‚Äôll probably install it into a ryzen 5 3600 system with another 32gb of cheap ddr4 ram that costs about another 150.&lt;/p&gt;\n\n&lt;p&gt;Any major issues with this build? &lt;/p&gt;\n\n&lt;p&gt;I‚Äôll probably install Ubuntu just to try it out, I‚Äôve always been a windows user but shouldn‚Äôt hurt to try. Always wanted to try out Linux anyways but never had a spare pc to work with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzijk2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzijk2/mi50_32gb_with_bios_flash/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzijk2/mi50_32gb_with_bios_flash/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752487768,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. \n\nThe screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. \n\nI've tried prompting hard (\"Use ONLY full complete traditional sentences and grammar, write like Hemingway\" and variations of the same), and I've tried bringing the Temperature right down, but nothing seems to help. \n\nI've had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek's R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.\n\nAny advice, or just some bitter commiseration, gratefully accepted.",
          "author_fullname": "t2_i5ettea7e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Responses keep dissolving into word salad - how to stop it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 97,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhqz8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/KT3zIRhU3VM0HArg0M_K5OvqTcxlyaFFQUhqYMvfTZU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752484685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. &lt;/p&gt;\n\n&lt;p&gt;The screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried prompting hard (&amp;quot;Use ONLY full complete traditional sentences and grammar, write like Hemingway&amp;quot; and variations of the same), and I&amp;#39;ve tried bringing the Temperature right down, but nothing seems to help. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek&amp;#39;s R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.&lt;/p&gt;\n\n&lt;p&gt;Any advice, or just some bitter commiseration, gratefully accepted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lr7kq1452tcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lr7kq1452tcf1.png?auto=webp&amp;s=4b3ae39d0f36d78e751373129a21148da7beecfe",
                  "width": 2106,
                  "height": 1468
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc920b45a1223c1528565fd604812590fd688bc1",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40aefee037971cf3d6b6efb316c7b3e51c7517f4",
                    "width": 216,
                    "height": 150
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dd6d6bf1658fd7fe587e5003dba9ba42a7ad36a",
                    "width": 320,
                    "height": 223
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7c9ff380a5c67f510c3b2b1cf4849772e667cf4",
                    "width": 640,
                    "height": 446
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce4eccc69c232a328a2c24e817b01c27f77cdfd0",
                    "width": 960,
                    "height": 669
                  },
                  {
                    "url": "https://preview.redd.it/lr7kq1452tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af4b94b12d7167b70c16f924fc869a65fe847a07",
                    "width": 1080,
                    "height": 752
                  }
                ],
                "variants": {},
                "id": "LwTV7b9YJU3kHEoNxw0COXZ7bbZXrcGjWQcefFCOVzU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzhqz8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gilgameshcomputing",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/",
          "stickied": false,
          "url": "https://i.redd.it/lr7kq1452tcf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752484685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**Testing method** \n\n* For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.\n* If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.\n* Only one question couldn't be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.\n* Note that quantizations are not same. It's just me, trying to find the best reasoning &amp; coding model for my setup. \n\n**Coloring strategy:**\n\n* Mark the solution green if it's accepted.\n* Use red if it fails in the pre-test cases.\n* Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.\n* Use orange if it fails in the test cases but still manages to pass over 90%.\n\n**A few observations:**\n\n* Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn‚Äôt treat them as failures, since they were limited to single character issues that clearly qualify as typos.\n* Hunyuan fell short of my expectations.\n* Qwen-32B and OpenCodeReasoning model both performed better than expected.\n* The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.\n\n**Hardware: 2x H100**\n\n**Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)**\n\nFeel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.\n\n**Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills**, since everyday programming tasks faced by typical users are usually far less complex.\n\nAll questions are recent, with no data leakage involved. So don‚Äôt come back saying ‚ÄúLeetCode problems are easy for models, this test isn‚Äôt meaningful‚Äù. It's just your test questions have been seen by the model before.\n\n",
          "author_fullname": "t2_slwqrxz3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhns3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 133,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 133,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WppDZCrZ0xycGtKVlAsBndunRlo8Km7IfOuHDADfvik.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752484340,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Testing method&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/li&gt;\n&lt;li&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/li&gt;\n&lt;li&gt;Only one question couldn&amp;#39;t be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.&lt;/li&gt;\n&lt;li&gt;Note that quantizations are not same. It&amp;#39;s just me, trying to find the best reasoning &amp;amp; coding model for my setup. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Coloring strategy:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Mark the solution green if it&amp;#39;s accepted.&lt;/li&gt;\n&lt;li&gt;Use red if it fails in the pre-test cases.&lt;/li&gt;\n&lt;li&gt;Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.&lt;/li&gt;\n&lt;li&gt;Use orange if it fails in the test cases but still manages to pass over 90%.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;A few observations:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn‚Äôt treat them as failures, since they were limited to single character issues that clearly qualify as typos.&lt;/li&gt;\n&lt;li&gt;Hunyuan fell short of my expectations.&lt;/li&gt;\n&lt;li&gt;Qwen-32B and OpenCodeReasoning model both performed better than expected.&lt;/li&gt;\n&lt;li&gt;The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware: 2x H100&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Keep in mind that strong performance on LeetCode doesn&amp;#39;t automatically reflect real world coding skills&lt;/strong&gt;, since everyday programming tasks faced by typical users are usually far less complex.&lt;/p&gt;\n\n&lt;p&gt;All questions are recent, with no data leakage involved. So don‚Äôt come back saying ‚ÄúLeetCode problems are easy for models, this test isn‚Äôt meaningful‚Äù. It&amp;#39;s just your test questions have been seen by the model before.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nyu5vpzx2tcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?auto=webp&amp;s=e45702995060cc687a645562d2df2d39d92ccdf8",
                  "width": 1565,
                  "height": 408
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ba57d01c877fa0d1bec3f4aef3af9baaad55463",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf11fe9849c7a2c0833889c03bd68754c8b4e45",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d5e29a1cd3ba5283fb529c5d5457c3cce7a36a0",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f48f8bee49ad1c403134b86ad6d3fc3d3c55b4",
                    "width": 640,
                    "height": 166
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=44e9301ed958519990d12a001176c79624b7a9fe",
                    "width": 960,
                    "height": 250
                  },
                  {
                    "url": "https://preview.redd.it/nyu5vpzx2tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7efed2bc4442573f344c5203ab1b05379a7429be",
                    "width": 1080,
                    "height": 281
                  }
                ],
                "variants": {},
                "id": "Yt8sdbd4WSl3QWw399ju3ntGhqCOHF8RdVFnkafe5Hs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzhns3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kyazoglu",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/",
          "stickied": false,
          "url": "https://i.redd.it/nyu5vpzx2tcf1.png",
          "subreddit_subscribers": 499292,
          "created_utc": 1752484340,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm building a local AI agent system using n8n to handle technical SQL Q&amp;A and dashboard generation based on database results ‚Äî with tool execution via MCP Server.\n\nMy setup:\n\nGPU: NVIDIA A10 (24GB VRAM)\n\nSo I‚Äôm limited to small to medium models (&lt;=14B): \n\n* llama3.1:8b-instruct-fp16\n* qwen2.5:14b-instruct\n\nIssue: Even though I explicitly prompt with something like: ‚ÄúStep 1: Call Tool A to run a SQL query. Step 2: Use Tool B to generate a chart.‚Äù ‚Ä¶the models just don‚Äôt follow ‚Äî they output generic answers instead of invoking the defined tools.:  \nFOR example:  \n\\-------------------------------------------------------------------------------\n\nquestion: can you name top sql id that make low cpu?  \n\\-------------------------------------------------------------------------------\n\nanswer:    \n\\-------------------------------------------------------------------------------  \nStep 0: Execute tool¬†`schema`¬†to get all information about Tables Definition for next step.\n\n{\"name\": \"schema\", \"parameters\": {}}\n\nStep 1: Try to execute tool¬†`Execute_sql_query`¬†based on user question to get new data. Always limit 25.\n\nSince the user asked about performance c·ªßa database theo aas kh√¥ng t√≥m t·∫Øt, I will assume they want to know the performance metrics of the database for AAS (Application and Service Architecture) without summarization. Here is the SQL query:\n\nSELECT \\* FROM public\\_2.aas\\_performance LIMIT 25;  \n... etc  \n\\----------------------------------------------------------------------------------------  \n\n\nI tested the same prompt with GPT-4, and it executes each step correctly ‚Äî calls tools properly, reasons well, and behaves exactly as expected.  \nHas anyone found a small-to-mid size local model that can reliably follow structured, tool-calling prompts like GPT-4 does or any technique that can fix this issue",
          "author_fullname": "t2_efnhbt7w0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "local model for SQL Q&amp;A + dashboard agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzhlvb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752484127,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm building a local AI agent system using n8n to handle technical SQL Q&amp;amp;A and dashboard generation based on database results ‚Äî with tool execution via MCP Server.&lt;/p&gt;\n\n&lt;p&gt;My setup:&lt;/p&gt;\n\n&lt;p&gt;GPU: NVIDIA A10 (24GB VRAM)&lt;/p&gt;\n\n&lt;p&gt;So I‚Äôm limited to small to medium models (&amp;lt;=14B): &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;llama3.1:8b-instruct-fp16&lt;/li&gt;\n&lt;li&gt;qwen2.5:14b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Issue: Even though I explicitly prompt with something like: ‚ÄúStep 1: Call Tool A to run a SQL query. Step 2: Use Tool B to generate a chart.‚Äù ‚Ä¶the models just don‚Äôt follow ‚Äî they output generic answers instead of invoking the defined tools.:&lt;br/&gt;\nFOR example:&lt;br/&gt;\n-------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;question: can you name top sql id that make low cpu?&lt;br/&gt;\n-------------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;answer:&lt;br/&gt;\n-------------------------------------------------------------------------------&lt;br/&gt;\nStep 0: Execute tool¬†&lt;code&gt;schema&lt;/code&gt;¬†to get all information about Tables Definition for next step.&lt;/p&gt;\n\n&lt;p&gt;{&amp;quot;name&amp;quot;: &amp;quot;schema&amp;quot;, &amp;quot;parameters&amp;quot;: {}}&lt;/p&gt;\n\n&lt;p&gt;Step 1: Try to execute tool¬†&lt;code&gt;Execute_sql_query&lt;/code&gt;¬†based on user question to get new data. Always limit 25.&lt;/p&gt;\n\n&lt;p&gt;Since the user asked about performance c·ªßa database theo aas kh√¥ng t√≥m t·∫Øt, I will assume they want to know the performance metrics of the database for AAS (Application and Service Architecture) without summarization. Here is the SQL query:&lt;/p&gt;\n\n&lt;p&gt;SELECT * FROM public_2.aas_performance LIMIT 25;&lt;br/&gt;\n... etc&lt;br/&gt;\n----------------------------------------------------------------------------------------  &lt;/p&gt;\n\n&lt;p&gt;I tested the same prompt with GPT-4, and it executes each step correctly ‚Äî calls tools properly, reasons well, and behaves exactly as expected.&lt;br/&gt;\nHas anyone found a small-to-mid size local model that can reliably follow structured, tool-calling prompts like GPT-4 does or any technique that can fix this issue&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzhlvb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Practical-Corgi-9906",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzhlvb/local_model_for_sql_qa_dashboard_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzhlvb/local_model_for_sql_qa_dashboard_agent/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752484127,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently using Gemini 2.5 pro, and I seem to be using about $100 per month. I plan to increase the usage by 10 fold, so then I thought of using my 4090+3090 on open source models as a possibility cheaper alternative (and protect my assets). I'm currently testing Deep seek r1 70b and 8b. 70b takes a while, 8b seems much faster, but I continued using Gemini because of the context window.\n\nNow I'm just wondering if deepseek r1 is my best bet for programming locally or Kimi 2 is worth more, even if the inference it's much slower? Or something else? \n\nAnd perhaps I should be using some better flavor than pure Deep seek r1?",
          "author_fullname": "t2_cv5ft",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What best model(s) to use for inference using a 4090+3090 for Aider?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzh0cf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752481760,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using Gemini 2.5 pro, and I seem to be using about $100 per month. I plan to increase the usage by 10 fold, so then I thought of using my 4090+3090 on open source models as a possibility cheaper alternative (and protect my assets). I&amp;#39;m currently testing Deep seek r1 70b and 8b. 70b takes a while, 8b seems much faster, but I continued using Gemini because of the context window.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m just wondering if deepseek r1 is my best bet for programming locally or Kimi 2 is worth more, even if the inference it&amp;#39;s much slower? Or something else? &lt;/p&gt;\n\n&lt;p&gt;And perhaps I should be using some better flavor than pure Deep seek r1?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzh0cf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cGalaxy",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752481760,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello!\n\nI have a server on my network housing the RTX Pro 6000. I'd like to run a few models so that I can 1. Generate video (open to the interface used, but it seems like comfyui works well) and 2. Run a chat (likely with openwebui).\n\nMy question is, what is the most efficient way to run the models? Openllama? I prefer to run it dockerized, but it seems you can really fine tune things using pytorch? openllama i have used, but pytorch i am not familiar with. I am willing to run the models baremetal if it is significantly more efficient/performant.\n\nIt would also be beneficial if the program would automatically load/unload models based on their usage as it would be someone non-technical utilizing them and likely not always at the same time with long periods of non-use.\n\nAny tips would be appreciated.  Feel free to roast me as long as I can learn something from it ;)",
          "author_fullname": "t2_7xyy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to run dockerized linux LLM server?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzggo2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752479561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I have a server on my network housing the RTX Pro 6000. I&amp;#39;d like to run a few models so that I can 1. Generate video (open to the interface used, but it seems like comfyui works well) and 2. Run a chat (likely with openwebui).&lt;/p&gt;\n\n&lt;p&gt;My question is, what is the most efficient way to run the models? Openllama? I prefer to run it dockerized, but it seems you can really fine tune things using pytorch? openllama i have used, but pytorch i am not familiar with. I am willing to run the models baremetal if it is significantly more efficient/performant.&lt;/p&gt;\n\n&lt;p&gt;It would also be beneficial if the program would automatically load/unload models based on their usage as it would be someone non-technical utilizing them and likely not always at the same time with long periods of non-use.&lt;/p&gt;\n\n&lt;p&gt;Any tips would be appreciated.  Feel free to roast me as long as I can learn something from it ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzggo2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "a_40oz_of_Mickeys",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752479561,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Do you have any good uses cases for using the stop-sequence functionality when calling the API?\n\nList them below, please.",
          "author_fullname": "t2_8h2i7wiei",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stop-Sequences - Real World Use Cases",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfwdj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752477324,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have any good uses cases for using the stop-sequence functionality when calling the API?&lt;/p&gt;\n\n&lt;p&gt;List them below, please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzfwdj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Physical_Ad9040",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfwdj/stopsequences_real_world_use_cases/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzfwdj/stopsequences_real_world_use_cases/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752477324,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Regarding the design of tools, I want the LLM to generate files directly for the user. My current approach is:\nDefine a tool: \n```\ngen_file\nargs: {\n  file_name:\n  content:\n  append:\n}\n```\nHowever, I now have a different perspective. Is it really reasonable to use `content` as an argument for a tool call? Do long tool calls pose any problems for LLMs?",
          "author_fullname": "t2_1hlcy0hukk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "about LLM tools design",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfsxt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752476933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Regarding the design of tools, I want the LLM to generate files directly for the user. My current approach is:\nDefine a tool: \n&lt;code&gt;\ngen_file\nargs: {\n  file_name:\n  content:\n  append:\n}\n&lt;/code&gt;\nHowever, I now have a different perspective. Is it really reasonable to use &lt;code&gt;content&lt;/code&gt; as an argument for a tool call? Do long tool calls pose any problems for LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzfsxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dizzy-Meet-3258",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfsxt/about_llm_tools_design/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzfsxt/about_llm_tools_design/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752476933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4](https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4) (paywall)\n\nI don't know how the French and European authorities could accept this.",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Apple ‚Äúwill seriously consider‚Äù buying Mistral | Bloomberg - Mark Gurman",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 117,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfhhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 536,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 536,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Oi81Df9SQch6JRWkXBdb70VbCbc5PHkhZHq2yJRoex0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752475719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4\"&gt;https://www.bloomberg.com/news/newsletters/2025-07-13/is-apple-going-to-replace-ceo-tim-cook-who-is-the-next-ceo-of-apple-ternus-md1mhrj4&lt;/a&gt; (paywall)&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know how the French and European authorities could accept this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/syyfccpldscf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/syyfccpldscf1.jpeg?auto=webp&amp;s=6c1efde69cd327275f5e2033e00c0702e28920d1",
                  "width": 1662,
                  "height": 1390
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89bbf9ebe8ad920a9decea85a04e8ddddce9143e",
                    "width": 108,
                    "height": 90
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf248f7dbf4358bb70f07ee9c1da43f8a5a1428d",
                    "width": 216,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbbee1c5b59b74677e14df9bdb2c959132e1da7b",
                    "width": 320,
                    "height": 267
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c267b676d172a191872cfbacda802bec7e6a2e8",
                    "width": 640,
                    "height": 535
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=940d958a3d989315f06b79867efa25519d7abd78",
                    "width": 960,
                    "height": 802
                  },
                  {
                    "url": "https://preview.redd.it/syyfccpldscf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=121ecef6d4b87299bfabd79e10028f6748fd5beb",
                    "width": 1080,
                    "height": 903
                  }
                ],
                "variants": {},
                "id": "QahBk6E1a44oImzdfLzsg1gZn_nkm-ZxKpD4RbU9wnc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lzfhhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 219,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfhhq/apple_will_seriously_consider_buying_mistral/",
          "stickied": false,
          "url": "https://i.redd.it/syyfccpldscf1.jpeg",
          "subreddit_subscribers": 499292,
          "created_utc": 1752475719,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We‚Äôve launched dedicated GPU clusters (India &amp; US zones) with no waitlist. Mostly serving inference, fine-tuning, and SDXL use cases. \n\n* A100 / H100 / L40S \n* Hourly or monthly billing \n* Accessible via REST or container \n\nIf anyone needs GPUs for open-source models, happy to offer test credits on [cyfuture.ai](https://cyfuture.ai).",
          "author_fullname": "t2_1bv91ygeod",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for affordable dedicated GPUs (A100, H100) outside AWS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzfdiw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.28,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752475292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We‚Äôve launched dedicated GPU clusters (India &amp;amp; US zones) with no waitlist. Mostly serving inference, fine-tuning, and SDXL use cases. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A100 / H100 / L40S &lt;/li&gt;\n&lt;li&gt;Hourly or monthly billing &lt;/li&gt;\n&lt;li&gt;Accessible via REST or container &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If anyone needs GPUs for open-source models, happy to offer test credits on &lt;a href=\"https://cyfuture.ai\"&gt;cyfuture.ai&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzfdiw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Trash_9030",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzfdiw/looking_for_affordable_dedicated_gpus_a100_h100/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzfdiw/looking_for_affordable_dedicated_gpus_a100_h100/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752475292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to do voice dubbing but since I have started I am not being to achieve audible output... The videos are in English I transcrbe then in English then I translate the text in french, then when I try to get the traduced text to be read with the text to speech it gives me a bunch of gibberish, I am asking myself if it's an issue with the M1 processor or the script I don't get it... The videos are short, between 1 min to 3 min... Below is the script I use:\n\n\n#!/usr/bin/env python3\nimport torch\nimport gradio as gr\nimport librosa\nimport numpy as np\nfrom chatterbox.tts import ChatterboxTTS\nimport tempfile\nimport os\nimport importlib.util  # For dependency checking\n\n# Define sampling rate (Chatterbox uses 22.05kHz)\nSAMPLING_RATE = 22050\n\n# Check if soundfile is available\nif importlib.util.find_spec(\"soundfile\"):\n    import soundfile as sf\n    has_soundfile = True\nelse:\n    print(\"Warning: soundfile not installed. Using scipy.io.wavfile instead.\")\n    from scipy.io import wavfile\n    has_soundfile = False\n\n# Initialize TTS model\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\ntts_model = ChatterboxTTS.from_pretrained(device=device)\n\ndef preprocess_french_text(text):\n    \"\"\"Preprocess French text for better TTS pronunciation\"\"\"\n    # Simple normalization - expand common abbreviations\n    replacements = {\n        \"M.\": \"Monsieur\",\n        \"Mme\": \"Madame\",\n        \"Mlle\": \"Mademoiselle\",\n        \"Dr.\": \"Docteur\",\n        \"St.\": \"Saint\",\n        \"n¬∞\": \"num√©ro\",\n        \"&amp;\": \"et\"\n    }\n    \n    for abbr, full in replacements.items():\n        text = text.replace(abbr, full)\n    \n    return text\n\ndef preprocess_voice_sample(voice_path):\n    \"\"\"Preprocess voice sample to meet Chatterbox requirements\"\"\"\n    if not voice_path or not os.path.exists(voice_path):\n        return None\n    \n    try:\n        # Load audio and convert to mono\n        y, sr = librosa.load(voice_path, sr=SAMPLING_RATE, mono=True)\n        \n        # Trim to 5 seconds (Chatterbox's optimal length)\n        max_samples = 5 * SAMPLING_RATE\n        if len(y) &gt; max_samples:\n            y = y[:max_samples]\n        \n        # Save processed sample to temporary file\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n            if has_soundfile:\n                sf.write(tmpfile.name, y, SAMPLING_RATE)\n            else:\n                wavfile.write(tmpfile.name, SAMPLING_RATE, (y * 32767).astype(np.int16))\n            return tmpfile.name\n    except Exception as e:\n        print(f\"Voice preprocessing error: {e}\")\n        return voice_path  # Fallback to original\n\ndef ensure_mono(audio):\n    \"\"\"Convert audio to mono (1D array) if it's stereo\"\"\"\n    if audio.ndim &gt; 1:\n        return np.mean(audio, axis=1)\n    return audio\n\ndef generate_tts_segment(text, voice_sample_path=None, exaggeration=0.5, cfg_weight=0.7, pace=1.0):\n    \"\"\"Generate French TTS audio for text segment\"\"\"\n    # Preprocess French text\n    text = preprocess_french_text(text)\n    \n    params = {\n        \"text\": text,\n        \"exaggeration\": exaggeration,\n        \"cfg_weight\": cfg_weight\n    }\n    \n    if voice_sample_path and os.path.exists(voice_sample_path):\n        params[\"audio_prompt_path\"] = voice_sample_path\n    \n    # Generate audio (returns a PyTorch tensor)\n    audio_tensor = tts_model.generate(**params)\n    \n    # Convert tensor to numpy array\n    audio = audio_tensor.cpu().numpy().astype(np.float32)\n    \n    # Ensure mono audio\n    audio = ensure_mono(audio)\n    \n    # Normalize audio to avoid clipping\n    max_val = np.max(np.abs(audio))\n    if max_val &gt; 0:\n        audio = audio / max_val\n    \n    # Apply pace adjustment\n    if pace != 1.0:\n        audio = librosa.effects.time_stretch(audio, rate=pace)\n    \n    return audio\n\ndef process_text_file(text_file, voice_sample=None, exaggeration=0.5, cfg_weight=0.7, pause_duration=0.5, pace=1.0):\n    \"\"\"Process text file and generate concatenated audio\"\"\"\n    # Get actual file path\n    txt_path = text_file.name\n    \n    # Preprocess voice sample if provided\n    preprocessed_voice_path = None\n    if voice_sample:\n        preprocessed_voice_path = preprocess_voice_sample(voice_sample)\n    \n    try:\n        with open(txt_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n    except Exception as e:\n        yield f\"Error opening text file: {str(e)}\", None\n        return\n    \n    # Split text into paragraphs\n    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n    \n    full_audio = np.array([], dtype=np.float32)\n    pause_samples = int(pause_duration * SAMPLING_RATE)\n    \n    for i, paragraph in enumerate(paragraphs):\n        try:\n            # Generate audio for paragraph\n            segment = generate_tts_segment(\n                text=paragraph,\n                voice_sample_path=preprocessed_voice_path,\n                exaggeration=exaggeration,\n                cfg_weight=cfg_weight,\n                pace=pace\n            )\n            full_audio = np.concatenate([full_audio, segment])\n            \n            # Add pause between paragraphs (except after last one)\n            if i &lt; len(paragraphs) - 1:\n                full_audio = np.concatenate([full_audio, np.zeros(pause_samples)])\n        except Exception as e:\n            yield f\"Error processing paragraph {i+1}: {str(e)}\", None\n            return\n        \n        yield f\"Processing paragraph {i+1}/{len(paragraphs)}\", None\n        \n    # Clean up temporary voice file\n    if preprocessed_voice_path and os.path.exists(preprocessed_voice_path):\n        try:\n            os.remove(preprocessed_voice_path)\n        except Exception:\n            pass  # Ignore cleanup errors\n        \n    # Save to temporary file\n    try:\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n            output_path = tmpfile.name\n            if has_soundfile:\n                sf.write(output_path, full_audio, SAMPLING_RATE)\n            else:\n                wavfile.write(output_path, SAMPLING_RATE, (full_audio * 32767).astype(np.int16))\n        yield \"Audio generated successfully!\", output_path\n    except Exception as e:\n        yield f\"Audio save error: {str(e)}\", None\n\n# Gradio UI\nwith gr.Blocks(title=\"French Text Audio Synthesizer\") as ui:\n    gr.Markdown(\"# üéß French Text-to-Speech Generator\")\n    gr.Markdown(\"Generate French audio from .txt files with natural pauses\")\n    \n    with gr.Row():\n        with gr.Column():\n            text_input = gr.File(label=\"Text File\", file_types=[\".txt\"])\n            voice_input = gr.Audio(\n                label=\"Voice Sample (Optional)\",\n                type=\"filepath\",\n                sources=[\"upload\"],\n                format=\"wav\"\n            )\n            emotion_slider = gr.Slider(0.0, 1.0, 0.5, label=\"Emotion Intensity\")\n            pause_slider = gr.Slider(0.0, 2.0, 0.5, label=\"Pause Duration (seconds)\")\n            pace_slider = gr.Slider(0.5, 1.5, 1.0, label=\"Speech Pace\")\n            generate_btn = gr.Button(\"Generate Audio\")\n        \n        with gr.Column():\n            status = gr.Textbox(label=\"Status\", interactive=False)\n            audio_output = gr.Audio(label=\"Generated Audio\", type=\"filepath\")\n    \n    generate_btn.click(\n        fn=process_text_file,\n        inputs=[text_input, voice_input, emotion_slider, pause_slider, pace_slider],\n        outputs=[status, audio_output]\n    )\n\nif __name__ == \"__main__\":\n    ui.launch(server_port=7860)\n",
          "author_fullname": "t2_77qlzs20",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Xttsv2 model, Chatterbox on MacBook air 8 gb",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzf6zi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752474601,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to do voice dubbing but since I have started I am not being to achieve audible output... The videos are in English I transcrbe then in English then I translate the text in french, then when I try to get the traduced text to be read with the text to speech it gives me a bunch of gibberish, I am asking myself if it&amp;#39;s an issue with the M1 processor or the script I don&amp;#39;t get it... The videos are short, between 1 min to 3 min... Below is the script I use:&lt;/p&gt;\n\n&lt;h1&gt;!/usr/bin/env python3&lt;/h1&gt;\n\n&lt;p&gt;import torch\nimport gradio as gr\nimport librosa\nimport numpy as np\nfrom chatterbox.tts import ChatterboxTTS\nimport tempfile\nimport os\nimport importlib.util  # For dependency checking&lt;/p&gt;\n\n&lt;h1&gt;Define sampling rate (Chatterbox uses 22.05kHz)&lt;/h1&gt;\n\n&lt;p&gt;SAMPLING_RATE = 22050&lt;/p&gt;\n\n&lt;h1&gt;Check if soundfile is available&lt;/h1&gt;\n\n&lt;p&gt;if importlib.util.find_spec(&amp;quot;soundfile&amp;quot;):\n    import soundfile as sf\n    has_soundfile = True\nelse:\n    print(&amp;quot;Warning: soundfile not installed. Using scipy.io.wavfile instead.&amp;quot;)\n    from scipy.io import wavfile\n    has_soundfile = False&lt;/p&gt;\n\n&lt;h1&gt;Initialize TTS model&lt;/h1&gt;\n\n&lt;p&gt;device = &amp;quot;mps&amp;quot; if torch.backends.mps.is_available() else &amp;quot;cpu&amp;quot;\ntts_model = ChatterboxTTS.from_pretrained(device=device)&lt;/p&gt;\n\n&lt;p&gt;def preprocess_french_text(text):\n    &amp;quot;&amp;quot;&amp;quot;Preprocess French text for better TTS pronunciation&amp;quot;&amp;quot;&amp;quot;\n    # Simple normalization - expand common abbreviations\n    replacements = {\n        &amp;quot;M.&amp;quot;: &amp;quot;Monsieur&amp;quot;,\n        &amp;quot;Mme&amp;quot;: &amp;quot;Madame&amp;quot;,\n        &amp;quot;Mlle&amp;quot;: &amp;quot;Mademoiselle&amp;quot;,\n        &amp;quot;Dr.&amp;quot;: &amp;quot;Docteur&amp;quot;,\n        &amp;quot;St.&amp;quot;: &amp;quot;Saint&amp;quot;,\n        &amp;quot;n¬∞&amp;quot;: &amp;quot;num√©ro&amp;quot;,\n        &amp;quot;&amp;amp;&amp;quot;: &amp;quot;et&amp;quot;\n    }&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for abbr, full in replacements.items():\n    text = text.replace(abbr, full)\n\nreturn text\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def preprocess_voice_sample(voice_path):\n    &amp;quot;&amp;quot;&amp;quot;Preprocess voice sample to meet Chatterbox requirements&amp;quot;&amp;quot;&amp;quot;\n    if not voice_path or not os.path.exists(voice_path):\n        return None&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;try:\n    # Load audio and convert to mono\n    y, sr = librosa.load(voice_path, sr=SAMPLING_RATE, mono=True)\n\n    # Trim to 5 seconds (Chatterbox&amp;#39;s optimal length)\n    max_samples = 5 * SAMPLING_RATE\n    if len(y) &amp;gt; max_samples:\n        y = y[:max_samples]\n\n    # Save processed sample to temporary file\n    with tempfile.NamedTemporaryFile(suffix=&amp;quot;.wav&amp;quot;, delete=False) as tmpfile:\n        if has_soundfile:\n            sf.write(tmpfile.name, y, SAMPLING_RATE)\n        else:\n            wavfile.write(tmpfile.name, SAMPLING_RATE, (y * 32767).astype(np.int16))\n        return tmpfile.name\nexcept Exception as e:\n    print(f&amp;quot;Voice preprocessing error: {e}&amp;quot;)\n    return voice_path  # Fallback to original\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def ensure_mono(audio):\n    &amp;quot;&amp;quot;&amp;quot;Convert audio to mono (1D array) if it&amp;#39;s stereo&amp;quot;&amp;quot;&amp;quot;\n    if audio.ndim &amp;gt; 1:\n        return np.mean(audio, axis=1)\n    return audio&lt;/p&gt;\n\n&lt;p&gt;def generate_tts_segment(text, voice_sample_path=None, exaggeration=0.5, cfg_weight=0.7, pace=1.0):\n    &amp;quot;&amp;quot;&amp;quot;Generate French TTS audio for text segment&amp;quot;&amp;quot;&amp;quot;\n    # Preprocess French text\n    text = preprocess_french_text(text)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;params = {\n    &amp;quot;text&amp;quot;: text,\n    &amp;quot;exaggeration&amp;quot;: exaggeration,\n    &amp;quot;cfg_weight&amp;quot;: cfg_weight\n}\n\nif voice_sample_path and os.path.exists(voice_sample_path):\n    params[&amp;quot;audio_prompt_path&amp;quot;] = voice_sample_path\n\n# Generate audio (returns a PyTorch tensor)\naudio_tensor = tts_model.generate(**params)\n\n# Convert tensor to numpy array\naudio = audio_tensor.cpu().numpy().astype(np.float32)\n\n# Ensure mono audio\naudio = ensure_mono(audio)\n\n# Normalize audio to avoid clipping\nmax_val = np.max(np.abs(audio))\nif max_val &amp;gt; 0:\n    audio = audio / max_val\n\n# Apply pace adjustment\nif pace != 1.0:\n    audio = librosa.effects.time_stretch(audio, rate=pace)\n\nreturn audio\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def process_text_file(text_file, voice_sample=None, exaggeration=0.5, cfg_weight=0.7, pause_duration=0.5, pace=1.0):\n    &amp;quot;&amp;quot;&amp;quot;Process text file and generate concatenated audio&amp;quot;&amp;quot;&amp;quot;\n    # Get actual file path\n    txt_path = text_file.name&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Preprocess voice sample if provided\npreprocessed_voice_path = None\nif voice_sample:\n    preprocessed_voice_path = preprocess_voice_sample(voice_sample)\n\ntry:\n    with open(txt_path, &amp;#39;r&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f:\n        text = f.read()\nexcept Exception as e:\n    yield f&amp;quot;Error opening text file: {str(e)}&amp;quot;, None\n    return\n\n# Split text into paragraphs\nparagraphs = [p.strip() for p in text.split(&amp;#39;\\n\\n&amp;#39;) if p.strip()]\n\nfull_audio = np.array([], dtype=np.float32)\npause_samples = int(pause_duration * SAMPLING_RATE)\n\nfor i, paragraph in enumerate(paragraphs):\n    try:\n        # Generate audio for paragraph\n        segment = generate_tts_segment(\n            text=paragraph,\n            voice_sample_path=preprocessed_voice_path,\n            exaggeration=exaggeration,\n            cfg_weight=cfg_weight,\n            pace=pace\n        )\n        full_audio = np.concatenate([full_audio, segment])\n\n        # Add pause between paragraphs (except after last one)\n        if i &amp;lt; len(paragraphs) - 1:\n            full_audio = np.concatenate([full_audio, np.zeros(pause_samples)])\n    except Exception as e:\n        yield f&amp;quot;Error processing paragraph {i+1}: {str(e)}&amp;quot;, None\n        return\n\n    yield f&amp;quot;Processing paragraph {i+1}/{len(paragraphs)}&amp;quot;, None\n\n# Clean up temporary voice file\nif preprocessed_voice_path and os.path.exists(preprocessed_voice_path):\n    try:\n        os.remove(preprocessed_voice_path)\n    except Exception:\n        pass  # Ignore cleanup errors\n\n# Save to temporary file\ntry:\n    with tempfile.NamedTemporaryFile(suffix=&amp;quot;.wav&amp;quot;, delete=False) as tmpfile:\n        output_path = tmpfile.name\n        if has_soundfile:\n            sf.write(output_path, full_audio, SAMPLING_RATE)\n        else:\n            wavfile.write(output_path, SAMPLING_RATE, (full_audio * 32767).astype(np.int16))\n    yield &amp;quot;Audio generated successfully!&amp;quot;, output_path\nexcept Exception as e:\n    yield f&amp;quot;Audio save error: {str(e)}&amp;quot;, None\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Gradio UI&lt;/h1&gt;\n\n&lt;p&gt;with gr.Blocks(title=&amp;quot;French Text Audio Synthesizer&amp;quot;) as ui:\n    gr.Markdown(&amp;quot;# üéß French Text-to-Speech Generator&amp;quot;)\n    gr.Markdown(&amp;quot;Generate French audio from .txt files with natural pauses&amp;quot;)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;with gr.Row():\n    with gr.Column():\n        text_input = gr.File(label=&amp;quot;Text File&amp;quot;, file_types=[&amp;quot;.txt&amp;quot;])\n        voice_input = gr.Audio(\n            label=&amp;quot;Voice Sample (Optional)&amp;quot;,\n            type=&amp;quot;filepath&amp;quot;,\n            sources=[&amp;quot;upload&amp;quot;],\n            format=&amp;quot;wav&amp;quot;\n        )\n        emotion_slider = gr.Slider(0.0, 1.0, 0.5, label=&amp;quot;Emotion Intensity&amp;quot;)\n        pause_slider = gr.Slider(0.0, 2.0, 0.5, label=&amp;quot;Pause Duration (seconds)&amp;quot;)\n        pace_slider = gr.Slider(0.5, 1.5, 1.0, label=&amp;quot;Speech Pace&amp;quot;)\n        generate_btn = gr.Button(&amp;quot;Generate Audio&amp;quot;)\n\n    with gr.Column():\n        status = gr.Textbox(label=&amp;quot;Status&amp;quot;, interactive=False)\n        audio_output = gr.Audio(label=&amp;quot;Generated Audio&amp;quot;, type=&amp;quot;filepath&amp;quot;)\n\ngenerate_btn.click(\n    fn=process_text_file,\n    inputs=[text_input, voice_input, emotion_slider, pause_slider, pace_slider],\n    outputs=[status, audio_output]\n)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    ui.launch(server_port=7860)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lzf6zi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Layonkizungu",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzf6zi/xttsv2_model_chatterbox_on_macbook_air_8_gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzf6zi/xttsv2_model_chatterbox_on_macbook_air_8_gb/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752474601,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.\n\nQ: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?",
          "author_fullname": "t2_1fp7huwh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can VRAM be combined of 2 brands",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lze20x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752470433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.&lt;/p&gt;\n\n&lt;p&gt;Q: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lze20x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tonyleungnl",
          "discussion_type": null,
          "num_comments": 85,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752470433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It's very cool to watch it do it's thing. Make sure to use the --diffusion-visual flag. It's still a PR but has been approved so it should be merged soon.",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Diffusion model support in llama.cpp.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lze1r3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 139,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 139,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=77e8aaca890b1dc8486701afa4da4f4e06d486be",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752470404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was browsing the llama.cpp PRs and saw that Am17an has added diffusion model support in llama.cpp. It works. It&amp;#39;s very cool to watch it do it&amp;#39;s thing. Make sure to use the --diffusion-visual flag. It&amp;#39;s still a PR but has been approved so it should be merged soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14644",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?auto=webp&amp;s=e183ff7e541a319425a36dcaf9b80b74c4ff9243",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f661fc69800730ffa673f5aa97b47d5b9e191899",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4c5410f2c6927b2130b0b8edfce13fc0ce8cd59",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aab06d8dbc7317d7ae49105599d33bc04e0c66cf",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c75c6786f093153f6a5dc5065d5f9e2b741b5086",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a1002549b1b328880a986274b59212fd91c0e1f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01b756f299543d2fda31db392dfcbc407ad0faa7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "X6RZ_QwBHXWQcBNsgWEp_Ow5ef9fjjqJddTY6M9a0cA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lze1r3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lze1r3/diffusion_model_support_in_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14644",
          "subreddit_subscribers": 499292,
          "created_utc": 1752470404,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created [**TorchLeet**](https://github.com/Exorust/TorchLeet)! It's a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.\n\nIt‚Äôs split into:\n\n* **PyTorch Problems** (Basic ‚Üí Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability\n* **LLM Problems**: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.\n\n  \nI'd love feedback from the community and help taking this forward!",
          "author_fullname": "t2_7zqffw4e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Practice Pytorch like Leetcode? (Also with cool LLM questions)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzdu0l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752469640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created &lt;a href=\"https://github.com/Exorust/TorchLeet\"&gt;&lt;strong&gt;TorchLeet&lt;/strong&gt;&lt;/a&gt;! It&amp;#39;s a collection of PyTorch and LLM problems inspired by real convos with researchers, engineers, and interview prep.&lt;/p&gt;\n\n&lt;p&gt;It‚Äôs split into:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch Problems&lt;/strong&gt; (Basic ‚Üí Hard): CNNs, RNNs, transformers, autograd, distributed training, explainability&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM Problems&lt;/strong&gt;: Build attention, RoPE, KV cache, BPE, speculative decoding, quantization, RLHF, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love feedback from the community and help taking this forward!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?auto=webp&amp;s=6fcfa2d9d7c19f8fa35afeefcbbce3bf504c2b85",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33bbaed8c03eedf7c1200615b0ca7a0d1815ce63",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c56109fdcb24e669c74e98d923ae3f1642d2e040",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08a19b35605d99c30080a37ea914e073131c141a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8084972ed24e594f6135f70051578b2e4855e936",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc05f2d37492136cc66d813fe3f3a1397d056030",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1a139fb45b571bcfb4f1178fa34f58b45cb9d7eb",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nQQojR8L8YtcMGN-GeV05jPYf4tBYwES0RlHjNkNq40"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzdu0l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "exorust_fire",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzdu0l/practice_pytorch_like_leetcode_also_with_cool_llm/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752469640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been looking all over for a frontend to simplify the use of FishSpeech, but I haven't been able to find one.\n\nI was wondering if anyone has found one ...",
          "author_fullname": "t2_hyklw8a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There will be some frontend for FishSpeech?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzdgc8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752468330,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking all over for a frontend to simplify the use of FishSpeech, but I haven&amp;#39;t been able to find one.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has found one ...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzdgc8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vk3r",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzdgc8/there_will_be_some_frontend_for_fishspeech/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzdgc8/there_will_be_some_frontend_for_fishspeech/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752468330,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:\n\n|Model|dense layer#|MoE layer#|shared|active/routed|Shared|Active|Params|Active%|fp16 kv@128k|kv%|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|DeepSeek-MoE-16B|1|27|2|6/64|1.42B|2.83B|16.38B|17.28%|28GB|85.47%|\n|DeepSeek-V2-Lite|1|26|2|6/64|1.31B|2.66B|15.71B|16.93%|3.8GB|12.09%|\n|DeepSeek-V2|1|59|2|6/160|12.98B|21.33B|235.74B|8.41%|8.44GB|1.78%|\n|DeepSeek-V3|3|58|1|8/256|17.01B|37.45B|671.03B|5.58%|8.578GB|0.64%|\n|Kimi-K2|1|60|1|8/384|11.56B|32.70B|1026.41B|3.19%|8.578GB|0.42%|\n|Qwen3-30B-A3B|0|48|0|8/128|1.53B|3.34B|30.53B|10.94%|12GB|19.65%|\n|Qwen3-235B-A22B|0|94|0|8/128|7.95B|22.14B|235.09B|9.42%|23.5GB|4.998%|\n|Llama-4-Scout-17B-16E|0|48|1|1/16|11.13B|17.17B|107.77B|15.93%|24GB|11.13%|\n|Llama-4-Maverick-17B-128E|24|24|1|1/128|14.15B|17.17B|400.71B|4.28%|24GB|2.99%|\n|Mixtral-8x7B|0|32|0|2/8|1.60B|12.88B|46.70B|27.58%|24GB|25.696%|\n|Mixtral-8x22B|0|56|0|2/8|5.33B|39.15B|140.62B|27.84%|28GB|9.956%|\n\nLooks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. \n\nModels using their own architecture is Kimi-VL and Kimi-Audio. \n\nEdited: Per u/Aaaaaaaaaeeeee 's request. I added a column called \"Shared\" which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.",
          "author_fullname": "t2_s6sfw4yy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi-K2 is a DeepSeek V3 with more experts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzcuom",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 219,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 219,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752495937,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752466353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;dense layer#&lt;/th&gt;\n&lt;th align=\"left\"&gt;MoE layer#&lt;/th&gt;\n&lt;th align=\"left\"&gt;shared&lt;/th&gt;\n&lt;th align=\"left\"&gt;active/routed&lt;/th&gt;\n&lt;th align=\"left\"&gt;Shared&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active&lt;/th&gt;\n&lt;th align=\"left\"&gt;Params&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active%&lt;/th&gt;\n&lt;th align=\"left\"&gt;fp16 kv@128k&lt;/th&gt;\n&lt;th align=\"left\"&gt;kv%&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-MoE-16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;27&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/64&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.42B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.83B&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.28%&lt;/td&gt;\n&lt;td align=\"left\"&gt;28GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;85.47%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V2-Lite&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/64&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.31B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.66B&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.71B&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.93%&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.8GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.09%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;59&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6/160&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.98B&lt;/td&gt;\n&lt;td align=\"left\"&gt;21.33B&lt;/td&gt;\n&lt;td align=\"left\"&gt;235.74B&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.41%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.44GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.78%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;58&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/256&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.01B&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.45B&lt;/td&gt;\n&lt;td align=\"left\"&gt;671.03B&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.58%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.578GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.64%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi-K2&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;60&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/384&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.56B&lt;/td&gt;\n&lt;td align=\"left\"&gt;32.70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1026.41B&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.19%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.578GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.42%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;48&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.53B&lt;/td&gt;\n&lt;td align=\"left\"&gt;3.34B&lt;/td&gt;\n&lt;td align=\"left\"&gt;30.53B&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.94%&lt;/td&gt;\n&lt;td align=\"left\"&gt;12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.65%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3-235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;94&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;8/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.95B&lt;/td&gt;\n&lt;td align=\"left\"&gt;22.14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;235.09B&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.42%&lt;/td&gt;\n&lt;td align=\"left\"&gt;23.5GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.998%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-4-Scout-17B-16E&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;48&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/16&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.13B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;107.77B&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.93%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.13%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/128&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.15B&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400.71B&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.28%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.99%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mixtral-8x7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;32&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.60B&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.88B&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.58%&lt;/td&gt;\n&lt;td align=\"left\"&gt;24GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;25.696%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mixtral-8x22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;56&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/8&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.33B&lt;/td&gt;\n&lt;td align=\"left\"&gt;39.15B&lt;/td&gt;\n&lt;td align=\"left\"&gt;140.62B&lt;/td&gt;\n&lt;td align=\"left\"&gt;27.84%&lt;/td&gt;\n&lt;td align=\"left\"&gt;28GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.956%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Looks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. &lt;/p&gt;\n\n&lt;p&gt;Models using their own architecture is Kimi-VL and Kimi-Audio. &lt;/p&gt;\n\n&lt;p&gt;Edited: Per &lt;a href=\"/u/Aaaaaaaaaeeeee\"&gt;u/Aaaaaaaaaeeeee&lt;/a&gt; &amp;#39;s request. I added a column called &amp;quot;Shared&amp;quot; which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzcuom",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Warning2146",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752466353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What would you build with that? does it give you something that is entry level, mid and top tier (consumer grade)\n\nOr does it make sense to step up to 10k? where does the incremental benefit diminish significantly as the budget increases?\n\nEdit: I think i would at a bare minimum run a 5090 on it? does that future proof most local LLM models? i would want to run things like hunyuan (tencent vid), audiogen, musicgen (Meta), musetalk, Qwen, Whisper, image gen tools.\n\ndo most of these things run below 48gb vram? i suppose that is the bottleneck? Does that mean if i want to future proof, i think something a little better. i would also want to use the rig for gaming",
          "author_fullname": "t2_4dhoqhre",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of rig would you build with a 5k budget for local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzbadq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752463759,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752461403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would you build with that? does it give you something that is entry level, mid and top tier (consumer grade)&lt;/p&gt;\n\n&lt;p&gt;Or does it make sense to step up to 10k? where does the incremental benefit diminish significantly as the budget increases?&lt;/p&gt;\n\n&lt;p&gt;Edit: I think i would at a bare minimum run a 5090 on it? does that future proof most local LLM models? i would want to run things like hunyuan (tencent vid), audiogen, musicgen (Meta), musetalk, Qwen, Whisper, image gen tools.&lt;/p&gt;\n\n&lt;p&gt;do most of these things run below 48gb vram? i suppose that is the bottleneck? Does that mean if i want to future proof, i think something a little better. i would also want to use the rig for gaming&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lzbadq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "songhaegyo",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzbadq/what_kind_of_rig_would_you_build_with_a_5k_budget/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzbadq/what_kind_of_rig_would_you_build_with_a_5k_budget/",
          "subreddit_subscribers": 499292,
          "created_utc": 1752461403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2u7dh8n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building ‚ÄúAuto-Analyst‚Äù ‚Äî A data analytics AI agentic system. LLM Agnostic can be used locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 79,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lzbad8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=140&amp;height=79&amp;crop=140:79,smart&amp;auto=webp&amp;s=7dc759be09089b99a98b8d7055e6ee537f839db7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752461401,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "firebird-technologies.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.firebird-technologies.com/p/building-auto-analyst-a-data-analytics",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?auto=webp&amp;s=978512887e76ff70477002d96947b7d9bcfa4c77",
                  "width": 788,
                  "height": 445
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7b0b8070f32c6abef26fb7d071c276a90b8747b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7058de702adf3cc481f241fd2054477ddbe04f4",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00741c3f8a782a48cf37c001446ef9bc418eccc4",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ff25f0b3f0b37cd0f676be8ed2a53bfd7286a81",
                    "width": 640,
                    "height": 361
                  }
                ],
                "variants": {},
                "id": "vEddjlfKSwBkY0H7dWZ5csxyd4YIiVuMU7XBlW3xFQ0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lzbad8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "phicreative1997",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lzbad8/building_autoanalyst_a_data_analytics_ai_agentic/",
          "stickied": false,
          "url": "https://www.firebird-technologies.com/p/building-auto-analyst-a-data-analytics",
          "subreddit_subscribers": 499292,
          "created_utc": 1752461401,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}