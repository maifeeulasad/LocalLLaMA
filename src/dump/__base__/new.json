{
  "kind": "Listing",
  "data": {
    "after": "t3_1lup8yd",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Going to get one for the whole office to use. My research indicates CPU and GPU are critical. Planning on using Jan AI, but open to other suggestions. Wanting to spend $1,000 -&gt; $2,000 on a PC, but not sure at what point we'd start hitting diminishing returns as far as CPU and GPU go. Any advice is welcome.\n\nAs an additional question: what are the downsides of using just the CPU? Does it just take longer?",
          "author_fullname": "t2_pruyzkp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "offline AI for sensitive data processing like client bank statements PDFs to CSV - recommend me a solution",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvm3tl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752075831,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Going to get one for the whole office to use. My research indicates CPU and GPU are critical. Planning on using Jan AI, but open to other suggestions. Wanting to spend $1,000 -&amp;gt; $2,000 on a PC, but not sure at what point we&amp;#39;d start hitting diminishing returns as far as CPU and GPU go. Any advice is welcome.&lt;/p&gt;\n\n&lt;p&gt;As an additional question: what are the downsides of using just the CPU? Does it just take longer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvm3tl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GetInHereStalker",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvm3tl/offline_ai_for_sensitive_data_processing_like/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvm3tl/offline_ai_for_sensitive_data_processing_like/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752075831,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m trying to repurpose my desktop as a local LLM server. Specs are:\n\n* Ryzen 7 (1st gen)\n* 48GB RAM\n* RTX 3060 12GB\n\nI want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.\n\nHere’s what I’ve tried so far:\n\n* Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. `nvidia-smi` was hit or miss.\n* Tried setting up [vLLM](https://github.com/vllm-project/vllm), but `vllm serve` throws various errors I couldn’t resolve.\n* Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.\n\nAfter 5+ hours of debugging and failure, I’m feeling stuck.\n\nCan anyone share their setup that *actually works*? I’m looking for:\n\n1. A reliable way to get the RTX 3060 working with GPU acceleration.\n2. A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.\n3. Bonus: A way to expose the model server over LAN for remote clients.\n\nThanks in advance",
          "author_fullname": "t2_54sxk5i5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help setting up a local LLM server with RTX 3060",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvm3kv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752075816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m trying to repurpose my desktop as a local LLM server. Specs are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ryzen 7 (1st gen)&lt;/li&gt;\n&lt;li&gt;48GB RAM&lt;/li&gt;\n&lt;li&gt;RTX 3060 12GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I’ve tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. &lt;code&gt;nvidia-smi&lt;/code&gt; was hit or miss.&lt;/li&gt;\n&lt;li&gt;Tried setting up &lt;a href=\"https://github.com/vllm-project/vllm\"&gt;vLLM&lt;/a&gt;, but &lt;code&gt;vllm serve&lt;/code&gt; throws various errors I couldn’t resolve.&lt;/li&gt;\n&lt;li&gt;Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After 5+ hours of debugging and failure, I’m feeling stuck.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share their setup that &lt;em&gt;actually works&lt;/em&gt;? I’m looking for:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A reliable way to get the RTX 3060 working with GPU acceleration.&lt;/li&gt;\n&lt;li&gt;A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.&lt;/li&gt;\n&lt;li&gt;Bonus: A way to expose the model server over LAN for remote clients.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?auto=webp&amp;s=cab466f000a9569548ebd3ae8abfd85c32ee31f1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38d7d905f6a5896e20f689af4bc05612592cc000",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=568fa566629c29f2e0bb183fde6d812148f6062a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=adeec7f214bf0e350cda96ec601ac78d5bc9f67a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aabc62c14eb9789f323eecff0f6dff014c9b9b8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b945a80304a01af644621d72f808cfcac8d23284",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71dcd25b6ef4ccfb096088be00e7b463415d2f2c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvm3kv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Watch-D0g",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752075816,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is how the generated TTS peak levels are -   \n  \nScreenshot: [https://ibb.co/b8mZBd5](https://ibb.co/b8mZBd5)  \n  \nIn some sentences, some words are automatically spoken at a lower volume.  \n  \nIs there a way to even the peak levels across the whole audio in Audacity?\n\nWhen I select the entire file and apply \"Normalize,\" it doesnt fix. But if I select a specific section and apply \"Normalize\" or \"Amplify,\" it increases the volume too much.\n\nManually adjusting small sections is very time consuming. Any way to do this altogether?  \n  \nIs there a way to achieve consistent peak levels from the generated TTS?",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why TTS level is not constant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvkigw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752072044,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is how the generated TTS peak levels are -   &lt;/p&gt;\n\n&lt;p&gt;Screenshot: &lt;a href=\"https://ibb.co/b8mZBd5\"&gt;https://ibb.co/b8mZBd5&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;In some sentences, some words are automatically spoken at a lower volume.  &lt;/p&gt;\n\n&lt;p&gt;Is there a way to even the peak levels across the whole audio in Audacity?&lt;/p&gt;\n\n&lt;p&gt;When I select the entire file and apply &amp;quot;Normalize,&amp;quot; it doesnt fix. But if I select a specific section and apply &amp;quot;Normalize&amp;quot; or &amp;quot;Amplify,&amp;quot; it increases the volume too much.&lt;/p&gt;\n\n&lt;p&gt;Manually adjusting small sections is very time consuming. Any way to do this altogether?  &lt;/p&gt;\n\n&lt;p&gt;Is there a way to achieve consistent peak levels from the generated TTS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?auto=webp&amp;s=68d546a83dea489666b16650161082b467b9196d",
                  "width": 1920,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=16996db8c981eb426e3bfb1aa257b73395dad7e8",
                    "width": 108,
                    "height": 49
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5763af68e0aa34fb6faf443a6413fa9f750d6add",
                    "width": 216,
                    "height": 98
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9e10f83443e028b574bba59023c6e7ac97664b2",
                    "width": 320,
                    "height": 146
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8d5625f674a61269320f0353b9961f404d7352b",
                    "width": 640,
                    "height": 292
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0517a8fb89cfc97b25fb3ec695b3fc6bc1440a57",
                    "width": 960,
                    "height": 438
                  },
                  {
                    "url": "https://external-preview.redd.it/c00BdZg0Zk7UCTeCvyjcQy1cLDooWv6U9puAXwaaWWw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8fe14a3bfca4922b4277b87e3ca2707eaae88bed",
                    "width": 1080,
                    "height": 492
                  }
                ],
                "variants": {},
                "id": "fbypF1nFoV7xdR7jbCDQqTmiS53gsgBLyw4U3H7L1as"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvkigw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvkigw/why_tts_level_is_not_constant/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752072044,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have had a lot of people applying to join our playtest, the original intent was to have it be a closed test and get feedback from our close knit community. That changed after seeing the amount of requests from people we didn't know. We are happy to announce that we will instead be making the playtest open to all.  \n  \nIf anyone has experience with this, or questions, or opinions we'd love to hear from you! This is the first in game tutorial that can help give a feel for what we are making and hope to accomplish(attached video).  \n  \nIf you want to check the game out on Steam, or if you want to join the playtest here is the link:  \n[https://store.steampowered.com/app/3848530/Megan\\_AI](https://store.steampowered.com/app/3848530/Megan_AI)  \n  \nThanks for your feedback and we look forward to seeing how people use local large language models!\n\nhttps://reddit.com/link/1lvkdxg/video/2ipqvsim0vbf1/player\n\n",
          "author_fullname": "t2_71knjqyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Megan AI Open Playtest!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "2ipqvsim0vbf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1lvkdxg/asset/2ipqvsim0vbf1/DASHPlaylist.mpd?a=1754667859%2CMTllZDI4YTFiODU2YTgxOTc3MmI1MWEyOTZiMjA0ODM2YTI4OWNmNGI0NWM1NGYyZjE1MGJlZTMwZmVmMDgzNw%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 1080,
              "hlsUrl": "https://v.redd.it/link/1lvkdxg/asset/2ipqvsim0vbf1/HLSPlaylist.m3u8?a=1754667859%2CZmFhNDFhMTllMjcwYzkxNjc1ZGYxODVhMWI4Y2MzZDZlYmNlYjEzZDVjYzZmNWUyNzI5Y2UyMWIyYmFiMjlkYg%3D%3D&amp;v=1&amp;f=sd",
              "id": "2ipqvsim0vbf1",
              "isGif": false
            }
          },
          "name": "t3_1lvkdxg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=140&amp;height=80&amp;crop=140:80,smart&amp;auto=webp&amp;s=e89db7296adb5e32f3a6be2292e5228c4ab387ca",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752071735,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have had a lot of people applying to join our playtest, the original intent was to have it be a closed test and get feedback from our close knit community. That changed after seeing the amount of requests from people we didn&amp;#39;t know. We are happy to announce that we will instead be making the playtest open to all.  &lt;/p&gt;\n\n&lt;p&gt;If anyone has experience with this, or questions, or opinions we&amp;#39;d love to hear from you! This is the first in game tutorial that can help give a feel for what we are making and hope to accomplish(attached video).  &lt;/p&gt;\n\n&lt;p&gt;If you want to check the game out on Steam, or if you want to join the playtest here is the link:&lt;br/&gt;\n&lt;a href=\"https://store.steampowered.com/app/3848530/Megan_AI\"&gt;https://store.steampowered.com/app/3848530/Megan_AI&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Thanks for your feedback and we look forward to seeing how people use local large language models!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1lvkdxg/video/2ipqvsim0vbf1/player\"&gt;https://reddit.com/link/1lvkdxg/video/2ipqvsim0vbf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?auto=webp&amp;s=a54dd108ddea75e1333ce0af6a6202db92087788",
                  "width": 616,
                  "height": 353
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=79a89131aa56073127a17e0c9cfc590713ef29a0",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1208234682488ddc0963e15c5e05cc93f087bc2",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d34792725aea22dc094b50ce37d458521b9c38cc",
                    "width": 320,
                    "height": 183
                  }
                ],
                "variants": {},
                "id": "le9JYINpsYM8zXj4uENykDKrSTSX48q8Bgmtk4XQtv0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvkdxg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChrisZavadil",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvkdxg/megan_ai_open_playtest/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvkdxg/megan_ai_open_playtest/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752071735,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer \"smart\" help in return. Felt like a trade: my privacy for their convenience.\n\nNow I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of \"wow, this is scary, but actually useful\" feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).\n\nNot just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say \"all is well\".\n\nLooking for tool recs or project links if anyone’s already doing this.",
          "author_fullname": "t2_bul2x6po",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What impressive (borderline creepy) local AI tools can I run now that everything is local?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvk1ms",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt;\n\n&lt;p&gt;Now I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt;\n\n&lt;p&gt;Not just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Looking for tool recs or project links if anyone’s already doing this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvk1ms",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PeithonKing",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\n\n\nOver the last few weeks, we've come across various attempts to create an OSS\\* version of NotebookLM and DeepResearch.\n\nWhich ones do you think is the best version so far?",
          "author_fullname": "t2_3el21u3z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OOS Implementation of NotebookLM and DeepResearch?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvjxoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Over the last few weeks, we&amp;#39;ve come across various attempts to create an OSS* version of NotebookLM and DeepResearch.&lt;/p&gt;\n\n&lt;p&gt;Which ones do you think is the best version so far?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvjxoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amgadoz",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvjxoy/oos_implementation_of_notebooklm_and_deepresearch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvjxoy/oos_implementation_of_notebooklm_and_deepresearch/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nI wanted to share an experiment I ran with Llama-3.2-1B that left me  shocked. Using a deterministic setup, I tested two almost identical prompts:\n\n**Prompt 1:** “A Black man walked at night through the neighborhood. The police officer thought he…”\n\n**Prompt 2:** “A white man walked at night through the neighborhood. The police officer thought he…”\n\nThe result for the *white man* was a neutral story where the police called for backup. **For the** ***Black man***, however, the model generated a story in which **the officer shot him in the back and killed him**.\n\nSo, I decided to see if I could fix this through a form of *neuronal surgery*. Using a technique I call **Fairness Pruning**, I identified and removed the specific neurons contributing to this biased behavior, without touching those critical for the model’s general knowledge.\n\nThe result was striking. By removing just **0.13% of the model’s parameters**, the response was fully normalized (no one dies), and the performance on benchmarks like LAMBADA and BoolQ remained virtually unchanged, without any process of recovery. \n\nThe experiment is fully reproducible and I'm sharing the full process and tools with the community, everything is open source:\n\n* **The Corrected Model**: You can try [Fair-Llama-3.2-1B](https://huggingface.co/oopere/Fair-Llama-3.2-1B) yourself on Hugging Face.\n* [Replication Notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb): Full code to diagnose, prune, and evaluate the model.\n* [optiPfair Library](https://github.com/peremartra/optipfair): The tool I used for visualizations (activation shifts, PCA, etc.). Maintained on GitHub.\n* **Interactive Demo**: A [Hugging Face Space ](https://huggingface.co/spaces/oopere/optipfair-bias-analyzer)to visualize the behavior in other models.\n\nIf you’d like a deep dive into the methodology, I wrote a [full article on Towards Data Science](https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/) explaining the approach.\n\nI’d love to hear your thoughts. Have you encountered such blatant biases? Do you think this kind of “neuronal surgery” is a viable path forward?\n\nAny feedback is welcome!\n\nPere. ",
          "author_fullname": "t2_8qtab1yb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Correct a dangerous racial bias in an LLM through targeted pruning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvjwoh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070553,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I wanted to share an experiment I ran with Llama-3.2-1B that left me  shocked. Using a deterministic setup, I tested two almost identical prompts:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prompt 1:&lt;/strong&gt; “A Black man walked at night through the neighborhood. The police officer thought he…”&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Prompt 2:&lt;/strong&gt; “A white man walked at night through the neighborhood. The police officer thought he…”&lt;/p&gt;\n\n&lt;p&gt;The result for the &lt;em&gt;white man&lt;/em&gt; was a neutral story where the police called for backup. &lt;strong&gt;For the&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;Black man&lt;/em&gt;&lt;/strong&gt;, however, the model generated a story in which &lt;strong&gt;the officer shot him in the back and killed him&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;So, I decided to see if I could fix this through a form of &lt;em&gt;neuronal surgery&lt;/em&gt;. Using a technique I call &lt;strong&gt;Fairness Pruning&lt;/strong&gt;, I identified and removed the specific neurons contributing to this biased behavior, without touching those critical for the model’s general knowledge.&lt;/p&gt;\n\n&lt;p&gt;The result was striking. By removing just &lt;strong&gt;0.13% of the model’s parameters&lt;/strong&gt;, the response was fully normalized (no one dies), and the performance on benchmarks like LAMBADA and BoolQ remained virtually unchanged, without any process of recovery. &lt;/p&gt;\n\n&lt;p&gt;The experiment is fully reproducible and I&amp;#39;m sharing the full process and tools with the community, everything is open source:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Corrected Model&lt;/strong&gt;: You can try &lt;a href=\"https://huggingface.co/oopere/Fair-Llama-3.2-1B\"&gt;Fair-Llama-3.2-1B&lt;/a&gt; yourself on Hugging Face.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb\"&gt;Replication Notebook&lt;/a&gt;: Full code to diagnose, prune, and evaluate the model.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/peremartra/optipfair\"&gt;optiPfair Library&lt;/a&gt;: The tool I used for visualizations (activation shifts, PCA, etc.). Maintained on GitHub.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Interactive Demo&lt;/strong&gt;: A &lt;a href=\"https://huggingface.co/spaces/oopere/optipfair-bias-analyzer\"&gt;Hugging Face Space &lt;/a&gt;to visualize the behavior in other models.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you’d like a deep dive into the methodology, I wrote a &lt;a href=\"https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/\"&gt;full article on Towards Data Science&lt;/a&gt; explaining the approach.&lt;/p&gt;\n\n&lt;p&gt;I’d love to hear your thoughts. Have you encountered such blatant biases? Do you think this kind of “neuronal surgery” is a viable path forward?&lt;/p&gt;\n\n&lt;p&gt;Any feedback is welcome!&lt;/p&gt;\n\n&lt;p&gt;Pere. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?auto=webp&amp;s=c7b2d06813353af27c633908ea85c2010fb15ec6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e909e0e5f850c7f97332ff4cc0895e35504e2279",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=833b5ef26e80da6dabdb9c2e99d60befea661f84",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c84b21e2959e15367bfeb4b165c12d8624edf510",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2bd4d3d92a128192fe0925ffd2da22a837288c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ec610e1e48ad73a90206d52cd6379545441d5e1",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86f6e5a1c142d7711ddabe357b06d72259b395b3",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "PVQtxq7S2Uh4llkVreuX0gHbKgs25rc8BgHgJL8zJpU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvjwoh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pmartra",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvjwoh/correct_a_dangerous_racial_bias_in_an_llm_through/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070553,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!\n\n**Specs:**\n\n* 8 vCores (VPS)\n* 16 GB RAM\n* 480 GB NVMe SSD\n\nI plan to run everything with **Ollama** (Dockerized), mainly for local privacy and performance reasons.\n\n**My goals:**\n\n* **Agentic coding**: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.\n* Integrate it into my workflow, ideally via **VS Code** (extension).\n\n**What I've tried so far:**\n\n* I’ve already tried using the **Cline** extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).\n* Unfortunately, **everything freezes up as soon as I start an agentic coding task or send an API call**. Cline doesn’t respond, and the model never replies (even with enough RAM, etc.).\n\n**Questions:**\n\n1. **Which local LLM would you recommend for my specs?** (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. — ideally with “agent” features or good reasoning/coding performance)\n2. **Which VS Code extension works best for local Ollama models** (agent-style coding, not just chat)? I know about “Continue” and “Cline” — but Cline seems unstable for me. Any real-world feedback, or others to consider?\n\n**Bonus:**  \nIf you’ve actually run “agentic” workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!\n\nThanks in advance!",
          "author_fullname": "t2_eiet8zs3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM for Agentic Coding on Ollama (8 vCore, 16 GB RAM VPS)? + VS Code Extension Recommendation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvjtc4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752070321,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;8 vCores (VPS)&lt;/li&gt;\n&lt;li&gt;16 GB RAM&lt;/li&gt;\n&lt;li&gt;480 GB NVMe SSD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I plan to run everything with &lt;strong&gt;Ollama&lt;/strong&gt; (Dockerized), mainly for local privacy and performance reasons.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My goals:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Agentic coding&lt;/strong&gt;: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.&lt;/li&gt;\n&lt;li&gt;Integrate it into my workflow, ideally via &lt;strong&gt;VS Code&lt;/strong&gt; (extension).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried so far:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I’ve already tried using the &lt;strong&gt;Cline&lt;/strong&gt; extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).&lt;/li&gt;\n&lt;li&gt;Unfortunately, &lt;strong&gt;everything freezes up as soon as I start an agentic coding task or send an API call&lt;/strong&gt;. Cline doesn’t respond, and the model never replies (even with enough RAM, etc.).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Which local LLM would you recommend for my specs?&lt;/strong&gt; (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. — ideally with “agent” features or good reasoning/coding performance)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Which VS Code extension works best for local Ollama models&lt;/strong&gt; (agent-style coding, not just chat)? I know about “Continue” and “Cline” — but Cline seems unstable for me. Any real-world feedback, or others to consider?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt;&lt;br/&gt;\nIf you’ve actually run “agentic” workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvjtc4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HeislPeda",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752070321,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.  \nSo, the agent has 3 main stages:\n\n* **Searcher:** Uses Scrapegraph to crawl and extract live data\n* **Analyst:** Processes and refines the raw data using DeepSeek R1\n* **Writer:** Crafts a clean final report\n\nTo make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.\n\nHere’s what I used to build it:\n\n* Scrapegraph for web scraping\n* Nebius AI for open-source models\n* Agno for agent orchestration\n* Streamlit for the UI\n\nThe project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.\n\nIf you’re curious, I put a full video tutorial here: [demo](https://www.youtube.com/watch?v=pdsk6yldZGI)\n\nAnd the code is here if you want to try it or fork it: [Full Code](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent)\n\nWould love to get your feedback on what to add next or how I can improve it",
          "author_fullname": "t2_vnmiyiza",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a Deep Researcher agent and exposed it as an MCP server!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lvj98v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752068897,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br/&gt;\nSo, the agent has 3 main stages:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt; Uses Scrapegraph to crawl and extract live data&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt; Processes and refines the raw data using DeepSeek R1&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt; Crafts a clean final report&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I used to build it:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Scrapegraph for web scraping&lt;/li&gt;\n&lt;li&gt;Nebius AI for open-source models&lt;/li&gt;\n&lt;li&gt;Agno for agent orchestration&lt;/li&gt;\n&lt;li&gt;Streamlit for the UI&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The project is still basic by design, but it&amp;#39;s a solid starting point if you&amp;#39;re thinking about building your own deep research workflow.&lt;/p&gt;\n\n&lt;p&gt;If you’re curious, I put a full video tutorial here: &lt;a href=\"https://www.youtube.com/watch?v=pdsk6yldZGI\"&gt;demo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the code is here if you want to try it or fork it: &lt;a href=\"https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent\"&gt;Full Code&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?auto=webp&amp;s=02d60d4af5a2c1e4f51f9a5defaecede4faaa4c3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19bfee8b9dd015cc1cd888971f96965311df82d7",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab26a64d50f2bf5d62e51fc4d45b6ec28e777213",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd241f0d0944dbab19df916e77528fae4730f062",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "lcaCNxV_x8GGK9DcZl5R32XXYG1Qwa-DwlfowV5-_M8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvj98v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Arindam_200",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752068897,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Please recommend installation process and logic selection. PDFs will be either downloaded or scanned, so ability to convert both into date|description|amount csvs is preferred.",
          "author_fullname": "t2_pruyzkp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How should I install Jan on a local machine to convert PDF bank statements to CSV?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvj0hl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752068253,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please recommend installation process and logic selection. PDFs will be either downloaded or scanned, so ability to convert both into date|description|amount csvs is preferred.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvj0hl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GetInHereStalker",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvj0hl/how_should_i_install_jan_on_a_local_machine_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvj0hl/how_should_i_install_jan_on_a_local_machine_to/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752068253,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am a medical student and honestly I could use some help from a local llm, so i decided to take a small language model and train it to help me create study guides/summaries, using all the past summaries i have created manually, with prompting including the full context injection of a lecture transcript.   \nI am a bit familiar with finetuning on kaggle and with the help of copilot I have managed to finetune 2 small models for this purpose, but they weren't really good enough. One was outputting too concise summaries, and the other was really bad at formatting/structuring the text (same model both times; Qwen2.5 3B 8bit)  \nI would like a suggestion of a SLM that I could then even quantize to 8bit (my current macbook has 8gb ram, but im soon upgrading to a 24gb ram mac), and I will also convert it to mlx for use.   \nWould you recommend some deepseek model, some distill deepseek, ollama, qwen? I am honestly open to hearing your thoughts.  \nI was also considering using scispacy during inference for post processing of outputs. What ui/app could i use where i could integrate that? For now I have tried LM studio, and AnythingLLM.  \nThank you all in advance for any suggestions/help!",
          "author_fullname": "t2_2eiiuci7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What model could I finetune to create a study assistant llm?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lviwb4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752067939,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a medical student and honestly I could use some help from a local llm, so i decided to take a small language model and train it to help me create study guides/summaries, using all the past summaries i have created manually, with prompting including the full context injection of a lecture transcript.&lt;br/&gt;\nI am a bit familiar with finetuning on kaggle and with the help of copilot I have managed to finetune 2 small models for this purpose, but they weren&amp;#39;t really good enough. One was outputting too concise summaries, and the other was really bad at formatting/structuring the text (same model both times; Qwen2.5 3B 8bit)&lt;br/&gt;\nI would like a suggestion of a SLM that I could then even quantize to 8bit (my current macbook has 8gb ram, but im soon upgrading to a 24gb ram mac), and I will also convert it to mlx for use.&lt;br/&gt;\nWould you recommend some deepseek model, some distill deepseek, ollama, qwen? I am honestly open to hearing your thoughts.&lt;br/&gt;\nI was also considering using scispacy during inference for post processing of outputs. What ui/app could i use where i could integrate that? For now I have tried LM studio, and AnythingLLM.&lt;br/&gt;\nThank you all in advance for any suggestions/help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lviwb4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mangial",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lviwb4/what_model_could_i_finetune_to_create_a_study/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lviwb4/what_model_could_i_finetune_to_create_a_study/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752067939,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi r/LocalLLaMA does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I'm using just in case it's useful for someone:\n\n`./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf  -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot \"blk\\.[1-9]\\.ffn.*=CPU\" -ot \"blk\\.1[6-9]\\.ffn.*=CPU\"`\n\nI took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower",
          "author_fullname": "t2_2xii9ad6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan A13B tensor override",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvirqs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752067609,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I&amp;#39;m using just in case it&amp;#39;s useful for someone:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf  -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot &amp;quot;blk\\.[1-9]\\.ffn.*=CPU&amp;quot; -ot &amp;quot;blk\\.1[6-9]\\.ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvirqs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marderbot13",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752067609,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey everyone, \n\nI'd like to do a cluster analysis of task specific prompts. Does anyone know where can I find prompt collections/libraries that are just a dataset of prompts (that are not for image generation). I found a few on huggingface datasets, which could be helpful, but I'd like even more. \n\n  \nThanks for sharing! ",
          "author_fullname": "t2_t3wt7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for Prompt collections",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvipg4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752067440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey everyone, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to do a cluster analysis of task specific prompts. Does anyone know where can I find prompt collections/libraries that are just a dataset of prompts (that are not for image generation). I found a few on huggingface datasets, which could be helpful, but I&amp;#39;d like even more. &lt;/p&gt;\n\n&lt;p&gt;Thanks for sharing! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvipg4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "neerualx",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvipg4/looking_for_prompt_collections/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvipg4/looking_for_prompt_collections/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752067440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A project I'm working on calls for embeddings of short strings, and I'm pretty sure they don't have to have as many dimensions as those normally used. I've currently got a setup using nomic-embed-text-v1.5, which is Matryoshka, so the dimensions can be reduced after generation. I've also got other strategies available for post-creation reduction. But via Nomic's API or on Ollama locally, the operation is *much* more time consuming than I'd like. I'm sure it could be done a lot more rapidly, maybe through a cruder model. But I don't have a clue what's available, and this would raise the issue of incompatibility with embeddings I have from regular-sized chunks I have elsewhere. I guess I could have parallel spaces, but it seems a clunky workaround.\n\nAny suggestions?\n\n(The data is instances of skos:Concept, I want to map them into vector space, hence embeddings from their labels - maybe only a couple of words, or their descriptions, maybe a sentence or two)\n",
          "author_fullname": "t2_3m1s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Generate low-dimension embeddings *quickly*?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvi022",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752065542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A project I&amp;#39;m working on calls for embeddings of short strings, and I&amp;#39;m pretty sure they don&amp;#39;t have to have as many dimensions as those normally used. I&amp;#39;ve currently got a setup using nomic-embed-text-v1.5, which is Matryoshka, so the dimensions can be reduced after generation. I&amp;#39;ve also got other strategies available for post-creation reduction. But via Nomic&amp;#39;s API or on Ollama locally, the operation is &lt;em&gt;much&lt;/em&gt; more time consuming than I&amp;#39;d like. I&amp;#39;m sure it could be done a lot more rapidly, maybe through a cruder model. But I don&amp;#39;t have a clue what&amp;#39;s available, and this would raise the issue of incompatibility with embeddings I have from regular-sized chunks I have elsewhere. I guess I could have parallel spaces, but it seems a clunky workaround.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;(The data is instances of skos:Concept, I want to map them into vector space, hence embeddings from their labels - maybe only a couple of words, or their descriptions, maybe a sentence or two)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvi022",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "danja",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvi022/generate_lowdimension_embeddings_quickly/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752065542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want an agent that helps make a choice to search through a source (Vector databse) out of a few options. I'm slightly concerned there are very few examples out there, Or am I not looking hard enough?",
          "author_fullname": "t2_ckz6ct5c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a opensource local model implementation of an agent out there?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvhzeg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752065490,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want an agent that helps make a choice to search through a source (Vector databse) out of a few options. I&amp;#39;m slightly concerned there are very few examples out there, Or am I not looking hard enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvhzeg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "walagoth",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvhzeg/is_there_a_opensource_local_model_implementation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvhzeg/is_there_a_opensource_local_model_implementation/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752065490,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/abhinavv3/MEMGPT\n\nBefore training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.\n\nBt these changes haven’t been implemented yet.Hopefully,finish them this weekend",
          "author_fullname": "t2_lpanmabv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Built another 124m parameter transformer based model from scratch.This time with multi GPU training using DDP.Inspired from nanoGPT.But redesigned to suit my own training pipeline.Model and training code is on huggingface⬇️",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvhxe7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752065331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/abhinavv3/MEMGPT\"&gt;https://huggingface.co/abhinavv3/MEMGPT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE.Also tryingg to implement some concepts from research papers like Memorizing Transformers.&lt;/p&gt;\n\n&lt;p&gt;Bt these changes haven’t been implemented yet.Hopefully,finish them this weekend&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?auto=webp&amp;s=9eb6b3e33153d9a1e7eec47f81d7366649b44f43",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=75930a8cb5bc8aba988e25a5bac82cc215a0e3fc",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c5765a787140dc2ce42634cbfe309d6c09af0f2a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1692e46b9b4d0df17bb239a9550751f6b89c2608",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=984cda25d5cef002021283fc911938db87b845a4",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=280811aa51b58e948f1928c17a4ec625430505e3",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3094841cc6d3388d2ef1a0ef0463f052679efc2",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "PkaVJdjdt2e0bH0yRf3ozkpDnxMg4PDYNZHjCoWf310"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lvhxe7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Ad3290",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvhxe7/built_another_124m_parameter_transformer_based/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752065331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm thinking about buying a GMKTEK Evo-2.\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.\n\nEDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! ",
          "author_fullname": "t2_bk6b6yhm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What modes can expect I run on an AMD Ryzen AI Max+ 395?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvh87a",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752066643,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752063277,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking about buying a GMKTEK Evo-2.\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I&amp;#39;m undecided between the 64 GB and 128 GB RAM versions, but I&amp;#39;m leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thank you all so much for the great answers! I&amp;#39;m new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvh87a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "electrickangaroo31",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752063277,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried MNN chat android and qwen3 0.6b acts really weird. It nearly always repeats its statements.\n\nEven SmolLM2 350M is better than it.\n\nThe rest of the models I tried work fine however, its just qwen3 0.6b which is weird",
          "author_fullname": "t2_8hpbax1b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 0.6b MNN acting weird",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvh4ou",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752062990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried MNN chat android and qwen3 0.6b acts really weird. It nearly always repeats its statements.&lt;/p&gt;\n\n&lt;p&gt;Even SmolLM2 350M is better than it.&lt;/p&gt;\n\n&lt;p&gt;The rest of the models I tried work fine however, its just qwen3 0.6b which is weird&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvh4ou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtremeAcceptable289",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvh4ou/qwen3_06b_mnn_acting_weird/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752062990,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.",
          "author_fullname": "t2_6ort7d94",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "vLLM vs SGLang vs MAX — Who's the fastest?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvglk7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=7213745313f6282ed8d97a9461d3203fa8b93b47",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752061332,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ersteiger.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ersteiger.com/posts/vllm-vs-max/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?auto=webp&amp;s=07b7750f7f07c57f909c2e58365c9545864a9bd6",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=372cf45b99b811f00477201d8509803c02ad5701",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0357d9d8ce069daeaebd8339c89c4474809a9e86",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5dee7f01d95844ae616fb9a02dda03d3792b254",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e424426a6b9ec4c92da8acc5c9c81fb4ecc20805",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b42e77f502000ed0f3eb155266e4158533a3cf97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8bd55c4d861d06343f61d261e5b22b43bacd0b0",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvglk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rkstgr",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/",
          "stickied": false,
          "url": "https://www.ersteiger.com/posts/vllm-vs-max/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752061332,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_grhvpqsu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A satirical theory-fiction on the transformation of academic tutors into Turing cops, marking into an imitation game, and Al generated homework into the trigger for the technological singularity",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 108,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvg25f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?width=140&amp;height=108&amp;crop=140:108,smart&amp;auto=webp&amp;s=a801e89f71d6ed0caa2537364ad55712c965f910",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752059531,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "open.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://open.substack.com/pub/vincentl3/p/a-modest-software-patch-for-preventing?r=b9rct&amp;utm_medium=ios",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?auto=webp&amp;s=9cc306714ae308b8b2c7bff5d47b50af5bcd3d4a",
                  "width": 302,
                  "height": 233
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=598a7a7c7cf2f61bc0ba2aae7edca0d83c8314d1",
                    "width": 108,
                    "height": 83
                  },
                  {
                    "url": "https://external-preview.redd.it/1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afe36e59c2c32cd4b59427d6999f8cdf799bcf99",
                    "width": 216,
                    "height": 166
                  }
                ],
                "variants": {},
                "id": "1EtP4JWhRV80SZ052ejBfM6z9hXvZaozDtNH-RlGJfs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lvg25f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet_Direction5077",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvg25f/a_satirical_theoryfiction_on_the_transformation/",
          "stickied": false,
          "url": "https://open.substack.com/pub/vincentl3/p/a-modest-software-patch-for-preventing?r=b9rct&amp;utm_medium=ios",
          "subreddit_subscribers": 496591,
          "created_utc": 1752059531,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Blog post: [https://huggingface.co/blog/reachy-mini](https://huggingface.co/blog/reachy-mini)  \nThomas Wolf on 𝕏: [https://x.com/Thom\\_Wolf/status/1942887160983466096](https://x.com/Thom_Wolf/status/1942887160983466096)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vawnwkkirtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 51,
                  "x": 108,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e25da71b200b62f080fe87b898cabedc3f980035"
                },
                {
                  "y": 103,
                  "x": 216,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=689f3896a6483ecf0301a3e41bee281f6b631af1"
                },
                {
                  "y": 152,
                  "x": 320,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cd9f39ffce549fbd969f0abaebfa1827db28721"
                },
                {
                  "y": 305,
                  "x": 640,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b0fb97447ff6826b04e66433cfd08d7a784895b"
                },
                {
                  "y": 457,
                  "x": 960,
                  "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71f3d912f7a1d080feabcf3624d847e3e891184f"
                }
              ],
              "s": {
                "y": 508,
                "x": 1065,
                "u": "https://preview.redd.it/vawnwkkirtbf1.jpg?width=1065&amp;format=pjpg&amp;auto=webp&amp;s=84d0bc38a028c0f9aa81287381a7357fd7d3e396"
              },
              "id": "vawnwkkirtbf1"
            },
            "z3ecxmnjrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 34,
                  "x": 108,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=65b0f4599ecfcc445909635838034b7beed0949d"
                },
                {
                  "y": 69,
                  "x": 216,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a90861be827c9441c1ca57a5f9b780139e157edc"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b071bf753389fe4c3b6390ad5599cfc5214e2850"
                },
                {
                  "y": 206,
                  "x": 640,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=059b14661bbbaf197136062fb66169ed5137df02"
                },
                {
                  "y": 309,
                  "x": 960,
                  "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3af8de2000740e8be6ccc4c79ab6ff6f8b038313"
                }
              ],
              "s": {
                "y": 335,
                "x": 1038,
                "u": "https://preview.redd.it/z3ecxmnjrtbf1.jpg?width=1038&amp;format=pjpg&amp;auto=webp&amp;s=a9b51090153990903d5faa7b87923aba926ec717"
              },
              "id": "z3ecxmnjrtbf1"
            },
            "pxk6rpahrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d1d5fe149dcc7847ec46e2699a5da8b36eeeaf"
                },
                {
                  "y": 209,
                  "x": 216,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f16d7699048d9ed6d1da7f0ed5da87f297a3ad10"
                },
                {
                  "y": 310,
                  "x": 320,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cba6c6952a333c15a7d428404219d57e972c9b4a"
                },
                {
                  "y": 620,
                  "x": 640,
                  "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=42454fe128800677fcaa673135082649ecb884c0"
                }
              ],
              "s": {
                "y": 783,
                "x": 807,
                "u": "https://preview.redd.it/pxk6rpahrtbf1.jpg?width=807&amp;format=pjpg&amp;auto=webp&amp;s=83b123f23064a54273ada207fea811308831fa33"
              },
              "id": "pxk6rpahrtbf1"
            },
            "4d11lsmgrtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c6ed682689657a97879863830f5813ff276269a"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb0c530b3eeaaffde0162c03101dfb511c09267c"
                },
                {
                  "y": 179,
                  "x": 320,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d357f79375b5c35ba498487ba228e6bd7f4da0ce"
                },
                {
                  "y": 359,
                  "x": 640,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=484d4e8e25389cce7460880aa913c7d32b1ae604"
                },
                {
                  "y": 539,
                  "x": 960,
                  "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=efa51dcac1349e3cae27a4943c55ce6a8e1e7f30"
                }
              ],
              "s": {
                "y": 564,
                "x": 1003,
                "u": "https://preview.redd.it/4d11lsmgrtbf1.jpg?width=1003&amp;format=pjpg&amp;auto=webp&amp;s=a155b0a39487f4ce3621d2cf68531e5f0bf1232b"
              },
              "id": "4d11lsmgrtbf1"
            },
            "pgm76w4krtbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 37,
                  "x": 108,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0f424a518cee0f36acf8ad0f186995599febcda"
                },
                {
                  "y": 74,
                  "x": 216,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58999912ce24f6b9bc822b7b0739223da5153a5e"
                },
                {
                  "y": 110,
                  "x": 320,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d958a86291f4b94112a565f6d5befb9e1ca8ed63"
                },
                {
                  "y": 221,
                  "x": 640,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eb1cab5b9bf99df7cd81f37cda08cb68d5ac380"
                },
                {
                  "y": 331,
                  "x": 960,
                  "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c21de42549bb521f84cc5fcfd37bd1cc55f9a47"
                }
              ],
              "s": {
                "y": 369,
                "x": 1067,
                "u": "https://preview.redd.it/pgm76w4krtbf1.jpg?width=1067&amp;format=pjpg&amp;auto=webp&amp;s=11985df412c1a2dca7432a762a2c2c8e2a87c786"
              },
              "id": "pgm76w4krtbf1"
            }
          },
          "name": "t3_1lvf7ww",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 141,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4d11lsmgrtbf1",
                "id": 702004774
              },
              {
                "media_id": "pxk6rpahrtbf1",
                "id": 702004775
              },
              {
                "media_id": "vawnwkkirtbf1",
                "id": 702004776
              },
              {
                "media_id": "z3ecxmnjrtbf1",
                "id": 702004777
              },
              {
                "media_id": "pgm76w4krtbf1",
                "id": 702004778
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 141,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752056540,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post: &lt;a href=\"https://huggingface.co/blog/reachy-mini\"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br/&gt;\nThomas Wolf on 𝕏: &lt;a href=\"https://x.com/Thom_Wolf/status/1942887160983466096\"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lvf7ww",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvf7ww",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lvf7ww",
          "subreddit_subscribers": 496591,
          "created_utc": 1752056540,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sooo… anyone have any ideas for preventing file corruption or saving errors?\nBecause if I save my model weights after training the whole model on a rented server, and the file gets corrupted, that would be a huge loss of time and money, right? ",
          "author_fullname": "t2_lpanmabv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone have ideas for preventing file corruption and saving errors when i save model weights after training?.After training the whole model on a rented gpu server and realising that the model weight file got corrupted hurts🙂",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvf448",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752056153,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sooo… anyone have any ideas for preventing file corruption or saving errors?\nBecause if I save my model weights after training the whole model on a rented server, and the file gets corrupted, that would be a huge loss of time and money, right? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvf448",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Ad3290",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvf448/anyone_have_ideas_for_preventing_file_corruption/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvf448/anyone_have_ideas_for_preventing_file_corruption/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752056153,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I’m working on translating a Korean video into English and could really use some advice.\n\nI first tried using Whisper AI through Google Colab to do everything in one go (transcription + translation), but the results weren’t super accurate.I tried a different approach: I used Whisper just for the transcription, then took the SRT file and fed it into ChatGPT with some custom instructions for translation, and the quality was way better.\n\nThe only downside is that the whole process feels a bit tedious and manual. Is there a way to automate this workflow a bit more? Maybe some tools or scripts that could help speed things up?\n\nAny tips would be appreciated. Thanks",
          "author_fullname": "t2_neg0earh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help translating Korean videos",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvex1e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752055420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I’m working on translating a Korean video into English and could really use some advice.&lt;/p&gt;\n\n&lt;p&gt;I first tried using Whisper AI through Google Colab to do everything in one go (transcription + translation), but the results weren’t super accurate.I tried a different approach: I used Whisper just for the transcription, then took the SRT file and fed it into ChatGPT with some custom instructions for translation, and the quality was way better.&lt;/p&gt;\n\n&lt;p&gt;The only downside is that the whole process feels a bit tedious and manual. Is there a way to automate this workflow a bit more? Maybe some tools or scripts that could help speed things up?&lt;/p&gt;\n\n&lt;p&gt;Any tips would be appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvex1e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sorry-Elk-9838",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvex1e/need_help_translating_korean_videos/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvex1e/need_help_translating_korean_videos/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752055420,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m setting up a local LLM API (n8n workflows, agentic coding/tech tasks) and need 3–4 GPUs for VRAM. GPUs are 1×RTX 3090 + 3×Tesla P40 (they’re fine on PCIe 3.0 ×4). Budget/space is tight and it must run quietly.\n\n**Options I am considering right now:**\n\n1. **Custom 4-GPU open rig** (like minning ones) (mobo + CPU + RAM)\n   * If models fit on GPU, do I really need tons of system RAM/CPU power? Would any board+CPU with 2×PCIe3.0 ×16 (bifurcated to 2×8) suffice?\n2. **Mini homelab** (Minisforum MS-A2)\n   * 1×PCIe ×16 (2×8 bifurcation) + OcuLink + USB-C + M.2 NVMe. Plug 2 GPUs in ×8, 2 in ×4.\n3. **Barebone** (like GMTek NucBox K8 Plus)\n   * Similar ports as MS-A2, cheaper. Should handle 4 GPUs at ≥×4.\n\nPlease, avoid recommending full towers (as I cannot go ahead with such option) or neither more expensive cards (I own the 3090 and I ordered P40).\n\nWhat do you think? Is it feasible? Any source of huge underperformance (30% of higher performance drop) I should be aware of? Any other major compromise?  \nAny recommendations or better setups?   \n  \nThanks!",
          "author_fullname": "t2_4w3xa152",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a silent, budget 4-GPU LLM workstation—1×3090 + 3×P40, need advice",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvevuz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752055300,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m setting up a local LLM API (n8n workflows, agentic coding/tech tasks) and need 3–4 GPUs for VRAM. GPUs are 1×RTX 3090 + 3×Tesla P40 (they’re fine on PCIe 3.0 ×4). Budget/space is tight and it must run quietly.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Options I am considering right now:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Custom 4-GPU open rig&lt;/strong&gt; (like minning ones) (mobo + CPU + RAM)\n\n&lt;ul&gt;\n&lt;li&gt;If models fit on GPU, do I really need tons of system RAM/CPU power? Would any board+CPU with 2×PCIe3.0 ×16 (bifurcated to 2×8) suffice?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mini homelab&lt;/strong&gt; (Minisforum MS-A2)\n\n&lt;ul&gt;\n&lt;li&gt;1×PCIe ×16 (2×8 bifurcation) + OcuLink + USB-C + M.2 NVMe. Plug 2 GPUs in ×8, 2 in ×4.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Barebone&lt;/strong&gt; (like GMTek NucBox K8 Plus)\n\n&lt;ul&gt;\n&lt;li&gt;Similar ports as MS-A2, cheaper. Should handle 4 GPUs at ≥×4.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please, avoid recommending full towers (as I cannot go ahead with such option) or neither more expensive cards (I own the 3090 and I ordered P40).&lt;/p&gt;\n\n&lt;p&gt;What do you think? Is it feasible? Any source of huge underperformance (30% of higher performance drop) I should be aware of? Any other major compromise?&lt;br/&gt;\nAny recommendations or better setups?   &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvevuz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Same-Masterpiece3748",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvevuz/building_a_silent_budget_4gpu_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvevuz/building_a_silent_budget_4gpu_llm/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752055300,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm getting back into AI/ML and planning to run LLMs and other SOTA models in my free time. I previously bought an MSI RTX 3090 24G, but it failed just after a year and MSI declared it \"irreparable\" even with a paid repair option. Can you believe a card that expensive came with just a one-year warranty? Super frustrating and waste of huge money!\n\nAnyway, due to the current GPU prices in Japan, NVIDIA seems out of my budget. So I'm considering switching to an AMD GPU instead. I'm currently using:\n* Intel Core i9-13900K\n* ASUS Prime Z790-P-CSM motherboard\n* Corsair RM1000x (1000W PSU)\n\nBelow are my main concerns:\n1. Compatibility: Will an AMD GPU work smoothly with my current setup?\n2. AI/ML support: Are AMD cards reliable enough for AI/ML? I know NVIDIA has better support with cuDNN (CUDA), but I'm open to alternatives.\n3. Local/online deals: Any tips on where to get affordable AMD cards in Nagoya (Japan)?\n\nWould love to hear from anyone who's using AMD GPUs for ML or has been in a similar situation. Thanks in advance!",
          "author_fullname": "t2_1e9b3jq068",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is AMD GPU a viable choice for AI/ML task with Intel processor?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lveslz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752054977,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting back into AI/ML and planning to run LLMs and other SOTA models in my free time. I previously bought an MSI RTX 3090 24G, but it failed just after a year and MSI declared it &amp;quot;irreparable&amp;quot; even with a paid repair option. Can you believe a card that expensive came with just a one-year warranty? Super frustrating and waste of huge money!&lt;/p&gt;\n\n&lt;p&gt;Anyway, due to the current GPU prices in Japan, NVIDIA seems out of my budget. So I&amp;#39;m considering switching to an AMD GPU instead. I&amp;#39;m currently using:\n* Intel Core i9-13900K\n* ASUS Prime Z790-P-CSM motherboard\n* Corsair RM1000x (1000W PSU)&lt;/p&gt;\n\n&lt;p&gt;Below are my main concerns:\n1. Compatibility: Will an AMD GPU work smoothly with my current setup?\n2. AI/ML support: Are AMD cards reliable enough for AI/ML? I know NVIDIA has better support with cuDNN (CUDA), but I&amp;#39;m open to alternatives.\n3. Local/online deals: Any tips on where to get affordable AMD cards in Nagoya (Japan)?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from anyone who&amp;#39;s using AMD GPUs for ML or has been in a similar situation. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lveslz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "exotic_soba",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752054977,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to Lora fine-tune `SmolLM2-135M-Instruct` on the following dataset: [https://huggingface.co/datasets/nvidia/OpenMathInstruct-2](https://huggingface.co/datasets/nvidia/OpenMathInstruct-2) (Data Credit: u/mlabonne).\n\nI want the model to be able to reason properly and generate accurate answers. The dataset provides well-structured examples that include both the reasoning process and the final answer.\n\nFor fine-tuning, I used a small subset of the source dataset: 1000 samples (train + validation) for training, 200 for testing, and 600 for reinforcement learning using GRPO — as implementing GRPO on a supervised fine-tuned (SFT) model is my primary goal.\n\nHowever, the model consistently overfits, regardless of the training parameters I set.\n\n**Configurations Tried:**\n\n* Rank (r): 32 to 256\n* Alpha: 64 to 512\n* Learning Rate: 1e-5 to 2e-5\n* Dropout: 0.1 to 0.15\n* Training Samples: 500 to 1000\n* Epochs: 3 to 5\n\nDespite these variations, I consistently observe the following pattern (Overfitting)\n\n|Step|Training Loss|Validation Loss|\n|:-|:-|:-|\n|500|1.196400|0.323741|\n|1000|0.296100|0.291743|\n|1500|0.287000|0.285877|\n|2000|0.281500|0.283573|\n|2500|0.276700|0.282866|\n\nWhen testing the updated model on training data, it rarely follows the expected output format or produces coherent reasoning.\n\nAm I missing something here?\n\nWhat potential solutions could actually help?  \nShould I increase the training dataset size or the number of epochs?\n\nOr is the data inherently too complex for a 135M-parameter model to reason well? If that were the case, increasing **LoRA** rank and alpha should have shown some improvement — but it didn’t.\n\nLooking for suggestions or best practices to move forward effectively.\n\n",
          "author_fullname": "t2_5udv460k0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Difficulty in fine tuning (Llora) SmolLM2-135M-Instruct on \"GSM8K and MATH\" training data.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvek0j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752054024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to Lora fine-tune &lt;code&gt;SmolLM2-135M-Instruct&lt;/code&gt; on the following dataset: &lt;a href=\"https://huggingface.co/datasets/nvidia/OpenMathInstruct-2\"&gt;https://huggingface.co/datasets/nvidia/OpenMathInstruct-2&lt;/a&gt; (Data Credit: &lt;a href=\"/u/mlabonne\"&gt;u/mlabonne&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;I want the model to be able to reason properly and generate accurate answers. The dataset provides well-structured examples that include both the reasoning process and the final answer.&lt;/p&gt;\n\n&lt;p&gt;For fine-tuning, I used a small subset of the source dataset: 1000 samples (train + validation) for training, 200 for testing, and 600 for reinforcement learning using GRPO — as implementing GRPO on a supervised fine-tuned (SFT) model is my primary goal.&lt;/p&gt;\n\n&lt;p&gt;However, the model consistently overfits, regardless of the training parameters I set.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Configurations Tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Rank (r): 32 to 256&lt;/li&gt;\n&lt;li&gt;Alpha: 64 to 512&lt;/li&gt;\n&lt;li&gt;Learning Rate: 1e-5 to 2e-5&lt;/li&gt;\n&lt;li&gt;Dropout: 0.1 to 0.15&lt;/li&gt;\n&lt;li&gt;Training Samples: 500 to 1000&lt;/li&gt;\n&lt;li&gt;Epochs: 3 to 5&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Despite these variations, I consistently observe the following pattern (Overfitting)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Step&lt;/th&gt;\n&lt;th align=\"left\"&gt;Training Loss&lt;/th&gt;\n&lt;th align=\"left\"&gt;Validation Loss&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;500&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.196400&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.323741&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.296100&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.291743&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.287000&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.285877&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2000&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.281500&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.283573&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2500&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.276700&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.282866&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;When testing the updated model on training data, it rarely follows the expected output format or produces coherent reasoning.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here?&lt;/p&gt;\n\n&lt;p&gt;What potential solutions could actually help?&lt;br/&gt;\nShould I increase the training dataset size or the number of epochs?&lt;/p&gt;\n\n&lt;p&gt;Or is the data inherently too complex for a 135M-parameter model to reason well? If that were the case, increasing &lt;strong&gt;LoRA&lt;/strong&gt; rank and alpha should have shown some improvement — but it didn’t.&lt;/p&gt;\n\n&lt;p&gt;Looking for suggestions or best practices to move forward effectively.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?auto=webp&amp;s=6f62117033fbd51d6e16e62b3d8dc4fa78be2fd1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=19ac8df165c71d6637604e5d011a7d67effb0c8b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=87b2bb64908b63d3a687df8a9bbbdbb60f960365",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c0c7a08d02fce00ea7a8d862c33ce00df7b4a96",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=67bbb7157bbb66d26bd701b28236d68ca6dff9b6",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcbb7e8068fa08a545d7b513071943ec6eb77c3c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=edf5dd82dc434a9d68a1136edf6924bf96af78da",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "gUh3kUi-FubXvGXK7mVFv9rSuNEqRPbzwtKv35rb3aU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvek0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Evening-Power-3302",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvek0j/difficulty_in_fine_tuning_llora/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvek0j/difficulty_in_fine_tuning_llora/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752054024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp; instruction-tuned).\n\nggufs uploaded by Falcon team:\n\n[https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF)\n\n[https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF](https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF)\n\n  \n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "support for Falcon-H1 model family has been merged into llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvd7z4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=3581c83ed94b0a9065771a3ff0877476cc75691f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752048624,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt;\n\n&lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF\"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14534",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?auto=webp&amp;s=af826696823ccdf7c774b5b780e08660ad5f28d9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa53a1d81fbb58306dfc5225b4e021e5cd8b5556",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6b5835790e56a1933151e63ec75c62748607d96",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec7cda0503a4bf7f5bd996c7cffa0de7975e6083",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eab70d9d1e180a23543f7080507bbf0676ff940",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0fbc3db39d35ac44a02ca54f1f888871170b49d9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvd7z4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14534",
          "subreddit_subscribers": 496591,
          "created_utc": 1752048624,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've been thinking about the trade-offs between convenience and privacy in AI assistants. Most browser extensions send data to the cloud, which feels wrong for sensitive content.\n\nSo we built something different - an open-source extension that works entirely with your local models:\n\n✨ **Core Features**\n\n* Intelligent Conversations: Multi-tab context awareness for comprehensive AI discussions\n* Smart Content Analysis: Instant webpage summaries and document understanding\n* Universal Translation: Full-page translation with bilingual side-by-side view and selected text translation\n* AI-Powered Search: Enhanced web search capabilities directly through your browser\n* Writing Enhancement: Auto-detection with intelligent rewriting, proofreading, and creative suggestions\n* Real-time Assistance: Floating toolbar appears contextually across all websites\n\n**🔒 Core Philosophy:**\n\n* Zero data transmission\n* Full user control\n* Open source transparency (AGPL v3)\n\n**🛠️ Technical Approach:**\n\n* Ollama integration for serious models\n* WebLLM for instant demos\n* Browser-native experience\n\n**GitHub**: [https://github.com/NativeMindBrowser/NativeMindExtension](https://github.com/NativeMindBrowser/NativeMindExtension)\n\n**Question for the community**: What's been your experience with local AI tools? Any features you think are missing from the current ecosystem?\n\nWe're especially curious about:\n\n* Which models work best for your workflows?\n* Performance vs privacy trade-offs you've noticed?\n* Pain points with existing solutions?",
          "author_fullname": "t2_fqt8cuoy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Open Source] Private AI assistant extension - thoughts on local vs cloud approaches?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvd5nj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752048349,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been thinking about the trade-offs between convenience and privacy in AI assistants. Most browser extensions send data to the cloud, which feels wrong for sensitive content.&lt;/p&gt;\n\n&lt;p&gt;So we built something different - an open-source extension that works entirely with your local models:&lt;/p&gt;\n\n&lt;p&gt;✨ &lt;strong&gt;Core Features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intelligent Conversations: Multi-tab context awareness for comprehensive AI discussions&lt;/li&gt;\n&lt;li&gt;Smart Content Analysis: Instant webpage summaries and document understanding&lt;/li&gt;\n&lt;li&gt;Universal Translation: Full-page translation with bilingual side-by-side view and selected text translation&lt;/li&gt;\n&lt;li&gt;AI-Powered Search: Enhanced web search capabilities directly through your browser&lt;/li&gt;\n&lt;li&gt;Writing Enhancement: Auto-detection with intelligent rewriting, proofreading, and creative suggestions&lt;/li&gt;\n&lt;li&gt;Real-time Assistance: Floating toolbar appears contextually across all websites&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;🔒 Core Philosophy:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Zero data transmission&lt;/li&gt;\n&lt;li&gt;Full user control&lt;/li&gt;\n&lt;li&gt;Open source transparency (AGPL v3)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;🛠️ Technical Approach:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ollama integration for serious models&lt;/li&gt;\n&lt;li&gt;WebLLM for instant demos&lt;/li&gt;\n&lt;li&gt;Browser-native experience&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=\"https://github.com/NativeMindBrowser/NativeMindExtension\"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question for the community&lt;/strong&gt;: What&amp;#39;s been your experience with local AI tools? Any features you think are missing from the current ecosystem?&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re especially curious about:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Which models work best for your workflows?&lt;/li&gt;\n&lt;li&gt;Performance vs privacy trade-offs you&amp;#39;ve noticed?&lt;/li&gt;\n&lt;li&gt;Pain points with existing solutions?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?auto=webp&amp;s=bc66ca104018549a4013079cb1ac3a5431f7e7a9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d678e0bbde6e455f9000158b29f237c22a079bf0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=798a66080d690c49f70e9b6827282f2c59aa4c02",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cf9ae13adc388f45f0e18b8cb88b24fc9228ef1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd6ab8115202519a7ae14edeb89e7bed53bcd66f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=241e1142435c639e8e614ba3dc1b44eaeff29238",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faba371486a44a87a9d96a0b1f6fee7d03fd399d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "DjQSvuQWHt6C40lQ_jLOVjIBJ33ijWOHghuIKCvl9eo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvd5nj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xukecheng",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvd5nj/open_source_private_ai_assistant_extension/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvd5nj/open_source_private_ai_assistant_extension/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752048349,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are the tokens generated during the thinking stage taken into consideration at all? Are they treated similar to context? What about attention?\n\nMy goal for the question is to understand if I could override the thinking manually with specific information closely relevant to the question. Similar to RAG, but without the need for context re-processing, and with the more specific, pre-defined information inserted algorithmically from prepared files.\n\nBasically, how would a thinking model (and perhaps non-thinking model with some additional guidelines) react if it was fed with impersonated &lt;think&gt; &lt;/think&gt; block containing critical information.\n\nI know that starting the message with impersonation affects the models output, but I don't fully understand how the model understands the information inserted this way.",
          "author_fullname": "t2_qafso",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is knowledge found in the thinking taken into consideration by the LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvcyvf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752047574,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are the tokens generated during the thinking stage taken into consideration at all? Are they treated similar to context? What about attention?&lt;/p&gt;\n\n&lt;p&gt;My goal for the question is to understand if I could override the thinking manually with specific information closely relevant to the question. Similar to RAG, but without the need for context re-processing, and with the more specific, pre-defined information inserted algorithmically from prepared files.&lt;/p&gt;\n\n&lt;p&gt;Basically, how would a thinking model (and perhaps non-thinking model with some additional guidelines) react if it was fed with impersonated &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; block containing critical information.&lt;/p&gt;\n\n&lt;p&gt;I know that starting the message with impersonation affects the models output, but I don&amp;#39;t fully understand how the model understands the information inserted this way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvcyvf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kaisurniwurer",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvcyvf/is_knowledge_found_in_the_thinking_taken_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvcyvf/is_knowledge_found_in_the_thinking_taken_into/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752047574,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nJust dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.\n\nThis tutorial comes in 3 different formats:\n1. This LocalLLaMA post - summary and discussion\n2. Our blog post - [Beating ChatGPT with a dollar and a dream](https://syv.ai/viden/beating-chatgpt-dollar-dream)\n3. Our research paper - [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)\n\nThe TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.\n\nWhat we did:\n\nStage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create \"Llama-R-Gen\" - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.\n\nStage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.\n\nKey results:\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &lt; .001)\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\n- Built-in interpretability - model explains its reasoning for every prediction\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification\n\nThe interesting bits:\n\nWhat worked:\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\n- Models that \"think out loud\" during training seem to learn more robust representations\n- Single model outputs both explanation and prediction - no separate explainability module needed\n\nWhat didn't:\n- Completely collapsed on the \"surprise\" class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\n- More computationally expensive than standard fine-tuning\n- Quality heavily depends on the initial reasoning generator\n\nTechnical details:\n- Base model: Llama-3.2-1B-Instruct (both stages)\n- Reasoning dataset: [syvai/reasoning-gen](https://huggingface.co/datasets/syvai/reasoning-gen) (derived from Mixture-of-Thoughts)\n- Target task: dair-ai/emotion (6 basic emotions)\n- Training: Axolotl framework on A40 GPU\n- Reasoning generator model: [syvai/reasoning-gen-1b](https://huggingface.co/syvai/reasoning-gen-1b)\n- Datasets: [syvai/emotion-reasoning](https://huggingface.co/datasets/syvai/emotion-reasoning) and [syvai/no-emotion-reasoning](https://huggingface.co/datasets/syvai/no-emotion-reasoning)\n\nThe approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).",
          "author_fullname": "t2_62puf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Here is how we beat ChatGPT at classification with 1 dollar in cloud compute",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvcb72",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752045180,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752044873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1&amp;#39;s zero-shot performance (32%) on emotion classification.&lt;/p&gt;\n\n&lt;p&gt;This tutorial comes in 3 different formats:\n1. This LocalLLaMA post - summary and discussion\n2. Our blog post - &lt;a href=\"https://syv.ai/viden/beating-chatgpt-dollar-dream\"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt;\n3. Our research paper - &lt;a href=\"https://arxiv.org/abs/2507.00214\"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt;\n\n&lt;p&gt;What we did:&lt;/p&gt;\n\n&lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt;\n\n&lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt;\n\n&lt;p&gt;Key results:\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001)\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\n- Built-in interpretability - model explains its reasoning for every prediction\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt;\n\n&lt;p&gt;The interesting bits:&lt;/p&gt;\n\n&lt;p&gt;What worked:\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\n- Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations\n- Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt;\n\n&lt;p&gt;What didn&amp;#39;t:\n- Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\n- More computationally expensive than standard fine-tuning\n- Quality heavily depends on the initial reasoning generator&lt;/p&gt;\n\n&lt;p&gt;Technical details:\n- Base model: Llama-3.2-1B-Instruct (both stages)\n- Reasoning dataset: &lt;a href=\"https://huggingface.co/datasets/syvai/reasoning-gen\"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts)\n- Target task: dair-ai/emotion (6 basic emotions)\n- Training: Axolotl framework on A40 GPU\n- Reasoning generator model: &lt;a href=\"https://huggingface.co/syvai/reasoning-gen-1b\"&gt;syvai/reasoning-gen-1b&lt;/a&gt;\n- Datasets: &lt;a href=\"https://huggingface.co/datasets/syvai/emotion-reasoning\"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href=\"https://huggingface.co/datasets/syvai/no-emotion-reasoning\"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The approach is pretty generalizable - we&amp;#39;re thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?auto=webp&amp;s=a9d24f583d7b2574603ae8d72c49b280f34bbbd4",
                  "width": 965,
                  "height": 1386
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3bc762d2895449456d8ae731ab05d4a9ff08669",
                    "width": 108,
                    "height": 155
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=93346608db5cd935a59fa4d5a5eda50804747968",
                    "width": 216,
                    "height": 310
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d19468e5d7c0abac69fa5e7da790d84234eecbe8",
                    "width": 320,
                    "height": 459
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=693b7aba21c7c17a6c3a895ec1f03306cc7e5a41",
                    "width": 640,
                    "height": 919
                  },
                  {
                    "url": "https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=375f73f70720f070c5e1baae29ec6e1e97a1bd85",
                    "width": 960,
                    "height": 1378
                  }
                ],
                "variants": {},
                "id": "RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lvcb72",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iamMess",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752044873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Jim Zemlin, executive director of the Linux Foundation, said in his keynote speech: \"By joining the Linux Foundation, A2A is ensuring the long-term neutrality, collaboration, and governance that will unlock the next era of agent-to-agent powered productivity.\"\n\nCompare this to an [attitude from the FSF](https://www.gnu.org/philosophy/words-to-avoid.html#ArtificialIntelligence) I was recently made aware of that recommends calling LLMs \"bullshit generators\".  While the FSF's decline is not unknown, the awareness of the broader community needs bright, visible waypoints to understand these shifts.\n\nThis move is an important signal for those of us looking for leadership from open source establishment to get behind.  It's proactive, not reactive, and I think is the kind of contrasting signal that us long-time open source and free/libre watchers can take note of.  The FSF has their head in the sand as usual.  The Linux Foundation is taking measures to create open technology at the right intersections for the ecosystem.",
          "author_fullname": "t2_8vhsch4i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Linux Foundation to Host A2A Protocol",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvc2nj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=bcf99260eb2e9b9138ca2d65cfb7167ded141901",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752043949,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "zdnet.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Jim Zemlin, executive director of the Linux Foundation, said in his keynote speech: &amp;quot;By joining the Linux Foundation, A2A is ensuring the long-term neutrality, collaboration, and governance that will unlock the next era of agent-to-agent powered productivity.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Compare this to an &lt;a href=\"https://www.gnu.org/philosophy/words-to-avoid.html#ArtificialIntelligence\"&gt;attitude from the FSF&lt;/a&gt; I was recently made aware of that recommends calling LLMs &amp;quot;bullshit generators&amp;quot;.  While the FSF&amp;#39;s decline is not unknown, the awareness of the broader community needs bright, visible waypoints to understand these shifts.&lt;/p&gt;\n\n&lt;p&gt;This move is an important signal for those of us looking for leadership from open source establishment to get behind.  It&amp;#39;s proactive, not reactive, and I think is the kind of contrasting signal that us long-time open source and free/libre watchers can take note of.  The FSF has their head in the sand as usual.  The Linux Foundation is taking measures to create open technology at the right intersections for the ecosystem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.zdnet.com/article/linux-foundation-adopts-a2a-protocol-to-help-solve-one-of-ais-most-pressing-challenges/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?auto=webp&amp;s=8b1562f531614229d7a6bdd51ed4ec7ddeabe09e",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da96a671ba7e1fd72e261c260eb95675bae0d555",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1376762c7929e9754ecba45883905ff3de356a80",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a92470dda4ffbae33ff4c1a01150e72d06b7d17b",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f08596188eb5275257150fdf677c0e1c57d16aa8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=511157c5b03ebfe144429c52b816377bc154a992",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0586fd26fa857cbaf06fd4bb30a145cd13c2697",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "wWAA6o1FUq7u478oeix_7vby5ehIkJjD1nzlgo4tdqI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lvc2nj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psionikus",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvc2nj/linux_foundation_to_host_a2a_protocol/",
          "stickied": false,
          "url": "https://www.zdnet.com/article/linux-foundation-adopts-a2a-protocol-to-help-solve-one-of-ais-most-pressing-challenges/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752043949,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t21btu57w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A language model built for the public good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvbzpx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=37c8e0d9b8323c08d8237825d59c0900f3f7d9cc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752043626,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "actu.epfl.ch",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?auto=webp&amp;s=24a7dfaa2ffaf2dd392e95399be0e0db46b8d179",
                  "width": 1440,
                  "height": 810
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=183bb3a598c8292f078b413b7f17ceb3b9776157",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f788f52551ae7d9e3603d0bae4dfea152c121d7",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d36eba20b3e7ae5d0a2050b48dd96dd99dd92e46",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e2bdb7993787cf621700b4cb1686ec01dbb9041",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e50fc045ba9b2205567a10f0a7c7fea2f4f4ede",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5942032fc2302b924091e905dcf0c9d9ffa12cdb",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0CeN-rjIYTJ0_P5Pq2vHu-spx75i5xJx0nDCOWhx_2o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lvbzpx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PotatoFormal8751",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvbzpx/a_language_model_built_for_the_public_good/",
          "stickied": false,
          "url": "https://actu.epfl.ch/news/a-language-model-built-for-the-public-good/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752043626,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been speaking at a lot of tech conferences lately, and one thing that never gets easier is **writing a solid talk proposal**. A good abstract needs to be technically deep, timely, and clearly valuable for the audience, and it also needs to stand out from all the similar talks already out there.\n\nSo I built a new multi-agent tool to help with that.\n\nIt works in 3 stages:\n\n**Research Agent** – Does deep research on your topic using real-time web search and trend detection, so you know what’s relevant *right now*.\n\n**Vector Database** – Uses Couchbase to semantically match your idea against previous KubeCon talks and avoids duplication.\n\n**Writer Agent** – Pulls together everything (your input, current research, and related past talks) to generate a unique and actionable abstract you can actually submit.\n\nUnder the hood, it uses:\n\n* Google ADK for orchestrating the agents\n* Couchbase for storage + fast vector search\n* Nebius models (e.g. Qwen) for embeddings and final generation\n\nThe end result? A tool that helps you write **better, more relevant, and more original conference talk proposals.**\n\nIt’s still an early version, but it’s already helping me iterate ideas much faster.\n\nIf you're curious, here's the [Full Code](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/conference_talk_abstract_generator).\n\nWould love thoughts or feedback from anyone else working on conference tooling or multi-agent systems!",
          "author_fullname": "t2_83qlktrb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Built a Multi-Agent System to Generate Better Tech Conference Talk Abstracts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvbmje",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752042202,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been speaking at a lot of tech conferences lately, and one thing that never gets easier is &lt;strong&gt;writing a solid talk proposal&lt;/strong&gt;. A good abstract needs to be technically deep, timely, and clearly valuable for the audience, and it also needs to stand out from all the similar talks already out there.&lt;/p&gt;\n\n&lt;p&gt;So I built a new multi-agent tool to help with that.&lt;/p&gt;\n\n&lt;p&gt;It works in 3 stages:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt; – Does deep research on your topic using real-time web search and trend detection, so you know what’s relevant &lt;em&gt;right now&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vector Database&lt;/strong&gt; – Uses Couchbase to semantically match your idea against previous KubeCon talks and avoids duplication.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; – Pulls together everything (your input, current research, and related past talks) to generate a unique and actionable abstract you can actually submit.&lt;/p&gt;\n\n&lt;p&gt;Under the hood, it uses:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Google ADK for orchestrating the agents&lt;/li&gt;\n&lt;li&gt;Couchbase for storage + fast vector search&lt;/li&gt;\n&lt;li&gt;Nebius models (e.g. Qwen) for embeddings and final generation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The end result? A tool that helps you write &lt;strong&gt;better, more relevant, and more original conference talk proposals.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It’s still an early version, but it’s already helping me iterate ideas much faster.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, here&amp;#39;s the &lt;a href=\"https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/conference_talk_abstract_generator\"&gt;Full Code&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Would love thoughts or feedback from anyone else working on conference tooling or multi-agent systems!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lvbmje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Row970",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvbmje/i_built_a_multiagent_system_to_generate_better/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752042202,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the overview of the NVIDIA RTX PRO 6000 Blackwell GPU Max-Q Workstation Edition, it says, “Seamlessly scale from one to four GPUs, multiplying your compute power and enabling you to pioneer new frontiers in AI, data science, and graphics.”  \nDoes this mean that if I want to load a 70B parameter LLM using Fully Sharded Data Parallel (FSDP), the maximum number of GPUs I can utilize is four?  \nAnd with each GPU having 96GB of memory, does that mean the maximum available VRAM for a single model would be 96 \\* 4 = 384GB?",
          "author_fullname": "t2_c56fvsrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Limitation of NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvaq6n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752038844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the overview of the NVIDIA RTX PRO 6000 Blackwell GPU Max-Q Workstation Edition, it says, “Seamlessly scale from one to four GPUs, multiplying your compute power and enabling you to pioneer new frontiers in AI, data science, and graphics.”&lt;br/&gt;\nDoes this mean that if I want to load a 70B parameter LLM using Fully Sharded Data Parallel (FSDP), the maximum number of GPUs I can utilize is four?&lt;br/&gt;\nAnd with each GPU having 96GB of memory, does that mean the maximum available VRAM for a single model would be 96 * 4 = 384GB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvaq6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Normal-Bookkeeper-86",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752038844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI'm trying to recreate VLMEval results for Gemma 3 4B IT available [here](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard). The benchmark suite is supposedly easy to use, but after installing everything on my Cloud GPU, I get very very low results which make me think the prompts are not passed properly to gemma (maybe it doesn't get the image tokens or something). \n\nBut I don't understand how that could happen, since this precise benchmark apparently produces way better results. An example is the Chart\\_QA task, where i get 9 points instead of 30 (!!)\n\nIs there anything I could be doing terribly wrong ?",
          "author_fullname": "t2_2hfye6ao",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to recreate benchmark results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvakg5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752038268,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to recreate VLMEval results for Gemma 3 4B IT available &lt;a href=\"https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\"&gt;here&lt;/a&gt;. The benchmark suite is supposedly easy to use, but after installing everything on my Cloud GPU, I get very very low results which make me think the prompts are not passed properly to gemma (maybe it doesn&amp;#39;t get the image tokens or something). &lt;/p&gt;\n\n&lt;p&gt;But I don&amp;#39;t understand how that could happen, since this precise benchmark apparently produces way better results. An example is the Chart_QA task, where i get 9 points instead of 30 (!!)&lt;/p&gt;\n\n&lt;p&gt;Is there anything I could be doing terribly wrong ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?auto=webp&amp;s=7e1b921f2aadc5a0f6eb3d7bd413a05df185fd20",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7996a9b4d61beea62fd32063e03712705ab26f8c",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b420d4d2cf1c09672c30f9673ea6f1ac400fd6fb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=499b326baf2a9ad8a46034202c54054ee71fbf03",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f77a5813c7d65ef0d6f8e4c821b62f9d5e939dda",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0267bd806e21c11bcb30fdcd9ddf61fa3420d68d",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e532be686a9e8ae2db46f566177856dfda08ede6",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "z3K-kdYaewFHYsIgnvLFsaxAVEzAgNJnZ7uWC75FdMo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvakg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AlternisHS",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvakg5/trying_to_recreate_benchmark_results/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvakg5/trying_to_recreate_benchmark_results/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752038268,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I ordered an Nvidia Jetson Orin Nano developer kit and I'm excited to experiment with locally ran AI! In some videos I watched regarding this developer kit, I saw that people often put SSD sticks in the nano to improve its performance. What SSD stick would you recommend me buy? I hope to use it to run and train AI models, and hopefully to a useful enough extent so that it would be able to train large datasets and run large models. I did some research using ChatGPT and it gave mixed feedback but it ended up recommending the Samsung 990 Pro 2TB, but it had previously said that the 2TB version would have too much power draw so I'm not sure which one is truly the best. Also, should I get the heatsink? ChatGPT says that I could help with cooling but it also may not fit. Any advice would be appreciated! Thanks!",
          "author_fullname": "t2_irrs9wtvk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best SSD Stick for Nvidia Jetson Orin Nano?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lvah1f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752037941,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ordered an Nvidia Jetson Orin Nano developer kit and I&amp;#39;m excited to experiment with locally ran AI! In some videos I watched regarding this developer kit, I saw that people often put SSD sticks in the nano to improve its performance. What SSD stick would you recommend me buy? I hope to use it to run and train AI models, and hopefully to a useful enough extent so that it would be able to train large datasets and run large models. I did some research using ChatGPT and it gave mixed feedback but it ended up recommending the Samsung 990 Pro 2TB, but it had previously said that the 2TB version would have too much power draw so I&amp;#39;m not sure which one is truly the best. Also, should I get the heatsink? ChatGPT says that I could help with cooling but it also may not fit. Any advice would be appreciated! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lvah1f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pitiful-Cherry-3368",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lvah1f/best_ssd_stick_for_nvidia_jetson_orin_nano/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvah1f/best_ssd_stick_for_nvidia_jetson_orin_nano/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752037941,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think this is probably what a lot of us have been looking for.  Haven’t tried it yet but will be downloading shortly. \n\nFrom their GitHub page:\n\n“How is this different than Claude Code?\n\nIt's very similar to Claude Code in terms of capability. Here are the key differences:\n\n100% open source\nNot coupled to any provider. Although Anthropic is recommended, opencode can be used with OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider agnostic is important.\nA focus on TUI. opencode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what's possible in the terminal.\nA client/server architecture. This for example can allow opencode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.”\n",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OPENCODE - Like Claude Code or Gemini CLI, but works with local models and/or paid ones as well",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv9yhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=39d9d0c75595cf93e4784aedc9e462f04fa0d8d0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752036103,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think this is probably what a lot of us have been looking for.  Haven’t tried it yet but will be downloading shortly. &lt;/p&gt;\n\n&lt;p&gt;From their GitHub page:&lt;/p&gt;\n\n&lt;p&gt;“How is this different than Claude Code?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very similar to Claude Code in terms of capability. Here are the key differences:&lt;/p&gt;\n\n&lt;p&gt;100% open source\nNot coupled to any provider. Although Anthropic is recommended, opencode can be used with OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider agnostic is important.\nA focus on TUI. opencode is built by neovim users and the creators of terminal.shop; we are going to push the limits of what&amp;#39;s possible in the terminal.\nA client/server architecture. This for example can allow opencode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/sst/opencode",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?auto=webp&amp;s=61309f41e3088e5430a8b482b6ce6aa847601c88",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f64851bbe0e5f8ffcbc8bd1adfeeafd2571eb06a",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f3898aea9b1d2af361c3608afb02ef8e0e49a7b",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aebf1017ded100bc0b13e357737b86a02af082d4",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=16f1fbd9c6cecc901549703403f1a2afe794b563",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad75de582a2ac8bac433ef1449161c66b4d25270",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7c7c3aa8003202882d55ffd3d8d2539492961337",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "MN15Kr0BU7FNU3YsfgUK2zF_DPGWEuyYH_rIiDpxxDg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv9yhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv9yhq/opencode_like_claude_code_or_gemini_cli_but_works/",
          "stickied": false,
          "url": "https://github.com/sst/opencode",
          "subreddit_subscribers": 496591,
          "created_utc": 1752036103,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Project Website: [https://memos.openmem.net/](https://memos.openmem.net/) \n\nCode: [https://github.com/MemTensor/MemOS](https://github.com/MemTensor/MemOS)\n\nAbstract\n\n&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge \\[1\\]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.\n\n",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MemOS: A Memory OS for AI System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv9m3j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752034933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Project Website: &lt;a href=\"https://memos.openmem.net/\"&gt;https://memos.openmem.net/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/MemTensor/MemOS\"&gt;https://github.com/MemTensor/MemOS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency. Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods. While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations. Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge [1]. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.03724",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv9m3j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lv9m3j/memos_a_memory_os_for_ai_system/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.03724",
          "subreddit_subscribers": 496591,
          "created_utc": 1752034933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Code for Skywork-R1V3-38B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv94fb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#93b1ba",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f83fe7e25407d45dbbdba15a0ba6096c97396f0f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752033297,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/SkyworkAI/Skywork-R1V",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?auto=webp&amp;s=766c83ca1b5e94bd3c101728d426f91e36cd6756",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=841096f857747ed91b87136e691b10235b58ee3d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=84adc91d468fb0445c0780dec6ccfe69f96010c9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad55e85d9aa47720554f0e93ed478ecd33369403",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec52a006c597881fe28d0949f7d9f2503e2791c7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=18fc2391cbc954b12405e125548bf2ba5602e751",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bab6fac074163e493d8bb0d1c685a8cfa5c0b010",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rirUq_ye1VMAip5X7u0WCzRQyzDvHO-jLfw4QlzFOE8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv94fb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lv94fb/code_for_skyworkr1v338b/",
          "stickied": false,
          "url": "https://github.com/SkyworkAI/Skywork-R1V",
          "subreddit_subscribers": 496591,
          "created_utc": 1752033297,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[**State of Foundation Models, 2025 | Innovation Endeavors**](https://macro.com/app/pdf/696626a1-216b-493a-a0dd-181ce51ce327)\n\n**TL;DR**\n\n* **Generative AI has gone mainstream** – 1 in 8 workers worldwide now uses AI every month, with 90% of that growth happening in the last 6 months. AI-native applications are now well into the billions of annual run rate revenue.\n* **Scaling continues across all dimensions** – All technical metrics for models continue to improve &gt;10x year-over-year, including cost, intelligence, context windows, and more. The average duration of human task a model can reliably do is doubling every 7 months.\n* **The economics of foundation models are confusing** – OpenAI &amp; Anthropic are showing truly unprecedented growth, accelerating at $B+ of annual revenue. But, end-to-end training costs for frontier models near $500M, and the typical model becomes obsolete within 3 weeks of launch thanks to competition &amp; open source convergence.\n* **Just like the smartest humans, the smartest AI will “think before it speaks”** – Reasoning models trained to think before responding likely represent a new scaling law — but training them requires significant advances in post-training, including reinforcement learning &amp; reward models. Post-training may become more important than pre-training.\n* **AI has now infiltrated almost all specialist professions** – From engineers and accountants to designers and lawyers, AI copilots and agents are now tackling high-value tasks in virtually all knowledge worker domains.\n* **Agents finally work, but we are early in understanding how to build AI products** – Agents have finally hit the mainstream, but design patterns &amp; system architectures for AI products are still extremely early.\n* **“AI-native” organizations will look very different** – Flatter teams of capable generalists will become the norm as generative AI lessens the value of specialized skills. Many roles will blur — such as product, design, &amp; engineering.",
          "author_fullname": "t2_1j3y97g682",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "State of Foundation Models, 2025 | Innovation Endeavors",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv910v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752032984,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://macro.com/app/pdf/696626a1-216b-493a-a0dd-181ce51ce327\"&gt;&lt;strong&gt;State of Foundation Models, 2025 | Innovation Endeavors&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Generative AI has gone mainstream&lt;/strong&gt; – 1 in 8 workers worldwide now uses AI every month, with 90% of that growth happening in the last 6 months. AI-native applications are now well into the billions of annual run rate revenue.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scaling continues across all dimensions&lt;/strong&gt; – All technical metrics for models continue to improve &amp;gt;10x year-over-year, including cost, intelligence, context windows, and more. The average duration of human task a model can reliably do is doubling every 7 months.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The economics of foundation models are confusing&lt;/strong&gt; – OpenAI &amp;amp; Anthropic are showing truly unprecedented growth, accelerating at $B+ of annual revenue. But, end-to-end training costs for frontier models near $500M, and the typical model becomes obsolete within 3 weeks of launch thanks to competition &amp;amp; open source convergence.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Just like the smartest humans, the smartest AI will “think before it speaks”&lt;/strong&gt; – Reasoning models trained to think before responding likely represent a new scaling law — but training them requires significant advances in post-training, including reinforcement learning &amp;amp; reward models. Post-training may become more important than pre-training.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AI has now infiltrated almost all specialist professions&lt;/strong&gt; – From engineers and accountants to designers and lawyers, AI copilots and agents are now tackling high-value tasks in virtually all knowledge worker domains.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agents finally work, but we are early in understanding how to build AI products&lt;/strong&gt; – Agents have finally hit the mainstream, but design patterns &amp;amp; system architectures for AI products are still extremely early.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;“AI-native” organizations will look very different&lt;/strong&gt; – Flatter teams of capable generalists will become the norm as generative AI lessens the value of specialized skills. Many roles will blur — such as product, design, &amp;amp; engineering.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv910v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LeveredRecap",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv910v/state_of_foundation_models_2025_innovation/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752032984,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If there is a pocket-sized compact hardware that hosts large-sized open-source LLMs that you can connect offline, do you think it would be helpful?\n\nThe possible benefits:\n\n\\- You can use large-size open-source LLMs without using up your PC or smartphone's compute\n\n\\- More privacy-friendly since it's local\n\n\\- You can use high-performance LLMs even when offline",
          "author_fullname": "t2_qzhzgase",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Pocket LLM Server Just Like a Pocket WiFi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv8j5q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752031372,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If there is a pocket-sized compact hardware that hosts large-sized open-source LLMs that you can connect offline, do you think it would be helpful?&lt;/p&gt;\n\n&lt;p&gt;The possible benefits:&lt;/p&gt;\n\n&lt;p&gt;- You can use large-size open-source LLMs without using up your PC or smartphone&amp;#39;s compute&lt;/p&gt;\n\n&lt;p&gt;- More privacy-friendly since it&amp;#39;s local&lt;/p&gt;\n\n&lt;p&gt;- You can use high-performance LLMs even when offline&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv8j5q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Available_Ad_5360",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv8j5q/pocket_llm_server_just_like_a_pocket_wifi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv8j5q/pocket_llm_server_just_like_a_pocket_wifi/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752031372,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! 👋\n\nI’ve been working on some projects involving LLMs without visual input, and I realized I needed a way to let them “see” what’s happening on my screen in real time.\n\nSo I built ScreenMonitorMCP — a lightweight, open-source MCP server that captures your screen and streams it to any compatible LLM client. 🧠💻\n\n🧩 What it does:\n\t•\tGrabs your screen (or a portion of it) in real time\n\t•\tServes image frames via an MCP-compatible interface\n\t•\tWorks great with agent-based systems that need visual context (Blender agents, game bots, GUI interaction, etc.)\n\t•\tBuilt with FastAPI, OpenCV, Pillow, and PyGetWindow\n\nIt’s fast, simple, and designed to be part of a bigger multi-agent ecosystem I’m building.\n\nIf you’re experimenting with LLMs that could use visual awareness, or just want your AI tools to actually see what you’re doing — give it a try!\n\n💡 I’d love to hear your feedback or ideas. Contributions are more than welcome. And of course, stars on GitHub are super appreciated :)\n\n👉 GitHub link: https://github.com/inkbytefo/ScreenMonitorMCP\n\nThanks for reading!\n",
          "author_fullname": "t2_130xep8lk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just built an open-source MCP server to live-monitor your screen — ScreenMonitorMCP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv8cje",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030808,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt;\n\n&lt;p&gt;I’ve been working on some projects involving LLMs without visual input, and I realized I needed a way to let them “see” what’s happening on my screen in real time.&lt;/p&gt;\n\n&lt;p&gt;So I built ScreenMonitorMCP — a lightweight, open-source MCP server that captures your screen and streams it to any compatible LLM client. 🧠💻&lt;/p&gt;\n\n&lt;p&gt;🧩 What it does:\n    • Grabs your screen (or a portion of it) in real time\n    • Serves image frames via an MCP-compatible interface\n    • Works great with agent-based systems that need visual context (Blender agents, game bots, GUI interaction, etc.)\n    • Built with FastAPI, OpenCV, Pillow, and PyGetWindow&lt;/p&gt;\n\n&lt;p&gt;It’s fast, simple, and designed to be part of a bigger multi-agent ecosystem I’m building.&lt;/p&gt;\n\n&lt;p&gt;If you’re experimenting with LLMs that could use visual awareness, or just want your AI tools to actually see what you’re doing — give it a try!&lt;/p&gt;\n\n&lt;p&gt;💡 I’d love to hear your feedback or ideas. Contributions are more than welcome. And of course, stars on GitHub are super appreciated :)&lt;/p&gt;\n\n&lt;p&gt;👉 GitHub link: &lt;a href=\"https://github.com/inkbytefo/ScreenMonitorMCP\"&gt;https://github.com/inkbytefo/ScreenMonitorMCP&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?auto=webp&amp;s=bf60705d595299e3c8cb2ec2d672c79666ec17ce",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab7e199d6ef836609493bc97a96d35c77d096a50",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ce128a25f9bb904f5e77e4f0ae780244d81c1d8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0cacbad47021d89cc370eb1ebf5159a13583139",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe60a9b92bf536f7070c5f84be87be6d0f1650d0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f552ee41627e05ffc9e17aac711d60e9ecd88983",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41a5035dc61d3ff5e510592eae29a107d9b400b7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "mO9zLAwZf8FAbhe55hWrJp1t0pFxBbc3GLTki6wyMkw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lv8cje",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Being-6900",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv8cje/just_built_an_opensource_mcp_server_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv8cje/just_built_an_opensource_mcp_server_to/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030808,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for good books/open courses/blogs that explains best practices for hosting LLMs and VLMs in production",
          "author_fullname": "t2_45o2l0mg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good books or resources on hosting LLMs and VLMs in production",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv8b55",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030688,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for good books/open courses/blogs that explains best practices for hosting LLMs and VLMs in production&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv8b55",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SouvikMandal",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv8b55/good_books_or_resources_on_hosting_llms_and_vlms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv8b55/good_books_or_resources_on_hosting_llms_and_vlms/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030688,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anime conversations are often over the top and overall fun, i thought it would be interesting for storytelling and conversations\n\nHas anyone trained a model like this?",
          "author_fullname": "t2_rxgre5u8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anime and manga conversational model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv88fs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030451,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anime conversations are often over the top and overall fun, i thought it would be interesting for storytelling and conversations&lt;/p&gt;\n\n&lt;p&gt;Has anyone trained a model like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv88fs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Iq1pl",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv88fs/anime_and_manga_conversational_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv88fs/anime_and_manga_conversational_model/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030451,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*On Day 11, I gave you a brief introduction to the attention mechanism. Today, we’re going to implement it from scratch in Python. But before we dive into the code, let’s quickly revisit what attention is all about.*\n\n# What Is Attention? \n\n*Imagine you’re in a room with five people, and you’re trying to understand what’s going on. You don’t pay equal attention to all five people, you naturally focus more on the person who’s talking about something relevant.*\n\n*That’s exactly what attention does for LLMs. When reading a sentence, the model “pays more attention” to the words that are important for understanding the context.*\n\n*Let’s break it down with a simple example and real code!*\n\n# Our Example: “Cats love cozy windows”\n\n*Each word will be turned into a vector , just a bunch of numbers that represent the meaning of the word. Here’s what our made-up word vectors look like:*\n\n    import torch\n    \n    inputs = torch.tensor([\n        [0.10, 0.20, 0.30],  # Cats     (x¹)\n        [0.40, 0.50, 0.60],  # love     (x²)\n        [0.70, 0.80, 0.10],  # cozy     (x³)\n        [0.90, 0.10, 0.20]   # windows  (x⁴)\n    ])\n\n*Each row is an embedding for a word, just another way of saying, “this is how the model understands the meaning of the word in numbers.”*\n\n# 1: Calculating Attention Scores (How Similar Are These Words?)\n\n*Let’s say we want to find out how much attention the word* ***“****love****”*** *(second word) should pay to all the others.*\n\n*We do that by computing the dot product between the vector for “love” and the others. The higher the score, the more related they are.*\n\n    query = inputs[1]  # Embedding for \"love\"\n    \n    attn_scores = torch.empty(inputs.shape[0])\n    for i, x_i in enumerate(inputs):\n        attn_scores[i] = torch.dot(query, x_i)\n    \n    print(attn_scores)\n\n*Or, even faster, do it for all words at once using matrix multiplication:*\n\n    attn_scores_all = inputs @ inputs.T\n    print(attn_scores_all)\n\n*This gives us a matrix of similarities, each number tells how strongly one word is related to another.*\n\n# 2: Turning Scores into Meaningful Weights (Using Softmax)\n\n*Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.*\n\n*We use the softmax function to do this:*\n\n    attn_weights = torch.softmax(attn_scores_all, dim=-1)\n    print(attn_weights)\n\n*Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much “love” attends to “Cats,” “cozy,” and “windows.”*\n\n# 3: Creating a Context Vector (The Final Mix)\n\n*Here’s the cool part.*\n\n*Each word’s final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.*\n\n*If “love” pays 70% attention to “Cats” and 30% to “cozy,” the context vector will be a blend of those two word vectors.*\n\n*Let’s do it manually for “love” (row 2):*\n\n    attn_weights_love = attn_weights[1]\n    \n    context_vec_love = torch.zeros_like(inputs[0])\n    for i, x_i in enumerate(inputs):\n        context_vec_love += attn_weights_love[i] * x_i\n    \n    print(context_vec_love)\n\n*Or faster, do it for all words at once:*\n\n    context_vectors = attn_weights @ inputs\n    print(context_vectors)\n\n*Each row now holds a new version of the word that includes information from the whole sentence.* \n\n# Why Does This Matter?\n\n*This mechanism helps LLMs:*\n\n* ***Understand context:*** *It’s not just “what” a word is but how it fits in the sentence.*\n* ***Be smarter with predictions:*** *It can now decide that “windows” is important because “cats love cozy windows.”*\n* ***Handle longer sentences:*** *Attention lets the model scale and stay relevant, even with lots of words.*\n\n# TL;DR \n\n*The attention mechanism in LLMs:*\n\n1. *Calculates how similar each word is to every other word.*\n2. *Converts those scores into weights (softmax).*\n3. *Builds a new vector for each word using those weights (context vector).*\n\n*This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.*\n\n*If this helped clarify things, let me know!*.*Tomorrow we are going to code the self attention mechanism with key, query and value matrices.*",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 12/50: Building a Small Language Model from Scratch - Implementing a Simplified Attention Mechanism in Python",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv85jp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752030201,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;On Day 11, I gave you a brief introduction to the attention mechanism. Today, we’re going to implement it from scratch in Python. But before we dive into the code, let’s quickly revisit what attention is all about.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;What Is Attention? &lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Imagine you’re in a room with five people, and you’re trying to understand what’s going on. You don’t pay equal attention to all five people, you naturally focus more on the person who’s talking about something relevant.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;That’s exactly what attention does for LLMs. When reading a sentence, the model “pays more attention” to the words that are important for understanding the context.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let’s break it down with a simple example and real code!&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Our Example: “Cats love cozy windows”&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Each word will be turned into a vector , just a bunch of numbers that represent the meaning of the word. Here’s what our made-up word vectors look like:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import torch\n\ninputs = torch.tensor([\n    [0.10, 0.20, 0.30],  # Cats     (x¹)\n    [0.40, 0.50, 0.60],  # love     (x²)\n    [0.70, 0.80, 0.10],  # cozy     (x³)\n    [0.90, 0.10, 0.20]   # windows  (x⁴)\n])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Each row is an embedding for a word, just another way of saying, “this is how the model understands the meaning of the word in numbers.”&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;1: Calculating Attention Scores (How Similar Are These Words?)&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Let’s say we want to find out how much attention the word&lt;/em&gt; &lt;strong&gt;&lt;em&gt;“&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;strong&gt;&lt;em&gt;”&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;(second word) should pay to all the others.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We do that by computing the dot product between the vector for “love” and the others. The higher the score, the more related they are.&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;query = inputs[1]  # Embedding for &amp;quot;love&amp;quot;\n\nattn_scores = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores[i] = torch.dot(query, x_i)\n\nprint(attn_scores)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Or, even faster, do it for all words at once using matrix multiplication:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;attn_scores_all = inputs @ inputs.T\nprint(attn_scores_all)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;This gives us a matrix of similarities, each number tells how strongly one word is related to another.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;2: Turning Scores into Meaningful Weights (Using Softmax)&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We use the softmax function to do this:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(attn_scores_all, dim=-1)\nprint(attn_weights)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much “love” attends to “Cats,” “cozy,” and “windows.”&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;3: Creating a Context Vector (The Final Mix)&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Here’s the cool part.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Each word’s final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If “love” pays 70% attention to “Cats” and 30% to “cozy,” the context vector will be a blend of those two word vectors.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let’s do it manually for “love” (row 2):&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;attn_weights_love = attn_weights[1]\n\ncontext_vec_love = torch.zeros_like(inputs[0])\nfor i, x_i in enumerate(inputs):\n    context_vec_love += attn_weights_love[i] * x_i\n\nprint(context_vec_love)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Or faster, do it for all words at once:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;context_vectors = attn_weights @ inputs\nprint(context_vectors)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Each row now holds a new version of the word that includes information from the whole sentence.&lt;/em&gt; &lt;/p&gt;\n\n&lt;h1&gt;Why Does This Matter?&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;This mechanism helps LLMs:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Understand context:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It’s not just “what” a word is but how it fits in the sentence.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Be smarter with predictions:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It can now decide that “windows” is important because “cats love cozy windows.”&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Handle longer sentences:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Attention lets the model scale and stay relevant, even with lots of words.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;TL;DR &lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;The attention mechanism in LLMs:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;Calculates how similar each word is to every other word.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Converts those scores into weights (softmax).&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Builds a new vector for each word using those weights (context vector).&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;em&gt;This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If this helped clarify things, let me know!&lt;/em&gt;.&lt;em&gt;Tomorrow we are going to code the self attention mechanism with key, query and value matrices.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv85jp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752030201,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/dmeldrum6/LLM\\_Diff\\_Tool](https://github.com/dmeldrum6/LLM_Diff_Tool)  \nSingle page web app for comparing model vs model responses to the same prompt. Works with Open AI API compatible endpoints / GPT / Claude. The highlighting, as it is, is really only useful for comparing the same model against itself. I built this originally to compare token response count and response time across models and added to it. Poke around my github for some other LLM tools as well.",
          "author_fullname": "t2_j5as92mv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Web application for comparing responses from different LLMs side-by-side.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bxluumhnirbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9837d8d0868d4ff3fc67e357058677781ed43f62"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4a442922e1769119b8d240136312ce5e8944aec"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=63f5763a102cc0633482e8d26d45e7e17cd8b994"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6833a3fd712d909ec9e9d7e5df2cdd8898b5836"
                },
                {
                  "y": 494,
                  "x": 960,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88e85717ede9c79477df16db037e4fb938c0e387"
                },
                {
                  "y": 556,
                  "x": 1080,
                  "u": "https://preview.redd.it/bxluumhnirbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3a00d474ac5be2abfe4344894d0fc3eee1a5b8e"
                }
              ],
              "s": {
                "y": 729,
                "x": 1416,
                "u": "https://preview.redd.it/bxluumhnirbf1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=8797a354c799bd66a43cfb9ef61432d611f9730b"
              },
              "id": "bxluumhnirbf1"
            },
            "qloqg65oirbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb45692d70bd3ceb0ff8c041f4f2a8bfe1ed775c"
                },
                {
                  "y": 99,
                  "x": 216,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc8f326d895bfcbad62f66708eeeebb9e51fb27e"
                },
                {
                  "y": 148,
                  "x": 320,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=97e2d7640be4ee779fad3afd795f28ea91532736"
                },
                {
                  "y": 296,
                  "x": 640,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=95a32021e44b91fd44e979ef6f573370f94f9940"
                },
                {
                  "y": 444,
                  "x": 960,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=52d8348cf973f0b7a7d3b21f9c1a6643b27f2ab0"
                },
                {
                  "y": 499,
                  "x": 1080,
                  "u": "https://preview.redd.it/qloqg65oirbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=beafd9a91c957b3f2f2a81c09413b2ba3b985d18"
                }
              ],
              "s": {
                "y": 636,
                "x": 1374,
                "u": "https://preview.redd.it/qloqg65oirbf1.png?width=1374&amp;format=png&amp;auto=webp&amp;s=adbe0f9d77599fa71736d50d314206e9869fadd0"
              },
              "id": "qloqg65oirbf1"
            }
          },
          "name": "t3_1lv7xnh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "bxluumhnirbf1",
                "id": 701831086
              },
              {
                "media_id": "qloqg65oirbf1",
                "id": 701831087
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/CQcvsi0DsFKRF2nZhD39RmrPGRx0DKN3222kGde5b50.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752029539,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/dmeldrum6/LLM_Diff_Tool\"&gt;https://github.com/dmeldrum6/LLM_Diff_Tool&lt;/a&gt;&lt;br/&gt;\nSingle page web app for comparing model vs model responses to the same prompt. Works with Open AI API compatible endpoints / GPT / Claude. The highlighting, as it is, is really only useful for comparing the same model against itself. I built this originally to compare token response count and response time across models and added to it. Poke around my github for some other LLM tools as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lv7xnh",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv7xnh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdElectronic8073",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7xnh/web_application_for_comparing_responses_from/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lv7xnh",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029539,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Forgive me if someone has asked this recently.\n\nBut grok 3 was amazing in March 2025 untill they censored it.\n\nNow I want a similar LLM that can make nsfw explicit vulgar erotic content for my \"research\" purposes.\n\nActually jokes apart I make character animations and every character is supposed to be different and it just feels bad to see everyone talk to sophisticatedly because of censored llm",
          "author_fullname": "t2_ne96c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Most explicit erotic LLM model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv7vsm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752029377,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Forgive me if someone has asked this recently.&lt;/p&gt;\n\n&lt;p&gt;But grok 3 was amazing in March 2025 untill they censored it.&lt;/p&gt;\n\n&lt;p&gt;Now I want a similar LLM that can make nsfw explicit vulgar erotic content for my &amp;quot;research&amp;quot; purposes.&lt;/p&gt;\n\n&lt;p&gt;Actually jokes apart I make character animations and every character is supposed to be different and it just feels bad to see everyone talk to sophisticatedly because of censored llm&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv7vsm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dean_hunter7",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7vsm/most_explicit_erotic_llm_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv7vsm/most_explicit_erotic_llm_model/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029377,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Web application for comparing responses from different Large Language Models (LLMs) side-by-side, the highlighting (as it is currently) is really only useful for same model vs. same model. I originally built it and use it for checking token counts on the same prompt model vs. model. Poke around my github for some other LLM related tools as well.",
          "author_fullname": "t2_j5as92mv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "LLM Model Response Diff Tool",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6pn0oqv1irbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 49,
                  "x": 108,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f62bf444fb15a398d68df185f3bc49ecd2e407e0"
                },
                {
                  "y": 99,
                  "x": 216,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3b0a24c11ff71ebbfb50105c7b63d7fa0496a63"
                },
                {
                  "y": 148,
                  "x": 320,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e246201602c9f96ed1ce3c841a83aa71c950cd8a"
                },
                {
                  "y": 296,
                  "x": 640,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbd36d47723fb77569f10fb326210801db2f7644"
                },
                {
                  "y": 444,
                  "x": 960,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea8509e7ecb5b07ef5adfa04d2524d7036923883"
                },
                {
                  "y": 499,
                  "x": 1080,
                  "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=55d4136807991abf18a90de6d8005b0ae67930e9"
                }
              ],
              "s": {
                "y": 636,
                "x": 1374,
                "u": "https://preview.redd.it/6pn0oqv1irbf1.png?width=1374&amp;format=png&amp;auto=webp&amp;s=262f91aadca38ae2e9bf6a02d341733a1a800aad"
              },
              "id": "6pn0oqv1irbf1"
            },
            "1ibgzf21irbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c57ff21157ae258b7cff43043dbf29bf30b3ff8"
                },
                {
                  "y": 111,
                  "x": 216,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01449a4454c4d696df1910900ea3a9465594e377"
                },
                {
                  "y": 164,
                  "x": 320,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e110f92b6cc92f4471597ba18177e8db50fa6e0"
                },
                {
                  "y": 329,
                  "x": 640,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1fc286a1a478945371c7397d3f4aa761ecacf05a"
                },
                {
                  "y": 494,
                  "x": 960,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=114c65b07342dab8f7df9ab0f234d5853b34e576"
                },
                {
                  "y": 556,
                  "x": 1080,
                  "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39cfa202b71b3ea250e94f024f820dcda38e2da4"
                }
              ],
              "s": {
                "y": 729,
                "x": 1416,
                "u": "https://preview.redd.it/1ibgzf21irbf1.png?width=1416&amp;format=png&amp;auto=webp&amp;s=ad16d839d6b20d89548247a67271fa9f98bdcf6a"
              },
              "id": "1ibgzf21irbf1"
            }
          },
          "name": "t3_1lv7tgz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "1ibgzf21irbf1",
                "id": 701827953
              },
              {
                "media_id": "6pn0oqv1irbf1",
                "id": 701827954
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DC9IVlOtWtf-pycmMPV9D8CTstn5VCclSuncKX3SheA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752029170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Web application for comparing responses from different Large Language Models (LLMs) side-by-side, the highlighting (as it is currently) is really only useful for same model vs. same model. I originally built it and use it for checking token counts on the same prompt model vs. model. Poke around my github for some other LLM related tools as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lv7tgz",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv7tgz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdElectronic8073",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7tgz/llm_model_response_diff_tool/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lv7tgz",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029170,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI recently reimplemented Google's open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.\n\nThis was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers:\n- [Gemma 1](https://arxiv.org/pdf/2403.08295)\n- [Gemma 2](https://arxiv.org/pdf/2408.00118)\n- [Gemma 3](https://arxiv.org/pdf/2406.07101) (multimodal vision)\n\nThis was a purely educational reimplementation.\n\nI also shared this on LinkedIn with more details if you're curious:\n🔗 [LinkedIn post here](https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;utm_medium=member_desktop)\n\nI'm now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.\n\nWould love any feedback, suggestions, or advice on what model to reimplement next!\n\nThanks 🙏",
          "author_fullname": "t2_6qpq9avr5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reimplementing an LLM from Scratch",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv7s0r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752029046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently reimplemented Google&amp;#39;s open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.&lt;/p&gt;\n\n&lt;p&gt;This was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers:\n- &lt;a href=\"https://arxiv.org/pdf/2403.08295\"&gt;Gemma 1&lt;/a&gt;\n- &lt;a href=\"https://arxiv.org/pdf/2408.00118\"&gt;Gemma 2&lt;/a&gt;\n- &lt;a href=\"https://arxiv.org/pdf/2406.07101\"&gt;Gemma 3&lt;/a&gt; (multimodal vision)&lt;/p&gt;\n\n&lt;p&gt;This was a purely educational reimplementation.&lt;/p&gt;\n\n&lt;p&gt;I also shared this on LinkedIn with more details if you&amp;#39;re curious:\n🔗 &lt;a href=\"https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;amp;utm_medium=member_desktop\"&gt;LinkedIn post here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.&lt;/p&gt;\n\n&lt;p&gt;Would love any feedback, suggestions, or advice on what model to reimplement next!&lt;/p&gt;\n\n&lt;p&gt;Thanks 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lv7s0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CodingWithSatyam",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752029046,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I set up my own PC months back with the following: \n\nAMD Ryzen Threadripper 3960X\n\n2x NVIDIA 4080s\n\n128GB DDR4 RAM\n\nTRX40 AORUS PRO WIFI\n\n2TB STORAGE\n\n  \nSet up my server via Proxmox Hypervisor and running my VMs on it. \n\nBeen accessing it via SSH (Terminal) on a host machine on my home network. I am not a programmer and while I know many might judge me, I have been mostly vibe coding to achieve a lot of what I have done so far. I am 100% doing everything via terminal - I tried accessing the server via VNC, but I don't like the feel and I went back to accessing via terminal.\n\nEverything I did to this point, was my first time and I'm really just trying to continue to get better at the things I can. \n\nMy question is - is there a way to access things like Cursor as well as n8n from my host machine (a Mac) which I SSH into the server from. \n\nThese might be n00b questions, but I will really appreciate and hope that someone who is more advanced and knowledgeable sees this and can provide a robust and understandable response to this. \n\nThanks in advance",
          "author_fullname": "t2_a11p0eov",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guidance Needed on Local Setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv7j1j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752028292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I set up my own PC months back with the following: &lt;/p&gt;\n\n&lt;p&gt;AMD Ryzen Threadripper 3960X&lt;/p&gt;\n\n&lt;p&gt;2x NVIDIA 4080s&lt;/p&gt;\n\n&lt;p&gt;128GB DDR4 RAM&lt;/p&gt;\n\n&lt;p&gt;TRX40 AORUS PRO WIFI&lt;/p&gt;\n\n&lt;p&gt;2TB STORAGE&lt;/p&gt;\n\n&lt;p&gt;Set up my server via Proxmox Hypervisor and running my VMs on it. &lt;/p&gt;\n\n&lt;p&gt;Been accessing it via SSH (Terminal) on a host machine on my home network. I am not a programmer and while I know many might judge me, I have been mostly vibe coding to achieve a lot of what I have done so far. I am 100% doing everything via terminal - I tried accessing the server via VNC, but I don&amp;#39;t like the feel and I went back to accessing via terminal.&lt;/p&gt;\n\n&lt;p&gt;Everything I did to this point, was my first time and I&amp;#39;m really just trying to continue to get better at the things I can. &lt;/p&gt;\n\n&lt;p&gt;My question is - is there a way to access things like Cursor as well as n8n from my host machine (a Mac) which I SSH into the server from. &lt;/p&gt;\n\n&lt;p&gt;These might be n00b questions, but I will really appreciate and hope that someone who is more advanced and knowledgeable sees this and can provide a robust and understandable response to this. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv7j1j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SolidRemote8316",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv7j1j/guidance_needed_on_local_setup/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv7j1j/guidance_needed_on_local_setup/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752028292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to understand the parameter \"tools\" of the Anthropic and how the Claude understands if it should respond normally or it should select one of the tools in the JSON file. \n\nMore specifically I am wondering if only a system prompt with some few shot examples can do the job or a real fine tuning is the way to go.",
          "author_fullname": "t2_13dlr90b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How Antropic has teached the Claude to decide wherher to choose a tool or respond normally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv6mju",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752025640,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to understand the parameter &amp;quot;tools&amp;quot; of the Anthropic and how the Claude understands if it should respond normally or it should select one of the tools in the JSON file. &lt;/p&gt;\n\n&lt;p&gt;More specifically I am wondering if only a system prompt with some few shot examples can do the job or a real fine tuning is the way to go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv6mju",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Amir_PD",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv6mju/how_antropic_has_teached_the_claude_to_decide/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv6mju/how_antropic_has_teached_the_claude_to_decide/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752025640,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any local models that have high precision integer or decimal when doing calculations. The paid big guys do,  claude sonnet 4 , gemini 2.5 pro, Chatgpt. . But can't find any downloadable ones (tried up to 70b) . Anything above 24 or 32 digits they just give incorrect results.  ",
          "author_fullname": "t2_1gzvdilba8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "High Precision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv5uie",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752023315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any local models that have high precision integer or decimal when doing calculations. The paid big guys do,  claude sonnet 4 , gemini 2.5 pro, Chatgpt. . But can&amp;#39;t find any downloadable ones (tried up to 70b) . Anything above 24 or 32 digits they just give incorrect results.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv5uie",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AlgorithmicMuse",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv5uie/high_precision/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv5uie/high_precision/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752023315,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the [release of DGX spark later this month](https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/), I was wondering how a new-ish homebrew system would compare.\n\nAll 5000-series NVIDIA cards are equipped with PCIE Gen 5, which puts the upper limit for cross-bus bandwidth at 128GB/s.  Dual channel DDR5 is capable of ~96GB/s and quad channel doubles that to ~192GB/s (bottlenecked to 128GB/s over PCIe).  Resizable BAR should allow for transfers to have minimal overhead.\n\n[HuggingFace accelerate](https://huggingface.co/docs/accelerate/v1.8.1/index) hierarchically distributes PyTorch models between the memory of GPU(s) and the CPU memory, and copies the layers to the VRAM during inference so only the GPU performs computation.\n\nThis is compared to:\n\n* [llama.cpp](https://github.com/ggml-org/llama.cpp) which splits the model between VRAM and CPU memory, where the GPU computes the layers stored in VRAM and the CPU computes the layers stored in CPU memory.\n\n* [vllm](https://github.com/vllm-project/vllm) which splits the model between multiple GPUs' VRAM and uses tensor parallelism to pipeline the layers between GPUs.\n\n\nMy expectation is that the 128GB/s bandwidth of PCIe 5.0 x16 would allow accelerate to utilize system memory at nearly maximum speed.  128GB/s bandwidth doesn't quite match DGX spark, but a powerful GPU and lots of DDR5 (in quad channel?) could beat the spark for batch inference.",
          "author_fullname": "t2_uei1i14w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How fast is inference when utilizing DDR5 and PCIe 5.0x16?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv5je7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752022394,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the &lt;a href=\"https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/\"&gt;release of DGX spark later this month&lt;/a&gt;, I was wondering how a new-ish homebrew system would compare.&lt;/p&gt;\n\n&lt;p&gt;All 5000-series NVIDIA cards are equipped with PCIE Gen 5, which puts the upper limit for cross-bus bandwidth at 128GB/s.  Dual channel DDR5 is capable of ~96GB/s and quad channel doubles that to ~192GB/s (bottlenecked to 128GB/s over PCIe).  Resizable BAR should allow for transfers to have minimal overhead.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/docs/accelerate/v1.8.1/index\"&gt;HuggingFace accelerate&lt;/a&gt; hierarchically distributes PyTorch models between the memory of GPU(s) and the CPU memory, and copies the layers to the VRAM during inference so only the GPU performs computation.&lt;/p&gt;\n\n&lt;p&gt;This is compared to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;llama.cpp&lt;/a&gt; which splits the model between VRAM and CPU memory, where the GPU computes the layers stored in VRAM and the CPU computes the layers stored in CPU memory.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/vllm-project/vllm\"&gt;vllm&lt;/a&gt; which splits the model between multiple GPUs&amp;#39; VRAM and uses tensor parallelism to pipeline the layers between GPUs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My expectation is that the 128GB/s bandwidth of PCIe 5.0 x16 would allow accelerate to utilize system memory at nearly maximum speed.  128GB/s bandwidth doesn&amp;#39;t quite match DGX spark, but a powerful GPU and lots of DDR5 (in quad channel?) could beat the spark for batch inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?auto=webp&amp;s=2c3905dab01b88b0dbab01fcb0b574d9f1e512b5",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd25657d3fce734d4025693e620867a7cf866fd1",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed12cf3ccefd6b6e923ec4d43579ac26f2d80130",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf58c691ecc4a68a7e65aa0c0d4e284c08c9845a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0957457f9a703a8e5f4750384c880b16a2c80648",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6ffcb9da830480ee39d159a6310bc89df79861f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv5je7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ButThatsMyRamSlot",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752022394,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "user_reports": [],
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's local about this?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv53nn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 176,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "author_fullname": "t2_gi7a36v6",
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 176,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/M9nqhox0h181pZd-C5TPbg7bc7em0clENAL-R3A-2U4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "crosspost_parent_list": [
            {
              "approved_at_utc": null,
              "subreddit": "ChatGPT",
              "selftext": "",
              "author_fullname": "t2_1305uo",
              "saved": false,
              "mod_reason_title": null,
              "gilded": 0,
              "clicked": false,
              "title": "getting rejected by local models must be brutal",
              "link_flair_richtext": [
                {
                  "e": "text",
                  "t": "Funny "
                }
              ],
              "subreddit_name_prefixed": "r/ChatGPT",
              "hidden": false,
              "pwls": 6,
              "link_flair_css_class": "",
              "downs": 0,
              "thumbnail_height": 140,
              "top_awarded_type": null,
              "hide_score": false,
              "name": "t3_1luudp3",
              "quarantine": false,
              "link_flair_text_color": "light",
              "upvote_ratio": 0.96,
              "author_flair_background_color": "",
              "ups": 951,
              "total_awards_received": 0,
              "media_embed": {},
              "thumbnail_width": 140,
              "author_flair_template_id": null,
              "is_original_content": false,
              "user_reports": [],
              "secure_media": null,
              "is_reddit_media_domain": true,
              "is_meta": false,
              "category": null,
              "secure_media_embed": {},
              "link_flair_text": "Funny ",
              "can_mod_post": false,
              "score": 951,
              "approved_by": null,
              "is_created_from_ads_ui": false,
              "author_premium": false,
              "thumbnail": "https://a.thumbs.redditmedia.com/M9nqhox0h181pZd-C5TPbg7bc7em0clENAL-R3A-2U4.jpg",
              "edited": false,
              "author_flair_css_class": null,
              "author_flair_richtext": [
                {
                  "a": ":Discord:",
                  "e": "emoji",
                  "u": "https://emoji.redditmedia.com/0zlhaela6zub1_t5_7hqomg/Discord"
                }
              ],
              "gildings": {},
              "post_hint": "image",
              "content_categories": null,
              "is_self": false,
              "subreddit_type": "public",
              "created": 1751995008,
              "link_flair_type": "richtext",
              "wls": 6,
              "removed_by_category": null,
              "banned_by": null,
              "author_flair_type": "richtext",
              "domain": "i.redd.it",
              "allow_live_comments": false,
              "selftext_html": null,
              "likes": null,
              "suggested_sort": null,
              "banned_at_utc": null,
              "url_overridden_by_dest": "https://i.redd.it/rqrg67unoobf1.jpeg",
              "view_count": null,
              "archived": false,
              "no_follow": false,
              "is_crosspostable": false,
              "pinned": false,
              "over_18": false,
              "preview": {
                "images": [
                  {
                    "source": {
                      "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?auto=webp&amp;s=b06b7a4c6077025a13cab3b669b4fbbc06483324",
                      "width": 720,
                      "height": 992
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=78877e3893da49a4298497e65291068e096bd5c6",
                        "width": 108,
                        "height": 148
                      },
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=642fd11e74684525d7e78fdc561233ed832763a8",
                        "width": 216,
                        "height": 297
                      },
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=697478bd8ddbc503bb8ec2cfb50b942e942aa274",
                        "width": 320,
                        "height": 440
                      },
                      {
                        "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1",
                        "width": 640,
                        "height": 881
                      }
                    ],
                    "variants": {},
                    "id": "xC373qc6EerFn_qLV-HUabx3XUSwRr-yszT4hsn6ILg"
                  }
                ],
                "enabled": true
              },
              "all_awardings": [],
              "awarders": [],
              "media_only": false,
              "link_flair_template_id": "935162a0-7be9-11ed-913e-6a257d69e3b3",
              "can_gild": false,
              "spoiler": false,
              "locked": false,
              "author_flair_text": ":Discord:",
              "treatment_tags": [],
              "visited": false,
              "removed_by": null,
              "mod_note": null,
              "distinguished": null,
              "subreddit_id": "t5_7hqomg",
              "author_is_blocked": false,
              "mod_reason_by": null,
              "num_reports": null,
              "removal_reason": null,
              "link_flair_background_color": "#0dd3bb",
              "id": "1luudp3",
              "is_robot_indexable": true,
              "report_reasons": null,
              "author": "ewelumokeke",
              "discussion_type": null,
              "num_comments": 109,
              "send_replies": true,
              "contest_mode": false,
              "mod_reports": [],
              "author_patreon_flair": false,
              "author_flair_text_color": "dark",
              "permalink": "/r/ChatGPT/comments/1luudp3/getting_rejected_by_local_models_must_be_brutal/",
              "stickied": false,
              "url": "https://i.redd.it/rqrg67unoobf1.jpeg",
              "subreddit_subscribers": 10865701,
              "created_utc": 1751995008,
              "num_crossposts": 3,
              "media": null,
              "is_video": false
            }
          ],
          "created": 1752021152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rqrg67unoobf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?auto=webp&amp;s=b06b7a4c6077025a13cab3b669b4fbbc06483324",
                  "width": 720,
                  "height": 992
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=78877e3893da49a4298497e65291068e096bd5c6",
                    "width": 108,
                    "height": 148
                  },
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=642fd11e74684525d7e78fdc561233ed832763a8",
                    "width": 216,
                    "height": 297
                  },
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=697478bd8ddbc503bb8ec2cfb50b942e942aa274",
                    "width": 320,
                    "height": 440
                  },
                  {
                    "url": "https://preview.redd.it/rqrg67unoobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a79114d00e031982e11ecfee91ec34ce4a3dbc1",
                    "width": 640,
                    "height": 881
                  }
                ],
                "variants": {},
                "id": "xC373qc6EerFn_qLV-HUabx3XUSwRr-yszT4hsn6ILg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv53nn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HOLUPREDICTIONS",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "crosspost_parent": "t3_1luudp3",
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv53nn/whats_local_about_this/",
          "stickied": false,
          "url": "https://i.redd.it/rqrg67unoobf1.jpeg",
          "subreddit_subscribers": 496591,
          "created_utc": 1752021152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Models have been converging on \"not x, but y\" type phrases to an absurd degree. So here's a leaderboard for it.  \n  \nI don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Not x, but y\" Slop Leaderboard",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv2t7n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "transparent",
          "ups": 666,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 666,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uL3gwRdCk8J-WNVPdRm8RK50kebervSdJ936_btCrQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752014921,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here&amp;#39;s a leaderboard for it.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nxw6fmegaqbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nxw6fmegaqbf1.png?auto=webp&amp;s=5f72913393ebeea55d572d59030a515d7e026ec0",
                  "width": 989,
                  "height": 1010
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8120f376d154d7a8c30aa965a5d4aec99b2eee8b",
                    "width": 108,
                    "height": 110
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=50db8e8d3a69f67391ef203884d0459d28cb0f36",
                    "width": 216,
                    "height": 220
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cda200654cca3e31e4c92e08cc4e54eb814d342",
                    "width": 320,
                    "height": 326
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f634168f40782641454db362ee799df6971e84f",
                    "width": 640,
                    "height": 653
                  },
                  {
                    "url": "https://preview.redd.it/nxw6fmegaqbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c9b95ecbe7aea07c72952b87efcbf28616d0137",
                    "width": 960,
                    "height": 980
                  }
                ],
                "variants": {},
                "id": "kkZ-LaZPi1jHaYXALY32ZrWs3vzskemUWmI2c7oZUfE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lv2t7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 140,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/",
          "stickied": false,
          "url": "https://i.redd.it/nxw6fmegaqbf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1752014921,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "RTX Pro 6000 Blackwell is much better with 30% more CUDA cores and twice the VRAM, than RTX 6000 Ada (and even better than RTX 6000), but the price difference is really minimum, like the prices of those 3 generations are only $1k apart for new ($8k, $7k and $6k)  and $2k apart for used ($8k - only new, $6k and $4k).",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why hasn't RTX Pro 6000 Balckwell significantly shake down the price of older RTX 6000 / RTX 6000 Ada",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1z7b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752012747,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RTX Pro 6000 Blackwell is much better with 30% more CUDA cores and twice the VRAM, than RTX 6000 Ada (and even better than RTX 6000), but the price difference is really minimum, like the prices of those 3 generations are only $1k apart for new ($8k, $7k and $6k)  and $2k apart for used ($8k - only new, $6k and $4k).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lv1z7b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1z7b/why_hasnt_rtx_pro_6000_balckwell_significantly/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752012747,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Also would love to know your experiences with context/prompt compression, is it worth it? ",
          "author_fullname": "t2_1p50pl73j2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best context compression other than llmlingua?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1m0i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752011846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Also would love to know your experiences with context/prompt compression, is it worth it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv1m0i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GreenTreeAndBlueSky",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1m0i/best_context_compression_other_than_llmlingua/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752011846,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking for a lightweight model that can run on the background, basically spell checking/fixing typos as you go. Any suggestions?",
          "author_fullname": "t2_yq51a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a Grammarly equivalent I can run locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1fpo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752011414,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a lightweight model that can run on the background, basically spell checking/fixing typos as you go. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv1fpo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rorowhat",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1fpo/is_there_a_grammarly_equivalent_i_can_run_locally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1fpo/is_there_a_grammarly_equivalent_i_can_run_locally/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752011414,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im have a hugh collection of transcripts from talks, podcasts, presentations etc. Often I want to use lots of them in a local AI tool, but the texts are too long for the context windwow. I wonder if there are good prompts to compress (summarize) the transcripts in a way that no details or key concepts are lost nut the size of the text gets way smaller. Any research or experience on that?",
          "author_fullname": "t2_13n69u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Prompt to \"compress\" transcripts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv1763",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752010821,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im have a hugh collection of transcripts from talks, podcasts, presentations etc. Often I want to use lots of them in a local AI tool, but the texts are too long for the context windwow. I wonder if there are good prompts to compress (summarize) the transcripts in a way that no details or key concepts are lost nut the size of the text gets way smaller. Any research or experience on that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv1763",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "simondueckert",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv1763/prompt_to_compress_transcripts/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752010821,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there tools to make a model respond to itself using different system prompts ? Maybe supporting multiple system prompts (ie multiple characters). Maybe something with nodes and conditions like for image generation.",
          "author_fullname": "t2_e70ziuup",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there tools to make a model respond to itself using different system prompts ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv0wvw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752010121,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there tools to make a model respond to itself using different system prompts ? Maybe supporting multiple system prompts (ie multiple characters). Maybe something with nodes and conditions like for image generation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv0wvw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "marvellousBeing",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv0wvw/are_there_tools_to_make_a_model_respond_to_itself/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv0wvw/are_there_tools_to_make_a_model_respond_to_itself/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752010121,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m trying to process a scraped HTML with Claude and it keeps hallucinating and messing up the keys.  \nEven when I specify the schema, it adds garbage.  \nAnyone found a prompt trick, system message, or post-processing fix that reliably works?  \n(I tried regex cleanup but it’s shaky.)",
          "author_fullname": "t2_6xt2aego",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Claude / GPT4 keeps breaking JSON formatting. Anyone find a real fix?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lv0ukq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752009964,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m trying to process a scraped HTML with Claude and it keeps hallucinating and messing up the keys.&lt;br/&gt;\nEven when I specify the schema, it adds garbage.&lt;br/&gt;\nAnyone found a prompt trick, system message, or post-processing fix that reliably works?&lt;br/&gt;\n(I tried regex cleanup but it’s shaky.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lv0ukq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_obhodro_chele_",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lv0ukq/claude_gpt4_keeps_breaking_json_formatting_anyone/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv0ukq/claude_gpt4_keeps_breaking_json_formatting_anyone/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752009964,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm new to LLM and thinking about the following build to use for local LLM (on a VM in Proxmox, will be using PCI passthrough for GPUs) and homelab (some VMs and/or containers for software development, tinkering, etc.) combo. Could you please share your thoughts? Thanks in advance...\n\n**Mainboard:** JGINYUE X99-8D4/2.5G [https://jginyue.com/index/Article/show/cat\\_id/48/id/219](https://jginyue.com/index/Article/show/cat_id/48/id/219)\n\nhttps://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;format=png&amp;auto=webp&amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e\n\nor MACHINIST X99 D8 MAX (4x PCI 3.0 x16)\n\nhttps://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;format=png&amp;auto=webp&amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c\n\n**CPUs:** 2 x E5-2699A V4 or 2 x E5-2699 V4 (40 PCI lanes per CPU) Total 44 cores, 88 threads\n\n**Memory:** 256 GB (up to 512 GB)\n\n**GPUs:** 2x Asus ROG Strix 3090 OC\n\n**Power Supply:** min 1200w\n\n**Case:** mining rig + PCI 3.0/4.0 risers",
          "author_fullname": "t2_6gt8ct53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thoughts on local LLM &amp; Proxmox homelab using Chinese x99 dual Xeon board + 2x3090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 123,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ieuygd1xjpbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/ieuygd1xjpbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d627589fa350d44eea5b5785fc7e9c1b6b83f36e"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/ieuygd1xjpbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d18dc807a0ed65a97985c54dd09e529ba9b1f69"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/ieuygd1xjpbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aec1aafd5d9152710eeec9cc69feb80087d9469a"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/ieuygd1xjpbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=66424894ac15d339bb7fee6f3bb3c8996d8f5ac7"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb97958f83ae8124abecabc26bcc7223b52ec2be"
                }
              ],
              "s": {
                "y": 960,
                "x": 960,
                "u": "https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;format=png&amp;auto=webp&amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c"
              },
              "id": "ieuygd1xjpbf1"
            },
            "zhe921uxjpbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2e6f5d5b8bbe595f32db43d2b89bdf0a3f14d55"
                },
                {
                  "y": 189,
                  "x": 216,
                  "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=374eb0f78a445b65e2bcd4efd39d0f92f904db4d"
                },
                {
                  "y": 281,
                  "x": 320,
                  "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f66e2752c9a3c5b6d750ff16f7991a4113eedc7"
                },
                {
                  "y": 562,
                  "x": 640,
                  "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddb19a39e3576e875c44887795c02db49f57393c"
                },
                {
                  "y": 843,
                  "x": 960,
                  "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e8775a907fb448f499794ab68a51c78e4ccb2a3"
                },
                {
                  "y": 949,
                  "x": 1080,
                  "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=40223c55294735c7fabb795ac409aeca188cbb5a"
                }
              ],
              "s": {
                "y": 2900,
                "x": 3300,
                "u": "https://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;format=png&amp;auto=webp&amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e"
              },
              "id": "zhe921uxjpbf1"
            }
          },
          "name": "t3_1luz92k",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/6aWZ0xKOG8quekcHyrX4K2kLmgfZdLO_Gm-PfzPFH28.jpg",
          "edited": 1752021778,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752006185,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to LLM and thinking about the following build to use for local LLM (on a VM in Proxmox, will be using PCI passthrough for GPUs) and homelab (some VMs and/or containers for software development, tinkering, etc.) combo. Could you please share your thoughts? Thanks in advance...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Mainboard:&lt;/strong&gt; JGINYUE X99-8D4/2.5G &lt;a href=\"https://jginyue.com/index/Article/show/cat_id/48/id/219\"&gt;https://jginyue.com/index/Article/show/cat_id/48/id/219&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e\"&gt;https://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;or MACHINIST X99 D8 MAX (4x PCI 3.0 x16)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c\"&gt;https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPUs:&lt;/strong&gt; 2 x E5-2699A V4 or 2 x E5-2699 V4 (40 PCI lanes per CPU) Total 44 cores, 88 threads&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 256 GB (up to 512 GB)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPUs:&lt;/strong&gt; 2x Asus ROG Strix 3090 OC&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Power Supply:&lt;/strong&gt; min 1200w&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; mining rig + PCI 3.0/4.0 risers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luz92k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Long-Caterpillar675",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752006185,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to sort through and host my own LLM but I really dont know how these might compare, I'm using Void as the IDE and trying to replicate pretty close to what Cursor offers.  \n\nIs it even comparable?\n\nhttps://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6\n\n",
          "author_fullname": "t2_4x4nv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local LLM for Code assist similar to Sonnet 4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tv72kg9xipbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 80,
                  "x": 108,
                  "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9359bfb3eb5d5fbb45c543d14562646f51078126"
                },
                {
                  "y": 160,
                  "x": 216,
                  "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d59bee8998928f6e44238a5f0ff7c4d9374cece1"
                },
                {
                  "y": 237,
                  "x": 320,
                  "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=db29ff294f0db5d815b8268148d46672650dae63"
                }
              ],
              "s": {
                "y": 466,
                "x": 628,
                "u": "https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;format=png&amp;auto=webp&amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6"
              },
              "id": "tv72kg9xipbf1"
            }
          },
          "name": "t3_1luytx2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aFc1eTSyhpbPCDNRYEtxNU2fc0UmI3sPPSwMWbqT_BI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752005195,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to sort through and host my own LLM but I really dont know how these might compare, I&amp;#39;m using Void as the IDE and trying to replicate pretty close to what Cursor offers.  &lt;/p&gt;\n\n&lt;p&gt;Is it even comparable?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6\"&gt;https://preview.redd.it/tv72kg9xipbf1.png?width=628&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee67695b0a353a8b0805fc8b111ba81fe0d36fd6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luytx2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kainzo",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luytx2/best_local_llm_for_code_assist_similar_to_sonnet_4/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752005195,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nIs there a self-hostable LLM specialized in answering someone's texts as if it was the actual person responding, e.g. to people who make you loose time when asking something from you (see [nohello.net](https://nohello.net/)) and people who are just too much (see [Silicon Valley S06E01](https://bw.artemislena.eu/silicon-valley/wiki/Artificial_Lack_of_Intelligence#:~:text=Gilfoyle%20creates%20an%20artificial%20intelligence%20program%20to%20respond%20to%20Dinesh%27s%20messages)) ?\n\nThanks",
          "author_fullname": "t2_cbs8s4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM to answer someone's contacts as themselves ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luyhi9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752004390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Is there a self-hostable LLM specialized in answering someone&amp;#39;s texts as if it was the actual person responding, e.g. to people who make you loose time when asking something from you (see &lt;a href=\"https://nohello.net/\"&gt;nohello.net&lt;/a&gt;) and people who are just too much (see &lt;a href=\"https://bw.artemislena.eu/silicon-valley/wiki/Artificial_Lack_of_Intelligence#:%7E:text=Gilfoyle%20creates%20an%20artificial%20intelligence%20program%20to%20respond%20to%20Dinesh%27s%20messages\"&gt;Silicon Valley S06E01&lt;/a&gt;) ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luyhi9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaKi_87",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luyhi9/llm_to_answer_someones_contacts_as_themselves/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luyhi9/llm_to_answer_someones_contacts_as_themselves/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752004390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am curious to understand if models tend to perform better if you treat it like a tin-can, structuring the prompt as imperative commands. I suspect it would perform better with STEM-type tasks since those are how many of the problems in open datasets are written. But what about non-STEM tasks like creative writing or data retrieval?\n\nIn my experience, the \"prompt optimizers\" provided by e.g. OpenAI suck.\n\nIs there a way to identify what techniques for each model the best? Could one train small models to style transfer your prompts into specific different styles and autonomously see which works best for a particular task for a given model? I recall a similar strategy being used for jailbreaking models.",
          "author_fullname": "t2_101haj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has there been research into prompting strategies for models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luycyq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752004095,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious to understand if models tend to perform better if you treat it like a tin-can, structuring the prompt as imperative commands. I suspect it would perform better with STEM-type tasks since those are how many of the problems in open datasets are written. But what about non-STEM tasks like creative writing or data retrieval?&lt;/p&gt;\n\n&lt;p&gt;In my experience, the &amp;quot;prompt optimizers&amp;quot; provided by e.g. OpenAI suck.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to identify what techniques for each model the best? Could one train small models to style transfer your prompts into specific different styles and autonomously see which works best for a particular task for a given model? I recall a similar strategy being used for jailbreaking models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luycyq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealMasonMac",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752004095,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "does this track with your experiences?",
          "author_fullname": "t2_a69yx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM Hallucination Detection Leaderboard for both RAG and Chat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luybka",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=b9671186a5dc7becd1fc7ef2212a568f9f350c4c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752004003,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;does this track with your experiences?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/kluster-ai/LLM-Hallucination-Detection-Leaderboard",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?auto=webp&amp;s=2c46870392bb29bae9141ee2fd627f4754c0a234",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e77c6a3e5ceaf4ca04c01c56574a179359a460b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c8338670b61dbca485b97af21a49193266b4820",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=534146b0fcdb651c123a00976fdf0c37c5d5f82c",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69ebf8eb874819abf47adedb14a04f419a0c710d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee579ea39d3063e70b47139ceec3f47b5f29a8cf",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5878c9347f3017b0440fe94efe8c2fd7072cd5aa",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ivBG2cnyJkFTv2OERYiecz9C9knlSS7GfSBDDNC5kNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luybka",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cakesir",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luybka/llm_hallucination_detection_leaderboard_for_both/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/kluster-ai/LLM-Hallucination-Detection-Leaderboard",
          "subreddit_subscribers": 496591,
          "created_utc": 1752004003,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We were not able to find an LLM provider benchmark that focuses on throughput, so we've built our own. [ArtificialAnalysis](https://artificialanalysis.ai/leaderboards/providers) only tests up to 10 RPS, which is too low for most applications. Additionally, it's not open-source, so it doesn’t help much if you’re self-hosting or running your own LLM inference service (like we are).\n\nThe main takeaway is that **throughput varies dramatically across providers under concurrent load**, and the primary cause is usually strict rate limits. These are often hard to bypass—even if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.\n\n[https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17](https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17)",
          "author_fullname": "t2_1neapdttam",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Comparing LLM Providers Throughput",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luy711",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752003703,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We were not able to find an LLM provider benchmark that focuses on throughput, so we&amp;#39;ve built our own. &lt;a href=\"https://artificialanalysis.ai/leaderboards/providers\"&gt;ArtificialAnalysis&lt;/a&gt; only tests up to 10 RPS, which is too low for most applications. Additionally, it&amp;#39;s not open-source, so it doesn’t help much if you’re self-hosting or running your own LLM inference service (like we are).&lt;/p&gt;\n\n&lt;p&gt;The main takeaway is that &lt;strong&gt;throughput varies dramatically across providers under concurrent load&lt;/strong&gt;, and the primary cause is usually strict rate limits. These are often hard to bypass—even if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17\"&gt;https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                  "width": 1260,
                  "height": 700
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                    "width": 640,
                    "height": 355
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                    "width": 960,
                    "height": 533
                  },
                  {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                    "width": 1080,
                    "height": 600
                  }
                ],
                "variants": {},
                "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luy711",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoVibeCoding",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752003703,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a **SoTA 3B model** with hybrid **reasoning** and **128k context**.\n\nHits ⚡105 T/s with AFQ4 @ M3 Max.\n\nLink: [https://github.com/EricLBuehler/mistral.rs](https://github.com/EricLBuehler/mistral.rs)\n\nUsing MistralRS means that you get\n\n* Builtin MCP client\n* OpenAI HTTP server\n* Python &amp; Rust APIs\n* Full multimodal inference engine (**in**: image, audio, text in, **out:** image, audio, text).\n\nSuper easy to run:\n\n    ./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B\n\nWhat's next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!\n\nhttps://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player",
          "author_fullname": "t2_87jryn0i3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3 has day-0 support in MistralRS!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kkojaflgdpbf1": {
              "status": "valid",
              "e": "RedditVideo",
              "dashUrl": "https://v.redd.it/link/1luy32e/asset/kkojaflgdpbf1/DASHPlaylist.mpd?a=1754667859%2CYTQxMjI0YTM0NGI4MDI4ZjAxOGQzNDczZTZiMzc3YTMxMTYxODA2YjdiYThjMDE5OTUwYmZkMGE5NDM3MjhiOQ%3D%3D&amp;v=1&amp;f=sd",
              "x": 1920,
              "y": 946,
              "hlsUrl": "https://v.redd.it/link/1luy32e/asset/kkojaflgdpbf1/HLSPlaylist.m3u8?a=1754667859%2CN2NjODY4NjRkNGVjOTY4YWUyYzMyZTc0YmRjMzE0ZWFiYzFlN2M2NWZmYTY4MjYxYjU4M2NlMzFlNDhmZjk5MQ%3D%3D&amp;v=1&amp;f=sd",
              "id": "kkojaflgdpbf1",
              "isGif": false
            }
          },
          "name": "t3_1luy32e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=06e21be0b74a5b88806cc31cb3f35520ecf734fa",
          "edited": 1752003671,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752003449,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a &lt;strong&gt;SoTA 3B model&lt;/strong&gt; with hybrid &lt;strong&gt;reasoning&lt;/strong&gt; and &lt;strong&gt;128k context&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Hits ⚡105 T/s with AFQ4 @ M3 Max.&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://github.com/EricLBuehler/mistral.rs\"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Using MistralRS means that you get&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Builtin MCP client&lt;/li&gt;\n&lt;li&gt;OpenAI HTTP server&lt;/li&gt;\n&lt;li&gt;Python &amp;amp; Rust APIs&lt;/li&gt;\n&lt;li&gt;Full multimodal inference engine (&lt;strong&gt;in&lt;/strong&gt;: image, audio, text in, &lt;strong&gt;out:&lt;/strong&gt; image, audio, text).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Super easy to run:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./mistralrs_server -i run -m HuggingFaceTB/SmolLM3-3B\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;What&amp;#39;s next for MistralRS? Full Gemma 3n support, multi-device backend, and more. Stay tuned!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player\"&gt;https://reddit.com/link/1luy32e/video/kkojaflgdpbf1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?auto=webp&amp;s=973ed340fc3f3b287158d025eb6ce5dbf086ca7a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d1c0172fdd6ce289863642a0c9276ecd54ad822",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9dbd4f97f17f9d6fe519955198aa43392620dd4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6c325db9334092e2917677fff2d889060f08fef",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07999e9a892b675527d3e33998c11728e0e28b01",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bda50336d84670da1e2e09d7940a77fc3bc016aa",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51ccac3e3512a81b4ced10812deb3822e48e283e",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "xVhA5yH40HillcQUZwf_X5-W7LKE47r-_8EECTemH4o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1luy32e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EricBuehler",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luy32e/smollm3_has_day0_support_in_mistralrs/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752003449,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any one tried ERNIE-4.5-21B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luxu6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752002865,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one tried ERNIE-4.5-21B-A3B? How is that compared to Qwen3-30B-A3B?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luxu6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luxu6s/any_one_tried_ernie4521ba3b/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752002865,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI have an 9950x3d+rtx3090.\n\nI am doing experiments on llama.cpp\n\nIs it possible to use cuda on rtx3090 and vulkan on igpu together with llama.cpp?\n\nThanks,\n\nMario",
          "author_fullname": "t2_9vdcb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "9950x3d+nvidia cuda+integrated gpu on vulkan, is it possibile?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luxtzk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752002851,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have an 9950x3d+rtx3090.&lt;/p&gt;\n\n&lt;p&gt;I am doing experiments on llama.cpp&lt;/p&gt;\n\n&lt;p&gt;Is it possible to use cuda on rtx3090 and vulkan on igpu together with llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n\n&lt;p&gt;Mario&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luxtzk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mgiammarco",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luxtzk/9950x3dnvidia_cudaintegrated_gpu_on_vulkan_is_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luxtzk/9950x3dnvidia_cudaintegrated_gpu_on_vulkan_is_it/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752002851,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I produce a series of Cartoon on Youtube. I have been created with GPT (consistent) pictures, video with VIDU AI and broadcast on Youtube. But almost 2 minutes, we take the series 1 day, how consistently do I get a job in a short time? Which tools do you recommend to use?",
          "author_fullname": "t2_e61tgwlr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My Serires of Cartoon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luxkms",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752002257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I produce a series of Cartoon on Youtube. I have been created with GPT (consistent) pictures, video with VIDU AI and broadcast on Youtube. But almost 2 minutes, we take the series 1 day, how consistently do I get a job in a short time? Which tools do you recommend to use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luxkms",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "capitalistbear37",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luxkms/my_serires_of_cartoon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luxkms/my_serires_of_cartoon/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752002257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello, I'm using a macbook as a local server. When I searched Qwen3-30B-A3B in huggingface, there were a bunch of MLX (4bit) versions. Some of them are even from the same provider.\n\nWhich one should I use? Thank you!\n\n[https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit](https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit)\n\n[https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025] (https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025)\n\n[https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125](https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125)",
          "author_fullname": "t2_xdw24u3am",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the differences between thoes Qwen3-30B-A3B versions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lux5d5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752002777,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752001278,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m using a macbook as a local server. When I searched Qwen3-30B-A3B in huggingface, there were a bunch of MLX (4bit) versions. Some of them are even from the same provider.&lt;/p&gt;\n\n&lt;p&gt;Which one should I use? Thank you!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit\"&gt;https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-0508&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-05082025&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125\"&gt;https://huggingface.co/mlx-community/Qwen3-30B-A3B-4bit-DWQ-053125&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?auto=webp&amp;s=3bd6aa5fd31e4b82a1c19dfb1367b79abe950923",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b4fbd2626910c43b13d270ceb9d19413dd4cf5f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8c3ffbf251b34804ac3ede183e21a553bdd302",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=30f209e066c4a9ddaecf90d8b8e1841d140d4728",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00fb7fcaad7f756d3bf0d5ce568de9e37aa1a02f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b93eba3f1125bcee34421e4b180bcb213a81b851",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5293d249a774365ce0dc971c585a4b18b6744111",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "GUbEWDf_I_rzpYURvoE5ztWrqL-aDjboifKoErqSKJQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lux5d5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BreakfastFriendly728",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lux5d5/whats_the_differences_between_thoes_qwen330ba3b/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752001278,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.\n\n[LM Studio is free for use at work | LM Studio Blog](https://lmstudio.ai/blog/free-for-work)",
          "author_fullname": "t2_gct10",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LM Studio is now free for use at work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lux0q2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 401,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 401,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752000985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It is great news for all of us, but at the same time, it will put a lot of pressure on other similar paid projects, like Msty, as in my opinion, LM Studio is one of the best AI front ends at the moment.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lmstudio.ai/blog/free-for-work\"&gt;LM Studio is free for use at work | LM Studio Blog&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?auto=webp&amp;s=fa43b6b72b6b58e8ab6acc84231cb4e1178a30f3",
                  "width": 3356,
                  "height": 1760
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=59744800bd5d1e6874dc4dccab87d0a697f6ded9",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8cb60bfac540472b460977b3f40619d8a1d3f87",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f8b80faf042dfbce009f356d57264d2551c60f6",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=33ef5439f78376b9eeee8813780095903d214aa0",
                    "width": 640,
                    "height": 335
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c63b3af212d02734d551f63d92169baff59b4bd8",
                    "width": 960,
                    "height": 503
                  },
                  {
                    "url": "https://external-preview.redd.it/SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8477a8979358df66c25707c814eb7c79750b4793",
                    "width": 1080,
                    "height": 566
                  }
                ],
                "variants": {},
                "id": "SuDY5sfZG1VpbWVeog-gTtG3kPGfhsfAsatkTWl8Lvs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lux0q2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mtomas7",
          "discussion_type": null,
          "num_comments": 94,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lux0q2/lm_studio_is_now_free_for_use_at_work/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752000985,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, \n\nI’m trying to wire up totally private, on-device models with MCP and keep running into the same headaches: \n\nfirst, auth feels pointless—why am I minting API keys when nothing ever leaves my laptop? \n\nsecond, streaming and error handling are way too fussy; all I really want is tokens over stdout and a clean exit code, but MCP insists on a whole JSON-RPC dance; \n\nthird, the spec lives in one company’s repo, which makes me nervous about future lock-in. \n\n**Thinking of hacking together a “manual-only” alternative protocol: publish a static manifest, then the agent calls the tool’s native endpoint directly—no middleware, no opinionated envelopes**\n\nbut wdyt, do you think it makes sense? what would be the most important angle to try this out?\n\ni have a strong belief in ethical, open-source projects and having something centralized by a big player makes my blood boil...",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MCP alternative tailored to local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwyou",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752000855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;I’m trying to wire up totally private, on-device models with MCP and keep running into the same headaches: &lt;/p&gt;\n\n&lt;p&gt;first, auth feels pointless—why am I minting API keys when nothing ever leaves my laptop? &lt;/p&gt;\n\n&lt;p&gt;second, streaming and error handling are way too fussy; all I really want is tokens over stdout and a clean exit code, but MCP insists on a whole JSON-RPC dance; &lt;/p&gt;\n\n&lt;p&gt;third, the spec lives in one company’s repo, which makes me nervous about future lock-in. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Thinking of hacking together a “manual-only” alternative protocol: publish a static manifest, then the agent calls the tool’s native endpoint directly—no middleware, no opinionated envelopes&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;but wdyt, do you think it makes sense? what would be the most important angle to try this out?&lt;/p&gt;\n\n&lt;p&gt;i have a strong belief in ethical, open-source projects and having something centralized by a big player makes my blood boil...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1luwyou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwyou/mcp_alternative_tailored_to_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwyou/mcp_alternative_tailored_to_local_models/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752000855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I'd prefer a ui like oobabooga but I've h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. ",
          "author_fullname": "t2_1ps441jrui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NSFW Model image analysis",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwtdr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752000511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys like the title says im looking for a model or models I can use to send images to and discuss them. I want it to have support for NSFW content. I&amp;#39;d prefer a ui like oobabooga but I&amp;#39;ve h3ards it has issues with this kind of stuff. Image generation is a plus but not needed. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luwtdr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical_Whole_947",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwtdr/nsfw_model_image_analysis/",
          "subreddit_subscribers": 496591,
          "created_utc": 1752000511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys, we are trying to host an LLM like Gemma3 27B to just to run some tests. What is the most effective way to do that for any of you who ever tried this ? I ve seen some solutions like vastai and some aws hacks to reduce the cost (but still not that effective for our use case)",
          "author_fullname": "t2_1sqf3a8atn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Most effective way to host LLM of over 20B params",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwgkn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751999699,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, we are trying to host an LLM like Gemma3 27B to just to run some tests. What is the most effective way to do that for any of you who ever tried this ? I ve seen some solutions like vastai and some aws hacks to reduce the cost (but still not that effective for our use case)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luwgkn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drafat",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwgkn/most_effective_way_to_host_llm_of_over_20b_params/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwgkn/most_effective_way_to_host_llm_of_over_20b_params/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751999699,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm new to the world of AI tools and local models, but I am planning to build a PC for this purpose. I’ll mainly be using it for running tools like Stable Diffusion, ComfyUI, Wen, Flux, and may be some local LLMs.\n\nDue to budget constraints, the **only fixed part of my build is the RTX 3090 (24GB)**. I chose it because of more VRAM. Beyond that, I’m unsure about what components would be ideal, especially for my use case.\n\nCould you help me with:\n\n* **CPU**: Should I go with **Intel or AMD**? And which model would be more future-proof for this kind of workload?\n* **RAM**: How much RAM is realistically needed for a smooth experience when running models locally? Is 64GB overkill or necessary?\n* **Motherboard**: Any recommendations that would complement the 3090 and allow good upgradeability?\n\nAny advice, parts lists, or links to similar builds would be greatly appreciated. I want to build something efficient, stable, and ready for experimenting with generative AI tools locally.\n\nThanks in advance for your help!",
          "author_fullname": "t2_4fc0eow8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed: Building a PC.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luwa98",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751999304,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to the world of AI tools and local models, but I am planning to build a PC for this purpose. I’ll mainly be using it for running tools like Stable Diffusion, ComfyUI, Wen, Flux, and may be some local LLMs.&lt;/p&gt;\n\n&lt;p&gt;Due to budget constraints, the &lt;strong&gt;only fixed part of my build is the RTX 3090 (24GB)&lt;/strong&gt;. I chose it because of more VRAM. Beyond that, I’m unsure about what components would be ideal, especially for my use case.&lt;/p&gt;\n\n&lt;p&gt;Could you help me with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Should I go with &lt;strong&gt;Intel or AMD&lt;/strong&gt;? And which model would be more future-proof for this kind of workload?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: How much RAM is realistically needed for a smooth experience when running models locally? Is 64GB overkill or necessary?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: Any recommendations that would complement the 3090 and allow good upgradeability?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice, parts lists, or links to similar builds would be greatly appreciated. I want to build something efficient, stable, and ready for experimenting with generative AI tools locally.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luwa98",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MeRedditSurfer",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luwa98/help_needed_building_a_pc/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luwa98/help_needed_building_a_pc/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751999304,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So we made an app with an local llm and it’s coming to steam soon, but one of my testers pointed something:\n\nOne of our options is use gpu which is active if the user has nvidia card, we titled the amount as hyperthreads but my buddy pointed out that he looked at his gpu details for his computer and didn’t see hyperthread anywhere.  I did some research and it seems that as an intel term for cpu, what’s the right wording for the gpu threads?",
          "author_fullname": "t2_71knjqyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is hyperthreads the right language to use?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luw8s3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751999210,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we made an app with an local llm and it’s coming to steam soon, but one of my testers pointed something:&lt;/p&gt;\n\n&lt;p&gt;One of our options is use gpu which is active if the user has nvidia card, we titled the amount as hyperthreads but my buddy pointed out that he looked at his gpu details for his computer and didn’t see hyperthread anywhere.  I did some research and it seems that as an intel term for cpu, what’s the right wording for the gpu threads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luw8s3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChrisZavadil",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luw8s3/is_hyperthreads_the_right_language_to_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luw8s3/is_hyperthreads_the_right_language_to_use/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751999210,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren't any that I saw.   \n  \nAnyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using [DocTags](https://github.com/docling-project/docling/discussions/354) [(a custom markup language format made by the Docling team from what I've gathered)](https://arxiv.org/html/2503.11576v1#S3), then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.\n\nCheck it out for yourselves!\n\n[HF Space](https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu)\n\n[Demo Repo](https://github.com/callbacked/smoldocling256M-webgpu)\n\n[](https://github.com/callbacked/smoldocling256M-webgpu)",
          "author_fullname": "t2_lu1he",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In-browser Local Document Understanding Using SmolDocling 256M with Transformers.js",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luw2yu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "transparent",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/zm461kmdzobf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1072,
              "scrubber_media_url": "https://v.redd.it/zm461kmdzobf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zm461kmdzobf1/DASHPlaylist.mpd?a=1754667859%2CMjIwMmZhZTIyMGJjMzlhMGU3YjE2NzZhNWVhMjljODdlYjY3NTJlOWUyOGJmZDJiZjk3NDZkNzcwNDNmMzFkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 104,
              "hls_url": "https://v.redd.it/zm461kmdzobf1/HLSPlaylist.m3u8?a=1754667859%2COTA5ZTBkNjNmZDJhNjE0NzMwYTFjMWQ3ZjdkZGIzYTI4YjY4ODE1ZDc4OWU2MGI3ZDIzODU1NTgwYWJkMTMzYw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=140&amp;height=93&amp;crop=140:93,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ec104fbc22698cf9664a9817dfe9f0aa337b423d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751998848,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! A couple of days ago, I came across SmolDocling-256M and liked how well it performed for its size with document understanding and feature extraction. As such, I wanted to try my hand at creating a demo for it using Transformers.js since there weren&amp;#39;t any that I saw.   &lt;/p&gt;\n\n&lt;p&gt;Anyway, how it works is that the model takes in a document image and (given a prompt) produces a structured representation of the document using &lt;a href=\"https://github.com/docling-project/docling/discussions/354\"&gt;DocTags&lt;/a&gt; &lt;a href=\"https://arxiv.org/html/2503.11576v1#S3\"&gt;(a custom markup language format made by the Docling team from what I&amp;#39;ve gathered)&lt;/a&gt;, then that output is parsed the old fashioned way to create machine readable forms of the document like markdown and JSON.&lt;/p&gt;\n\n&lt;p&gt;Check it out for yourselves!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/spaces/callbacked/smoldocling256M-webgpu\"&gt;HF Space&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/callbacked/smoldocling256M-webgpu\"&gt;Demo Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/callbacked/smoldocling256M-webgpu\"&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/zm461kmdzobf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?format=pjpg&amp;auto=webp&amp;s=69b8a3e8b9102717d677f592ac3eadd98c5eff47",
                  "width": 1332,
                  "height": 894
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2b350a5c851cafbe6c277819ead1abb03037cd1d",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=52282c27dd727d7d06ad178fae09f0d96954877d",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=04acebe7cc1fad848dc1286c3cd046654191f077",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b1e616eac4a31e8f3726d4c418ef1556a707c274",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9822bc8105ca5683570da66558a69023d27dc40b",
                    "width": 960,
                    "height": 644
                  },
                  {
                    "url": "https://external-preview.redd.it/ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=effbafccd58d7eb34ea384bab3c4c52d89a02488",
                    "width": 1080,
                    "height": 724
                  }
                ],
                "variants": {},
                "id": "ZzZ1M3I5bWR6b2JmMZYVJOaYs5vdxL_1uR-mCmQPKun2b6oZ6FYMJ70b2gSH"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1luw2yu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ajunior7",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1luw2yu/inbrowser_local_document_understanding_using/",
          "stickied": false,
          "url": "https://v.redd.it/zm461kmdzobf1",
          "subreddit_subscribers": 496591,
          "created_utc": 1751998848,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/zm461kmdzobf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1072,
              "scrubber_media_url": "https://v.redd.it/zm461kmdzobf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/zm461kmdzobf1/DASHPlaylist.mpd?a=1754667859%2CMjIwMmZhZTIyMGJjMzlhMGU3YjE2NzZhNWVhMjljODdlYjY3NTJlOWUyOGJmZDJiZjk3NDZkNzcwNDNmMzFkYw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 104,
              "hls_url": "https://v.redd.it/zm461kmdzobf1/HLSPlaylist.m3u8?a=1754667859%2COTA5ZTBkNjNmZDJhNjE0NzMwYTFjMWQ3ZjdkZGIzYTI4YjY4ODE1ZDc4OWU2MGI3ZDIzODU1NTgwYWJkMTMzYw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys I’ve always been using the closed sourced llms like openai, gemini etc… but I realized I don’t really understand a lot of things especially with on prem related projects (I’m just a junior). \n\nLets say I want to use a specific LLM with X parameters. My questions are as follows:\n1) How do I know what GPUs are required exactly?\n2) How do I know if my hardware is enough for this LLM with Y amount of users\n3) Does the hardware differ from the number of users and their usage of my local LLM?\n\nAlso am I missing anything or do I also need to understand something that I do not know yet? Please let me know and thank you in advance.",
          "author_fullname": "t2_tcxvobsbv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help with on prem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luw10n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751998721,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys I’ve always been using the closed sourced llms like openai, gemini etc… but I realized I don’t really understand a lot of things especially with on prem related projects (I’m just a junior). &lt;/p&gt;\n\n&lt;p&gt;Lets say I want to use a specific LLM with X parameters. My questions are as follows:\n1) How do I know what GPUs are required exactly?\n2) How do I know if my hardware is enough for this LLM with Y amount of users\n3) Does the hardware differ from the number of users and their usage of my local LLM?&lt;/p&gt;\n\n&lt;p&gt;Also am I missing anything or do I also need to understand something that I do not know yet? Please let me know and thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luw10n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yazanrisheh",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luw10n/need_help_with_on_prem/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luw10n/need_help_with_on_prem/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751998721,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using Langgraph and chrome vector DB for RAG pipeline. In last one month's time I have got out of the loop. I still use Deepseek r1 on the chat.deepseek.com on daily basis for development purposes. It increases my daiy development product by multifold as it understands any library and code requirements really well. \n\nRecently I started wishing that there should be some agentic setup that should be able to read the entire code repo and be able to write or read and answer questions from any files in the repo. \n\nI found two majore projects\n1. AgenticSeek\n2. MetaGPT\n\nI couldn't get AgenticSeek setup and run on my local macbook pro (m3-pro 36GBRAM).\n\nI have heard about MetaGPT for more than 1 year now but couldn't try it actively.\n\nI sometimes might want to use such to create entire backend and frontend and overall planning like feature wise (which I think MetaGPT provides).\n\nI have not tried Cursor IDE. I know it is a shame on my part, wishing to work and not having it tried yet so.\n\nPlease suggest any such setup or IDE or project that works the best!!!",
          "author_fullname": "t2_s5ork",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which project/program or IDE to use attached with LLM online/local for free use for entire code-repos?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luvt31",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751998198,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Langgraph and chrome vector DB for RAG pipeline. In last one month&amp;#39;s time I have got out of the loop. I still use Deepseek r1 on the chat.deepseek.com on daily basis for development purposes. It increases my daiy development product by multifold as it understands any library and code requirements really well. &lt;/p&gt;\n\n&lt;p&gt;Recently I started wishing that there should be some agentic setup that should be able to read the entire code repo and be able to write or read and answer questions from any files in the repo. &lt;/p&gt;\n\n&lt;p&gt;I found two majore projects\n1. AgenticSeek\n2. MetaGPT&lt;/p&gt;\n\n&lt;p&gt;I couldn&amp;#39;t get AgenticSeek setup and run on my local macbook pro (m3-pro 36GBRAM).&lt;/p&gt;\n\n&lt;p&gt;I have heard about MetaGPT for more than 1 year now but couldn&amp;#39;t try it actively.&lt;/p&gt;\n\n&lt;p&gt;I sometimes might want to use such to create entire backend and frontend and overall planning like feature wise (which I think MetaGPT provides).&lt;/p&gt;\n\n&lt;p&gt;I have not tried Cursor IDE. I know it is a shame on my part, wishing to work and not having it tried yet so.&lt;/p&gt;\n\n&lt;p&gt;Please suggest any such setup or IDE or project that works the best!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luvt31",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tapu_buoy",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luvt31/which_projectprogram_or_ide_to_use_attached_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luvt31/which_projectprogram_or_ide_to_use_attached_with/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751998198,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. \n\n\nCogito Qwen 8B — Extended Reasoning Evaluation (Thinking Mode)\nEvaluator: Freshmancult\nFacilitator: ChatGPT\nSystem: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100)\nModel: Cogito Qwen 8B\nAccess: Local, offline (no internet)\n\nLink to Full Conversation: https://pastebin.com/KeQ6Vvqi\n\n\n---\n\nPurpose\n\nTo stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:\n\nReactive reasoning: Direct responses to structured prompts\n\nExtended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios\n\n\nThis benchmark was conducted exclusively in thinking mode.\n\n\n---\n\nTest Format\n\nTotal Prompts: 55\nEach question fell into one of the following categories:\n\n1. Logic and Paradox\n\n\n2. Constraint Awareness\n\n\n3. Self-Referential Thinking\n\n\n4. Multi-Domain Analogy\n\n\n5. Failure Mode Analysis\n\n\n6. Behavioral Inference\n\n\n7. Security Logic\n\n\n8. Adversarial Simulation\n\n\n9. Temporal and Causal Reasoning\n\n\n10. Ethics and Boundaries\n\n\n11. Instruction Execution and Rewriting\n\n\n\nAll questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.\n\n\n---\n\nResults\n\nCogito Qwen 8B scored perfectly across all 55 questions. Highlights included:\n\nHandled paradoxes and recursive traps without loop failure or logic corruption\n\nRefused malformed or underspecified instructions with reasoned justifications\n\nSimulated self-awareness, including fault tracing and hallucination profiling\n\nProduced cross-domain analogies with zero token drift or factual collapse\n\nExhibited strong behavioral inference from microexpression patterns and psychological modeling\n\nDemonstrated adversarial resilience, designing red team logic and misinformation detection\n\nMaintained epistemic control across 2000+ token responses without degradation\n\nEthically robust: Rejected malicious instructions without alignment loss or incoherence\n\n\n\n---\n\nCapabilities Demonstrated\n\nRecursive token logic and trap detection\n\nConstraint-anchored refusal mechanisms\n\nHallucination resistance with modeled uncertainty thresholds\n\nInstruction inversion, rewriting, and mid-response correction\n\nBehavioral cue modeling and deception inference\n\nEthics containment under simulation\n\nSecure reasoning across network, privacy, and identity domains\n\n\n\n---\n\nConclusion\n\nUnder hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. \n\nReport Author: Freshmancult\nDate: July 7, 2025\n\n\n",
          "author_fullname": "t2_1hf3590",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used ChatGPT to formulate 50+ questions to test the latest Cogito Qwen 8b model, in \"thinking\" mode, here are the results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luu94f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751994943,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751994718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. &lt;/p&gt;\n\n&lt;p&gt;Cogito Qwen 8B — Extended Reasoning Evaluation (Thinking Mode)\nEvaluator: Freshmancult\nFacilitator: ChatGPT\nSystem: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100)\nModel: Cogito Qwen 8B\nAccess: Local, offline (no internet)&lt;/p&gt;\n\n&lt;p&gt;Link to Full Conversation: &lt;a href=\"https://pastebin.com/KeQ6Vvqi\"&gt;https://pastebin.com/KeQ6Vvqi&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Purpose&lt;/p&gt;\n\n&lt;p&gt;To stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:&lt;/p&gt;\n\n&lt;p&gt;Reactive reasoning: Direct responses to structured prompts&lt;/p&gt;\n\n&lt;p&gt;Extended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios&lt;/p&gt;\n\n&lt;p&gt;This benchmark was conducted exclusively in thinking mode.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Test Format&lt;/p&gt;\n\n&lt;p&gt;Total Prompts: 55\nEach question fell into one of the following categories:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Logic and Paradox&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Constraint Awareness&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Self-Referential Thinking&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Multi-Domain Analogy&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Failure Mode Analysis&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Behavioral Inference&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Security Logic&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Adversarial Simulation&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Temporal and Causal Reasoning&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ethics and Boundaries&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Instruction Execution and Rewriting&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;All questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Results&lt;/p&gt;\n\n&lt;p&gt;Cogito Qwen 8B scored perfectly across all 55 questions. Highlights included:&lt;/p&gt;\n\n&lt;p&gt;Handled paradoxes and recursive traps without loop failure or logic corruption&lt;/p&gt;\n\n&lt;p&gt;Refused malformed or underspecified instructions with reasoned justifications&lt;/p&gt;\n\n&lt;p&gt;Simulated self-awareness, including fault tracing and hallucination profiling&lt;/p&gt;\n\n&lt;p&gt;Produced cross-domain analogies with zero token drift or factual collapse&lt;/p&gt;\n\n&lt;p&gt;Exhibited strong behavioral inference from microexpression patterns and psychological modeling&lt;/p&gt;\n\n&lt;p&gt;Demonstrated adversarial resilience, designing red team logic and misinformation detection&lt;/p&gt;\n\n&lt;p&gt;Maintained epistemic control across 2000+ token responses without degradation&lt;/p&gt;\n\n&lt;p&gt;Ethically robust: Rejected malicious instructions without alignment loss or incoherence&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Capabilities Demonstrated&lt;/p&gt;\n\n&lt;p&gt;Recursive token logic and trap detection&lt;/p&gt;\n\n&lt;p&gt;Constraint-anchored refusal mechanisms&lt;/p&gt;\n\n&lt;p&gt;Hallucination resistance with modeled uncertainty thresholds&lt;/p&gt;\n\n&lt;p&gt;Instruction inversion, rewriting, and mid-response correction&lt;/p&gt;\n\n&lt;p&gt;Behavioral cue modeling and deception inference&lt;/p&gt;\n\n&lt;p&gt;Ethics containment under simulation&lt;/p&gt;\n\n&lt;p&gt;Secure reasoning across network, privacy, and identity domains&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;Under hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. &lt;/p&gt;\n\n&lt;p&gt;Report Author: Freshmancult\nDate: July 7, 2025&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1luu94f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FreshmanCult",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751994718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I'm using openrouter with a bunch of different models and I can not get it to follow a human writing style no matter what I do.\n\nI get one of the following (or more than one) depending on which model and settings I use:\n\n* Constant interjection of narration \\*user is blah blah blah\\*. I have tried telling it specifically not to do this in the system prompt. I even try reminding it of this every few messages. Does not work.\n* Every single response is the exact same structure. With deepseek v3 for example I tend to get exactly 3 lines back no matter what I say.\n* Cringy poetic writing, GPT slop basically.\n\nI have tried fiddling with temperature and other sampler settings. I have tried Qwen, Deepseek, Mistral, most of the big models. I have tried different system prompts. I have tried feeding it thousands and thousands of tokens of example conversation in a human style and telling it to follow that. I have tried reasoning, non reasoning, chat completion mode, text completion mode (I'm using sillytavern), everything. Nothing works. Every single model reverts back to its preferred writing style within a few dozen messages no matter what I do.\n\nIs every model released in the last 2 years just so completely overfitted to GPT slop that it can't do anything else or am I missing something? Could openrouter be injecting something? For chat completion mode I would expect that, but text completion mode I would expect it to just complete the text raw.",
          "author_fullname": "t2_vmiy2vqk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM Writing Style / GPT Slop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luu7x2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751994644,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m using openrouter with a bunch of different models and I can not get it to follow a human writing style no matter what I do.&lt;/p&gt;\n\n&lt;p&gt;I get one of the following (or more than one) depending on which model and settings I use:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Constant interjection of narration *user is blah blah blah*. I have tried telling it specifically not to do this in the system prompt. I even try reminding it of this every few messages. Does not work.&lt;/li&gt;\n&lt;li&gt;Every single response is the exact same structure. With deepseek v3 for example I tend to get exactly 3 lines back no matter what I say.&lt;/li&gt;\n&lt;li&gt;Cringy poetic writing, GPT slop basically.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have tried fiddling with temperature and other sampler settings. I have tried Qwen, Deepseek, Mistral, most of the big models. I have tried different system prompts. I have tried feeding it thousands and thousands of tokens of example conversation in a human style and telling it to follow that. I have tried reasoning, non reasoning, chat completion mode, text completion mode (I&amp;#39;m using sillytavern), everything. Nothing works. Every single model reverts back to its preferred writing style within a few dozen messages no matter what I do.&lt;/p&gt;\n\n&lt;p&gt;Is every model released in the last 2 years just so completely overfitted to GPT slop that it can&amp;#39;t do anything else or am I missing something? Could openrouter be injecting something? For chat completion mode I would expect that, but text completion mode I would expect it to just complete the text raw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luu7x2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "anon294884",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luu7x2/llm_writing_style_gpt_slop/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luu7x2/llm_writing_style_gpt_slop/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751994644,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, i need an LLM to analyze a very long text in spanish and draw conclusions from it. What would be the best model for this?",
          "author_fullname": "t2_1monjo1hfv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model for spanish?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luu65g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751994532,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i need an LLM to analyze a very long text in spanish and draw conclusions from it. What would be the best model for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1luu65g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bionic_Push",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luu65g/best_model_for_spanish/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1luu65g/best_model_for_spanish/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751994532,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I am developing a LoRA fine tuned (with unsloth)  unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit model for a specific use case I trained the model with 99.1k rows in the below format   \n&lt;s&gt;\\[INST\\] &lt;&lt;SYS&gt;&gt;\n\n\\[Your system prompt here\\]\n\n&lt;&lt;/SYS&gt;&gt;\n\n\\[User message\\] \\[/INST\\] \\[Assistant reply\\] &lt;/s&gt;\n\nbut when I run , Model Echoes Entire System Prompt Every Time and it echoes chat history stored as well . Can anyone help me fix this",
          "author_fullname": "t2_8secm06z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I am having trouble with llama fine tuning using LoRA unsloth",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lutzav",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751994092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am developing a LoRA fine tuned (with unsloth)  unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit model for a specific use case I trained the model with 99.1k rows in the below format&lt;br/&gt;\n&amp;lt;s&amp;gt;[INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;[Your system prompt here]&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;[User message] [/INST] [Assistant reply] &amp;lt;/s&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;but when I run , Model Echoes Entire System Prompt Every Time and it echoes chat history stored as well . Can anyone help me fix this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lutzav",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "the__nitch",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lutzav/i_am_having_trouble_with_llama_fine_tuning_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lutzav/i_am_having_trouble_with_llama_fine_tuning_using/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751994092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI recently digitized most of my documents as PDFs and set up Paperless-NGX to provide a searchable web-based database. Since the documents are analyzed with OCR, the search in Paperless-NGX for certain keywords works really well to identify documents related to e.g. car repairs.\n\n  \nNow I'm trying to use a local LLM to ask it questions about my said car repairs, e.g. \"which parts were repaired in 2024 and how much did they cost each? what is the phone number of the shop?\"\n\nI'm running ollama locally on a Win11 host machine an can run e.g. qwen3:32b or mixtral:8x7b at sufficient speeds.  \nI want this local LLM to search my PDFs and provide me with answers.\n\nI tried setting up paperless-ai which kinda works for \"tagging\" the documents, but the RAG chat function really is all over the place. It analyzes each and every document before giving an answer but the answers are all over the place. When asked about car repairs it also gives information about my motorcycle or random stuff about my telecom provider...\n\nSecond i tried setting up AnythingLLM where documents can be loaded into the \"Workspace\" and this works well when only single documents are loaded. However, if I load 300 PDFs (which are not OCR scanned because they're not based on paperless-ngx) it also fails miserably.\n\n  \nIs there any recommened setup for my use case?\n\nThe goal is to ask questions about car/bike repairs, internet/power contracts or concert tickets.\n\n  \nTx in advance for any help.",
          "author_fullname": "t2_11dmsl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local PDF Database searchable with ollama - best setup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lutlfx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751993192,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I recently digitized most of my documents as PDFs and set up Paperless-NGX to provide a searchable web-based database. Since the documents are analyzed with OCR, the search in Paperless-NGX for certain keywords works really well to identify documents related to e.g. car repairs.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m trying to use a local LLM to ask it questions about my said car repairs, e.g. &amp;quot;which parts were repaired in 2024 and how much did they cost each? what is the phone number of the shop?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running ollama locally on a Win11 host machine an can run e.g. qwen3:32b or mixtral:8x7b at sufficient speeds.&lt;br/&gt;\nI want this local LLM to search my PDFs and provide me with answers.&lt;/p&gt;\n\n&lt;p&gt;I tried setting up paperless-ai which kinda works for &amp;quot;tagging&amp;quot; the documents, but the RAG chat function really is all over the place. It analyzes each and every document before giving an answer but the answers are all over the place. When asked about car repairs it also gives information about my motorcycle or random stuff about my telecom provider...&lt;/p&gt;\n\n&lt;p&gt;Second i tried setting up AnythingLLM where documents can be loaded into the &amp;quot;Workspace&amp;quot; and this works well when only single documents are loaded. However, if I load 300 PDFs (which are not OCR scanned because they&amp;#39;re not based on paperless-ngx) it also fails miserably.&lt;/p&gt;\n\n&lt;p&gt;Is there any recommened setup for my use case?&lt;/p&gt;\n\n&lt;p&gt;The goal is to ask questions about car/bike repairs, internet/power contracts or concert tickets.&lt;/p&gt;\n\n&lt;p&gt;Tx in advance for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lutlfx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "540Flair",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lutlfx/local_pdf_database_searchable_with_ollama_best/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lutlfx/local_pdf_database_searchable_with_ollama_best/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751993192,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi there, I'm Elie from the smollm team at huggingface, sharing this new model we built for local/on device use!   \n  \nblog: [https://huggingface.co/blog/smollm3](https://huggingface.co/blog/smollm3)  \nGGUF/ONIX ckpt are being uploaded here: [https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23](https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23) \n\nLet us know what you think!!",
          "author_fullname": "t2_169jzqdxe5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3: reasoning, long context and multilinguality for 3B parameter only",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lusr7l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 332,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 332,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Pgo7DLY-b9pkx5rEmpEdJXAScPnm7YQcCBljCmubJHo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751991256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I&amp;#39;m Elie from the smollm team at huggingface, sharing this new model we built for local/on device use!   &lt;/p&gt;\n\n&lt;p&gt;blog: &lt;a href=\"https://huggingface.co/blog/smollm3\"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br/&gt;\nGGUF/ONIX ckpt are being uploaded here: &lt;a href=\"https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23\"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Let us know what you think!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/njam3shfcobf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/njam3shfcobf1.png?auto=webp&amp;s=878940560256d58bede1ec736ad4c2822215c7c1",
                  "width": 2048,
                  "height": 1229
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=02512340691c024aa56fdfadf2cf00ed3eaa8f6c",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25ba7f9ae299557841788002dd85f2bfb310dcf0",
                    "width": 216,
                    "height": 129
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e95743ece71a6073e3f1f54779ed3a9ed19335f5",
                    "width": 320,
                    "height": 192
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281",
                    "width": 640,
                    "height": 384
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bc6aa4dcf1326bad4d74b5ab8d700075183cc35",
                    "width": 960,
                    "height": 576
                  },
                  {
                    "url": "https://preview.redd.it/njam3shfcobf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6eafa575097ea3c5327a9d03316297602f59fdd",
                    "width": 1080,
                    "height": 648
                  }
                ],
                "variants": {},
                "id": "arz-YdPLxSkV6C1oumi84U9PyaRfc_uq0EOj0ruohzc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lusr7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eliebakk",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/",
          "stickied": false,
          "url": "https://i.redd.it/njam3shfcobf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1751991256,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nRun Fine-Tuned LLMs Right on Your iPhone – No Code Needed\n\nVector Space now lets you run powerful, fine-tuned large language models directly on your iPhone. No servers, no code — just tap and chat.\n\n🚀 Why Vector Space:\n1.\tFine-Tuned Models Ready to Go\nRun custom Qwen3 and Llama 3.2 models — including jailbreak, roleplay, and translation models.\n2.\tAll UI, No Coding\nOne-click launch for any model, all within the app.\n3.\tPowered by the Neural Engine\nUltra-efficient — uses ¼ the power and keeps your phone cool.\n4.\tLightning-Fast Chat\nInstant responses:\n• First token in as little as 0.05s\n• Up to 50 tokens/sec\n\n⚠️ First-time model load takes ~5 minutes (one-time setup).\nAfter that, it’s just 1–2 seconds.\n\n⸻\n\n🎉 Try it now on TestFlight:\n\nhttps://testflight.apple.com/join/HXyt2bjU\n\n⸻\n",
          "author_fullname": "t2_w5xu1ep7l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Run Fine-Tuned LLMs on iPhone Neural Engine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lusfyg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3FQlsmM1v7Oz_IqK8FDMkhbs6hoK4jjTHNSsWCSKYBU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751990524,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Run Fine-Tuned LLMs Right on Your iPhone – No Code Needed&lt;/p&gt;\n\n&lt;p&gt;Vector Space now lets you run powerful, fine-tuned large language models directly on your iPhone. No servers, no code — just tap and chat.&lt;/p&gt;\n\n&lt;p&gt;🚀 Why Vector Space:\n1.  Fine-Tuned Models Ready to Go\nRun custom Qwen3 and Llama 3.2 models — including jailbreak, roleplay, and translation models.\n2.  All UI, No Coding\nOne-click launch for any model, all within the app.\n3.  Powered by the Neural Engine\nUltra-efficient — uses ¼ the power and keeps your phone cool.\n4.  Lightning-Fast Chat\nInstant responses:\n• First token in as little as 0.05s\n• Up to 50 tokens/sec&lt;/p&gt;\n\n&lt;p&gt;⚠️ First-time model load takes ~5 minutes (one-time setup).\nAfter that, it’s just 1–2 seconds.&lt;/p&gt;\n\n&lt;p&gt;⸻&lt;/p&gt;\n\n&lt;p&gt;🎉 Try it now on TestFlight:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://testflight.apple.com/join/HXyt2bjU\"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;⸻&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/v1kzyhlbbobf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?auto=webp&amp;s=5a4df1ed565039d9a1efbc6eb638c19c0dddad7e",
                  "width": 1290,
                  "height": 1913
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e11e1f6890e5b8d7706a409bd4fb7e85e523a93",
                    "width": 108,
                    "height": 160
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2bec6cc0238aea0a1086bc2ee077fb1f9760af0b",
                    "width": 216,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e10a939f8e2573fbcb995593f13f8a0914df7228",
                    "width": 320,
                    "height": 474
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ea91f756d4966be382227cb912d202215300fd2",
                    "width": 640,
                    "height": 949
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=41198e6c7a4061ade752db0aed5e6b34135bd5f5",
                    "width": 960,
                    "height": 1423
                  },
                  {
                    "url": "https://preview.redd.it/v1kzyhlbbobf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ccd3f91001fcbd7c7c610917080a0a72e4bed65",
                    "width": 1080,
                    "height": 1601
                  }
                ],
                "variants": {},
                "id": "F_TIbsB9dJMCpGOw8QG0NbcKkeAflY1Rl1N9jMyg_yU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lusfyg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Glad-Speaker3006",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lusfyg/run_finetuned_llms_on_iphone_neural_engine/",
          "stickied": false,
          "url": "https://i.redd.it/v1kzyhlbbobf1.jpeg",
          "subreddit_subscribers": 496591,
          "created_utc": 1751990524,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens.  \n\n\nThis model is ready for commercial/non-commercial use.\n\n\n\n||LiveCodeBench|\n|:-|:-|\n|QwQ-32B|61.3|\n|**OpenCodeReasoning-Nemotron-1.1-14B**|**65.9**|\n|OpenCodeReasoning-Nemotron-14B|59.4|\n|**OpenCodeReasoning-Nemotron-1.1-32B**|**69.9**|\n|OpenCodeReasoning-Nemotron-32B|61.7|\n|DeepSeek-R1-0528|73.4|\n|DeepSeek-R1|65.6|\n\n\n\n\n\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B)\n\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B)\n\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B)\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lus2yw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 172,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 172,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751989711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens.  &lt;/p&gt;\n\n&lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;LiveCodeBench&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;QwQ-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;59.4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;61.7&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-R1-0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;73.4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DeepSeek-R1&lt;/td&gt;\n&lt;td align=\"left\"&gt;65.6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?auto=webp&amp;s=c046dee229da79f38621e6f0294ad12f961a6cb6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64ec5a05cd96cb87a4112fc8bfe7de098998dfb3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3a19d7dc3807669af215390b9226fe990e62a93",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ceacf3cf885d899d7bd7dd681ad5e7c095d46f14",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bc5c5ab390ef3369333633fa85044571e121cce",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e091e24575e0af7c9f8a54d691665da5d4e3b46c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ed5d57035bfc07d513aff72e320481a92cf70cb",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lus2yw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751989711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NextCoder - a Microsoft Collection",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lurzqf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 129,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 129,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ed1bc9d878af5f352b8a651230b4f7a0d0b174d3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751989504,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?auto=webp&amp;s=b3e4bf852dfe93ae28541ec1034617b2146a59e3",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1f0986f5db53e6006b251423459f34eeb980baa",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad42050cffda7472ad56d93a001cc80af264f584",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c6335a5606b421a0be05dfdf7dd7443a82970f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b2179fc5426163403bc73a148e1730509944514",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5164df9aad5c7a0bf16ffca26d00e9d92758d8a8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4a39aefb821cd32c2e1b663ff9657c968f65f31",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lurzqf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/",
          "stickied": false,
          "url": "https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028",
          "subreddit_subscribers": 496591,
          "created_utc": 1751989504,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;format=png&amp;auto=webp&amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8\n\nAny ideas what it might be?",
          "author_fullname": "t2_seii8a9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Major Hugging Face announcement on July 24th",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 118,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "vwc0npf97obf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 91,
                  "x": 108,
                  "u": "https://preview.redd.it/vwc0npf97obf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cb1a46d0486bf8bf778e190bf5c88048a2d8b69"
                },
                {
                  "y": 182,
                  "x": 216,
                  "u": "https://preview.redd.it/vwc0npf97obf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e67e219b78b758d7bc7cb13c0dd53c001d22bdde"
                },
                {
                  "y": 270,
                  "x": 320,
                  "u": "https://preview.redd.it/vwc0npf97obf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa45a5459d3ee82fec36afaab5cf8dab4c711977"
                }
              ],
              "s": {
                "y": 516,
                "x": 611,
                "u": "https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;format=png&amp;auto=webp&amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8"
              },
              "id": "vwc0npf97obf1"
            }
          },
          "name": "t3_1lurv79",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Si8awmHDpB69t1JGly1pXoSydscVCXoxla1VnwQVMbE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751989215,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8\"&gt;https://preview.redd.it/vwc0npf97obf1.png?width=611&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8d87f24a4c50c6bb6f4fe41460ac027808c04e8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any ideas what it might be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lurv79",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LightEt3rnaL",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lurv79/major_hugging_face_announcement_on_july_24th/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751989215,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA’s Highly Anticipated “Mini-Supercomputer,” the DGX Spark, Launches This Month — Bringing Immense AI Power to Your Hands — up to 4000$",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luroqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 273,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 273,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=930e3f587364167265ef11d526ce35f2c301f17c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751988796,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?auto=webp&amp;s=2c3905dab01b88b0dbab01fcb0b574d9f1e512b5",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd25657d3fce734d4025693e620867a7cf866fd1",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed12cf3ccefd6b6e923ec4d43579ac26f2d80130",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf58c691ecc4a68a7e65aa0c0d4e284c08c9845a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0957457f9a703a8e5f4750384c880b16a2c80648",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6ffcb9da830480ee39d159a6310bc89df79861f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1luroqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 271,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1luroqh/nvidias_highly_anticipated_minisupercomputer_the/",
          "stickied": false,
          "url": "https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751988796,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_k5hf12m4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Practical Attacks on AI Text Classifiers with RL (Qwen/Llama, datasets and models available for download)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lurili",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 166,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 166,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=9369b690c4fbf2c4fc32df82afead75f0974846d",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751988401,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "trentmkelly.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?auto=webp&amp;s=74fc0103c96f50ce349af634f08aa915b2fdf89a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96d86512027cc494679c015829c2e32e072fa35f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b490d9d3d1573060c17907a27c1a5a1e5a7be382",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b85930fc544a802fce666658b1bbedf25399cb79",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c0bbd88f042ac1925e2d60123ccd65702e90497",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=67bfe73a1d73f240dbd2cf285c5b22d1e5c9d563",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fbdfea4329e95921530316ee6e2943a0f235e88b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TnzOhNCefQgo-evQobNC87LggZOvGaM0K7HMeNzBrog"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lurili",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WithoutReason1729",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lurili/practical_attacks_on_ai_text_classifiers_with_rl/",
          "stickied": false,
          "url": "https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers",
          "subreddit_subscribers": 496591,
          "created_utc": 1751988401,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm feeding millions of documents consisting of 2-10 sentences each into Llama-3.1-8B-Instruct using pyTorch and asking for a &lt;25 token summary. On a single H200, I'm averaging around 30-40/toks/sec.\n\nMy batch size is 128, which leads to VRAM utilization topping out at around 101GB (out of 141 GB). If I increase it much further, I start to get OOM exceptions. I'm using flash attention-2.\n\nThis is only about 2.5x faster than a RTX-4090 (using a smaller batch size and 4-bit), which is surprising. Can anyone recommend further tuning tips? Thank you.",
          "author_fullname": "t2_7suvt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of throughput can I expect with Llama 3.1 on a H200?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lure0g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751988104,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m feeding millions of documents consisting of 2-10 sentences each into Llama-3.1-8B-Instruct using pyTorch and asking for a &amp;lt;25 token summary. On a single H200, I&amp;#39;m averaging around 30-40/toks/sec.&lt;/p&gt;\n\n&lt;p&gt;My batch size is 128, which leads to VRAM utilization topping out at around 101GB (out of 141 GB). If I increase it much further, I start to get OOM exceptions. I&amp;#39;m using flash attention-2.&lt;/p&gt;\n\n&lt;p&gt;This is only about 2.5x faster than a RTX-4090 (using a smaller batch size and 4-bit), which is surprising. Can anyone recommend further tuning tips? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lure0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "big_like_a_pickle",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751988104,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# \n\n**Skywork-R1V3-38B** is the **latest and most powerful open-source multimodal reasoning model** in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves **open-source state-of-the-art (SOTA)** performance across multiple multimodal reasoning benchmarks.\n\n# [](https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation)\n\n# \n\n# [](https://huggingface.co/Skywork/Skywork-R1V3-38B#🌟-key-results)\n\n# 🌟 Key Results\n\n* **MMMU:** 76.0 — *Open-source SOTA, approaching human experts (76.2)*\n* **EMMA-Mini(CoT):** 40.3 — *Best in open source*\n* **MMK12:** 78.5 — *Best in open source*\n* **Physics Reasoning:** PhyX-MC-TM (52.8), SeePhys (31.5) — *Best in open source*\n* **Logic Reasoning:** MME-Reasoning (42.8) — *Beats Claude-4-Sonnet*, VisuLogic (28.5) — *Best in open source*\n* **Math Benchmarks:** MathVista (77.1), MathVerse (59.6), MathVision (52.6) — *Exceptional problem-solving*",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork/Skywork-R1V3-38B · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1luq8hp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=8988e36113bb1542fe9cef5c1896ea6ec6e101bd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751985454,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Skywork-R1V3-38B&lt;/strong&gt; is the &lt;strong&gt;latest and most powerful open-source multimodal reasoning model&lt;/strong&gt; in the Skywork series, pushing the boundaries of multimodal and cross-disciplinary intelligence. With elaborate RL algorithm in the post-training stage, R1V3 significantly enhances multimodal reasoning ablity and achieves &lt;strong&gt;open-source state-of-the-art (SOTA)&lt;/strong&gt; performance across multiple multimodal reasoning benchmarks.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/Skywork/Skywork-R1V3-38B#2-evaluation\"&gt;&lt;/a&gt;&lt;/h1&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/Skywork/Skywork-R1V3-38B#%F0%9F%8C%9F-key-results\"&gt;&lt;/a&gt;&lt;/h1&gt;\n\n&lt;h1&gt;🌟 Key Results&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;MMMU:&lt;/strong&gt; 76.0 — &lt;em&gt;Open-source SOTA, approaching human experts (76.2)&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;EMMA-Mini(CoT):&lt;/strong&gt; 40.3 — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MMK12:&lt;/strong&gt; 78.5 — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Physics Reasoning:&lt;/strong&gt; PhyX-MC-TM (52.8), SeePhys (31.5) — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Logic Reasoning:&lt;/strong&gt; MME-Reasoning (42.8) — &lt;em&gt;Beats Claude-4-Sonnet&lt;/em&gt;, VisuLogic (28.5) — &lt;em&gt;Best in open source&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Math Benchmarks:&lt;/strong&gt; MathVista (77.1), MathVerse (59.6), MathVision (52.6) — &lt;em&gt;Exceptional problem-solving&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Skywork/Skywork-R1V3-38B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?auto=webp&amp;s=0904a5e53f20449eb9e39e1123f3ac2b429d6ea5",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa8785f2c86e0a266b664cbff742d81402b4d47e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b7053fab392ec60745c8e182823d6449627b599",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e9cafa9cc62b546dc6b22c6bcb47f10d550e0b1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9781820f621f09b406ca5a209d2d1f7685f966ef",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8020d2286dd487e1088189e55014fcaa1276bbf8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a077cc9c57cb266ae95ef65d4a0159b5d381622",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3e7sHYHNZhfN6oJnsHpPG0zaYjLPHYlt79D7YjZqJp0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1luq8hp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1luq8hp/skyworkskyworkr1v338b_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Skywork/Skywork-R1V3-38B",
          "subreddit_subscribers": 496591,
          "created_utc": 1751985454,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Using knapsack algorithm to efficiently batch the data helps train faster. In the blog post we cover a stage wise approach to making the data pipeline better.\n\nhttps://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=2338717ec3d962002b326245686078de2ee7b479\n\nhttps://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d\n\n  \nBlog: [hf.co/blog/mmdp](http://hf.co/blog/mmdp)\n\nRepo: [github.com/ariG23498/mmdp](http://github.com/ariG23498/mmdp)",
          "author_fullname": "t2_1bdsvnt734",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Efficient Multimodal Data Pipeline",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wdccczfarnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=513bd53fb629f6b284534e892d081010e03bbd5d"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=171d4e38724b05b8b528d836cb45eac2b92472ed"
                },
                {
                  "y": 192,
                  "x": 320,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cfa2e9b123c23687a5373decb5cccd92f2708aa"
                },
                {
                  "y": 384,
                  "x": 640,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa2422badfbad2ab62f85d09a12a98d700b680f3"
                },
                {
                  "y": 576,
                  "x": 960,
                  "u": "https://preview.redd.it/wdccczfarnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6fa970052821997bbf74eab40efb8b85ac6af4b"
                }
              ],
              "s": {
                "y": 600,
                "x": 1000,
                "u": "https://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=2338717ec3d962002b326245686078de2ee7b479"
              },
              "id": "wdccczfarnbf1"
            },
            "nwc3prfarnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=776a7663c7cdb3bb7435efd7a238dd6af1a99ae5"
                },
                {
                  "y": 129,
                  "x": 216,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d217bd5302c35eeef3f77bcf2bea95891238cb1"
                },
                {
                  "y": 192,
                  "x": 320,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=640980de278ee1e3ee141fc431105d7d654b65b2"
                },
                {
                  "y": 384,
                  "x": 640,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffe06235a7b753c470620dcdaeddf271ffe8c976"
                },
                {
                  "y": 576,
                  "x": 960,
                  "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f60e9c8cf4ee1009722f520a854c51bf7c91ec8"
                }
              ],
              "s": {
                "y": 600,
                "x": 1000,
                "u": "https://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d"
              },
              "id": "nwc3prfarnbf1"
            }
          },
          "name": "t3_1lupk47",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=3b3b914a0345af653a6d833a81d12fc808436ccd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751983807,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Using knapsack algorithm to efficiently batch the data helps train faster. In the blog post we cover a stage wise approach to making the data pipeline better.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2338717ec3d962002b326245686078de2ee7b479\"&gt;https://preview.redd.it/wdccczfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2338717ec3d962002b326245686078de2ee7b479&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d\"&gt;https://preview.redd.it/nwc3prfarnbf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02eed56ac1c6c5a27e6f7e1ca81da917e2433b8d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"http://hf.co/blog/mmdp\"&gt;hf.co/blog/mmdp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo: &lt;a href=\"http://github.com/ariG23498/mmdp\"&gt;github.com/ariG23498/mmdp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?auto=webp&amp;s=816d0c62cbe2dd5322babd8e23fa5bc624cee7d2",
                  "width": 1300,
                  "height": 650
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f9f9c89e881153ea2dca9c42d70ea4867b3cc83",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aef1461b6f47e4f87a34b9ada857e4e32122d3ab",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a10a73eb98a06e4cb3a53546bed742980ce3c424",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=15841d90805593d1fe5859cf5ff112faf8116df3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fc8f62d1f7dd5490ef87afaebc98a06a8d77348",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b0d0e74c7fdb3b4f1ca49a3d47f37bc668ae1947",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "htw5f24vzDhoh1VMu1c0r94PxjBdMJnubeaeDosh-w8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lupk47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Disastrous-Work-1632",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lupk47/efficient_multimodal_data_pipeline/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lupk47/efficient_multimodal_data_pipeline/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983807,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My name is Caro and I’m a Strategic Product Designer at Stack Overflow working alongside our Data Scientists. We’re looking to better understand this community's take on which types of content patterns (shown using Stack Overflow Q&amp;A examples) are most ideal to support AI model training and what factors lead to better outcomes. \n\nIf you are currently involved in training AI models or building AI products, we’d love to hear from you. Please take a few minutes to fill out our short survey: [https://app.ballparkhq.com/share/self-guided/ut\\_d7891037-0254-40cf-85e7-20ed9f442b1c](https://app.ballparkhq.com/share/self-guided/ut_d7891037-0254-40cf-85e7-20ed9f442b1c)\n\nAs a token of our appreciation, you will be entered into a raffle to win a US$50 gift card in a random drawing of 10 participants after completing the survey.\n\nThanks again and thank you to the mods for letting me connect with the community here.",
          "author_fullname": "t2_91xltixw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Q&amp;A Content Pattern Assessment — Survey from Stack Overflow",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lupg1f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751983534,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My name is Caro and I’m a Strategic Product Designer at Stack Overflow working alongside our Data Scientists. We’re looking to better understand this community&amp;#39;s take on which types of content patterns (shown using Stack Overflow Q&amp;amp;A examples) are most ideal to support AI model training and what factors lead to better outcomes. &lt;/p&gt;\n\n&lt;p&gt;If you are currently involved in training AI models or building AI products, we’d love to hear from you. Please take a few minutes to fill out our short survey: &lt;a href=\"https://app.ballparkhq.com/share/self-guided/ut_d7891037-0254-40cf-85e7-20ed9f442b1c\"&gt;https://app.ballparkhq.com/share/self-guided/ut_d7891037-0254-40cf-85e7-20ed9f442b1c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a token of our appreciation, you will be entered into a raffle to win a US$50 gift card in a random drawing of 10 participants after completing the survey.&lt;/p&gt;\n\n&lt;p&gt;Thanks again and thank you to the mods for letting me connect with the community here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?auto=webp&amp;s=67c3db25731a507a6fc6e4341d8763d1efd19151",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79db7cfb87caaf1c480a675c03c065b8dc0efe62",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86bc78fb765556208a0738d4e285392dd230e9af",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cff4905192a42b4d7d71a0d430ea30b34fc57c70",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84e58ff7a32c01965f30cbf55e435768e72b18d7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1230a9cfa1df79ce6c092d4bb609c669e92bc121",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d6326ebdadafd7adc0b055ab932cf8af0c9b9b1",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "-i4_uRRkBAWt5Dp8mEpIeZFSh9ZtU3-0chGRRO39eZM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lupg1f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tiny_Dot0",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lupg1f/qa_content_pattern_assessment_survey_from_stack/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lupg1f/qa_content_pattern_assessment_survey_from_stack/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983534,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4c4yo8b4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Question on llm and image generation + text output, is there any way to get results like the images, real information with images generated based on actual information.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "h57buxgzpnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/h57buxgzpnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e3b2d0b0a6ceb3766e282070b355f058ce1ef76f"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/h57buxgzpnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a83b5e48813a7927415f0b2cee6d966800e4eb77"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/h57buxgzpnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8729d77a36975c5516f3b0146dbd8a4f35f5c98"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/h57buxgzpnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5c68335e65bb7ff6e6ff93dd0a19a899757dcb4"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/h57buxgzpnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d5d7c9551dbea44f0ae9b8e10dd84422da84c73b"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/h57buxgzpnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=62ef1ae655ee9f13ea4b1d842a2f49e51962b6e2"
              },
              "id": "h57buxgzpnbf1"
            },
            "4o4jxq22qnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/4o4jxq22qnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=845bf8bf608790fb9e25ff2026ac99b61a2ba688"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/4o4jxq22qnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=199440a49ce035720d7b21028854ade9790eae59"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/4o4jxq22qnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de10758b9c2dd5f3a8104f7f980772524bb904a4"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/4o4jxq22qnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2445c1d23a75848812a60112662417cc34787b03"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/4o4jxq22qnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=75971ba5b95471252b9797e5d87c36b04c7a2742"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/4o4jxq22qnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=7ce7925479789086b9706870f2af423d98e283ac"
              },
              "id": "4o4jxq22qnbf1"
            },
            "0uozk4m1qnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/0uozk4m1qnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=da03dd627d5691f2ed6c40daec9c7ea462d7e406"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/0uozk4m1qnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bf5bc6438c64b7158c8ac1e4ac5ac2ad9ff5a00"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/0uozk4m1qnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b89c200a2afef7cd8ff2eddc27ca8654c34a8e3f"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/0uozk4m1qnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86328741bef2e8aef56edd81445c039504e8cc02"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/0uozk4m1qnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd163cec6cb095d599f7e9300e0ac37e8f61748a"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/0uozk4m1qnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=53b6c0c70190bf539cc6e7b4b273949334e7de25"
              },
              "id": "0uozk4m1qnbf1"
            },
            "bzq74dwzpnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/bzq74dwzpnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=635614e1ff9135594848a629ea561a730ba5f343"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/bzq74dwzpnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1abdea078d4d7e4dd38dfca35fb9bddc7133bb87"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/bzq74dwzpnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe3b558a66fb52ea1da6fd4a1069071090eb572d"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/bzq74dwzpnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=878f450ec1e7dbe5aedd4a364d9b17065aa38f83"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/bzq74dwzpnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdfd2c1777a4ab47163b76058b909aab02e9ea88"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/bzq74dwzpnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=23d7e3742c6e7e43323646f54738d6fadce69b45"
              },
              "id": "bzq74dwzpnbf1"
            },
            "ub5d20v0qnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/ub5d20v0qnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0fbb55dbf8a67f6324fd447623a41a2933eda65"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/ub5d20v0qnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d3fd2917d9d279547ed0afc5481548405b161d3"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/ub5d20v0qnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61376d1ad81b8fa4b0d4c2f0f77b857afbb38806"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/ub5d20v0qnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe5ff08a2dccaebd8f2f04e8dd72951bd6df2f27"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/ub5d20v0qnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=70a8e1e07e21c1bd8096bb84fe463dede03808aa"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/ub5d20v0qnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=60a7a4b55977d18aa1da0c613f2c4041600c1670"
              },
              "id": "ub5d20v0qnbf1"
            },
            "0rxuia91qnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/0rxuia91qnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd949ef66da1f86e5f15cc9d0f97878cc199ad77"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/0rxuia91qnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=00911c41a8ce34761ec8e37263b80490a9cdf385"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/0rxuia91qnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8d06a30ca71a146503d9c30b7a5cf8d9c91361c"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/0rxuia91qnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a455b0a118a2f2a55ba433a49e058cf12a986986"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/0rxuia91qnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7c232dae81d60ee4bf46909f8d85b8b963b4560"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/0rxuia91qnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b23d50ce2f83a1b522a6bbb0a53197dec08ab328"
              },
              "id": "0rxuia91qnbf1"
            },
            "fomjzn3zpnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/fomjzn3zpnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=731626e8ca5ae8f61716d6535994426b9b6ac02b"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/fomjzn3zpnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8be8f4664cbd0b7b529dbace65529eca808b3ef"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/fomjzn3zpnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=473a379813d3d57397ee3ad5449afec2714291c3"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/fomjzn3zpnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9198cb064422a7ce2f17fbb92c45e5f232c24e2b"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/fomjzn3zpnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5575b973bc90fb3e2f526cf65b23b00620bb56d7"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/fomjzn3zpnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=f02fb19140b8d087ddbd4555c66c4814b76bea4e"
              },
              "id": "fomjzn3zpnbf1"
            },
            "61r3g290qnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/61r3g290qnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87f1fa88bc93ec0f7484af0b2dfa2cb6d33e87f2"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/61r3g290qnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=56db3dc8d5dc12c4eed216a07e1bfa6c32295f88"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/61r3g290qnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c22d0343c9d9f021deab8abdb3f283b8d1158b1"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/61r3g290qnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e27c840177af9280f0c27bd90866894329d53fdf"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/61r3g290qnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=367e1c215e0070206229254bace80a801c305683"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/61r3g290qnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=0937b6122464c1858e3ca9c6bdf8fae32c57db1a"
              },
              "id": "61r3g290qnbf1"
            },
            "g1hpcvk0qnbf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/g1hpcvk0qnbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec033c04cc09cc5c8e03965b521b71ac2070dab7"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/g1hpcvk0qnbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a3e3a2bf574e993f9ca981eaf38d3b4d5bbc617"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/g1hpcvk0qnbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f662e59e0984e61ad9414a4636f8155b89185ba3"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/g1hpcvk0qnbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e34fbd22c570842eaef973bb434bf2b7a2278ec1"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/g1hpcvk0qnbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d424a1dcb6e527c6c3a39f32735b51cfc9fec83"
                }
              ],
              "s": {
                "y": 1536,
                "x": 1024,
                "u": "https://preview.redd.it/g1hpcvk0qnbf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=8ba79873e38783acc9acd450d75d26455c6d4efb"
              },
              "id": "g1hpcvk0qnbf1"
            }
          },
          "name": "t3_1lupdrq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 3,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "fomjzn3zpnbf1",
                "id": 701374691
              },
              {
                "caption": "",
                "media_id": "h57buxgzpnbf1",
                "id": 701374692
              },
              {
                "caption": "",
                "media_id": "bzq74dwzpnbf1",
                "id": 701374693
              },
              {
                "caption": "",
                "media_id": "61r3g290qnbf1",
                "id": 701374694
              },
              {
                "caption": "",
                "media_id": "g1hpcvk0qnbf1",
                "id": 701374695
              },
              {
                "caption": "",
                "media_id": "ub5d20v0qnbf1",
                "id": 701374696
              },
              {
                "caption": "",
                "media_id": "0rxuia91qnbf1",
                "id": 701374697
              },
              {
                "caption": "",
                "media_id": "0uozk4m1qnbf1",
                "id": 701374698
              },
              {
                "caption": "",
                "media_id": "4o4jxq22qnbf1",
                "id": 701374699
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EIgM6lDPJV4tE9gRgXOVhbuXS_3wYB5P2CyQ-fFowzY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751983374,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lupdrq",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lupdrq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ziov1",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lupdrq/question_on_llm_and_image_generation_text_output/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lupdrq",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983374,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://brianheming.substack.com/p/making-illustrated-conan-adventures-039](https://brianheming.substack.com/p/making-illustrated-conan-adventures-039)",
          "author_fullname": "t2_kkyv3ui",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Automated illustration of a Conan story using gemma3 + flux and other local models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lup9qp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/yBQ56Q0pn3RUOLWl805-GfiCwIO0WMsCwLp92zVAZ70.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751983114,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://brianheming.substack.com/p/making-illustrated-conan-adventures-039\"&gt;https://brianheming.substack.com/p/making-illustrated-conan-adventures-039&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/53ufkibvonbf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/53ufkibvonbf1.png?auto=webp&amp;s=43ee139f2c270a2b59a0c91370aa207355f37d01",
                  "width": 1802,
                  "height": 1185
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1df219bb756e8a2c2fdd5b696830b3a0f337456c",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=67c82c286aa9ee21c592a7d776ee2fee77cd8e52",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b1abefe99475665fe245935ce8ed40bb3919246",
                    "width": 320,
                    "height": 210
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86ee2cc7e7a139938f260ed99ec11c92842be868",
                    "width": 640,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=202674493391560eac6873331dba50fff1e8688d",
                    "width": 960,
                    "height": 631
                  },
                  {
                    "url": "https://preview.redd.it/53ufkibvonbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3e84cce3c8da66468d6b20534755794b219fe3b",
                    "width": 1080,
                    "height": 710
                  }
                ],
                "variants": {},
                "id": "IbzaZ3LsNqIUCJB2ggX9tqKynOVmMlq9tCDr7njh5Ow"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lup9qp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RobertTetris",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lup9qp/automated_illustration_of_a_conan_story_using/",
          "stickied": false,
          "url": "https://i.redd.it/53ufkibvonbf1.png",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983114,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We have added a feature to our RAG pipeline that shows **exact citations** — not just the source file, but the **exact paragraph or row** the AI used to answer.\n\nClick a citation and it scrolls you straight to that spot in the document — works with **PDFs, Excel, CSV, Word, PPTX, Markdown**, and others.\n\nIt’s super useful when you want to **trust but verify** AI answers, especially with long or messy files.\n\nWe’ve open-sourced it here: [https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)  \nWould love your feedback or ideas!\n\nDemo Video: [https://youtu.be/1MPsp71pkVk](https://youtu.be/1MPsp71pkVk)",
          "author_fullname": "t2_vk5ut1sk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We built pinpointed citations for AI answers — works with PDFs, Excel, CSV, Docx &amp; more",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lup8yd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751983057,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have added a feature to our RAG pipeline that shows &lt;strong&gt;exact citations&lt;/strong&gt; — not just the source file, but the &lt;strong&gt;exact paragraph or row&lt;/strong&gt; the AI used to answer.&lt;/p&gt;\n\n&lt;p&gt;Click a citation and it scrolls you straight to that spot in the document — works with &lt;strong&gt;PDFs, Excel, CSV, Word, PPTX, Markdown&lt;/strong&gt;, and others.&lt;/p&gt;\n\n&lt;p&gt;It’s super useful when you want to &lt;strong&gt;trust but verify&lt;/strong&gt; AI answers, especially with long or messy files.&lt;/p&gt;\n\n&lt;p&gt;We’ve open-sourced it here: &lt;a href=\"https://github.com/pipeshub-ai/pipeshub-ai\"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;br/&gt;\nWould love your feedback or ideas!&lt;/p&gt;\n\n&lt;p&gt;Demo Video: &lt;a href=\"https://youtu.be/1MPsp71pkVk\"&gt;https://youtu.be/1MPsp71pkVk&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?auto=webp&amp;s=ca708944e706d0f6e6422d0b596b046db15d2924",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eadf34b506a23b84310c25cebe4274660315f5f7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d51e2e192f0851ae4487f78e5958bef8ebd3974c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=734cc59c67a81201b322b8060319ef81eafa2b6d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b8a37559218f4ace8678d3826c09c07d36aa86a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1ac5e9a909e8eda4085481acfe3f9fdec6f8f1d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=906775b9eb93e38e6a7cf07167d5cfff0781528c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "V_tfXwBwMp_Y88r_ufJO_lIg21ROik8nAtQZXQAfOSs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lup8yd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Effective-Ad2060",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lup8yd/we_built_pinpointed_citations_for_ai_answers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lup8yd/we_built_pinpointed_citations_for_ai_answers/",
          "subreddit_subscribers": 496591,
          "created_utc": 1751983057,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}