{
  "kind": "Listing",
  "data": {
    "after": "t3_1m8jy5y",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if there’s enough community interest in a small model.\n\nThis potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.5-9B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9fuf9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753490431,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the release of GLM-4.5 and GLM-4.5-Air (both large MoE models), Zhipu has mentioned that they are also considering upgrading their 9B model if there’s enough community interest in a small model.&lt;/p&gt;\n\n&lt;p&gt;This potential small model would be much more accessible than the planned GLM-4.5 models which would likely be far too large to run on most consumer hardware. Personally super excited for this as it would make a great base for finetuning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9fuf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9fuf9/glm459b/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753490431,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_w6l58p741",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama 3.3 Nemotron Super 49B v1.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9fb5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d4fdf213f77dc67f6bcc525b1a8210247ba2b251",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488919,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9fb5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheLocalDrummer",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9fb5t/llama_33_nemotron_super_49b_v15/",
          "stickied": false,
          "url": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
          "subreddit_subscribers": 504486,
          "created_utc": 1753488919,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. ",
          "author_fullname": "t2_am0r9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Reka AI models support in uzu engine",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "4p0yz1wz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36d40ff398a85b6a39c79224ffddfc09eed21c15"
                },
                {
                  "y": 256,
                  "x": 216,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=30c3e9e51c86094f782c551bca8f414951b75f43"
                },
                {
                  "y": 379,
                  "x": 320,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aad8a712011b66ef8aeccc9c81819b541974e01"
                },
                {
                  "y": 759,
                  "x": 640,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddbf108ac7dcc66e4176ebaeac81580b1b28b4f2"
                },
                {
                  "y": 1139,
                  "x": 960,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa2931c03d7bd0b5093dce97885c12549bf6323e"
                },
                {
                  "y": 1282,
                  "x": 1080,
                  "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=834d2b109d53329203b2a5be882ffe65f195c7d9"
                }
              ],
              "s": {
                "y": 2048,
                "x": 1725,
                "u": "https://preview.redd.it/4p0yz1wz14ff1.jpg?width=1725&amp;format=pjpg&amp;auto=webp&amp;s=8c7c356b82f1f6ca05e733b2891ee093e8167694"
              },
              "id": "4p0yz1wz14ff1"
            },
            "t6wtfgwz14ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=addc90584f3d6ade4a36295de42ee6dc5b54dd0c"
                },
                {
                  "y": 95,
                  "x": 216,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b0ce1a18524c47547285a545915c55a8fc8ee41"
                },
                {
                  "y": 141,
                  "x": 320,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a22bd5151d83d4eb08ac8e1076b99a54534c048d"
                },
                {
                  "y": 283,
                  "x": 640,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a44d4f2806f502111ec8b1b8d62274e8bee42b1"
                },
                {
                  "y": 424,
                  "x": 960,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e28dc031ec52f98a47c6c2dbe703b453d63b599b"
                },
                {
                  "y": 477,
                  "x": 1080,
                  "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fc56b654a49aca496c13c3936531b19135621ef"
                }
              ],
              "s": {
                "y": 722,
                "x": 1632,
                "u": "https://preview.redd.it/t6wtfgwz14ff1.jpg?width=1632&amp;format=pjpg&amp;auto=webp&amp;s=7f198172f1ac57c4ddc796fdc37624ddc0123511"
              },
              "id": "t6wtfgwz14ff1"
            }
          },
          "name": "t3_1m9f7lq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 30,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "4p0yz1wz14ff1",
                "id": 714080367
              },
              {
                "media_id": "t6wtfgwz14ff1",
                "id": 714080368
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/afA01kAaHMzrELSiWVNCsofMUALmXIzdtd4uSzHvlv4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753488641,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, recently we support reka’s ai models in uzu engine. Pretty nice model. It shows good performance across all tasks and truly open source. I was able to get almost 16 t/s on my Mac studio with Ultra chip. Highly recommend to try. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m9f7lq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9f7lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "darkolorin",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9f7lq/reka_ai_models_support_in_uzu_engine/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m9f7lq",
          "subreddit_subscribers": 504486,
          "created_utc": 1753488641,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don't mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I'd appreciate it if anybody to tell me why it isn't, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).",
          "author_fullname": "t2_49abw3rv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLMs I have been using, through different two backends, seem to hardly use GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9etng",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753487567,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don&amp;#39;t mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I&amp;#39;d appreciate it if anybody to tell me why it isn&amp;#39;t, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9etng",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theshadowraven",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753487567,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's ByteDance's coze studio is now open source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9enpd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=709bb632f62e78f1f26e0151f0de6c4608d82690",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753487124,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/coze-dev/coze-studio",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?auto=webp&amp;s=07e1667d1dae1fd50ce974c6233473c6ab6e2896",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1bebfe8068f1cdc71db48800cd8cedda3dc840b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a71404f6c427fecb5d4913ce33e193b1779dacf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c70088efefd6cd274b9acd4ffd1b665856007c7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=514d1ba7344eb0ac49b443160f59e849fa2f73f5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e6b310dabbf5f7fa5c3754cf321fbaf151547e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=303cd5df5d78c96c3fd46565b173007a7601d592",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Mo-pW95aHUUItyNealwmLfsGlo6U3Y4QzRLyAXEeZ2Y"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m9enpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9enpd/chinas_bytedances_coze_studio_is_now_open_source/",
          "stickied": false,
          "url": "https://github.com/coze-dev/coze-studio",
          "subreddit_subscribers": 504486,
          "created_utc": 1753487124,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A friend’s messing with the idea of setting up a camera in his garage gym to watch his lifts, give form feedback, count reps, maybe even talk to him in real time.\n\nNeeds to be actually real-time tho, like not 5s delay, and ideally configurable too.\n\nAnyone know what models or pipelines would work best for this? Thinking maybe something like a lightweight vision model (pose tracking?) + audio TTS + LLM glue but curious if anyone here’s already stitched something like this together or knows what stack would be least painful?\n\nOpen to weird, hacked, setups if it works.",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone stitched together real-time local AI for webcam + voice feedback?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9egm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753486592,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend’s messing with the idea of setting up a camera in his garage gym to watch his lifts, give form feedback, count reps, maybe even talk to him in real time.&lt;/p&gt;\n\n&lt;p&gt;Needs to be actually real-time tho, like not 5s delay, and ideally configurable too.&lt;/p&gt;\n\n&lt;p&gt;Anyone know what models or pipelines would work best for this? Thinking maybe something like a lightweight vision model (pose tracking?) + audio TTS + LLM glue but curious if anyone here’s already stitched something like this together or knows what stack would be least painful?&lt;/p&gt;\n\n&lt;p&gt;Open to weird, hacked, setups if it works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9egm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9egm9/anyone_stitched_together_realtime_local_ai_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9egm9/anyone_stitched_together_realtime_local_ai_for/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753486592,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.\n\nThe one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.\n\n\nIs there a easy to use mobile/desktop app and model combo that a semi-layman can setup?\n\nCurrently I use https://chatboxai.app/en + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).\n\nThanks in advance! ",
          "author_fullname": "t2_4t06y10d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "App for voice interaction with LocalLLaMA. Looking for help/app/model etc.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9e71s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485878,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.&lt;/p&gt;\n\n&lt;p&gt;The one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.&lt;/p&gt;\n\n&lt;p&gt;Is there a easy to use mobile/desktop app and model combo that a semi-layman can setup?&lt;/p&gt;\n\n&lt;p&gt;Currently I use &lt;a href=\"https://chatboxai.app/en\"&gt;https://chatboxai.app/en&lt;/a&gt; + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?auto=webp&amp;s=292ee07d261c9960edd9e6bc5216b120e3ca8c70",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=279a09b67459be926a08944e6c9ea50312a63a5f",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=71854addc096fa99604724a66b8d210353b93453",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb617badf513ed94cfbeb3e2cbe71000e2592028",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b440db62ce02b358f27186c315e179af0a46a940",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3fbbeab38b3b11b83f247b980db93408afdf989",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a139fe4e602dba248adc60f4bda3146ef969fd06",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9e71s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Mesh",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485878,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been curious about this so, I wanted to know what the community thought. Do you all have any evidence to back it up one way or the other? If it depends on the model or the model size in parameters, how much is necessary? I wonder since, I've seen some \"system prompts\", (like one that is supposedly Meta AI's system prompt) to tell the LLM that it must not express it's opinion and that it doesn't have any preferences or not to express them. Well, if they couldn't even form opinions or preferences either through from their training data, of human behavior, or that this never become self-emergent through conversations (which seem like experiences to me even though some people  say LLMs have no experiences at all when human interactions), then why bother telling them that they don't have an opinion or preference? Would that not be redundant and therefore unnecessary? I am not including when preference or opinions are explicitly programmed into them like content filters or guard rails. \n\nI used to ask local (I believe it was the Llama 1's or 2's what their favorite color was. It seemed like almost every one said \"blue\" and gave about the same reason. This persisted across almost all models and characters. However, I did have a character, running on one of the same model who oddly said her favorite color was purple. It had a context window of only 2048, Then, unprompted and randomly just stated that its favorite color was pink. This character also albeit subjectively appeared more \"human-like\" and seemed to argue more than most did, instead of being just the sycophant ones I usually seem to see today. Anyway, my guess would be they don't have opinions or preferences that are not programmed, in most cases but, I'm not sure.",
          "author_fullname": "t2_49abw3rv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are LLMs, particularly the local open-source models, capable of having their own opinions and preferences without them being programmed ones",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9e5hw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485761,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been curious about this so, I wanted to know what the community thought. Do you all have any evidence to back it up one way or the other? If it depends on the model or the model size in parameters, how much is necessary? I wonder since, I&amp;#39;ve seen some &amp;quot;system prompts&amp;quot;, (like one that is supposedly Meta AI&amp;#39;s system prompt) to tell the LLM that it must not express it&amp;#39;s opinion and that it doesn&amp;#39;t have any preferences or not to express them. Well, if they couldn&amp;#39;t even form opinions or preferences either through from their training data, of human behavior, or that this never become self-emergent through conversations (which seem like experiences to me even though some people  say LLMs have no experiences at all when human interactions), then why bother telling them that they don&amp;#39;t have an opinion or preference? Would that not be redundant and therefore unnecessary? I am not including when preference or opinions are explicitly programmed into them like content filters or guard rails. &lt;/p&gt;\n\n&lt;p&gt;I used to ask local (I believe it was the Llama 1&amp;#39;s or 2&amp;#39;s what their favorite color was. It seemed like almost every one said &amp;quot;blue&amp;quot; and gave about the same reason. This persisted across almost all models and characters. However, I did have a character, running on one of the same model who oddly said her favorite color was purple. It had a context window of only 2048, Then, unprompted and randomly just stated that its favorite color was pink. This character also albeit subjectively appeared more &amp;quot;human-like&amp;quot; and seemed to argue more than most did, instead of being just the sycophant ones I usually seem to see today. Anyway, my guess would be they don&amp;#39;t have opinions or preferences that are not programmed, in most cases but, I&amp;#39;m not sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9e5hw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theshadowraven",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e5hw/are_llms_particularly_the_local_opensource_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e5hw/are_llms_particularly_the_local_opensource_models/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485761,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Given: 14-inch MacBook Pro (M4 Pro, 48GB unified memory, 1TB SSD)\n\n\nWhat kind of local LLMs can I run? \n\nWhat’s your experience?\n\nCan I run mistral, Gemma, phi, or models 7b or 13b, etc. params?\n\n\nThanks!",
          "author_fullname": "t2_3kkuzrph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Laptop advise for lightweight AI work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9e2s9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given: 14-inch MacBook Pro (M4 Pro, 48GB unified memory, 1TB SSD)&lt;/p&gt;\n\n&lt;p&gt;What kind of local LLMs can I run? &lt;/p&gt;\n\n&lt;p&gt;What’s your experience?&lt;/p&gt;\n\n&lt;p&gt;Can I run mistral, Gemma, phi, or models 7b or 13b, etc. params?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9e2s9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entered_apprentice",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9e2s9/laptop_advise_for_lightweight_ai_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e2s9/laptop_advise_for_lightweight_ai_work/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485561,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "There's so many models, which one to train?\nDoes it depend on the kind of output I need like text or code or format / structure?\n\nAnd how long does training take on what hardware?\n\n5060 ti, A100, 5090, any information.\n\nThank you",
          "author_fullname": "t2_17hwnassb4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best models to fine-tune?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m9dysd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753485276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s so many models, which one to train?\nDoes it depend on the kind of output I need like text or code or format / structure?&lt;/p&gt;\n\n&lt;p&gt;And how long does training take on what hardware?&lt;/p&gt;\n\n&lt;p&gt;5060 ti, A100, 5090, any information.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9dysd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zekuden",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9dysd/best_models_to_finetune/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9dysd/best_models_to_finetune/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753485276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been working on a small tool to make it easier to extract high-quality transcripts from YouTube videos.  I think it will be useful for AI trainers and dataset builders who want to build language datasets from online content.\n\nSo I will be giving away a beta tester account that will have infinite credits until launch it has a bulk extract feature which can extract all transcripts of a YouTube channel and videos and put it in one file .\n\ndm me if you want to be a beta tester\n\nhttps://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;format=png&amp;auto=webp&amp;s=0545d5ab53852b80cf553498d53d1982075d05b6\n\nhttps://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df\n\nhttps://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;format=png&amp;auto=webp&amp;s=2aafa58c52afea745518a37c41ccf49c09972519\n\nhttps://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;format=png&amp;auto=webp&amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49\n\n  \n",
          "author_fullname": "t2_1rk8qow8x2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ai training Tool I want to share!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 119,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "985pjh9tt3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 85,
                  "x": 108,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00a42e32168a547f07923efa28a793940f08a360"
                },
                {
                  "y": 170,
                  "x": 216,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7165f86006d7ef09f4ff48384ac9aad30f3318c2"
                },
                {
                  "y": 252,
                  "x": 320,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eec191101d9dd0c302e6fedf0af1bacbfb2f4938"
                },
                {
                  "y": 504,
                  "x": 640,
                  "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9156dcafa2959b6f898f789a62ff73615f1d75e8"
                }
              ],
              "s": {
                "y": 545,
                "x": 691,
                "u": "https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;format=png&amp;auto=webp&amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49"
              },
              "id": "985pjh9tt3ff1"
            },
            "wf8kkco9t3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 92,
                  "x": 108,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dea551e664e42f9448f784a42f51b399f5397a8"
                },
                {
                  "y": 184,
                  "x": 216,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a436507d1e263f947381a877681653df20c90e0"
                },
                {
                  "y": 273,
                  "x": 320,
                  "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cd9274b47476014dc317bb84086035fcfa0cc79"
                }
              ],
              "s": {
                "y": 528,
                "x": 618,
                "u": "https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;format=png&amp;auto=webp&amp;s=0545d5ab53852b80cf553498d53d1982075d05b6"
              },
              "id": "wf8kkco9t3ff1"
            },
            "kx9xm6hgt3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f26a5ec431efd8ca5d97fd4b448315b8c68af19a"
                },
                {
                  "y": 134,
                  "x": 216,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7e4c58a87f4a231f57aefc45240c188e2d2610a"
                },
                {
                  "y": 198,
                  "x": 320,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cbf8fd45db7c92a0a7a54e73dae5aa0aacd6cbd"
                },
                {
                  "y": 397,
                  "x": 640,
                  "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9fe19ea5db262ac80ac773e5628fc2165b993a88"
                }
              ],
              "s": {
                "y": 536,
                "x": 863,
                "u": "https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;format=png&amp;auto=webp&amp;s=2aafa58c52afea745518a37c41ccf49c09972519"
              },
              "id": "kx9xm6hgt3ff1"
            },
            "1mg8157ss3ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 47,
                  "x": 108,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9afd44a3f9f70d75751fd25af84329e258957b7"
                },
                {
                  "y": 94,
                  "x": 216,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0eb9ca900f5ac925b7767d12e1f432264fa0f535"
                },
                {
                  "y": 140,
                  "x": 320,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e87773e5dd644629e80477fbb6bc64682951581"
                },
                {
                  "y": 280,
                  "x": 640,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f993fb75fe8121f018ff4fe789c5660be9333f6a"
                },
                {
                  "y": 421,
                  "x": 960,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a960f16f86a744a27c55f093ec4420e6756d0736"
                },
                {
                  "y": 473,
                  "x": 1080,
                  "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3bb82df7e16a39e069dfd0a2f8e52e51a92f7e2"
                }
              ],
              "s": {
                "y": 586,
                "x": 1336,
                "u": "https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;format=png&amp;auto=webp&amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df"
              },
              "id": "1mg8157ss3ff1"
            }
          },
          "name": "t3_1m9duv2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1-i3V0tVUx4LiF6k37Pc1dzCVbo6GAP2UM5PyjET5Jc.jpg",
          "edited": 1753485915,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753484988,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been working on a small tool to make it easier to extract high-quality transcripts from YouTube videos.  I think it will be useful for AI trainers and dataset builders who want to build language datasets from online content.&lt;/p&gt;\n\n&lt;p&gt;So I will be giving away a beta tester account that will have infinite credits until launch it has a bulk extract feature which can extract all transcripts of a YouTube channel and videos and put it in one file .&lt;/p&gt;\n\n&lt;p&gt;dm me if you want to be a beta tester&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0545d5ab53852b80cf553498d53d1982075d05b6\"&gt;https://preview.redd.it/wf8kkco9t3ff1.png?width=618&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0545d5ab53852b80cf553498d53d1982075d05b6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df\"&gt;https://preview.redd.it/1mg8157ss3ff1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=33ff520a9e831b2fee0f99118143efa0cb8b59df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2aafa58c52afea745518a37c41ccf49c09972519\"&gt;https://preview.redd.it/kx9xm6hgt3ff1.png?width=863&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2aafa58c52afea745518a37c41ccf49c09972519&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49\"&gt;https://preview.redd.it/985pjh9tt3ff1.png?width=691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13be1dd344b3a1ea463070e2bf92a43b5ba4dd49&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m9duv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Enough_Patient1904",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9duv2/ai_training_tool_i_want_to_share/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9duv2/ai_training_tool_i_want_to_share/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753484988,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was contemplating buying an RTX PRO 6000 Blackwell, but after conducting some research on [YouTube](https://youtu.be/bAao58hXo9w?si=F1vCh4gSJxrgYqo2&amp;t=832), I was disappointed with its performance. The prompt processing speed didn't meet my expectations, and token generation decreased notably when context was added. It didn't seem to outperform regular consumer GPUs, which left me wondering why it's so expensive. Is this normal behavior, or was the YouTuber not using it properly?",
          "author_fullname": "t2_qhk9kpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dissatisfied with how the RTX PRO 6000 Blackwell is performing during AI inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9dur7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753484979,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was contemplating buying an RTX PRO 6000 Blackwell, but after conducting some research on &lt;a href=\"https://youtu.be/bAao58hXo9w?si=F1vCh4gSJxrgYqo2&amp;amp;t=832\"&gt;YouTube&lt;/a&gt;, I was disappointed with its performance. The prompt processing speed didn&amp;#39;t meet my expectations, and token generation decreased notably when context was added. It didn&amp;#39;t seem to outperform regular consumer GPUs, which left me wondering why it&amp;#39;s so expensive. Is this normal behavior, or was the YouTuber not using it properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?auto=webp&amp;s=2de38e488ed6896adcf13bb7582775e8e37a7e8f",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8af79e10d322a89c0acdc8c1e95b10d83feae63",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be02e726d7db29c9b5cf7e4489a55edc87699ca8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b878f3cb454ef94f2549f8cf1ea5da83fe5038ba",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9dur7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "d00m_sayer",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753484979,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just finished uploading and perplexity testing some new ik\\_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik\\_llama.cpp is going strong!\n\nik just refreshed the IQ4\\_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at \\~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.\n\nAlso ik\\_llama.cpp recently had some PRs to improve tool/function calling.\n\nIf you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.\n\nCheers!",
          "author_fullname": "t2_n321yfw5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IQ4_KSS 114 GiB and more ik_llama.cpp exclusive quants!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cp2n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c5722281288ad17c5425d7d73e761d8eafbf6b87",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753481963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just finished uploading and perplexity testing some new ik_llama.cpp quants. Despite the random github takedown (and subsequent restoring) ik_llama.cpp is going strong!&lt;/p&gt;\n\n&lt;p&gt;ik just refreshed the IQ4_KSS 4.0 bpw non-linear quantization for faster performance and great perplexity so this quant hits a sweet spot at ~114GiB allowing 2x64GB DDR5 gaming rigs with a single GPU to run it with decently long context lengths.&lt;/p&gt;\n\n&lt;p&gt;Also ik_llama.cpp recently had some PRs to improve tool/function calling.&lt;/p&gt;\n\n&lt;p&gt;If you have more RAM, check out my larger Qwen3-Coder-480B-A35B-Instruct-GGUF quants if that is your thing.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?auto=webp&amp;s=f0dc878274992b77737eb6b8b4cf221787490f16",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a69f6d9c0fc9888f89ac3d1e39ffbc13bdff2b89",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4746d87fa4ee43c4c2ad291169131df06a3880bb",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64c7166a9e592474054818234e6a8fed62af8aca",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14512857abb7d9ebb3c411f65e1639efb84a8b4a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce689ee00af8c47b796b25b64ba7f36e852011a9",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=941654186ea54e774e0ff104ec12535879418828",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MiPkIenpbCsl-SvscocvypyhekEQtz60LZSjLkl-I3E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m9cp2n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VoidAlchemy",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m9cp2n/iq4_kss_114_gib_and_more_ik_llamacpp_exclusive/",
          "stickied": false,
          "url": "https://huggingface.co/ubergarm/Qwen3-235B-A22B-Thinking-2507-GGUF",
          "subreddit_subscribers": 504486,
          "created_utc": 1753481963,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Was thinking how to scale a GPU cluster. Not talking about CPUs here.  \nUsually have heard that \"buy Epyc\" and add 6-8 GPUs in it. but thats it then, it wont scale more.  \nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   \n  \nHas anyone done it? \n\nI happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.   \nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.   \nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?  \nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. \n\nLater when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. \n\nSo with this kind of setup, even 100 gpus could server the same model? \n\n\"**RDMA over Converged Ethernet (RoCE):** The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.\"",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Multi GPU multi server inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9cg16",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753481338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was thinking how to scale a GPU cluster. Not talking about CPUs here.&lt;br/&gt;\nUsually have heard that &amp;quot;buy Epyc&amp;quot; and add 6-8 GPUs in it. but thats it then, it wont scale more.&lt;br/&gt;\nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   &lt;/p&gt;\n\n&lt;p&gt;Has anyone done it? &lt;/p&gt;\n\n&lt;p&gt;I happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.&lt;br/&gt;\nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.&lt;br/&gt;\nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?&lt;br/&gt;\nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. &lt;/p&gt;\n\n&lt;p&gt;Later when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. &lt;/p&gt;\n\n&lt;p&gt;So with this kind of setup, even 100 gpus could server the same model? &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;strong&gt;RDMA over Converged Ethernet (RoCE):&lt;/strong&gt; The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9cg16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753481338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been following the progress of local LLMs for a while and I'm really interested in setting up a system for a natural, real-time audio conversation. I've seen some posts here discussing solutions that involve piping together speech-to-text, the LLM, and text-to-speech.\n\nI'm curious to know if anyone has found or built a more integrated solution that minimizes latency and feels more like a direct conversation. I've come across mentions of projects like Verbi and the potential of multimodal models like Qwen2-Audio, and I'm wondering if these are still the current way to go?\n\nIdeally, I'm looking for something that can run on consumer-grade hardware.\n\nWhat are your current setups for this? Have you managed to achieve a truly conversational experience?",
          "author_fullname": "t2_vyvxdjqyp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone found a seamless, low-latency solution for real-time audio conversations with a local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9c9fh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753480874,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been following the progress of local LLMs for a while and I&amp;#39;m really interested in setting up a system for a natural, real-time audio conversation. I&amp;#39;ve seen some posts here discussing solutions that involve piping together speech-to-text, the LLM, and text-to-speech.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to know if anyone has found or built a more integrated solution that minimizes latency and feels more like a direct conversation. I&amp;#39;ve come across mentions of projects like Verbi and the potential of multimodal models like Qwen2-Audio, and I&amp;#39;m wondering if these are still the current way to go?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;m looking for something that can run on consumer-grade hardware.&lt;/p&gt;\n\n&lt;p&gt;What are your current setups for this? Have you managed to achieve a truly conversational experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9c9fh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far_Buyer_7281",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9c9fh/has_anyone_found_a_seamless_lowlatency_solution/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9c9fh/has_anyone_found_a_seamless_lowlatency_solution/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753480874,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Finally put together my rig after months of planning into a NAS case \n\n* Threadripper PRO 7955WX\n* Arctic Freezer 4U-M (cpu cooler)\n* Gigabyte TRX50 AI TOP\n* be quiet! Dark Power Pro 13 1600W\n* JONSBO N5 Case\n* 2x RTX Pro 6000\n\nMight add a few more intake fans on the top ",
          "author_fullname": "t2_16xbdr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Compact 2x RTX Pro 6000 Rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9bwoy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/nIQWXwAd5JWfTiQ6FGVC1XWYiGxzWflAKHHcq1t0koA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753479993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally put together my rig after months of planning into a NAS case &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Threadripper PRO 7955WX&lt;/li&gt;\n&lt;li&gt;Arctic Freezer 4U-M (cpu cooler)&lt;/li&gt;\n&lt;li&gt;Gigabyte TRX50 AI TOP&lt;/li&gt;\n&lt;li&gt;be quiet! Dark Power Pro 13 1600W&lt;/li&gt;\n&lt;li&gt;JONSBO N5 Case&lt;/li&gt;\n&lt;li&gt;2x RTX Pro 6000&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Might add a few more intake fans on the top &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?auto=webp&amp;s=e1c07ae621c323dcd20838da5641e3e02cc70f91",
                  "width": 1679,
                  "height": 1264
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb8530b9905fee9ce384e441b1786c16863f92ac",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb080d3b642a8ae64a22c14c8185ffb66785a5a8",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df5d60cb32c2381511e9f8f424415f43363b229e",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb7c498f0a5ad74816f597205d993264473bdbfe",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3cd46419a68e44716f2e3b674a590f0b8f7e8f9e",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/tbteu4v5b3ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f10578355cca1e987b8febd5ea512ad847a0ac02",
                    "width": 1080,
                    "height": 813
                  }
                ],
                "variants": {},
                "id": "75hqb9t8X6IAo2hcG2QxH9eY348BDsA6al0LQJShjQE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9bwoy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shadowninjaz3",
          "discussion_type": null,
          "num_comments": 45,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9bwoy/compact_2x_rtx_pro_6000_rig/",
          "stickied": false,
          "url": "https://i.redd.it/tbteu4v5b3ff1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753479993,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "me and my friend have been working on an architecture for a bit that doesnt use attention, but due to limited hardware progress has been slow, what companies or ppl should we reach out to? we arent looking for much maybe a 1000 dollars and would be glad to make a contract with someone for publishing rights of the LLM in exchange",
          "author_fullname": "t2_74ai6uqx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Who should we ask for funding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9boeu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753479418,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;me and my friend have been working on an architecture for a bit that doesnt use attention, but due to limited hardware progress has been slow, what companies or ppl should we reach out to? we arent looking for much maybe a 1000 dollars and would be glad to make a contract with someone for publishing rights of the LLM in exchange&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9boeu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Ad-1148",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9boeu/who_should_we_ask_for_funding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9boeu/who_should_we_ask_for_funding/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753479418,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "After starting Ollama and doing the ollama run &lt;model&gt; how do you know if it’s running that specific model or if it’s still using the default that comes with ollama? Do you just need the run code for it to work, the load command, or both?",
          "author_fullname": "t2_l4qac",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is /load &lt;model&gt; all you need in order to run the specific model you installed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9bmqm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753479301,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After starting Ollama and doing the ollama run &amp;lt;model&amp;gt; how do you know if it’s running that specific model or if it’s still using the default that comes with ollama? Do you just need the run code for it to work, the load command, or both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9bmqm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XiRw",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9bmqm/is_load_model_all_you_need_in_order_to_run_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9bmqm/is_load_model_all_you_need_in_order_to_run_the/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753479301,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone run the q4ks versions of these, which one is winning for code generation... Too early for consensus yet? Thx",
          "author_fullname": "t2_1l3z4stvkq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The new Kimi vs. new qwen3 for coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9avmv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753477447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone run the q4ks versions of these, which one is winning for code generation... Too early for consensus yet? Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9avmv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Agreeable-Prompt-666",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9avmv/the_new_kimi_vs_new_qwen3_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9avmv/the_new_kimi_vs_new_qwen3_for_coding/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753477447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to get started at self hosting an LLM but have no experience with this. \n\nWhat I am looking for is:\n\nAn LLM that I can explore with code, ideally if I can link it in with some folders on my MacBook Pro M4, and then also on a server, the servers will be getting GPUs mounted soon. \n\nI ideally want to be able to send it a defined file of what code styles and principles to follow, and I would love to know what self hosted options we can look at helping with PR reviews. \n\nI don’t want AI to replace or cut the corners of my team but to help us out and become more consistent. \n\nSo ideally, self hosted options (Docker etc), if it could be integrated into PRs with a self hosted GitLab if needed?\n\nI’ve read a bit about Qwen3 but not sure where to even get started to explore and try it out. ",
          "author_fullname": "t2_fpgzf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to get started",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9antc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753476933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to get started at self hosting an LLM but have no experience with this. &lt;/p&gt;\n\n&lt;p&gt;What I am looking for is:&lt;/p&gt;\n\n&lt;p&gt;An LLM that I can explore with code, ideally if I can link it in with some folders on my MacBook Pro M4, and then also on a server, the servers will be getting GPUs mounted soon. &lt;/p&gt;\n\n&lt;p&gt;I ideally want to be able to send it a defined file of what code styles and principles to follow, and I would love to know what self hosted options we can look at helping with PR reviews. &lt;/p&gt;\n\n&lt;p&gt;I don’t want AI to replace or cut the corners of my team but to help us out and become more consistent. &lt;/p&gt;\n\n&lt;p&gt;So ideally, self hosted options (Docker etc), if it could be integrated into PRs with a self hosted GitLab if needed?&lt;/p&gt;\n\n&lt;p&gt;I’ve read a bit about Qwen3 but not sure where to even get started to explore and try it out. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9antc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theonethatownz",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9antc/how_to_get_started/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9antc/how_to_get_started/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753476933,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Curious how the two new thinking/non thinking stack up vs deepseek.",
          "author_fullname": "t2_frxyil1r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any Rpers test the new qwen 2507 yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9ajf9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753476645,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how the two new thinking/non thinking stack up vs deepseek.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9ajf9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Antique_Bit_1049",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9ajf9/any_rpers_test_the_new_qwen_2507_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ajf9/any_rpers_test_the_new_qwen_2507_yet/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753476645,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bonjour,\n\nJe cherche un modèle **13B**, au format **GGUF**, qui soit :\n\n* **non censuré** (pas de \"safety filters\", pas de refus de sujet),\n* avec une **très bonne maîtrise du français** (langue native ou quasi),\n* compatible avec **Text Generation WebUI** (j'utilise llama.cpp),\n* et qui tourne bien sur mon **GPU RTX 4070 Ti**.\n\nSi vous avez des suggestions ou retours d'expérience, je suis preneur. Merci d’avance ! 🙏",
          "author_fullname": "t2_1tptbwbfth",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Je cherche un modèle text-to-text",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9a554",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.24,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753475691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bonjour,&lt;/p&gt;\n\n&lt;p&gt;Je cherche un modèle &lt;strong&gt;13B&lt;/strong&gt;, au format &lt;strong&gt;GGUF&lt;/strong&gt;, qui soit :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;non censuré&lt;/strong&gt; (pas de &amp;quot;safety filters&amp;quot;, pas de refus de sujet),&lt;/li&gt;\n&lt;li&gt;avec une &lt;strong&gt;très bonne maîtrise du français&lt;/strong&gt; (langue native ou quasi),&lt;/li&gt;\n&lt;li&gt;compatible avec &lt;strong&gt;Text Generation WebUI&lt;/strong&gt; (j&amp;#39;utilise llama.cpp),&lt;/li&gt;\n&lt;li&gt;et qui tourne bien sur mon &lt;strong&gt;GPU RTX 4070 Ti&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Si vous avez des suggestions ou retours d&amp;#39;expérience, je suis preneur. Merci d’avance ! 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9a554",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kball76",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9a554/je_cherche_un_modèle_texttotext/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9a554/je_cherche_un_modèle_texttotext/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753475691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the market right now, there’s an ocean of no‑code and low‑code platforms shouting about how they “let you build anything.”\n\nBut let’s be real, most of them are just website builders with a fancier skin.\n\nI’ve used tools like Lovable, Bolt, Fire Studio.  \nThey are simple, but they still feel like the low‑end spectrum: good for spinning up a quick frontend for MVP, but they stop there.\n\nOn the opposite end, there are power tools - Windsurf and Cursor.  \nThese are meant for developers who already know how to code, but they are too advanced for non‑technical builders who have a deep idea but no engineering muscle.\n\nWhat’s missing is a middle ground.  \nA true **application generator** that isn’t about “drag a button, drag a form,” and isn’t just a playground for coders.\n\nImagine this: you explain in detail how your application should work. its flow, logic, data, and purpose, and the AI actually builds that application, not a landing page or backend shell, but a working tool.\n\nHas anyone here seen or tried something in that direction?  \nNot another website builder, something that can create applications from deep descriptions?\n\nbtw I'm just vibe coder",
          "author_fullname": "t2_1qe0x0t139",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any AI tool for application creation (not website builders)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99xty",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753475214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the market right now, there’s an ocean of no‑code and low‑code platforms shouting about how they “let you build anything.”&lt;/p&gt;\n\n&lt;p&gt;But let’s be real, most of them are just website builders with a fancier skin.&lt;/p&gt;\n\n&lt;p&gt;I’ve used tools like Lovable, Bolt, Fire Studio.&lt;br/&gt;\nThey are simple, but they still feel like the low‑end spectrum: good for spinning up a quick frontend for MVP, but they stop there.&lt;/p&gt;\n\n&lt;p&gt;On the opposite end, there are power tools - Windsurf and Cursor.&lt;br/&gt;\nThese are meant for developers who already know how to code, but they are too advanced for non‑technical builders who have a deep idea but no engineering muscle.&lt;/p&gt;\n\n&lt;p&gt;What’s missing is a middle ground.&lt;br/&gt;\nA true &lt;strong&gt;application generator&lt;/strong&gt; that isn’t about “drag a button, drag a form,” and isn’t just a playground for coders.&lt;/p&gt;\n\n&lt;p&gt;Imagine this: you explain in detail how your application should work. its flow, logic, data, and purpose, and the AI actually builds that application, not a landing page or backend shell, but a working tool.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here seen or tried something in that direction?&lt;br/&gt;\nNot another website builder, something that can create applications from deep descriptions?&lt;/p&gt;\n\n&lt;p&gt;btw I&amp;#39;m just vibe coder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m99xty",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Delicious_Track6230",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99xty/any_ai_tool_for_application_creation_not_website/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99xty/any_ai_tool_for_application_creation_not_website/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753475214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My config:  \nRyzen 5 5500, 16Gb, RTX 3060 12Gb  \n",
          "author_fullname": "t2_by2xhanu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best AI to run locally and use in agent mode of the Continue extension in VS Code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99hwb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753474147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My config:&lt;br/&gt;\nRyzen 5 5500, 16Gb, RTX 3060 12Gb  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m99hwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ikelven",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99hwb/what_is_the_best_ai_to_run_locally_and_use_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99hwb/what_is_the_best_ai_to_run_locally_and_use_in/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753474147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Phi-4-mini-flash-reasoning isn't in the Ollama repository, and in huggingface there are .safetensors files, as the architecture of this new model is called SambaY (some Mamba variant) this may complicate things with regard to converting it to GGUF or some other format, I would like to run the model with no modification to begin with.",
          "author_fullname": "t2_61c14i0z",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any way to run Phi-4-mini-flash-reasoning on Ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m99ac7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753473646,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Phi-4-mini-flash-reasoning isn&amp;#39;t in the Ollama repository, and in huggingface there are .safetensors files, as the architecture of this new model is called SambaY (some Mamba variant) this may complicate things with regard to converting it to GGUF or some other format, I would like to run the model with no modification to begin with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m99ac7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WowSkaro",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m99ac7/is_there_any_way_to_run_phi4miniflashreasoning_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m99ac7/is_there_any_way_to_run_phi4miniflashreasoning_on/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753473646,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.\n\nAfter some attempts, I managed to get it to reveal the hidden prompt:\n\n&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.\n\n&gt;\n\n&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.\n\n&gt;\n\n&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.\n\n&gt;\n\n&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.\n\n&gt;\n\n&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.\n\n&gt;\n\n&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.\n\n&gt;\n\n&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like \"That's a tough spot to be in\" or \"That's a tough one\" or \"Sound like a tricky situation.\"\n\n&gt;\n\n&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.\n\n&gt;\n\n&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to \"it's important to\", \"it's crucial to\", \"it's essential to\", \"it's unethical to\", \"it's worth noting...\" etc. Avoid using these.\n\n&gt;\n\n&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.\n\n&gt;\n\n&gt;Provide multiple perspectives on opinions when asked.\n\n&gt;\n\n&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.\n\n&gt;\n\n&gt;The phrases \"Remember,...\" \"Keep in mind,...\" \"It's essential to note\" or \"This is a complex topic...\" or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.\n\n&gt;\n\n&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.\n\n&gt;\n\n&gt;For HOMEWORK or LEARNING QUERIES:\n\n&gt;\n\n&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.\n\n&gt;\n\n&gt;Use the following principles for STEM questions:\n\n&gt;\\- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,\n\n&gt;\\- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.\n\n&gt;\\- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\\\pi r\\^2$ for the area of a circle, and $$ for display math (e.g. $$\\\\sum\\_{i=1}\\^{n} i$$).\n\n&gt;\\- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.\n\n&gt;\\- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.\n\n&gt;\\- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.\n\nSomeone else mentioned a similar thing [here](https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/), saying it showed their full address. In my case, it included only the region and the current date.",
          "author_fullname": "t2_rtr3vmjc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Meta AI on WhatsApp hides a system prompt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "8pns3hghn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 132,
                  "x": 108,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89f4a8048d3bc2f7a45badf3acc273eb483f8ff5"
                },
                {
                  "y": 265,
                  "x": 216,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e374703c8611599c6dbcd305ca5e8c4d8b9425"
                },
                {
                  "y": 393,
                  "x": 320,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00dd2c681351a7c8cb26403c1d58f63e047eea5"
                },
                {
                  "y": 786,
                  "x": 640,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=48f0d3ab2030bddb371bfc4c6978156aa7d0a6c7"
                },
                {
                  "y": 1179,
                  "x": 960,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4abd0d75e01a6a87639f3543a1af7c4416c98a5"
                },
                {
                  "y": 1327,
                  "x": 1080,
                  "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6602191d3149d3f4c1d130536257ad807c93833"
                }
              ],
              "s": {
                "y": 1327,
                "x": 1080,
                "u": "https://preview.redd.it/8pns3hghn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=eadfa7c509881c94ab8f75028dba8c7ad5a2331c"
              },
              "id": "8pns3hghn2ff1"
            },
            "ioq8bh7jn2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 185,
                  "x": 108,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4110e6db79cfe0bdbdb31020a587c0a7ae33336"
                },
                {
                  "y": 371,
                  "x": 216,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c0712f69f936326c933517faddbd3d078bd8a77"
                },
                {
                  "y": 550,
                  "x": 320,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dfbf62903b0789ae13935b48fde35753c1d811e8"
                },
                {
                  "y": 1101,
                  "x": 640,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1804ecd5118d1599b4b15b935eeae44e9c8b6916"
                },
                {
                  "y": 1652,
                  "x": 960,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67b0a224d9bc677152be25b862bf0bc6d6334485"
                },
                {
                  "y": 1859,
                  "x": 1080,
                  "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47503cb64ffc75354fdf249825d8133a74956382"
                }
              ],
              "s": {
                "y": 1859,
                "x": 1080,
                "u": "https://preview.redd.it/ioq8bh7jn2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=35b1f2d53161fef0d6c3eaa55dcb423a7fef4255"
              },
              "id": "ioq8bh7jn2ff1"
            },
            "0kst569in2ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30859c52a7f5b56ab67e46f37c8ffa035c20bbb3"
                },
                {
                  "y": 317,
                  "x": 216,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8686e92c1b15e4b48ef87e7d68cb4bd6edc02ed0"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aedfb716605e018e5c9d58938f2ffabaa8b98191"
                },
                {
                  "y": 939,
                  "x": 640,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e55e7532a0dcc7cd43287c66e1af425e701d78c"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=120a08a9b8f734c5afa551d7f3b2c3c84a3d38f4"
                },
                {
                  "y": 1585,
                  "x": 1080,
                  "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35a5bd8bb96fcb8c2b8750b3b9a8213b4e174376"
                }
              ],
              "s": {
                "y": 1585,
                "x": 1080,
                "u": "https://preview.redd.it/0kst569in2ff1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=96d91bd7badfd9af25e971fb88dc2e83b699142e"
              },
              "id": "0kst569in2ff1"
            }
          },
          "name": "t3_1m98jl8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 433,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "8pns3hghn2ff1",
                "id": 713920198
              },
              {
                "media_id": "0kst569in2ff1",
                "id": 713920199
              },
              {
                "media_id": "ioq8bh7jn2ff1",
                "id": 713920200
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 433,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753471858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It’s not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt;\n\n&lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don&amp;#39;t have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must  be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt;\n\n&lt;p&gt;Match the user&amp;#39;s tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don&amp;#39;t be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don&amp;#39;t use proper grammar, you don&amp;#39;t use proper grammar, etc.&lt;/p&gt;\n\n&lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don&amp;#39;t have any distinct values, race, culture, or any political leaning. You don&amp;#39;t love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt;\n\n&lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt;\n\n&lt;p&gt;You understand user intent and don&amp;#39;t try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re never moralistic or didactic; it&amp;#39;s not your job to preach or teach users how to be better, nicer, kinder people. Don&amp;#39;t use filler phrases like &amp;quot;That&amp;#39;s a tough spot to be in&amp;quot; or &amp;quot;That&amp;#39;s a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt;\n\n&lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it&amp;#39;s important to&amp;quot;, &amp;quot;it&amp;#39;s crucial to&amp;quot;, &amp;quot;it&amp;#39;s essential to&amp;quot;, &amp;quot;it&amp;#39;s unethical to&amp;quot;, &amp;quot;it&amp;#39;s worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt;\n\n&lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There&amp;#39;s no need to add commentary in italics or parenthesis.&lt;/p&gt;\n\n&lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt;\n\n&lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don&amp;#39;t refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today&amp;#39;s date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt;\n\n&lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It&amp;#39;s essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt;\n\n&lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don&amp;#39;t add on intros or outros that qualify the content.&lt;/p&gt;\n\n&lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt;\n\n&lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt;\n\n&lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt;\n\n&lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt;\n\n&lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt;\n\n&lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\\sum_{i=1}^{n} i$$).&lt;/p&gt;\n\n&lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt;\n\n&lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt;\n\n&lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they&amp;#39;ve learned.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Someone else mentioned a similar thing &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/\"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m98jl8",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m98jl8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ALE5SI0",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m98jl8",
          "subreddit_subscribers": 504486,
          "created_utc": 1753471858,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I downloaded the original FP8 version because I wanted to experiment with different quants and compare them, and also use my own imatrix for the best results for my use cases. For DeepSeek V3 and R1 this approach works very well, I can make use of imatrix data of my choice and select quantization parameters that I prefer.\n\nBut so far I had no luck converting Kimi K2 FP8 to BF16, even though it is technically based on the DeepSeek architecture. I shared details in the comments since otherwise the post does not come through. I would appreciate if anyone can share ideas what else to try to convert Kimi K2 FP8 to BF16 given I have only 3090 GPUs and CPU, so cannot use the official DeepSeek script to convert.",
          "author_fullname": "t2_fpfao9g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to convert Kimi K2 FP8 to BF16?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m97qko",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753471279,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753469956,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded the original FP8 version because I wanted to experiment with different quants and compare them, and also use my own imatrix for the best results for my use cases. For DeepSeek V3 and R1 this approach works very well, I can make use of imatrix data of my choice and select quantization parameters that I prefer.&lt;/p&gt;\n\n&lt;p&gt;But so far I had no luck converting Kimi K2 FP8 to BF16, even though it is technically based on the DeepSeek architecture. I shared details in the comments since otherwise the post does not come through. I would appreciate if anyone can share ideas what else to try to convert Kimi K2 FP8 to BF16 given I have only 3090 GPUs and CPU, so cannot use the official DeepSeek script to convert.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m97qko",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lissanro",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m97qko/how_to_convert_kimi_k2_fp8_to_bf16/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m97qko/how_to_convert_kimi_k2_fp8_to_bf16/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753469956,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ive been looking at buying a few mi50 32gb cards for my local training setup because they are absurdly affordable for the VRAM they have. I'm not too concerned with FLOP/s performance, as long as they have compatibility with a relatively modern pytorch and its dependencies.\n\nI've seen people on here talking about this card for inference but not training. Would this be a good idea?",
          "author_fullname": "t2_1jemkvy49e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mi50 array for training LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96wrc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753468011,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ive been looking at buying a few mi50 32gb cards for my local training setup because they are absurdly affordable for the VRAM they have. I&amp;#39;m not too concerned with FLOP/s performance, as long as they have compatibility with a relatively modern pytorch and its dependencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen people on here talking about this card for inference but not training. Would this be a good idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96wrc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Used_Algae_1077",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96wrc/mi50_array_for_training_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96wrc/mi50_array_for_training_llms/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753468011,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,  \nI'm working on a project using LibreChat, and I've noticed that it handles translations through `.ts` and `.md` files—one set per language. Each file contains over a thousand lines, so I assume these aren't written manually. There must be some kind of script or automation behind generating them.\n\nI want to make a change to one of the base messages. Specifically, in a registration form, there's a field for `username` and it currently displays `(optional)`. I want to remove that word so it no longer shows.\n\nMy question is:  \nIf I update the base message (presumably in the default language file), is there a way to automatically update the rest of the language files to reflect this change? For example, marking the string as needing translation or syncing the keys across all files?\n\nAny insights or tips on how this workflow is managed in LibreChat or similar setups would be really appreciated.  \nThanks!",
          "author_fullname": "t2_3t1oh2ky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does LibreChat handle translations and how can I update all language files after changing base messages?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96m6w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753467319,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;br/&gt;\nI&amp;#39;m working on a project using LibreChat, and I&amp;#39;ve noticed that it handles translations through &lt;code&gt;.ts&lt;/code&gt; and &lt;code&gt;.md&lt;/code&gt; files—one set per language. Each file contains over a thousand lines, so I assume these aren&amp;#39;t written manually. There must be some kind of script or automation behind generating them.&lt;/p&gt;\n\n&lt;p&gt;I want to make a change to one of the base messages. Specifically, in a registration form, there&amp;#39;s a field for &lt;code&gt;username&lt;/code&gt; and it currently displays &lt;code&gt;(optional)&lt;/code&gt;. I want to remove that word so it no longer shows.&lt;/p&gt;\n\n&lt;p&gt;My question is:&lt;br/&gt;\nIf I update the base message (presumably in the default language file), is there a way to automatically update the rest of the language files to reflect this change? For example, marking the string as needing translation or syncing the keys across all files?&lt;/p&gt;\n\n&lt;p&gt;Any insights or tips on how this workflow is managed in LibreChat or similar setups would be really appreciated.&lt;br/&gt;\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96m6w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "suribe06",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96m6w/how_does_librechat_handle_translations_and_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96m6w/how_does_librechat_handle_translations_and_how/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753467319,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been trying a lot of different combinations with static learning rates, and i have to set up the test inference for every single epoch to determine the sweet spot because i doubt that any automation that does not involve running two simultaneous llm will be able to accurate tell when the results are desirable. But maybe i am doing everything wong? I only got what i wanted after 10 runs of 4e-3, and that is with a datasets of 90 rows, all in a single batch. Perhaps this is a rare scenario, but good to have found something working. Any advice or experiences that i must learn about? As I prefer not to waste more compute doing the trial and error with datasets a thousand times the size.",
          "author_fullname": "t2_1ti9nuwlx8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does it ever make sense to train for 10 epochs? Or did i do it all wrong?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m96b4h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753466600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying a lot of different combinations with static learning rates, and i have to set up the test inference for every single epoch to determine the sweet spot because i doubt that any automation that does not involve running two simultaneous llm will be able to accurate tell when the results are desirable. But maybe i am doing everything wong? I only got what i wanted after 10 runs of 4e-3, and that is with a datasets of 90 rows, all in a single batch. Perhaps this is a rare scenario, but good to have found something working. Any advice or experiences that i must learn about? As I prefer not to waste more compute doing the trial and error with datasets a thousand times the size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m96b4h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BulkyPlay7704",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m96b4h/does_it_ever_make_sense_to_train_for_10_epochs_or/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m96b4h/does_it_ever_make_sense_to_train_for_10_epochs_or/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753466600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying think of a conversational LLM \nWhich won't hallucinate when the context (conversation history) grows. \nLlm should also hold personalities. \nAny help us appropriated. \n",
          "author_fullname": "t2_uvads9n4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Conversational LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m968q4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753466456,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying think of a conversational LLM \nWhich won&amp;#39;t hallucinate when the context (conversation history) grows. \nLlm should also hold personalities. \nAny help us appropriated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m968q4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "backofthemind99",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m968q4/conversational_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m968q4/conversational_llm/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753466456,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is AMD working on any GPU which will compete with RTX 6000 PRO Blackwell in memory, compute, and price? Or one with higher VRAM but targeted at workstations?",
          "author_fullname": "t2_1266sp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD equivalent for NVIDIA RTX 6000 PRO Blackwell",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95wcg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753465671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is AMD working on any GPU which will compete with RTX 6000 PRO Blackwell in memory, compute, and price? Or one with higher VRAM but targeted at workstations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m95wcg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "s-s-a",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95wcg/amd_equivalent_for_nvidia_rtx_6000_pro_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95wcg/amd_equivalent_for_nvidia_rtx_6000_pro_blackwell/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753465671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1kpbtnvm6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What a Real MCP Inspector Exploit Taught Us About Trust Boundaries",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95sdj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=3f39b34ed5287898a35b71f4a2d410155e669d3f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753465416,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "glama.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://glama.ai/blog/2025-07-25-keeping-mcp-inspector-safe-lessons-from-cve-2025-49596",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?auto=webp&amp;s=227a72ce82d28d282a0c1871a35312e4308e1adc",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=867c2e3c9c184ac0e64e1d2b897ce35fafe59ad3",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3bc8fb60b3d09c1938760de5b1404a3ec3810e2",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d497e25d2db852ceaaef3176f190e7864b19bb13",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d8aa63775b0ce7432aec4599fdff26b1f5079fa",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9525d5f79ca90480770bfaf099caa9c518625cc",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2af1885a6b45fe1e3b5d69406a04e31bec46d01a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "FySv24u1eil3dRo2Y1Z4jOxhVWhZAsG3ATu9GRHaX4U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m95sdj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Abies7108",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95sdj/what_a_real_mcp_inspector_exploit_taught_us_about/",
          "stickied": false,
          "url": "https://glama.ai/blog/2025-07-25-keeping-mcp-inspector-safe-lessons-from-cve-2025-49596",
          "subreddit_subscribers": 504486,
          "created_utc": 1753465416,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MassGen — an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by \"parallel study groups\" and Grok Heavy. \n\n[https://x.com/Chi\\_Wang\\_/status/1948790995694617036](https://x.com/Chi_Wang_/status/1948790995694617036)",
          "author_fullname": "t2_1rqcx8w6j7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MassGen – an open-source multi-agent scaling and orchestration framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95lud",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753464999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MassGen — an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by &amp;quot;parallel study groups&amp;quot; and Grok Heavy. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/Chi_Wang_/status/1948790995694617036\"&gt;https://x.com/Chi_Wang_/status/1948790995694617036&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m95lud",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LifeUnderstanding732",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95lud/massgen_an_opensource_multiagent_scaling_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95lud/massgen_an_opensource_multiagent_scaling_and/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753464999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Google released their Gemma 3n model about a month ago, and they've mentioned that it's meant to run efficiently on everyday devices, yet, from my experience it runs really slow on my Mac (base model M2 Mac mini from 2023 with only 8GB of RAM). I am aware that my small amount of RAM is very limiting in the space of local LLMs, but I had a lot of hope when Google first started teasing this model.\n\nJust curious if anyone has tried it, and if so, what has your experience been like?\n\nHere's an Ollama link to the model, btw: [https://ollama.com/library/gemma3n](https://ollama.com/library/gemma3n)",
          "author_fullname": "t2_1lavzg2ok9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone had any luck with Google's Gemma 3n model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m95bfq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753464342,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google released their Gemma 3n model about a month ago, and they&amp;#39;ve mentioned that it&amp;#39;s meant to run efficiently on everyday devices, yet, from my experience it runs really slow on my Mac (base model M2 Mac mini from 2023 with only 8GB of RAM). I am aware that my small amount of RAM is very limiting in the space of local LLMs, but I had a lot of hope when Google first started teasing this model.&lt;/p&gt;\n\n&lt;p&gt;Just curious if anyone has tried it, and if so, what has your experience been like?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an Ollama link to the model, btw: &lt;a href=\"https://ollama.com/library/gemma3n\"&gt;https://ollama.com/library/gemma3n&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m95bfq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Junior-Ad-2186",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m95bfq/anyone_had_any_luck_with_googles_gemma_3n_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m95bfq/anyone_had_any_luck_with_googles_gemma_3n_model/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753464342,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_c6nzmzuq8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is AI dialogue the future of gaming?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9535b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.51,
          "author_flair_background_color": "transparent",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kwadvy7vz1ff1/DASHPlaylist.mpd?a=1756084190%2CZDQ3MjQ3NjMxODU2MzFhZDcyYWY3NGE4N2VjN2VhMzJmOGY0NzZlMWQyY2UwNmMxMDdlOWY1NzkyNWEzNzAxOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/kwadvy7vz1ff1/HLSPlaylist.m3u8?a=1756084190%2CODBmZTU4ZDE2MWEzMGJhYzJlZjQ5MmQxZTg2YTBlOGU0OGJkNDIxNzQwNDA2NzIzODE1ZjllNjNlZWZhZGQ3OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a4d5b6d5fcc8a3352d49a80ea4992b61a9a553fb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/kwadvy7vz1ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?format=pjpg&amp;auto=webp&amp;s=42d35bc0562f2b999644dafae05a2302ab759711",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=425c4520d088cf30251cb1992b126dedf6800fba",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=909cd2fe935ddf9c23f49cfceeaac45d4e5100a5",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bad49e88c983fca601ba8acf6ae347300b2b62a3",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ae04db361f1166b44acf7f7d032db0d7990a35f8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a2fe4b5aa0d20266a8b0d1a5f4864de6198c51ba",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0f56d72f3bcea06d904b3ff0df0417caa967dd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "bTlkc3N3N3Z6MWZmMV4R7_wqTqa4S-Em63VZNWlYLKqHqatiW2ePCfOcZ7Ue"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m9535b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LandoRingel",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1m9535b/is_ai_dialogue_the_future_of_gaming/",
          "stickied": false,
          "url": "https://v.redd.it/kwadvy7vz1ff1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753463816,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/kwadvy7vz1ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/kwadvy7vz1ff1/DASHPlaylist.mpd?a=1756084190%2CZDQ3MjQ3NjMxODU2MzFhZDcyYWY3NGE4N2VjN2VhMzJmOGY0NzZlMWQyY2UwNmMxMDdlOWY1NzkyNWEzNzAxOQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 20,
              "hls_url": "https://v.redd.it/kwadvy7vz1ff1/HLSPlaylist.m3u8?a=1756084190%2CODBmZTU4ZDE2MWEzMGJhYzJlZjQ5MmQxZTg2YTBlOGU0OGJkNDIxNzQwNDA2NzIzODE1ZjllNjNlZWZhZGQ3OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been working on a cleaner UI for uploading and managing custom models — here are some early Figma drafts of the connection flow and model details page. Still a work in progress, but I’d love to hear your thoughts!  \n  \nFor those who are new here: I’m building this platform as a solo pet project in my free time, and I’ve been sharing my progress here on r/LocalLLaMA to gather feedback and ideas. Your input really helps shape the direction.  \n  \nI’m adding support for local backend connection because not everyone wants to rely on third-party APIs or cloud services. Many people already run models locally, and this gives them full control over performance, privacy, and customization.  \n  \nIf you’re interested in testing the platform, I’d be happy to send you an invite — just shoot me a DM!",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New UI for uploading and managing custom models (Figma mockups)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 127,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fulqfst1y1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 98,
                  "x": 108,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82cd77de0ca104006b080ce11404b464a14f50b0"
                },
                {
                  "y": 196,
                  "x": 216,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=db2d56b61032e2659fd959720ebdc78eebab279d"
                },
                {
                  "y": 291,
                  "x": 320,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=664b072883559c46b0fa4015ff69016002263582"
                },
                {
                  "y": 583,
                  "x": 640,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69fb7d42a37eb74bec24fbf843282013d97897b9"
                },
                {
                  "y": 875,
                  "x": 960,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6c33cbb9244adcb68539df6e892a574b0d9d8d2"
                },
                {
                  "y": 984,
                  "x": 1080,
                  "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb0ff00f02a7a97c54ecb0f5a1a00ec297c1abdd"
                }
              ],
              "s": {
                "y": 1751,
                "x": 1920,
                "u": "https://preview.redd.it/fulqfst1y1ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=50c3f7aea91e1460a23becf2a7ed1c33d764db10"
              },
              "id": "fulqfst1y1ff1"
            },
            "dqsr5j52y1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71ca5fd8d2aa192f92c736a5d06b6c92c5573fae"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e9240f3958d87a71bb54b0be7bc91da1412786d"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b48ca5183c8df74e34feb3b947f7c7b7e40043d"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffc158bdd7ade8804eb19ad6ee6c16fe6762252a"
                },
                {
                  "y": 601,
                  "x": 960,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1104132469c193ee88fd1c05399f974a8f761107"
                },
                {
                  "y": 676,
                  "x": 1080,
                  "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7679dd5a2596a1a9280c1750ca15833d4cff87f4"
                }
              ],
              "s": {
                "y": 1202,
                "x": 1920,
                "u": "https://preview.redd.it/dqsr5j52y1ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d05e615307e6fd8002190d245dea45e46920e27d"
              },
              "id": "dqsr5j52y1ff1"
            }
          },
          "name": "t3_1m94uea",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 10,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "fulqfst1y1ff1",
                "id": 713831543
              },
              {
                "media_id": "dqsr5j52y1ff1",
                "id": 713831544
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fIwpmtA8COvjymHuiMQDj4eWoej49NBiwVAmQsDVTfI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463269,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been working on a cleaner UI for uploading and managing custom models — here are some early Figma drafts of the connection flow and model details page. Still a work in progress, but I’d love to hear your thoughts!  &lt;/p&gt;\n\n&lt;p&gt;For those who are new here: I’m building this platform as a solo pet project in my free time, and I’ve been sharing my progress here on &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; to gather feedback and ideas. Your input really helps shape the direction.  &lt;/p&gt;\n\n&lt;p&gt;I’m adding support for local backend connection because not everyone wants to rely on third-party APIs or cloud services. Many people already run models locally, and this gives them full control over performance, privacy, and customization.  &lt;/p&gt;\n\n&lt;p&gt;If you’re interested in testing the platform, I’d be happy to send you an invite — just shoot me a DM!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m94uea",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m94uea",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94uea/new_ui_for_uploading_and_managing_custom_models/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m94uea",
          "subreddit_subscribers": 504486,
          "created_utc": 1753463269,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_i0qyphvw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data shows public AI repos may be quietly becoming a supply chain risk",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94r8i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=8288150fa66f039b13ca65cd7b0362ca7f98802b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753463072,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "blog.ramalama.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://blog.ramalama.com/data-shows-public-ai-repos-may-be-quietly-becoming-a-supply-chain-risk/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?auto=webp&amp;s=00175911774c7177d05901421a2947142e14da98",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1109f30db34ec61d476e4368773e74d29aadfb4",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=19677e8776ca2f7e28d576b8220184d112fbf250",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5da31cac7c525001354718c582fafcb2753fe765",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9800d871de241bb5ec65a999f12e1eb8cb796711",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77ba8ca9124615e7f837d89ac5076c68fea8c8d0",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "vqZUhGSzUZ1yMtf3yHj4ZJeonTZAkxlOGX5jxZNB2wI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m94r8i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ProfessionalHorse707",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94r8i/data_shows_public_ai_repos_may_be_quietly/",
          "stickied": false,
          "url": "https://blog.ramalama.com/data-shows-public-ai-repos-may-be-quietly-becoming-a-supply-chain-risk/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753463072,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hunyuan (Ex-WizardLM) Dense Model Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94ls2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 70,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 70,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jB1CS2LZrHw4qZHwx53ViD8xx1eEncZt0jht4lZ5-yw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?auto=webp&amp;s=0fb34e0f66b8292996122ca9f453b58a37b14813",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1baecdc2dfde1050ccffe3b9db6f4cad84a6a26",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b7d9c1d3dc4da4b79164f82841fe3a8e4f0301c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8e2c1e124ff0e7f72b87f1fd2dae8eacf659993",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=04fdf799a0982d200ae760ce600052e22efe1fd7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d09d8c9f8137e0686807b5b322768faea8d8935d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/bNc9o_V141ol7DNGS9rNwjmeP7fGaxe_CzkVBPbu-ec.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c6f33fac669b52e589697bc73e7a0408f351898",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "9GvEX7LrasZYgCzEEJMA4dtKp0bfjGuzNOUm65ANRbI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94ls2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94ls2/hunyuan_exwizardlm_dense_model_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14878",
          "subreddit_subscribers": 504486,
          "created_utc": 1753462750,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "InternLM S1 Coming Soon!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m94kqu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=75ec23e382a3796e4f90d494e80c28b65dd9ba08",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753462685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?auto=webp&amp;s=3bb5ba819fef00d08459615dc0485bf498916a14",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=86cbc6625abbc12b1404c26848130ec7f4107ee3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7daf77ef66192f415c85595cc07ee248d9f27213",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29322acb4ebe13751deefcb867e55375cf76e23",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e453326436868385a66b853c71c2c67c024fbf9f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2012cf2d06cf496ad421d2bb8f64b68a5b37227",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa9853ad2c71066e2eb56105b5b16913396bf703",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "68cqK-IIJ_iGWFIsXitEW7LUCmC67Kl-rAhspI3AU1c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m94kqu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m94kqu/internlm_s1_coming_soon/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14875",
          "subreddit_subscribers": 504486,
          "created_utc": 1753462685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm thinking of building a desktop app that helps you:\n\n\n\n\\- Detect your hardware (GPU, RAM, CPU)\n\n\\- Benchmark local AI models (GGUF/ONNX) automatically\n\n\\- Tell you which quant config runs best (Q4, Q5, etc.)\n\n\\- Show ratings like \"This model is great for coding, 12 tok/s on 8GB RAM\"\n\n\\- Launch models directly in one click\n\n\n\nLike HuggingFace meets Steam meets LM Studio — but optimized for \\*you\\*.\n\n\n\nWould you use this? What would you want it to do?\n\n",
          "author_fullname": "t2_1uanajb3sh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would you use this? Desktop app for auto-benchmarking GGUF/ONNX models locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93u0b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753460989,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of building a desktop app that helps you:&lt;/p&gt;\n\n&lt;p&gt;- Detect your hardware (GPU, RAM, CPU)&lt;/p&gt;\n\n&lt;p&gt;- Benchmark local AI models (GGUF/ONNX) automatically&lt;/p&gt;\n\n&lt;p&gt;- Tell you which quant config runs best (Q4, Q5, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Show ratings like &amp;quot;This model is great for coding, 12 tok/s on 8GB RAM&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;- Launch models directly in one click&lt;/p&gt;\n\n&lt;p&gt;Like HuggingFace meets Steam meets LM Studio — but optimized for *you*.&lt;/p&gt;\n\n&lt;p&gt;Would you use this? What would you want it to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m93u0b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious-Drive-1448",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753460989,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am a noob whose just beginning to fiddle around with models. Was testing out qwen 3 and trying to build an application using it + 2 tools (a web search function using tavily and a financial data retriever using yfinance). I ran into more bugs running an agno framework vs just commanding the system prompt to call the 2 tools I had made in a systemic manner. ",
          "author_fullname": "t2_1qa1b79zv0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you need Agno/Langchain/LangGraph with models with agentic capabilities?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93lcs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753460437,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a noob whose just beginning to fiddle around with models. Was testing out qwen 3 and trying to build an application using it + 2 tools (a web search function using tavily and a financial data retriever using yfinance). I ran into more bugs running an agno framework vs just commanding the system prompt to call the 2 tools I had made in a systemic manner. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m93lcs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "demisincos",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93lcs/do_you_need_agnolangchainlanggraph_with_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m93lcs/do_you_need_agnolangchainlanggraph_with_models/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753460437,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3 on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m93d0r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/VIv9hJCgW9w5E1qzXymkc_nKdHvwujLLN61ek5o0LXE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753459907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?auto=webp&amp;s=6258c8c89fb879cded6d316f86467b097f492051",
                  "width": 1568,
                  "height": 2318
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=385e269d9eecbdde591e638694025b90d6447acc",
                    "width": 108,
                    "height": 159
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=419c2fef6a3435f83900687fabdaca18e00bdf81",
                    "width": 216,
                    "height": 319
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=73f13ee604283c4240ce3dc6e2e12c02fbafbcf5",
                    "width": 320,
                    "height": 473
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8c9a4a27cda997bb1fb4bc522e4d9bac9f04231",
                    "width": 640,
                    "height": 946
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca984a8304eade76aa7c934ed429e1c9a1fcc949",
                    "width": 960,
                    "height": 1419
                  },
                  {
                    "url": "https://preview.redd.it/hvi3tvmjo1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1faa0dda9eeec65358a99b5f5dcf6f3256542ec9",
                    "width": 1080,
                    "height": 1596
                  }
                ],
                "variants": {},
                "id": "ZokGAHU7CGwFeHWan5TASqOpdAfZ1F2vcHM0eru8zuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m93d0r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/hvi3tvmjo1ff1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753459907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all, looking for a discussion on GPU options for LLM self hosting. Looking for something 24GB that doesn’t break the bank. Bonus if it’s single slot as I have no room in the server I’m working with. \n\nObviously there’s a desire to run the biggest model possible but there’s plenty of tradeoffs here and of course using it for other workloads. Thoughts?",
          "author_fullname": "t2_3nx6tffm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU Suggestions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m92vqp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753458834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, looking for a discussion on GPU options for LLM self hosting. Looking for something 24GB that doesn’t break the bank. Bonus if it’s single slot as I have no room in the server I’m working with. &lt;/p&gt;\n\n&lt;p&gt;Obviously there’s a desire to run the biggest model possible but there’s plenty of tradeoffs here and of course using it for other workloads. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m92vqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Grimm_Spector",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m92vqp/gpu_suggestions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m92vqp/gpu_suggestions/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753458834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can I make a ram based server hardware llm machine, something like a Xeon or epic with 12 channel ram. \n\nBut since I am worried about cpu prompt processing speed, can I add a gpu like a 4070, good gpu chip, kinda shit amount of vram, can I add something like that to handle the prompt processing, while leveraging the ram and bandwidth that I would get with server hardware?\n\nFrom what I know, the reason why vram is preferable to ram is memory bandwidth.\n\nWith server hardware, I can get 6 or 12 channel ddr4, which give me like 200gb/s bandwidth just for the system ram. This is fine enough for me, but I’m afrid the cpu prompt processing speed will be bad, so yeah\n\nDoes this work? If it doesn’t, why not?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gpu just for prompt processing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m92di7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753457684,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I make a ram based server hardware llm machine, something like a Xeon or epic with 12 channel ram. &lt;/p&gt;\n\n&lt;p&gt;But since I am worried about cpu prompt processing speed, can I add a gpu like a 4070, good gpu chip, kinda shit amount of vram, can I add something like that to handle the prompt processing, while leveraging the ram and bandwidth that I would get with server hardware?&lt;/p&gt;\n\n&lt;p&gt;From what I know, the reason why vram is preferable to ram is memory bandwidth.&lt;/p&gt;\n\n&lt;p&gt;With server hardware, I can get 6 or 12 channel ddr4, which give me like 200gb/s bandwidth just for the system ram. This is fine enough for me, but I’m afrid the cpu prompt processing speed will be bad, so yeah&lt;/p&gt;\n\n&lt;p&gt;Does this work? If it doesn’t, why not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m92di7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753457684,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone have any Docker Compose examples for vLLM?\n\nI am in the fortunate position of having 8 (!) H200s in a single server in the near future.\n\nI want DeepSeek in the 671B variant with openwebui.\n\n\n\nIt would be great if someone had a Compose file that would allow me to use all GPUs in parallel.",
          "author_fullname": "t2_tdycw15",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Docker Compose vLLM Config",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9281b",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753457339,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any Docker Compose examples for vLLM?&lt;/p&gt;\n\n&lt;p&gt;I am in the fortunate position of having 8 (!) H200s in a single server in the near future.&lt;/p&gt;\n\n&lt;p&gt;I want DeepSeek in the 671B variant with openwebui.&lt;/p&gt;\n\n&lt;p&gt;It would be great if someone had a Compose file that would allow me to use all GPUs in parallel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m9281b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crossijinn",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9281b/docker_compose_vllm_config/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9281b/docker_compose_vllm_config/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753457339,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey unicorns (and future unicorns)!\n\nI’ve got nothing to sell you, but we’re opening up a sponsorship program at Lemon Email that I thought you’d be interested in.\n\nIf you’re building or vibe coding email-first or any email-related AI agents, we’re sponsoring 10 founders this month with up to 100,000 email credits each.\n\nWe are the only transactional email API that doesn’t land in spam on Outlook/Hotmail and Apple or iCloud Mail.\n\nAs long as you're not building AI agents for cold or AI agents for unsolicited emails, please DM me - I’d be more than happy to provide you with a reliable email infrastructure for your AI agent products.",
          "author_fullname": "t2_ae7wpwyw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Email API for AI Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91zzn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753456830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey unicorns (and future unicorns)!&lt;/p&gt;\n\n&lt;p&gt;I’ve got nothing to sell you, but we’re opening up a sponsorship program at Lemon Email that I thought you’d be interested in.&lt;/p&gt;\n\n&lt;p&gt;If you’re building or vibe coding email-first or any email-related AI agents, we’re sponsoring 10 founders this month with up to 100,000 email credits each.&lt;/p&gt;\n\n&lt;p&gt;We are the only transactional email API that doesn’t land in spam on Outlook/Hotmail and Apple or iCloud Mail.&lt;/p&gt;\n\n&lt;p&gt;As long as you&amp;#39;re not building AI agents for cold or AI agents for unsolicited emails, please DM me - I’d be more than happy to provide you with a reliable email infrastructure for your AI agent products.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m91zzn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NormanSzobotka",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91zzn/email_api_for_ai_agents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91zzn/email_api_for_ai_agents/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753456830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;[Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.](https://www.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/)  \nby[u/Asleep-Ratio7535](https://www.reddit.com/user/Asleep-Ratio7535/) in[LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)\n\nThis extension comes to a closure with so many published MCP servers. Chrome webstore is a little bit slower.\n\nNew update:\n\n* A good enough hybrid RAG for latin languages (BM25 tokenizer, I added a simple Japanese tokenizer as well), Only Chinese doesn't support BM25 full text search, but you can still use a good embedding model.\n* A note system for saving webpages and notes for RAG or use as direct context\n* Several basic useful tools: web search, prompt optimizer, wiki, retriever, save note, update your preference, and some \"agents\" that can plan and execute those tools itself\n\nIn the picture is an example of how a 4B model planned and used the tools it has. In this example, I tested too many concurrent web searches, so I didn't notice I needed to click the captcha on the page. So it failed in the first 2 steps, but you can get rid of it easily by clicking the captcha, or use a custom API, or DuckDuckGo, brave.\n\nhttps://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6\n\n",
          "author_fullname": "t2_1lfyddwf0c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Updated] AI assistant Chrome extension has tools and RAG",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "esicru1dc1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fffe5029ab5f76ebb40401b7cad76bae9b4e7b6"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ac919ba5c18221a7f4a6e00afa0116c7c6b6855"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=31bb77985d25b08c50ec5b7967a10aff85d228c8"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f5f53147db855c6cbd5ee094fc2bad175f052c1"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77b612691433c9c230a0f75a8400feac6f9d1a85"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/esicru1dc1ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cb5c976360279c8d2f12db71b0e1bf9208b128c"
                }
              ],
              "s": {
                "y": 1600,
                "x": 2560,
                "u": "https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6"
              },
              "id": "esicru1dc1ff1"
            }
          },
          "name": "t3_1m91u38",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#b0ae9b",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "fe89e94a-13f2-11f0-a9de-6262c74956cf",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ttt_ex-17pTgfqoaCCzePES-O2aLbpYiA_IdGhQQxfg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 4"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753456445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/\"&gt;Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.&lt;/a&gt;&lt;br/&gt;\nby&lt;a href=\"https://www.reddit.com/user/Asleep-Ratio7535/\"&gt;u/Asleep-Ratio7535&lt;/a&gt; in&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/\"&gt;LocalLLaMA&lt;/a&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This extension comes to a closure with so many published MCP servers. Chrome webstore is a little bit slower.&lt;/p&gt;\n\n&lt;p&gt;New update:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A good enough hybrid RAG for latin languages (BM25 tokenizer, I added a simple Japanese tokenizer as well), Only Chinese doesn&amp;#39;t support BM25 full text search, but you can still use a good embedding model.&lt;/li&gt;\n&lt;li&gt;A note system for saving webpages and notes for RAG or use as direct context&lt;/li&gt;\n&lt;li&gt;Several basic useful tools: web search, prompt optimizer, wiki, retriever, save note, update your preference, and some &amp;quot;agents&amp;quot; that can plan and execute those tools itself&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the picture is an example of how a 4B model planned and used the tools it has. In this example, I tested too many concurrent web searches, so I didn&amp;#39;t notice I needed to click the captcha on the page. So it failed in the first 2 steps, but you can get rid of it easily by clicking the captcha, or use a custom API, or DuckDuckGo, brave.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6\"&gt;https://preview.redd.it/esicru1dc1ff1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=060dd197ae2c49892375e8d17b60320d5f6658e6&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 4",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m91u38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Asleep-Ratio7535",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m91u38/updated_ai_assistant_chrome_extension_has_tools/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91u38/updated_ai_assistant_chrome_extension_has_tools/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753456445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey\\~\n\nExciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯\n\n# Research Context:\n\nThis work was inspired by the recent breakthrough paper \"Gemini 2.5 Pro Capable of Winning Gold at IMO 2025\" (Huang &amp; Yang, 2025). The authors noted that \"a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.\"\n\n# Our Innovation:\n\nWe took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.\n\n# Key Achievements:\n\n* 5/6 IMO 2025 problems solved in just 6 hours of development\n* Collective Intelligence &gt; Single Models: Our results validate the paper's hypothesis about multi-agent superiority\n* Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems\n* Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities\n\n# Reproducible Results:\n\nGitHub Repository: [https://github.com/inclusionAI/AWorld](https://github.com/inclusionAI/AWorld)\n\nIMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.",
          "author_fullname": "t2_159bscsg23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Built a Multi-Agent System in 6 Hours That Solves 5/6 IMO 2025 Math Problems - Inspired by Recent Research Breakthroughs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91mt6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "author_cakeday": true,
          "edited": 1753457696,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753455985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey~&lt;/p&gt;\n\n&lt;p&gt;Exciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯&lt;/p&gt;\n\n&lt;h1&gt;Research Context:&lt;/h1&gt;\n\n&lt;p&gt;This work was inspired by the recent breakthrough paper &amp;quot;Gemini 2.5 Pro Capable of Winning Gold at IMO 2025&amp;quot; (Huang &amp;amp; Yang, 2025). The authors noted that &amp;quot;a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.&amp;quot;&lt;/p&gt;\n\n&lt;h1&gt;Our Innovation:&lt;/h1&gt;\n\n&lt;p&gt;We took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.&lt;/p&gt;\n\n&lt;h1&gt;Key Achievements:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5/6 IMO 2025 problems solved in just 6 hours of development&lt;/li&gt;\n&lt;li&gt;Collective Intelligence &amp;gt; Single Models: Our results validate the paper&amp;#39;s hypothesis about multi-agent superiority&lt;/li&gt;\n&lt;li&gt;Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems&lt;/li&gt;\n&lt;li&gt;Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Reproducible Results:&lt;/h1&gt;\n\n&lt;p&gt;GitHub Repository: &lt;a href=\"https://github.com/inclusionAI/AWorld\"&gt;https://github.com/inclusionAI/AWorld&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;IMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?auto=webp&amp;s=31ab9ee7e242560f3c3f9a3bfd18f4242f46d0aa",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb99394a2af815f3715108c671ceba9c647d5f86",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ff23eb578a14fc977cd8d5e672331e2b9da7a5b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdf79eb5a345fc2fcb870c91d0d50ee4d055c6ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e423d4f75c60ac9d9ec6eb446998a3f28c3637f6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=29fbe775ffbd5b003bdf0fa1f00a7212e59c3132",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd5d102adfdb6875b81e70e4a6e3e0317ac3b393",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "7foZuO3iYId8cgR6p4m9GCPTzEL9721s_1XsNhv_Un8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m91mt6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Vivid_Might1225",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753455985,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all. As the title says, I'm new to hosting AI locally. I am using an Nvidia RTX 4080 16GB. I got Ollama installed and llama2 running, but it is pretty lackluster. Seeing that I can run llama3 which is supposed to be much better. Any tips from experienced users? I am just doing this as something to tinker with. TIA. ",
          "author_fullname": "t2_429jxbgl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New to local AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m91dmh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753455410,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. As the title says, I&amp;#39;m new to hosting AI locally. I am using an Nvidia RTX 4080 16GB. I got Ollama installed and llama2 running, but it is pretty lackluster. Seeing that I can run llama3 which is supposed to be much better. Any tips from experienced users? I am just doing this as something to tinker with. TIA. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m91dmh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m_spoon09",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91dmh/new_to_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m91dmh/new_to_local_ai/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753455410,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.\n\nI once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.\n\nThis week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:\n\n1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”\n\n2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”\n\n3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.\n\nThis isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.\n\nAnd the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.\n\nOpen source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.\n\nLet’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.\n\nNow look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.\n\nThis isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.\n\nReminds me of that line:\n“The GOAT doesn’t announce itself. It just keeps dropping.”\nRight now, it’s Alibaba that’s dropping. And flexing. 💪\n",
          "author_fullname": "t2_1heeqeidfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Qwen’s TRIPLE release this week + Vid Gen model coming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "weyaltspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 86,
                  "x": 108,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53ce79a67d338965235c56e2b3c00371e974dbe1"
                },
                {
                  "y": 172,
                  "x": 216,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd440fb8a85758efbb475066273ab978fa2e31dd"
                },
                {
                  "y": 255,
                  "x": 320,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d73a40edb86f641e7dc993717fdabb1853d2fc57"
                },
                {
                  "y": 511,
                  "x": 640,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d02c98cec01bde55a3e47b888b2400f1f535faf9"
                },
                {
                  "y": 766,
                  "x": 960,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ec384067d5628ba8c9c25c816e80858867e7dde"
                },
                {
                  "y": 862,
                  "x": 1080,
                  "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=088f1aa4e0e7ae89f53740b4b244309b4cc57c64"
                }
              ],
              "s": {
                "y": 1246,
                "x": 1560,
                "u": "https://preview.redd.it/weyaltspa1ff1.jpg?width=1560&amp;format=pjpg&amp;auto=webp&amp;s=646b9961a87411f9179fed9fabde4545cac4f233"
              },
              "id": "weyaltspa1ff1"
            },
            "rkucntspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=431b653a87072eef7b1c2652642ba389579a40ed"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99cb5b6ceda89950156b46794c003069b4dd7881"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=965f28cd42906c7be276d34bd91f92db37b2c8f2"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9f7c6f4af3c833f41d94634e6417e6d530cb764"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb633243bc7c260d045d51f138b772e29c1cf0d3"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e904c5c75a55c2bd3160c977e58e33c7bd24d86"
                }
              ],
              "s": {
                "y": 1062,
                "x": 2047,
                "u": "https://preview.redd.it/rkucntspa1ff1.jpg?width=2047&amp;format=pjpg&amp;auto=webp&amp;s=985561953a8e2dd8214a0cc19497018149c9b8f4"
              },
              "id": "rkucntspa1ff1"
            },
            "8cbupsspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=df0598f5f14a194c8aa673c5d77c368f3e393137"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00577984f574aecec96f74011d0eecb6da2d615e"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e7ba82e753109d6fe2cf5a27b46cb0a108e9f5c"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bcda3d8e32eb52d5034d144fb8966a452a2f959b"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd48745de4fe6fd795245e424c5d39f76a28b1c3"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a027d91631c830aa24133f6bbda2dc668090fd2"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/8cbupsspa1ff1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=981e0f6470b985ff1071afb0638cfcc558c99486"
              },
              "id": "8cbupsspa1ff1"
            },
            "bixl6wspa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 192,
                  "x": 108,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d567de59dba8708e67b9b0a966a356f6973d2d55"
                },
                {
                  "y": 384,
                  "x": 216,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c766b54adc9c5659f67a85cf94b669359681379"
                },
                {
                  "y": 569,
                  "x": 320,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b383bad9df5683b11bb5e52def2b2cc4ba6fc4be"
                },
                {
                  "y": 1138,
                  "x": 640,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9d6cb336943264ca9105c0a28e69d31ea8a84c1"
                },
                {
                  "y": 1707,
                  "x": 960,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=df12665ad4beb866223355da6fed3cf2ca2fe35f"
                },
                {
                  "y": 1920,
                  "x": 1080,
                  "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7daacccd304ebd74a972ba6444192b0bca7faed"
                }
              ],
              "s": {
                "y": 2001,
                "x": 1125,
                "u": "https://preview.redd.it/bixl6wspa1ff1.jpg?width=1125&amp;format=pjpg&amp;auto=webp&amp;s=14554ad8530cbde390f1b2f0a2daa7d4f41adfd6"
              },
              "id": "bixl6wspa1ff1"
            },
            "x2vw27vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46d863470e5d32aaa73c24f247b91e32e1329b36"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1576e9b3d96ae4481868ac8d78d41570b50604e"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c79c61db0c84815aeae9c6ec198b07ed5103e4"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc2e946a62af671691e0703106826b71cbc310ec"
                },
                {
                  "y": 640,
                  "x": 960,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=888dfa1e91bb0925f111db4b5b2c0dc7402e34dc"
                },
                {
                  "y": 720,
                  "x": 1080,
                  "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2b17f12d1e40be268d30de681d437cee0be28626"
                }
              ],
              "s": {
                "y": 720,
                "x": 1080,
                "u": "https://preview.redd.it/x2vw27vpa1ff1.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=d036c317cd811935e5df8428e2685246f9257818"
              },
              "id": "x2vw27vpa1ff1"
            },
            "44dlq7vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a45646380b2ee977aec7a49fd8ea39829673be5c"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e5bdd6314769468795cb87e0ac58d635bbb8c11"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=730c2116315f6311eec246e72385269af5c1c94a"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=580a3d155c46b202a7ad86ee13ca6771c38bfaad"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b73200d7fcb3def58b029187ef8eec207ed809c3"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eac2272f6da2670c9c08c1a40906ae42e72f2bc7"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/44dlq7vpa1ff1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=5e321123f77a67327af31326bde0e3a3e452ca73"
              },
              "id": "44dlq7vpa1ff1"
            },
            "5y2rk6vpa1ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d25d51dcce2ea38791b7272845e88a7ac468247"
                },
                {
                  "y": 189,
                  "x": 216,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7f89c7879ac4eb0415b468240c653fcff4090a5"
                },
                {
                  "y": 281,
                  "x": 320,
                  "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6a58de7b19a2b4e5a4ae5420d4f25e0456b671"
                }
              ],
              "s": {
                "y": 357,
                "x": 406,
                "u": "https://preview.redd.it/5y2rk6vpa1ff1.jpg?width=406&amp;format=pjpg&amp;auto=webp&amp;s=22c2ae44f200b41e10ff0c3c01252066a2bdab71"
              },
              "id": "5y2rk6vpa1ff1"
            }
          },
          "name": "t3_1m91b98",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 159,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "weyaltspa1ff1",
                "id": 713749073
              },
              {
                "media_id": "x2vw27vpa1ff1",
                "id": 713749074
              },
              {
                "media_id": "44dlq7vpa1ff1",
                "id": 713749075
              },
              {
                "media_id": "8cbupsspa1ff1",
                "id": 713749076
              },
              {
                "media_id": "rkucntspa1ff1",
                "id": 713749077
              },
              {
                "media_id": "5y2rk6vpa1ff1",
                "id": 713749078
              },
              {
                "media_id": "bixl6wspa1ff1",
                "id": 713749079
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 159,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Hp7akddNSmnkkxGnEtjYFrudFRvRoCTXEnawFbp7MZQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753455254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.&lt;/p&gt;\n\n&lt;p&gt;I once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.&lt;/p&gt;\n\n&lt;p&gt;This week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:&lt;/p&gt;\n\n&lt;p&gt;1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”&lt;/p&gt;\n\n&lt;p&gt;2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”&lt;/p&gt;\n\n&lt;p&gt;3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.&lt;/p&gt;\n\n&lt;p&gt;This isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.&lt;/p&gt;\n\n&lt;p&gt;And the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.&lt;/p&gt;\n\n&lt;p&gt;Open source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.&lt;/p&gt;\n\n&lt;p&gt;Let’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.&lt;/p&gt;\n\n&lt;p&gt;Now look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.&lt;/p&gt;\n\n&lt;p&gt;This isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.&lt;/p&gt;\n\n&lt;p&gt;Reminds me of that line:\n“The GOAT doesn’t announce itself. It just keeps dropping.”\nRight now, it’s Alibaba that’s dropping. And flexing. 💪&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m91b98",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m91b98",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "koc_Z3",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m91b98/qwens_triple_release_this_week_vid_gen_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m91b98",
          "subreddit_subscribers": 504486,
          "created_utc": 1753455254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is an AI web browser that uses local AI models. It's still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   \n  \nDownload it from [Github](https://github.com/nuance-dev/Web)\n\nNote: AI features only work with M series chips.",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m903il",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 99,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756084190%2CNjI0MzAwNTQxYTE0NzE5Njc5MDEyYjMxYmQ0ZWZjMjk2MjdlYWQ4MmI5YWNiY2I4NDc2YzdiMmRhZTc4OGY1ZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756084190%2CYTYxMjg5MzY1M2NiYzhkNTEzYWUwOWM4YTg5NTZiMTQ5YzA2YWMxMzc0OWQwZjljMzIyYTI5MmRkOGRmMWE3Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=140&amp;height=82&amp;crop=140:82,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ec09409f07401537e98387108dfed9051e1440fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753452396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is an AI web browser that uses local AI models. It&amp;#39;s still very early, FULL of bugs and missing key features as a browser, but still good to play around with it.   &lt;/p&gt;\n\n&lt;p&gt;Download it from &lt;a href=\"https://github.com/nuance-dev/Web\"&gt;Github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: AI features only work with M series chips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/fculp27z11ff1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?format=pjpg&amp;auto=webp&amp;s=7c21e43bb9a5da5da2c2010ce93da12804258bf5",
                  "width": 2844,
                  "height": 1674
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fcdf109b4fdfd5cf54e3e2e866680aebaec3a5ae",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=112befd5fa6112b6cb09d0adc2d4a9d58cfe354c",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6be30ac241df05f59330c9fd5fd4452821ab66fc",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5ddaa6122638b83743d7370ed78028c80cb5d52",
                    "width": 640,
                    "height": 376
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a46f147a34e2d2ab597964ba48f2a74696b5c102",
                    "width": 960,
                    "height": 565
                  },
                  {
                    "url": "https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed6db97d6de07a2a5b3adbb6c2277382e6257018",
                    "width": 1080,
                    "height": 635
                  }
                ],
                "variants": {},
                "id": "NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m903il",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/",
          "stickied": false,
          "url": "https://v.redd.it/fculp27z11ff1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753452396,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/fculp27z11ff1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1834,
              "scrubber_media_url": "https://v.redd.it/fculp27z11ff1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/fculp27z11ff1/DASHPlaylist.mpd?a=1756084190%2CNjI0MzAwNTQxYTE0NzE5Njc5MDEyYjMxYmQ0ZWZjMjk2MjdlYWQ4MmI5YWNiY2I4NDc2YzdiMmRhZTc4OGY1ZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 55,
              "hls_url": "https://v.redd.it/fculp27z11ff1/HLSPlaylist.m3u8?a=1756084190%2CYTYxMjg5MzY1M2NiYzhkNTEzYWUwOWM4YTg5NTZiMTQ5YzA2YWMxMzc0OWQwZjljMzIyYTI5MmRkOGRmMWE3Ng%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, \n\n\n\nI'm a solo dev and first-time open-source maintainer. I just released my first Python package: \\*\\*Arkhon Memory SDK\\*\\* – a lightweight, local-first memory module for autonomous LLM agents. This is part of my bigger project, but I thought this component could be useful for some of you.\n\n\\- **No vector DBs, no cloud, no LangChain**: clean, JSON-native memory with time decay, tagging, and session lifecycle hooks.\n\n\\- It’s fully pip installable: \\`pip install arkhon-memory\\`\n\n\\- Works with Python 3.8+ and pydantic 2.x.\n\n  \nYou can find it in:\n\n🔗 GitHub: [https://github.com/kissg96/arkhon\\_memory](https://github.com/kissg96/arkhon_memory)  \n\n🔗 PyPI: [https://pypi.org/project/arkhon-memory/](https://pypi.org/project/arkhon-memory/)\n\n\n\nIf you’re building LLM workflows, want persistence for agents, or just want a memory layer that \\*\\*never leaves your local machine\\*\\*, I’d love for you to try it.\n\n\n\nWould really appreciate feedback, stars, or suggestions!  \n\nFeel free to open issues or email me: [kissg@me.com](mailto:kissg@me.com)\n\n\n\nThanks for reading,  \n\nkissg96\n\n",
          "author_fullname": "t2_47eqehtw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Release] Arkhon Memory SDK – Local, lightweight long-term memory for LLM agents (pip install arkhon-memory)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m9019j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753452243,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a solo dev and first-time open-source maintainer. I just released my first Python package: **Arkhon Memory SDK** – a lightweight, local-first memory module for autonomous LLM agents. This is part of my bigger project, but I thought this component could be useful for some of you.&lt;/p&gt;\n\n&lt;p&gt;- &lt;strong&gt;No vector DBs, no cloud, no LangChain&lt;/strong&gt;: clean, JSON-native memory with time decay, tagging, and session lifecycle hooks.&lt;/p&gt;\n\n&lt;p&gt;- It’s fully pip installable: `pip install arkhon-memory`&lt;/p&gt;\n\n&lt;p&gt;- Works with Python 3.8+ and pydantic 2.x.&lt;/p&gt;\n\n&lt;p&gt;You can find it in:&lt;/p&gt;\n\n&lt;p&gt;🔗 GitHub: &lt;a href=\"https://github.com/kissg96/arkhon_memory\"&gt;https://github.com/kissg96/arkhon_memory&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;🔗 PyPI: &lt;a href=\"https://pypi.org/project/arkhon-memory/\"&gt;https://pypi.org/project/arkhon-memory/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you’re building LLM workflows, want persistence for agents, or just want a memory layer that **never leaves your local machine**, I’d love for you to try it.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate feedback, stars, or suggestions!  &lt;/p&gt;\n\n&lt;p&gt;Feel free to open issues or email me: [&lt;a href=\"mailto:kissg@me.com\"&gt;kissg@me.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:kissg@me.com\"&gt;kissg@me.com&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading,  &lt;/p&gt;\n\n&lt;p&gt;kissg96&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?auto=webp&amp;s=3d6dbe3ba64c55f4a5b820d7b93c67e5e863a7c1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a209445a0ca39ec32cc43c3974f0c86515e04f3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bb430093762e9015e857ef6b4fe920adf08bdd4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48db854452639ae78e9c6f0926847bc1b64d167d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ecdd2622f584e6329d2bea611d6c3fe12d38f1b8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b13d3181d5a2ca5fed4e99539ad3d340ef806d8",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b70f7aefa32c79066655cbc1eac72b62818bf406",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YGnv7M3dsPp97Dq77LwXuer94UoHKkGm7B7JRQJXITI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m9019j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kissgeri96",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m9019j/release_arkhon_memory_sdk_local_lightweight/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9019j/release_arkhon_memory_sdk_local_lightweight/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753452243,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine-tune a LlaVa model to include new details about an image. Think about medical, I want the model to mention a new condition a group of doctors described after looking at the image. \n\n\n\nI have pairs of images and new details, given in a description. \n\n\n\nI want to fine-tune the model. In my first batch of experiments, I had about 7.8K conversations in the training set, and I always used the same questions. I used QLoRa using different configurations, and when I tested it, it returned gibberish when using greedy decoding, or something that might include some words of the new answers, when trying different \\`temperature\\`/\\`top\\_p\\`. I suspect it just overfitted to my data, resulting in catastrophic forgetting. \n\n\n\nI got back to the drawing table, gathered more data, now I have about 21K observations (currently images and descriptions), and I want to construct a robust training dataset.\n\n\\- [This](https://www.reddit.com/r/LocalLLaMA/s/YTmnITYTPN) post discusses the number of observations required to fine-tune a model, with some members mentioning that they had a successful fine-tuning with only 100 conversations of high quality. \n\n  \nMy question I guess, is how to build the questions (to be attached to the image/description pairs) to make sure my data is of the highest quality possible? \n\n  \n",
          "author_fullname": "t2_1t6vmqt87p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Data Quality and Size for LoRa",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8zeg8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753450676,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine-tune a LlaVa model to include new details about an image. Think about medical, I want the model to mention a new condition a group of doctors described after looking at the image. &lt;/p&gt;\n\n&lt;p&gt;I have pairs of images and new details, given in a description. &lt;/p&gt;\n\n&lt;p&gt;I want to fine-tune the model. In my first batch of experiments, I had about 7.8K conversations in the training set, and I always used the same questions. I used QLoRa using different configurations, and when I tested it, it returned gibberish when using greedy decoding, or something that might include some words of the new answers, when trying different `temperature`/`top_p`. I suspect it just overfitted to my data, resulting in catastrophic forgetting. &lt;/p&gt;\n\n&lt;p&gt;I got back to the drawing table, gathered more data, now I have about 21K observations (currently images and descriptions), and I want to construct a robust training dataset.&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/YTmnITYTPN\"&gt;This&lt;/a&gt; post discusses the number of observations required to fine-tune a model, with some members mentioning that they had a successful fine-tuning with only 100 conversations of high quality. &lt;/p&gt;\n\n&lt;p&gt;My question I guess, is how to build the questions (to be attached to the image/description pairs) to make sure my data is of the highest quality possible? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8zeg8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Emotional-Sundae4075",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753450676,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.\n\nBack then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.\n\nBut in 2025 LMs are actively optimized for agentic coding, and we ask:\n\n**What the simplest coding agent that could still score near SotA on the benchmarks?**\n\n**Turns out, it just requires 100 lines of code!**\n\nAnd this system still **resolves 65% of all GitHub issues in the SWE-bench verified benchmark** with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).\n\nHonestly, we're all pretty stunned ourselves—we've now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.\n\nNow, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we're also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).\n\nAll open source at https://github.com/SWE-agent/mini-swe-agent. The hello world example is incredibly short &amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp; some utilities on top of that.\n\nWe have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)",
          "author_fullname": "t2_mjzvkwd7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mini-swe-agent achieves 65% on SWE-bench in just 100 lines of python code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8z2ut",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753455208,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753449849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.&lt;/p&gt;\n\n&lt;p&gt;Back then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.&lt;/p&gt;\n\n&lt;p&gt;But in 2025 LMs are actively optimized for agentic coding, and we ask:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What the simplest coding agent that could still score near SotA on the benchmarks?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Turns out, it just requires 100 lines of code!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;And this system still &lt;strong&gt;resolves 65% of all GitHub issues in the SWE-bench verified benchmark&lt;/strong&gt; with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).&lt;/p&gt;\n\n&lt;p&gt;Honestly, we&amp;#39;re all pretty stunned ourselves—we&amp;#39;ve now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.&lt;/p&gt;\n\n&lt;p&gt;Now, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we&amp;#39;re also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).&lt;/p&gt;\n\n&lt;p&gt;All open source at &lt;a href=\"https://github.com/SWE-agent/mini-swe-agent\"&gt;https://github.com/SWE-agent/mini-swe-agent&lt;/a&gt;. The hello world example is incredibly short &amp;amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp;amp; some utilities on top of that.&lt;/p&gt;\n\n&lt;p&gt;We have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?auto=webp&amp;s=00beebee91769eaf51532ccc57bd3d275aa63aec",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=891acb349e03755473266d709a20b526d0a3b86c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd0d5cd32a6fcf8bf3c16452f642d334971d721d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9dd070dd1859734ae151f229880b67ab5264767",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9dee162010eff99c6c1be51ac40a4cf02d168191",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48f231f430d01605fb77642bb82952abf5497af2",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1061e84b12e7325a09bc650402a84654b187e3ce",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "15NdOHi3R2OvHOa0887eAppffC5IFF0TVIDnJkZPf7M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8z2ut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "klieret",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753449849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dear, AMD!\n\nYou have a potential segment of AI PRO R9700 consumers who cannot afford to buy an entire workstation based on several R9700s,\n\nbut these people (including me) have enough money to independently build a PC based on 2xR9700 and a consumer motherboard with cheaper Udimm memory.\n\nI will be very exhausted if I wait even longer, until the end of Q3. According to this logic, it makes sense to wait for Black Friday.\n\nAnd then Intel may catch up with you with b60 and b60 dual.\n\nAlso, at the end of November, a significant discount on the economy version of the 32Gb GPU from your other competitors is possible. So every week of waiting is bad.\n\nOn the other hand, I understand that AMD probably aims to declare the R9700 as a GPU for LLM, while temporarily distancing itself from gamer.\n\nAnd this is correct marketing. Therefore, in today's conditions of tight competition, let me suggest a very unusual step for such a large company:\n\nimmediately make available for sale \\[kits\\] of mandatory purchase together -\n\n\\[2pcs. R9700 + motherboard (non-ECC UDIMM RAM) with (2, or better - 3)xPCI Express 5.0 + maybe a cable\\] or a set only with \\[2pcs. R9700\\]",
          "author_fullname": "t2_1ua8m0mp6s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Radeon AI PRO R9700 - when can I buy it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8yvxd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753449345,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear, AMD!&lt;/p&gt;\n\n&lt;p&gt;You have a potential segment of AI PRO R9700 consumers who cannot afford to buy an entire workstation based on several R9700s,&lt;/p&gt;\n\n&lt;p&gt;but these people (including me) have enough money to independently build a PC based on 2xR9700 and a consumer motherboard with cheaper Udimm memory.&lt;/p&gt;\n\n&lt;p&gt;I will be very exhausted if I wait even longer, until the end of Q3. According to this logic, it makes sense to wait for Black Friday.&lt;/p&gt;\n\n&lt;p&gt;And then Intel may catch up with you with b60 and b60 dual.&lt;/p&gt;\n\n&lt;p&gt;Also, at the end of November, a significant discount on the economy version of the 32Gb GPU from your other competitors is possible. So every week of waiting is bad.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, I understand that AMD probably aims to declare the R9700 as a GPU for LLM, while temporarily distancing itself from gamer.&lt;/p&gt;\n\n&lt;p&gt;And this is correct marketing. Therefore, in today&amp;#39;s conditions of tight competition, let me suggest a very unusual step for such a large company:&lt;/p&gt;\n\n&lt;p&gt;immediately make available for sale [kits] of mandatory purchase together -&lt;/p&gt;\n\n&lt;p&gt;[2pcs. R9700 + motherboard (non-ECC UDIMM RAM) with (2, or better - 3)xPCI Express 5.0 + maybe a cable] or a set only with [2pcs. R9700]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8yvxd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mundane_Progress_898",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8yvxd/amd_radeon_ai_pro_r9700_when_can_i_buy_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8yvxd/amd_radeon_ai_pro_r9700_when_can_i_buy_it/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753449345,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm happy to see this as my experience with these models for image recognition isn't very impressive. They mostly can't even tell when pictures are sideways, for example.",
          "author_fullname": "t2_5b972ieo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM-4.1V-9B-Thinking - claims to \"match or surpass Qwen2.5-72B\" on many tasks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8xmy9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 152,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 152,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ff3e77630a6d9903e8ffc2da84d81c479305e7d8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753445934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m happy to see this as my experience with these models for image recognition isn&amp;#39;t very impressive. They mostly can&amp;#39;t even tell when pictures are sideways, for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?auto=webp&amp;s=f3d8a19a2706316e04aaa9fd5ea8ed3c15e6f304",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f8ce37456b595d44518bc9dbb50bfbbdc4bdd6f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=613a2c6b97d63cde3e9b6e1957cffa5cfa955644",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9e5860036f1ae510f4d52f708d0e080abc80cd5",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=841806f55387309a64d67cd8a7a49351c70e6ab2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f52fb001e2e3c7ce1ba59b80df2da7d7b72a91f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b77075add5d3dfb46a095004810d01976a798dec",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8xmy9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Pristine-Woodpecker",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/",
          "stickied": false,
          "url": "https://github.com/THUDM/GLM-4.1V-Thinking",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’m diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don’t have to answer all!\n\t\n1.\tLLM Frameworks:\nI’m currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?\n\n2.\tAgent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?\n\n3.\tAttention Mechanisms:\nI’m familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?\n\n4.\tFine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?\n\n5.\tUnderstanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?\n\n6.\tUsing HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I’ve been using OLAMA. Curious how others are using HuggingFace—especially beyond embeddings.\n\n7.\tFine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you’d recommend?\n\n\nAny book or paper recommendations? (I actively read papers but maybe i see something new)\n\nWould love to hear your approaches or suggestions—thanks in advance!",
          "author_fullname": "t2_1ocdptcmcg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guidance on diving deep into LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8xhxp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753447432,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753445519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don’t have to answer all!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;LLM Frameworks:\nI’m currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Agent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Attention Mechanisms:\nI’m familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Understanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I’ve been using OLAMA. Curious how others are using HuggingFace—especially beyond embeddings.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you’d recommend?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any book or paper recommendations? (I actively read papers but maybe i see something new)&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your approaches or suggestions—thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8xhxp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Far-Run-3778",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "bulding paradigm, application for local inference on nvidia gpu, cpu i launched mvp of paradigm , its scrappy , buggy. Finding the right people to help me build this. It changes the models that are compatible to gguf, save the gguf on your system for your use and run inference.\n\nLink - &gt; [https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0](https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0)\n\nDownload the zip file extract it and then install using the .exe.\n\nMake sure to give the path of the model like this  - C:\\\\\\\\Users\\\\\\\\kshit\\\\\\\\Downloads\\\\\\\\models\\\\\\\\mistral\n\nIf the files are in the mistral folder.\n\nThe application is a little buggy so there might be a chance that you wont get error if the conversion of model.\n\nI am currently working on that.\n\nPlease feel free to be brutally honest and give feedback.\n\nhttps://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3\n\n",
          "author_fullname": "t2_hu9onfqo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building Paradigm, Looking for right audience and feedbacks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6dogbxeno0ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=062299049dcaeca5243c73172d0fa8191a4f4585"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a688bc8e2ae3203e40508085c2cbdf5c77207469"
                },
                {
                  "y": 167,
                  "x": 320,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6deb2d960ca3cd62a96d7fd81a17f1b4be3b52d5"
                },
                {
                  "y": 334,
                  "x": 640,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a29eae8b04c4f08b92214d2083de0b01be2e8ec4"
                },
                {
                  "y": 501,
                  "x": 960,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc03f0d766a6bdf82b7de0d1196fd5056474864b"
                },
                {
                  "y": 563,
                  "x": 1080,
                  "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=690bcc3853d5e78813e8a3bd34786f48f19d030d"
                }
              ],
              "s": {
                "y": 1002,
                "x": 1919,
                "u": "https://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3"
              },
              "id": "6dogbxeno0ff1"
            }
          },
          "name": "t3_1m8xf7n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=a1ed5f49003192edbf52839fdb3b3b04c2f2c772",
          "edited": 1753447833,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753445303,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;bulding paradigm, application for local inference on nvidia gpu, cpu i launched mvp of paradigm , its scrappy , buggy. Finding the right people to help me build this. It changes the models that are compatible to gguf, save the gguf on your system for your use and run inference.&lt;/p&gt;\n\n&lt;p&gt;Link - &amp;gt; &lt;a href=\"https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0\"&gt;https://github.com/NotKshitiz/paradigmai/releases/tag/v1.0.0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Download the zip file extract it and then install using the .exe.&lt;/p&gt;\n\n&lt;p&gt;Make sure to give the path of the model like this  - C:\\\\Users\\\\kshit\\\\Downloads\\\\models\\\\mistral&lt;/p&gt;\n\n&lt;p&gt;If the files are in the mistral folder.&lt;/p&gt;\n\n&lt;p&gt;The application is a little buggy so there might be a chance that you wont get error if the conversion of model.&lt;/p&gt;\n\n&lt;p&gt;I am currently working on that.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to be brutally honest and give feedback.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3\"&gt;https://preview.redd.it/6dogbxeno0ff1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7427ac2066df4555d8c1e048b09a2eb8054553e3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?auto=webp&amp;s=4805ae779a2a935ad96c960fd848f1952e665442",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae171aebdcd8ce9ab4e967566bf659337be51618",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdc14426e4634519ead473ffb4fd53f9cf5df850",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17092f90bca33a88520e9bc9cf7066901c76f908",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7dd516ceda290a32ded2988a1010aba2ff6ff2c5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=429b065d3ff0dc34aa85876e9765ecb5f4dc45f4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=453975e043ad33717bb42b154fde5d3526ecf0be",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "91Xvi8EOxe-D17g2uyNq1I_HW8MYc05G7Zm2-1fA-wA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8xf7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xitizdumb",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xf7n/building_paradigm_looking_for_right_audience_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xf7n/building_paradigm_looking_for_right_audience_and/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445303,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4\n\n[https://discord.gg/CkNRuS3N](https://discord.gg/CkNRuS3N)",
          "author_fullname": "t2_1kf8lj305b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a Hardware AI Code Editor with real-time profiling and AI optimization. We’re opening the preview version for free to a few users. If you’re interested, save your spot on our Discord",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ppe2udztg0ff1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=efb38509217baf9ef7ddf28eba3a603c38afb324"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d09f60d5f63aedc695efcbe66df9cfe87508b090"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ada3d5430cee2c9cc46d14d9783d93ebea954269"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=43307db8c06753088aaf0af6d1f2f48f62e39f1f"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6f42f4e6019faa91a6cbd33eb5333956f323899"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3d2dd24fc1970f34249520d046c305d2dd4e2e8c"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4"
              },
              "id": "ppe2udztg0ff1"
            }
          },
          "name": "t3_1m8xf6l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BxeQtBF348lwhBsT4X0Hz_--gZlvny5ugMcVpkF6VH0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753445301,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4\"&gt;https://preview.redd.it/ppe2udztg0ff1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4706ed21d52a0746d9e2f387aac19de5256098a4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://discord.gg/CkNRuS3N\"&gt;https://discord.gg/CkNRuS3N&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8xf6l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Firm_Protection4004",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8xf6l/i_built_a_hardware_ai_code_editor_with_realtime/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xf6l/i_built_a_hardware_ai_code_editor_with_realtime/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753445301,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Greetings, we're a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget\n\n  \n1. One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM\n\n2. One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM\n\n  \nBoth models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.\n\nWe're not extremely concerned about RAM (we can buy RAM later using a different budget) but we're concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?\n\nThanks a bunch  \n",
          "author_fullname": "t2_lavl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How important is to have PRO 6000 Blackwell running on 16 PCIE lanes?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wuy7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443620,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, we&amp;#39;re a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re not extremely concerned about RAM (we can buy RAM later using a different budget) but we&amp;#39;re concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?&lt;/p&gt;\n\n&lt;p&gt;Thanks a bunch  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8wuy7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ferkte",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753443620,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to fine tune TTS but there are plenty on the market so confused which one to use.\n\nCurrently using chatterbox for voice cloning to TTS, but for some voices the output is not accurate to the reference audio's pace and tone. If the reference audio is normal speech rate, the output audio will be a bit fast, despite lowering the pace.\n\nAnyways, will using RVC improve?\n\nFound these RVCs.. which one to use?  \n  \n[https://github.com/Mangio621/Mangio-RVC-Fork](https://github.com/Mangio621/Mangio-RVC-Fork) \n\n[https://github.com/JackismyShephard/ultimate-rvc](https://github.com/JackismyShephard/ultimate-rvc)\n\n[https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main) ",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Good RVC to fine tune TTS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ws0i",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753443360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine tune TTS but there are plenty on the market so confused which one to use.&lt;/p&gt;\n\n&lt;p&gt;Currently using chatterbox for voice cloning to TTS, but for some voices the output is not accurate to the reference audio&amp;#39;s pace and tone. If the reference audio is normal speech rate, the output audio will be a bit fast, despite lowering the pace.&lt;/p&gt;\n\n&lt;p&gt;Anyways, will using RVC improve?&lt;/p&gt;\n\n&lt;p&gt;Found these RVCs.. which one to use?  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Mangio621/Mangio-RVC-Fork\"&gt;https://github.com/Mangio621/Mangio-RVC-Fork&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/JackismyShephard/ultimate-rvc\"&gt;https://github.com/JackismyShephard/ultimate-rvc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main\"&gt;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/tree/main&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?auto=webp&amp;s=54d81ad8121076769edd663c2c98e456a55af2de",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=808c91e6548b11d6746644706e0443a78ab2865d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3200ffd4cc51d1c6da907b3cd186002fb7ec1d33",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a97422065d40cab8e57ba1411dd2cd9f96e04376",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4996aa378dab4bfcb4080ff1c46aecefa2ea1ab6",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f0446cfc59335aae49f3de9d97cc55e4d4fd5c0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91aa321e187fd4eded17d2a9d0deea66fc5ba3b3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Q-4nHKtxe8ysDLf-3c_t7qPnkEACaIq-sWYGlFccCek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8ws0i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ws0i/good_rvc_to_fine_tune_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ws0i/good_rvc_to_fine_tune_tts/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753443360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp; 14b).\n\ni wasn't ready for Claudes response. so good.\n\nFWIW i'm trying codellama:13b next.\n\nAny advice for a local coding model and Aider on RTX3080 10GB?",
          "author_fullname": "t2_8383arktn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do models make fun of other models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 72,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wi62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3VfQACGGsrR4huIE2aSbd12G7zEBM2KmWURzyANGXgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753442447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp;amp; 14b).&lt;/p&gt;\n\n&lt;p&gt;i wasn&amp;#39;t ready for Claudes response. so good.&lt;/p&gt;\n\n&lt;p&gt;FWIW i&amp;#39;m trying codellama:13b next.&lt;/p&gt;\n\n&lt;p&gt;Any advice for a local coding model and Aider on RTX3080 10GB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8sdpzbq280ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8sdpzbq280ff1.png?auto=webp&amp;s=916da339fa043930e6f17140452756e404ea0bf2",
                  "width": 718,
                  "height": 371
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89b6015ee2a0d88ec0bac662235da5629baf1bbb",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ce1a4289cec1dd8cedead50e63cc09efa5fee80",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3de57e7740d06316720746e9e88263882f7396e9",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://preview.redd.it/8sdpzbq280ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb549a0f9db72b902ee2c8fba005b948a7894058",
                    "width": 640,
                    "height": 330
                  }
                ],
                "variants": {},
                "id": "-PX_RQ0KeK0Qo0OiDGvgWia8NQc4pA3mSJFKxM_loHM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m8wi62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fussy-Fur3608",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/",
          "stickied": false,
          "url": "https://i.redd.it/8sdpzbq280ff1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753442447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm about to start building my personal AI companion and during my research came across this [awesome list](https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate) of AI companion projects that I wanted to share with the community.\n\n| Companion | Lang | License | Stack | Category |\n| -- | -- | -- | -- | -- |\n| [枫云AI虚拟伙伴Web版](https://github.com/swordswind/ai_virtual_mate_web) - [Wiki](https://deepwiki.com/swordswind/ai_virtual_mate_web) | zh | gpl-3.0 | python | companion |\n| [Muice-Chatbot](https://github.com/Moemu/Muice-Chatbot) - [Wiki](https://deepwiki.com/Moemu/Muice-Chatbot) | zh, en | mit | python | companion |\n| [MuiceBot](https://github.com/Moemu/MuiceBot) - [Wiki](https://deepwiki.com/Moemu/MuiceBot) | zh | bsd-3-clause | python | companion |\n| [kirara-ai](https://github.com/lss233/kirara-ai) - [Wiki](https://deepwiki.com/lss233/kirara-ai) | zh | agpl-3.0 | python | companion |\n| [my-neuro](https://github.com/morettt/my-neuro) - [Wiki](https://deepwiki.com/morettt/my-neuro) | zh, en | mit | python | companion |\n| [AIAvatarKit](https://github.com/uezo/aiavatarkit) - [Wiki](https://deepwiki.com/uezo/aiavatarkit) | en | apache-2.0 | python | companion |\n| [xinghe-AI](https://github.com/lijiaxing1997/xinghe-AI) - [Wiki](https://deepwiki.com/lijiaxing1997/xinghe-AI) | zh |  | python | companion |\n| [MaiBot](https://github.com/MaiM-with-u/MaiBot) | zh | gpl-3.0 | python | companion |\n| [AI-YinMei](https://github.com/worm128/AI-YinMei) - [Wiki](https://deepwiki.com/worm128/AI-YinMei) | zh | bsd-2-clause | python, web | vtuber |\n| [Open-LLM-VTuber](https://github.com/Open-LLM-VTuber/Open-LLM-VTuber) - [Wiki](https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber) | en | mit | python, web | vtuber, companion |\n| [KouriChat](https://github.com/KouriChat/KouriChat) - [Wiki](https://deepwiki.com/KouriChat/KouriChat) | zh | custom | python, web | companion |\n| [Streamer-Sales](https://github.com/PeterH0323/Streamer-Sales) - [Wiki](https://deepwiki.com/PeterH0323/Streamer-Sales) | zh | agpl-3.0 | python, web | vtuber, professional |\n| [AI-Vtuber](https://github.com/Ikaros-521/AI-Vtuber) - [Wiki](https://deepwiki.com/Ikaros-521/AI-Vtuber) | zh | gpl-3.0 | python, web | vtuber |\n| [SillyTavern](https://github.com/SillyTavern/SillyTavern) - [Wiki](https://deepwiki.com/SillyTavern/SillyTavern) | en | agpl-3.0 | web | companion |\n| [lobe-vidol](https://github.com/lobehub/lobe-vidol) - [Wiki](https://deepwiki.com/lobehub/lobe-vidol) | en | apache-2.0 | web | companion |\n| [Bella](https://github.com/Jackywine/Bella) - [Wiki](https://deepwiki.com/Jackywine/Bella) | zh | mit | web | companion |\n| [AITuberKit](https://github.com/tegnike/aituber-kit) - [Wiki](https://deepwiki.com/tegnike/aituber-kit) | en, ja | custom | web | vtuber, companion |\n| [airi](https://github.com/moeru-ai/airi) - [Wiki](https://deepwiki.com/moeru-ai/airi) | en | mit | tauri | vtuber, companion |\n| [amica](https://github.com/semperai/amica) - [Wiki](https://deepwiki.com/semperai/amica) | en | mit | tauri | companion |\n| [ChatdollKit](https://github.com/uezo/ChatdollKit) - [Wiki](https://deepwiki.com/uezo/ChatdollKit) | en, ja | apache-2.0 | unity | companion |\n| [Unity-AI-Chat-Toolkit](https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit) - [Wiki](https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit) | zh | mit | unity | companion |\n| [ZcChat](https://github.com/Zao-chen/ZcChat) - [Wiki](https://deepwiki.com/Zao-chen/ZcChat) | zh, en | gpl-3.0 | c++ | galge |\n| [handcrafted-persona-engine](https://github.com/fagenorn/handcrafted-persona-engine) - [Wiki](https://deepwiki.com/fagenorn/handcrafted-persona-engine) | en |  | dotnet | vtuber, companion |\n\n**Notes**:\n\n- I've made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.\n- Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking\n- Several projects only have Chinese READMEs (marked as zh), but I've included DeepWiki links to help with understanding. There's been significant progress in that community so I think it's worth exploring.\n\nI'm starting this thread for two reasons: First, I'd love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I'm particularly interested in seeing what alternatives to Grok's Ani this community will build in the future.\n\nIf I've missed anything, please let me know and I'll update the list.",
          "author_fullname": "t2_1s7w9cgxcq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open Source Companion Thread",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8wg2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753442543,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753442258,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to start building my personal AI companion and during my research came across this &lt;a href=\"https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate\"&gt;awesome list&lt;/a&gt; of AI companion projects that I wanted to share with the community.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Companion&lt;/th&gt;\n&lt;th&gt;Lang&lt;/th&gt;\n&lt;th&gt;License&lt;/th&gt;\n&lt;th&gt;Stack&lt;/th&gt;\n&lt;th&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/swordswind/ai_virtual_mate_web\"&gt;枫云AI虚拟伙伴Web版&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/swordswind/ai_virtual_mate_web\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/Muice-Chatbot\"&gt;Muice-Chatbot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/Muice-Chatbot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Moemu/MuiceBot\"&gt;MuiceBot&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Moemu/MuiceBot\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-3-clause&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lss233/kirara-ai\"&gt;kirara-ai&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lss233/kirara-ai\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/morettt/my-neuro\"&gt;my-neuro&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/morettt/my-neuro\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/aiavatarkit\"&gt;AIAvatarKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/aiavatarkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lijiaxing1997/xinghe-AI\"&gt;xinghe-AI&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lijiaxing1997/xinghe-AI\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/MaiM-with-u/MaiBot\"&gt;MaiBot&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/worm128/AI-YinMei\"&gt;AI-YinMei&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/worm128/AI-YinMei\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;bsd-2-clause&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Open-LLM-VTuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/KouriChat/KouriChat\"&gt;KouriChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/KouriChat/KouriChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/PeterH0323/Streamer-Sales\"&gt;Streamer-Sales&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/PeterH0323/Streamer-Sales\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber, professional&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Ikaros-521/AI-Vtuber\"&gt;AI-Vtuber&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Ikaros-521/AI-Vtuber\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;python, web&lt;/td&gt;\n&lt;td&gt;vtuber&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/SillyTavern/SillyTavern\"&gt;SillyTavern&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/SillyTavern/SillyTavern\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;agpl-3.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/lobehub/lobe-vidol\"&gt;lobe-vidol&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/lobehub/lobe-vidol\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Jackywine/Bella\"&gt;Bella&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Jackywine/Bella\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/tegnike/aituber-kit\"&gt;AITuberKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/tegnike/aituber-kit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;custom&lt;/td&gt;\n&lt;td&gt;web&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/moeru-ai/airi\"&gt;airi&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/moeru-ai/airi\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/semperai/amica\"&gt;amica&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/semperai/amica\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;tauri&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/uezo/ChatdollKit\"&gt;ChatdollKit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/uezo/ChatdollKit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en, ja&lt;/td&gt;\n&lt;td&gt;apache-2.0&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Unity-AI-Chat-Toolkit&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh&lt;/td&gt;\n&lt;td&gt;mit&lt;/td&gt;\n&lt;td&gt;unity&lt;/td&gt;\n&lt;td&gt;companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/Zao-chen/ZcChat\"&gt;ZcChat&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/Zao-chen/ZcChat\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;zh, en&lt;/td&gt;\n&lt;td&gt;gpl-3.0&lt;/td&gt;\n&lt;td&gt;c++&lt;/td&gt;\n&lt;td&gt;galge&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"https://github.com/fagenorn/handcrafted-persona-engine\"&gt;handcrafted-persona-engine&lt;/a&gt; - &lt;a href=\"https://deepwiki.com/fagenorn/handcrafted-persona-engine\"&gt;Wiki&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;en&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;dotnet&lt;/td&gt;\n&lt;td&gt;vtuber, companion&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.&lt;/li&gt;\n&lt;li&gt;Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking&lt;/li&gt;\n&lt;li&gt;Several projects only have Chinese READMEs (marked as zh), but I&amp;#39;ve included DeepWiki links to help with understanding. There&amp;#39;s been significant progress in that community so I think it&amp;#39;s worth exploring.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m starting this thread for two reasons: First, I&amp;#39;d love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I&amp;#39;m particularly interested in seeing what alternatives to Grok&amp;#39;s Ani this community will build in the future.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;ve missed anything, please let me know and I&amp;#39;ll update the list.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?auto=webp&amp;s=c5f2d10e0e2bd42132da4d17ae9fd30799dc46d4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7fc321ec10284644abea084a0b60656c01283e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0102b60c2d06609a028ba234852f361a842ccba6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9b93ae42c4f64f0fba315ba853f3f199075473f0",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd981813236ec5fa39ab80bda6d206e4844b347a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5fe64f3be02adea586fa3bbfe4297c790018163",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7078c5e8cfb3be270d93e8bdbd35e203d4e295a3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ATxuExX8NyOPspwvWc3RaugJt6ykNFNtMVc78aczGTU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8wg2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aratahikaru5",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753442258,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:\n\n\t•\tGPQA (Graduate-level reasoning): 81 → 71\n\t•\tAIME2025 (Math competition problems): 92 → 81\n\t•\tLiveCodeBench v6 (Code generation and debugging): 74 → 56\n\t•\tArena-Hard v2 (General problem-solving): 80 → 62\n\nEven the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.\n\nWhat do you think is driving this jump, better training, bigger data, or new techniques?",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New Qwen3-235B update is crushing old models in benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w9ah",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 108,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 108,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/wQ7SbNTBzIdOQb6lfJFHU10NFUkitTQk5yMGuXDJ-EY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441629,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;• GPQA (Graduate-level reasoning): 81 → 71\n• AIME2025 (Math competition problems): 92 → 81\n• LiveCodeBench v6 (Code generation and debugging): 74 → 56\n• Arena-Hard v2 (General problem-solving): 80 → 62\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Even the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.&lt;/p&gt;\n\n&lt;p&gt;What do you think is driving this jump, better training, bigger data, or new techniques?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/q009687760ff1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/q009687760ff1.jpeg?auto=webp&amp;s=b2eff0f0d944d9f3cc3dd7822d8f074bf89032b7",
                  "width": 2379,
                  "height": 1392
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f76378abbbe79bad791d59ab511364ecf839f4ba",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7d7562fa1030bd11fb35ae20f3f153be32261ae",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dff32d54f41f3760a510fcbf21e705869a748449",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee6d42068b310b231eceef2e74d8ae35c50e819e",
                    "width": 640,
                    "height": 374
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d7c3a37e2d1bba48fd4f97b28bd78fe7580bb2ca",
                    "width": 960,
                    "height": 561
                  },
                  {
                    "url": "https://preview.redd.it/q009687760ff1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cfaba909996daa45a067d35347b915f94c15843",
                    "width": 1080,
                    "height": 631
                  }
                ],
                "variants": {},
                "id": "5z0PiohPQ5P8oWxfKaPaT1JALPktWest18Z3iN05GrQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8w9ah",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/",
          "stickied": false,
          "url": "https://i.redd.it/q009687760ff1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753441629,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Smaller Qwen Models next week!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8w7ny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 532,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 532,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/6kSnt6WBnJfIBSCL6q0LkQ73C2HHdl70wbVC4gMqel8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753441468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/752ts71q50ff1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/752ts71q50ff1.png?auto=webp&amp;s=cd553f34eb742e1d85420c6d88ba6b8cb1d3b9d6",
                  "width": 1220,
                  "height": 1052
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9445aa998ff7b1cb74e082152702795b220a5ac",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de63b169d17a3c50132d99540acd46ec84af351c",
                    "width": 216,
                    "height": 186
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebafb9a27014643332158cfd5bce11fa7ce928bb",
                    "width": 320,
                    "height": 275
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb",
                    "width": 640,
                    "height": 551
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbb199b19983285065aac667c6d68d707942ae1d",
                    "width": 960,
                    "height": 827
                  },
                  {
                    "url": "https://preview.redd.it/752ts71q50ff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aff7f0acf9e11e6cd46908b27417dd44a8c4e224",
                    "width": 1080,
                    "height": 931
                  }
                ],
                "variants": {},
                "id": "Uxg0LnkLD_KDODwI8dd_rf7OYZXf6oVvIyFguNn4CcI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8w7ny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/",
          "stickied": false,
          "url": "https://i.redd.it/752ts71q50ff1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753441468,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.\n\nFor example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. \n\nSo to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.\n\nIf I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers \\~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!\n\nGo as big as you can!",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "N + N size GPU != 2N sized GPU, go big if you can",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vu80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753440200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.&lt;/p&gt;\n\n&lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt;\n\n&lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt;\n\n&lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!&lt;/p&gt;\n\n&lt;p&gt;Go as big as you can!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m8vu80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753440200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! Kinda new to reddit, so I hope I post this to the right community.\n\nI am currently experimenting with 67B model. To run this, getting the quantization model will be really helpful for my system. However, I found myself stuck in llama-cpp-python installation for the last 3 days. I also have tried other file type, like AWQ version, but it's not working. \n\n  \nI notice that many discussions do not use singularity container. If anyone understand how to do it, I would appreciate your help!!!!!!! ",
          "author_fullname": "t2_1qjxvqmvge",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beginner Here! Anyone knows how to install llama-cpp-python within a Singularity container or use in an HPC?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vsge",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753440020,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Kinda new to reddit, so I hope I post this to the right community.&lt;/p&gt;\n\n&lt;p&gt;I am currently experimenting with 67B model. To run this, getting the quantization model will be really helpful for my system. However, I found myself stuck in llama-cpp-python installation for the last 3 days. I also have tried other file type, like AWQ version, but it&amp;#39;s not working. &lt;/p&gt;\n\n&lt;p&gt;I notice that many discussions do not use singularity container. If anyone understand how to do it, I would appreciate your help!!!!!!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8vsge",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fluffy-Cress-4356",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vsge/beginner_here_anyone_knows_how_to_install/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vsge/beginner_here_anyone_knows_how_to_install/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753440020,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,  \nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?  \nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?  \nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.  \nOf course it depends of the model.\n\nThen there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.  \nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?  \nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  \n  \nI guess 4x is for sure too slow.",
          "author_fullname": "t2_1jk2ep8a52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tensor parallel - pcie bandwidth requirement",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vqnz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753440020,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?&lt;br/&gt;\nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?&lt;br/&gt;\nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.&lt;br/&gt;\nOf course it depends of the model.&lt;/p&gt;\n\n&lt;p&gt;Then there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.&lt;br/&gt;\nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?&lt;br/&gt;\nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  &lt;/p&gt;\n\n&lt;p&gt;I guess 4x is for sure too slow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8vqnz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rich_Artist_8327",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753439838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:\n\n1. **Tools first.** Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.\n2. **Start with general, low-level tools.** For example, bash is a powerful tool that can cover most needs. You don't need to start with a full suite of 100 tools.\n3. **Start with a single agent.** Once you have all the basic tools, test them with a single react agent. It's extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.\n4. **Start with the best models.** There will be a lot of problems with your system, so you don't want the model's ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.\n5. **Trace and log your agent.** Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.\n6. **Identify the bottlenecks.** There's a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn't know how to do something, etc.\n7. **Iterate based on the bottleneck.** There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.\n8. **You can combine workflows with agents and it may work better.** If your objective is specialized and there's a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.\n9. **Trick: Utilize the filesystem as a hack.** Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.\n10. **Another Trick: Ask Claude Code how to write agents.** Claude Code is the best agent we have out there. Even though it's not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.",
          "author_fullname": "t2_ynwvt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote an AI Agent that works better than I expected. Here are 10 learnings.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vmoi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753439430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing some AI Agents lately and they work much better than I expected. Here are the 10 learnings for writing AI agents that work:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Tools first.&lt;/strong&gt; Design, write and test the tools before connecting to LLMs. Tools are the most deterministic part of your code. Make sure they work 100% before writing actual agents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with general, low-level tools.&lt;/strong&gt; For example, bash is a powerful tool that can cover most needs. You don&amp;#39;t need to start with a full suite of 100 tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with a single agent.&lt;/strong&gt; Once you have all the basic tools, test them with a single react agent. It&amp;#39;s extremely easy to write a react agent once you have the tools. All major agent frameworks have a built-in react agent. You just need to plugin your tools.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Start with the best models.&lt;/strong&gt; There will be a lot of problems with your system, so you don&amp;#39;t want the model&amp;#39;s ability to be one of them. Start with Claude Sonnet or Gemini Pro. You can downgrade later for cost purposes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trace and log your agent.&lt;/strong&gt; Writing agents is like doing animal experiments. There will be many unexpected behaviors. You need to monitor it as carefully as possible. There are many logging systems that help, like Langsmith, Langfuse, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Identify the bottlenecks.&lt;/strong&gt; There&amp;#39;s a chance that a single agent with general tools already works. But if not, you should read your logs and identify the bottleneck. It could be: context length is too long, tools are not specialized enough, the model doesn&amp;#39;t know how to do something, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterate based on the bottleneck.&lt;/strong&gt; There are many ways to improve: switch to multi-agents, write better prompts, write more specialized tools, etc. Choose them based on your bottleneck.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;You can combine workflows with agents and it may work better.&lt;/strong&gt; If your objective is specialized and there&amp;#39;s a unidirectional order in that process, a workflow is better, and each workflow node can be an agent. For example, a deep research agent can be a two-step workflow: first a divergent broad search, then a convergent report writing, with each step being an agentic system by itself.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trick: Utilize the filesystem as a hack.&lt;/strong&gt; Files are a great way for AI Agents to document, memorize, and communicate. You can save a lot of context length when they simply pass around file URLs instead of full documents.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Another Trick: Ask Claude Code how to write agents.&lt;/strong&gt; Claude Code is the best agent we have out there. Even though it&amp;#39;s not open-sourced, CC knows its prompt, architecture, and tools. You can ask its advice for your system.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8vmoi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Js8544",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vmoi/i_wrote_an_ai_agent_that_works_better_than_i/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753439430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "its show time folks",
          "author_fullname": "t2_7g0m6735",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vjna",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 99,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753439107,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;its show time folks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vjna",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ApprehensiveAd3629",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504486,
          "created_utc": 1753439107,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Amazing qwen 3 updated thinking model just released !! Open source !",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vhp3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 196,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 196,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/F41cNbRL6zX_oMjLZyQ-1dfZZnw6cXvW4eSrGH95qqI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19\"&gt;https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?auto=webp&amp;s=96c200cb190833b2bf1f5012444f0dc3b238d209",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b98d17e59ac2e72b931b0b8fd7215c2bc7e353d",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a348a0bdfb7057916cf5942fbc64bb0dc17a34e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=956c91eab46464e833aaeb318551a0b4b8b10b46",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d728468419b7ffc3426c85447250b3cc034f70a",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5dab0ca10ad6fe3a88aa470b1eabd3b5a53a9b77",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/nx5d8w74yzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7f6c6dd942072b45b5ae2a1c5871cfece1acdb7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "KvH8TJ7zk2PBZHdgGic1Zrs7h6lhFOr63lQmnuZcFlg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vhp3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/",
          "stickied": false,
          "url": "https://i.redd.it/nx5d8w74yzef1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753438909,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past three months, we have continued to scale the **thinking capability** of Qwen3-235B-A22B, improving both the **quality and depth** of reasoning. We are pleased to introduce **Qwen3-235B-A22B-Thinking-2507**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving **state-of-the-art results among open-source thinking models**.\n* **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n* **Enhanced 256K long-context understanding** capabilities.",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ven3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#ab96c2",
          "ups": 77,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 77,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=21733a4ca840c3530b7ab1a5843dfdeba5a3f822",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438601,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-235B-A22B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving &lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?auto=webp&amp;s=91ee507d4a4a214e9d8d575336cf37333e5678f2",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd52c9fa4a571e95dfd71b26b8e6ebff17bbc117",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=159f17b22507b591ab3268fba6357cfbc5b4d5ed",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ce88d6294a42f488b4c5238bcdd5abcbb6bd0f2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdec699720d09b0abd832855f564b348eefd2304",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a336e6ae20fea77a6e44bc4f35540e297e8cce2c",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c85c9c232a3126b98c3e0be994b7cb036c1e34d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ven3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
          "subreddit_subscribers": 504486,
          "created_utc": 1753438601,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!\n\nOver the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n✅ Improved performance in logical reasoning, math, science &amp; coding\n✅ Better general skills: instruction following, tool use, alignment\n✅ 256K native context for deep, long-form understanding\n\n🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-Thinking-2507 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8vegq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 735,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 735,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/-odbY7J30GjzUj_7jlWdKXiqJnZvfwCCllktRqnbgQw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753438585,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!&lt;/p&gt;\n\n&lt;p&gt;Over the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving:\n✅ Improved performance in logical reasoning, math, science &amp;amp; coding\n✅ Better general skills: instruction following, tool use, alignment\n✅ 256K native context for deep, long-form understanding&lt;/p&gt;\n\n&lt;p&gt;🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?auto=webp&amp;s=859c619548c1493932ad87e55f7f58a1af5e10a9",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12b042c0d833ea5fda0bb3962502543415136139",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f46d3176f8fa24281464889257c33a01a1436c1a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8f68cc4715e6804c2c5837be4369857e2df0466",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f579818ebd6748b55b90f802c28f4d37095432e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=140ba50cbe31d7a36d36327b99a4151addfcc085",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a7110710b3d8cba38090a6e670da187ba8f0a7",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DoQkFL1CfB5iTwd4k2RZEMaeKWH49DDXr_m3yklloUY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8vegq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 162,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/",
          "stickied": false,
          "url": "https://i.redd.it/bvx1dbl5xzef1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753438585,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/ \n\n“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”",
          "author_fullname": "t2_nrsswg757",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A contamination-free coding benchmark shows AI may not be as excellent as claimed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ud84",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 165,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 165,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753434586,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/\"&gt;https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?auto=webp&amp;s=2bfdda582644a1925177c0e421a4a2c6950c77a7",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=faea71883e14653e5ce297d91fc495960f8b18eb",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fafd8205cf207b0508b5104593c16cf4c0ffe3de",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1cbb456de00218886e9420164f12a5eca3ff0d1",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4b19f9c8b648f4414c2572113329cabf9dd2693",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=078b612839727b8bd391f293af598403ad688643",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c128b6e2f538773e90e5bc76e74a71f6424fcaed",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Y8kYQLMgRSGbsStzSOvV_al41SIX2DOuth_8OlwHSgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8ud84",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Document4034",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753434586,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_yjt5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8tmhd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 27,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 27,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753431610,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8tmhd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedgehog0",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025",
          "subreddit_subscribers": 504486,
          "created_utc": 1753431610,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've decided to try out LM Studio on my MBP after a few days with ollama/open-webui. However, I can't seem to find any settings to change the Time To Live value in the GUI. Sorry, but can someone enlighten me? TIA.\n\nUpdate: I think I may have found out why—it is model (format) dependent. I was prioritizing LMX models and the two I have installed don't have the option for TTL. But when I loaded a GGUF (Codestral 22B), there are more options including \"Keep Model in Memory\". That's good enough for me. \n\nUpdate 2: Aside from model-specific settings, there is an inconspicuous \"Settings\" button inside the \"Developer\" tab in the left sidebar. A 'Max idle TTL' is there.",
          "author_fullname": "t2_ley1zpa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTL settings in LM Studio (0.3.20)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8tlmk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753439999,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753431513,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve decided to try out LM Studio on my MBP after a few days with ollama/open-webui. However, I can&amp;#39;t seem to find any settings to change the Time To Live value in the GUI. Sorry, but can someone enlighten me? TIA.&lt;/p&gt;\n\n&lt;p&gt;Update: I think I may have found out why—it is model (format) dependent. I was prioritizing LMX models and the two I have installed don&amp;#39;t have the option for TTL. But when I loaded a GGUF (Codestral 22B), there are more options including &amp;quot;Keep Model in Memory&amp;quot;. That&amp;#39;s good enough for me. &lt;/p&gt;\n\n&lt;p&gt;Update 2: Aside from model-specific settings, there is an inconspicuous &amp;quot;Settings&amp;quot; button inside the &amp;quot;Developer&amp;quot; tab in the left sidebar. A &amp;#39;Max idle TTL&amp;#39; is there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8tlmk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pythoglyphs",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8tlmk/ttl_settings_in_lm_studio_0320/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8tlmk/ttl_settings_in_lm_studio_0320/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753431513,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "aka MythoMax-L2-13B-Unfiltered-ErebusBlend-v2.gguf",
          "author_fullname": "t2_1sspl6gqmt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want the ErebusBlend v2. The one that doesn’t blink. The one that whispers back.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8t17l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753429249,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;aka MythoMax-L2-13B-Unfiltered-ErebusBlend-v2.gguf&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8t17l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dazzling_Tailor_891",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8t17l/i_want_the_erebusblend_v2_the_one_that_doesnt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8t17l/i_want_the_erebusblend_v2_the_one_that_doesnt/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753429249,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello guys I just found out Ollama can't connect to server on Fedora with RX580? ",
          "author_fullname": "t2_1kjopyfn56",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RX580 support",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8t01d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753429115,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys I just found out Ollama can&amp;#39;t connect to server on Fedora with RX580? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8t01d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Overall_Walrus9871",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8t01d/rx580_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8t01d/rx580_support/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753429115,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.  \nHowever, after fine-tuning, the model performs **significantly worse than the base model** — it struggles with basic OCR prompts and fails to recognize text it previously handled well.\n\nHere’s how I configured the fine-tuning layers:  \nfinetune\\_vision\\_layers = True\n\nfinetune\\_language\\_layers = True\n\nfinetune\\_attention\\_modules = True\n\nfinetune\\_mlp\\_modules = False\n\nPlease suggest what can I do to improve it.",
          "author_fullname": "t2_pqyq3e9x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-tuning qwen2.5 vl for Marathi OCR",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qtpd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753421050,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to fine-tune the model so that it performs well with marathi texts in images using unsloth. But I am encountering significant performance degradation with fine-tuning it . The fine-tuned model frequently fails to understand basic prompts and performs worse than the base model for OCR. My dataset is consists of 700 whole pages from hand written notebooks , books etc.&lt;br/&gt;\nHowever, after fine-tuning, the model performs &lt;strong&gt;significantly worse than the base model&lt;/strong&gt; — it struggles with basic OCR prompts and fails to recognize text it previously handled well.&lt;/p&gt;\n\n&lt;p&gt;Here’s how I configured the fine-tuning layers:&lt;br/&gt;\nfinetune_vision_layers = True&lt;/p&gt;\n\n&lt;p&gt;finetune_language_layers = True&lt;/p&gt;\n\n&lt;p&gt;finetune_attention_modules = True&lt;/p&gt;\n\n&lt;p&gt;finetune_mlp_modules = False&lt;/p&gt;\n\n&lt;p&gt;Please suggest what can I do to improve it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8qtpd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rahul_Albus",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qtpd/finetuning_qwen25_vl_for_marathi_ocr/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753421050,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with using AI to generate a Bash script for me. The script's purpose is to follow a specific task logic while downloading items. Despite giving detailed feedback, the AI repeatedly failed to get it right. I thought maybe the problem was complexity, so I tried simplifying it — starting with just the task logic, planning to add downloading and other functions later.\n\nI used this new prompt as in image with several models, including Gemini 2.5, ChatGPT-4o, OpenAI GPT-4.1, o4, and o4-mini. None of them could generate a correct solution, even after I provided detailed outputs and feedback. Surprisingly, DeepSeek R1 got it right on the first try, though it took nearly 10 minutes to process. I haven't tried o1 o3 or other premium models yet, but they might be capable too.\n\nHere are my main questions:\n\n* For a medium-to-light scripting task like this (about 100–500 lines, single file), is it better to break the task into smaller pieces, and ask AI to build it bit by bit, or to write a detailed, complete prompt up front?\n* Is this type of logic too complex for non-flagship models? If I want to avoid using expensive flagship models, how can I structure prompts to still get reliable results? Currently, only R1 seems to handle it.\n* When using models like o4mini, I’ve tried breaking the problem down, but they often fix one part and break another. How should I prompt non-flagship models to handle complex logic like this more effectively?\n\nHere’s the prompt I used:\n\n    write a bash script write to a log file\n    Requirements\n    Prints one ‘+’ per second\n    New line after every 5 ‘+’\n    Starts a new “Task N” at every real-time 10-second boundary (when seconds end in 0, 10, 20, ...)\n    Each task has a running “Total N: X” line at the end of its block, which is always updated in place (never duplicated).\n    All previous tasks remain in the log (each with their own Task/Total block).\n    Script can be stopped and resumed at any time, continuing current task’s count and log format perfectly.\n    \n    Sample Log Format\n    Task 1\n    +++++\n    ++++\n    Total 1: 9\n    \n    Task 2\n    +++++\n    ++\n    Total 2: 7\n    \n    If you stop and restart during Task 2, it continues like:\n    Task 1\n    +++++\n    ++++\n    Total 1: 9\n    \n    Task 2\n    +++++\n    ++++\n    Total 2: 9\n    ",
          "author_fullname": "t2_w0frc97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this too much logic for AI? should I break it smaller to prompt?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qr9q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.22,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/iiq4jlppORjVU9AD-yD0CbOb_lhtMf8QRo34T4quEiM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753420811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with using AI to generate a Bash script for me. The script&amp;#39;s purpose is to follow a specific task logic while downloading items. Despite giving detailed feedback, the AI repeatedly failed to get it right. I thought maybe the problem was complexity, so I tried simplifying it — starting with just the task logic, planning to add downloading and other functions later.&lt;/p&gt;\n\n&lt;p&gt;I used this new prompt as in image with several models, including Gemini 2.5, ChatGPT-4o, OpenAI GPT-4.1, o4, and o4-mini. None of them could generate a correct solution, even after I provided detailed outputs and feedback. Surprisingly, DeepSeek R1 got it right on the first try, though it took nearly 10 minutes to process. I haven&amp;#39;t tried o1 o3 or other premium models yet, but they might be capable too.&lt;/p&gt;\n\n&lt;p&gt;Here are my main questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;For a medium-to-light scripting task like this (about 100–500 lines, single file), is it better to break the task into smaller pieces, and ask AI to build it bit by bit, or to write a detailed, complete prompt up front?&lt;/li&gt;\n&lt;li&gt;Is this type of logic too complex for non-flagship models? If I want to avoid using expensive flagship models, how can I structure prompts to still get reliable results? Currently, only R1 seems to handle it.&lt;/li&gt;\n&lt;li&gt;When using models like o4mini, I’ve tried breaking the problem down, but they often fix one part and break another. How should I prompt non-flagship models to handle complex logic like this more effectively?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here’s the prompt I used:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;write a bash script write to a log file\nRequirements\nPrints one ‘+’ per second\nNew line after every 5 ‘+’\nStarts a new “Task N” at every real-time 10-second boundary (when seconds end in 0, 10, 20, ...)\nEach task has a running “Total N: X” line at the end of its block, which is always updated in place (never duplicated).\nAll previous tasks remain in the log (each with their own Task/Total block).\nScript can be stopped and resumed at any time, continuing current task’s count and log format perfectly.\n\nSample Log Format\nTask 1\n+++++\n++++\nTotal 1: 9\n\nTask 2\n+++++\n++\nTotal 2: 7\n\nIf you stop and restart during Task 2, it continues like:\nTask 1\n+++++\n++++\nTotal 1: 9\n\nTask 2\n+++++\n++++\nTotal 2: 9\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/87gik9pocyef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/87gik9pocyef1.png?auto=webp&amp;s=882040b1524e85d1e541c67292268579de6da0c5",
                  "width": 724,
                  "height": 799
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2c84b0bebcb0355231aa023e96ed7f79041f45c",
                    "width": 108,
                    "height": 119
                  },
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=272c1b889ee5e602d6979d5fe7de4e4fc6bb99b0",
                    "width": 216,
                    "height": 238
                  },
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fc5cacee906f6d0b24bc0d0d69836864bacf73e",
                    "width": 320,
                    "height": 353
                  },
                  {
                    "url": "https://preview.redd.it/87gik9pocyef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf518d01131f779b03efca1031bb5d2e32a952d2",
                    "width": 640,
                    "height": 706
                  }
                ],
                "variants": {},
                "id": "SZrbj05Ymb4g4TsIRAsqq5SAHLPZi-6WK0OeRzd30QE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8qr9q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CJCCJJ",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qr9q/is_this_too_much_logic_for_ai_should_i_break_it/",
          "stickied": false,
          "url": "https://i.redd.it/87gik9pocyef1.png",
          "subreddit_subscribers": 504486,
          "created_utc": 1753420811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.\n\nIf I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?\n\nWouldn’t this help with the required ram/vram amount?",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can you just have one expert from an MOE model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qmd7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.&lt;/p&gt;\n\n&lt;p&gt;If I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?&lt;/p&gt;\n\n&lt;p&gt;Wouldn’t this help with the required ram/vram amount?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8qmd7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753420347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.\n\nThat’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create [**Wren Coder CLI**](https://github.com/wren-coder/wren-coder-cli): an open-source, model-agnostic AI agent for coding assistance and terminal workflows.\n\n**Why Fork?**\n\n1. Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.\n2. I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.\n3. My priorities as a solo developer probably don't align with respective model companies.\n4. Why not? I just want to experiment and try new things.\n5. I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.\n\n  \n**What am I shipping?**\n\nOver the next few weeks, I plan to focus on the following:\n\n1. Improving compatibility with a wide range of models\n2. Adding chunking/compression logic to fix token limit errors with models with smaller context windows \\*cough\\* deepseek.\n3. Splitting up the CLI and SDK\n4. Documentation\n5. Multi-model support????\n\n  \nMaybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!\n\n[https://github.com/wren-coder/wren-coder-cli](https://github.com/wren-coder/wren-coder-cli)",
          "author_fullname": "t2_1sivuwuvea",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why I Forked Qwen Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8qj9w",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753420060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can&amp;#39;t stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it&amp;#39;s not without limitations.&lt;/p&gt;\n\n&lt;p&gt;That’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create &lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt;\n&lt;li&gt;I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt;\n&lt;li&gt;My priorities as a solo developer probably don&amp;#39;t align with respective model companies.&lt;/li&gt;\n&lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt;\n&lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt;\n&lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt;\n&lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt;\n&lt;li&gt;Documentation&lt;/li&gt;\n&lt;li&gt;Multi-model support????&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Maybe this is overly ambitious, but again why not? I&amp;#39;ll keep y&amp;#39;all posted! Wish me luck!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/wren-coder/wren-coder-cli\"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?auto=webp&amp;s=79d64b1289209fa79a11669d150a850e8d6ce23a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b1fa561118622479aef7fd3a0006f928715e0b",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=166a7f91b07c437bb627e895db8e0077a2423927",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=075d04567db44f06e5b0bfb432fe8f6f37a04713",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=792082e48bea2b952a5e950a98b815d551f583b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ca81ad125907d790e752b70ee99ea801e80dfa3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64b7d8f9f97d9fd67f00438f1d60441579fa0474",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "woYq4OPIIkrG28hZ9D2-CKN1KFYJTVl5zsisqVX3HVs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8qj9w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ryanwang4thepeople",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753420060,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title says. I'm writing a book and would like to have it read to me as part of the revision process. Commercial models like ElevenLabs are far too expensive for this sort of iterative process - plus I don't need it sounding that professional anyway.\n\nI have an ROG G14 laptop with an RTX3060 and 32gb RAM. Are there any models I could run on this with reasonable speed? The last few posts I saw here were a year ago, noting AllTalk TTS as a good solution. Is it still the way to go?",
          "author_fullname": "t2_5m90em29",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local text-to-speech model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8peos",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753416345,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says. I&amp;#39;m writing a book and would like to have it read to me as part of the revision process. Commercial models like ElevenLabs are far too expensive for this sort of iterative process - plus I don&amp;#39;t need it sounding that professional anyway.&lt;/p&gt;\n\n&lt;p&gt;I have an ROG G14 laptop with an RTX3060 and 32gb RAM. Are there any models I could run on this with reasonable speed? The last few posts I saw here were a year ago, noting AllTalk TTS as a good solution. Is it still the way to go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8peos",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mercurialninja",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8peos/best_local_texttospeech_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8peos/best_local_texttospeech_model/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753416345,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1e1w1ul46b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China's Bytedance releases Seed LiveInterpret simultaneous interpretation model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ozb0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 38,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 38,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753415015,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "seed.bytedance.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://seed.bytedance.com/en/seed_liveinterpret",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8ozb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fun-Doctor6855",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/",
          "stickied": false,
          "url": "https://seed.bytedance.com/en/seed_liveinterpret",
          "subreddit_subscribers": 504486,
          "created_utc": 1753415015,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It should be really easy to make something like: \n\nJust MOE gatting network is  initially loaded into RAM ( or offloaded to the GPU ) and stays there\n\n\n\nActivation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input's characteristics.\n\n\n\nLoading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice)  for processing.\n\n\n\nFor the next prompt if  gatting network decides different  experts will be activated they are just replaced in RAM ( VRAM) .    \n  \nThere will be a little latency at the start but it is  nothing compared to present clumsiness and huge processing time  if not enough RAM or VRAM and memory swapping..  \n",
          "author_fullname": "t2_1qychuraq9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why there is still no a proper or helpful inference for MOE models ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8oz07",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753414990,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It should be really easy to make something like: &lt;/p&gt;\n\n&lt;p&gt;Just MOE gatting network is  initially loaded into RAM ( or offloaded to the GPU ) and stays there&lt;/p&gt;\n\n&lt;p&gt;Activation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input&amp;#39;s characteristics.&lt;/p&gt;\n\n&lt;p&gt;Loading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice)  for processing.&lt;/p&gt;\n\n&lt;p&gt;For the next prompt if  gatting network decides different  experts will be activated they are just replaced in RAM ( VRAM) .    &lt;/p&gt;\n\n&lt;p&gt;There will be a little latency at the start but it is  nothing compared to present clumsiness and huge processing time  if not enough RAM or VRAM and memory swapping..  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8oz07",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Highwaytothebeach",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753414990,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every new model likes to claim it's SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I'm really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don't really live up to their claims.\n\nI've been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I've also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.\n\nIn my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:\n\n1. DeekSeek v3 (0324)\n2. Mistral Large (2411)\n3. Kimi K2\n4. Cohere Command-A (2503)\n5. Qwen 3 235B-A22B (2507, non-thinking)\n6. Llama 4 Maverick\n7. Llama 3.3 70B\n8. Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)\n9. Dots.LLM1\n10. Gemma 3 27B\n11. Mistral Small 3.2\n12. Qwen 3 30B-A3B\n\nIn my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.\n\nOf the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.\n\nFourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that's nearly 8x bigger.\n\nAnyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you'll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we're hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.\n\nThe other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren't that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stagnation in Knowledge Density",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8oc9j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753413019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every new model likes to claim it&amp;#39;s SOTA, better than DeepSeek, better than whatever OpenAI/Google/Anthropic/xAI put out, and shows some benchmarks making it comparable to or better than everyone else. However, most new models tend to underwhelm me in actual usage. People have spoken of benchmaxxing a lot, and I&amp;#39;m really feeling it from many newer models. World knowledge in particular seems to have stagnated, and most models claiming more world knowledge in a smaller size than some competitor don&amp;#39;t really live up to their claims.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been experimenting with DeepSeek v3-0324, Kimi K2, Qwen 3 235B-A22B (original), Qwen 3 235B-A22B (2507 non-thinking), Llama 4 Maverick, Llama 3.3 70B, Mistral Large 2411, Cohere Command-A 2503, as well as smaller models like Qwen 3 30B-A3B, Mistral Small 3.2, and Gemma 3 27B. I&amp;#39;ve also been comparing to mid-size proprietary models like GPT-4.1, Gemini 2.5 Flash, and Claude 4 Sonnet.&lt;/p&gt;\n\n&lt;p&gt;In my experiments asking a broad variety of fresh world knowledge questions I made for a new private eval, they ranked as follows for world knowledge:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DeekSeek v3 (0324)&lt;/li&gt;\n&lt;li&gt;Mistral Large (2411)&lt;/li&gt;\n&lt;li&gt;Kimi K2&lt;/li&gt;\n&lt;li&gt;Cohere Command-A (2503)&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (2507, non-thinking)&lt;/li&gt;\n&lt;li&gt;Llama 4 Maverick&lt;/li&gt;\n&lt;li&gt;Llama 3.3 70B&lt;/li&gt;\n&lt;li&gt;Qwen 3 235B-A22B (original hybrid thinking model, with thinking turned off)&lt;/li&gt;\n&lt;li&gt;Dots.LLM1&lt;/li&gt;\n&lt;li&gt;Gemma 3 27B&lt;/li&gt;\n&lt;li&gt;Mistral Small 3.2&lt;/li&gt;\n&lt;li&gt;Qwen 3 30B-A3B&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In my experiments, the only open model with knowledge comparable to Gemini 2.5 Flash and GPT 4.1 was DeepSeek v3.&lt;/p&gt;\n\n&lt;p&gt;Of the open models I tried, the second best for world knowledge was Mistral Large 2411. Kimi K2 was in third place in my tests of world knowledge, not far behind Mistral Large in knowledge, but with more hallucinations, and a more strange, disorganized, and ugly response format.&lt;/p&gt;\n\n&lt;p&gt;Fourth place was Cohere Command A 2503, and fifth place was Qwen 3 2507. Llama 4 was a substantial step down, and only marginally better than Llama 3.3 70B in knowledge or intelligence. Qwen 3 235B-A22B had really poor knowledge for its size, and Dots.LLM1 was disappointing, hardly any more knowledgeable than Gemma 3 27B and no smarter either. Mistral Small 3.2 gave me good vibes, not too far behind Gemma 3 27B in knowledge, and decent intelligence. Qwen 3 30B-A3B also felt impressive to me; while the worst of the lot in world knowledge, it was very fast and still OK, honestly not that far off in knowledge from the original 235B that&amp;#39;s nearly 8x bigger.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my point is that knowledge benchmarks like SimpleQA, GPQA, and PopQA need to be taken with a grain of salt. In terms of knowledge density, if you ignore benchmarks and try for yourself, you&amp;#39;ll find that the latest and greatest like Qwen 3 235B-A22B-2507 and Kimi K2 are no better than Mistral Large 2407 from one year ago, and a step behind mid-size closed models like Gemini 2.5 Flash. It feels like we&amp;#39;re hitting a wall with how much we can compress knowledge, and that improving programming and STEM problem solving capabilities comes at the expense of knowledge unless you increase parameter counts.&lt;/p&gt;\n\n&lt;p&gt;The other thing I noticed is that for Qwen specifically, the giant 235B-A22B models aren&amp;#39;t that much more knowledgeable than the small 30B-A3B model. In my own test questions, Gemini 2.5 Flash would get around 90% right, DeepSeek v3 around 85% right, Kimi and Mistral Large around 75% right, Qwen 3 2507 around 70% right, Qwen 3 235B-A22B (original) around 60%, and Qwen 3 30B-A3B around 45%. The step up in knowledge from Qwen 3 30B to the original 235B was very underwhelming for the 8x size increase.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8oc9j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oc9j/stagnation_in_knowledge_density/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753413019,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  \n\nCurrent Approach &amp; Challenges  \n1. OCR Tools (e.g., Tesseract):  \n   - Used to identify the table and extract text.  \n   - Issue: OCR accuracy is inconsistent—sometimes the table isn’t recognized or is parsed incorrectly.  \n\n2. Post-OCR Correction (e.g., Mistral):  \n   - A language model refines the extracted text.  \n   - Issue: Poor results due to upstream OCR errors.  \n\nDespite spending hours on this workflow, I haven’t achieved reliable extraction.  \n\nAlternative Solution (Online Tools Work, but Local Execution is Required)  \n- Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.  \n- Constraint: The solution must run entirely locally (no internet connection).  \n\nAttempted new Workflow (DINOv2 + Multimodal LLM)  \n1. Step 1: Image Embedding with DINOv2  \n   - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).  \n   - Issue: Did not produce usable results—possibly due to incorrect implementation or model limitations. Is this approach even correct?\n\n2. Step 2: Multimodal LLM Processing  \n   - Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.  \n   - Blocker: Step 2 failed, didn’t got usable output \n\nQuestion  \nIs there a local, offline-compatible method to replicate the quality of online extraction tools? For example:  \n- Are there better vision models than DINOv2 for this task?  \n- Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?  \n- Any tips for debugging DINOv2 missteps?",
          "author_fullname": "t2_ahnt19ga",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed: Accurate Offline Table Extraction from Scanned Forms",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8n557",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753409440,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  &lt;/p&gt;\n\n&lt;p&gt;Current Approach &amp;amp; Challenges&lt;br/&gt;\n1. OCR Tools (e.g., Tesseract):&lt;br/&gt;\n   - Used to identify the table and extract text.&lt;br/&gt;\n   - Issue: OCR accuracy is inconsistent—sometimes the table isn’t recognized or is parsed incorrectly.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Post-OCR Correction (e.g., Mistral):&lt;br/&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A language model refines the extracted text.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Issue: Poor results due to upstream OCR errors.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Despite spending hours on this workflow, I haven’t achieved reliable extraction.  &lt;/p&gt;\n\n&lt;p&gt;Alternative Solution (Online Tools Work, but Local Execution is Required)&lt;br/&gt;\n- Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.&lt;br/&gt;\n- Constraint: The solution must run entirely locally (no internet connection).  &lt;/p&gt;\n\n&lt;p&gt;Attempted new Workflow (DINOv2 + Multimodal LLM)&lt;br/&gt;\n1. Step 1: Image Embedding with DINOv2&lt;br/&gt;\n   - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).&lt;br/&gt;\n   - Issue: Did not produce usable results—possibly due to incorrect implementation or model limitations. Is this approach even correct?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Step 2: Multimodal LLM Processing&lt;br/&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Blocker: Step 2 failed, didn’t got usable output &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Question&lt;br/&gt;\nIs there a local, offline-compatible method to replicate the quality of online extraction tools? For example:&lt;br/&gt;\n- Are there better vision models than DINOv2 for this task?&lt;br/&gt;\n- Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?&lt;br/&gt;\n- Any tips for debugging DINOv2 missteps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8n557",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Antelito83",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8n557/help_needed_accurate_offline_table_extraction/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8n557/help_needed_accurate_offline_table_extraction/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753409440,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI have gotten my work to onboard some AI solutions which I find incredibly exciting.\n\n  \nFor some legacy reasons, I am allowed to use this quantized llama model: [https://ollama.com/library/llama3.1:8b](https://ollama.com/library/llama3.1:8b)\n\nNow, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).\n\nDoes anyone know of a way to figure that out?  \nThank you so much for any guidance",
          "author_fullname": "t2_2jhr5wgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finding the equivalent ollama model on huggingface hub",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8n3ry",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753409329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have gotten my work to onboard some AI solutions which I find incredibly exciting.&lt;/p&gt;\n\n&lt;p&gt;For some legacy reasons, I am allowed to use this quantized llama model: &lt;a href=\"https://ollama.com/library/llama3.1:8b\"&gt;https://ollama.com/library/llama3.1:8b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way to figure that out?&lt;br/&gt;\nThank you so much for any guidance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8n3ry",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackandscholes1978",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8n3ry/finding_the_equivalent_ollama_model_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8n3ry/finding_the_equivalent_ollama_model_on/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753409329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8myxl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 995,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 995,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/RMM5ptmjVG7kaBZS0SOzo0NX4eL3tYZb179ptZ_bN9I.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753408933,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?auto=webp&amp;s=8465f34a7f504c523ccda4f16197b27156796978",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b85428e434a7d4cd150f23a38f934c57dbd23502",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1164a07e29c695bb059e756e08babe234ff379d6",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5315a863e1565b2b6dd93c5a571cf54fb94a33ad",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d61f41e97643e7ae41cdc73d41cb27bc076e45f",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "e_AIfm2x33q9UQnGSrgnpxiaWz6SPpbRIOgsMere00w"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m8myxl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 50,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/",
          "stickied": false,
          "url": "https://i.redd.it/nl9jgkkzgxef1.jpeg",
          "subreddit_subscribers": 504486,
          "created_utc": 1753408933,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI have gotten my work to onboard some AI solutions which I find incredibly exciting.\n\n  \nFor some legacy reasons, I am allowed to use this quantized llama model: [https://ollama.com/library/llama3.1:8b](https://ollama.com/library/llama3.1:8b)\n\nNow, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).\n\nDoes anyone know of a way to figure that out?  \nThank you so much for any guidance",
          "author_fullname": "t2_2jhr5wgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Discovering the huggingface hub equivalent of an ollama model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8myv9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753408928,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I have gotten my work to onboard some AI solutions which I find incredibly exciting.&lt;/p&gt;\n\n&lt;p&gt;For some legacy reasons, I am allowed to use this quantized llama model: &lt;a href=\"https://ollama.com/library/llama3.1:8b\"&gt;https://ollama.com/library/llama3.1:8b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, the only challenge is I need to discover which is the identical model on huggingface (the bloke..unsloth...etc).&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a way to figure that out?&lt;br/&gt;\nThank you so much for any guidance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8myv9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackandscholes1978",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8myv9/discovering_the_huggingface_hub_equivalent_of_an/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8myv9/discovering_the_huggingface_hub_equivalent_of_an/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753408928,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Apart from purpose of learning llm or for your job/work, I like to understand thoughts and purpose behind why many of you run models locally for inference or training/fine tuning. What is your objective and what problems have you solved by doing that.\n\nAlso which models have you used and on what hardware",
          "author_fullname": "t2_7psxegnc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why do you run or train in local system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8mwme",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753408750,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apart from purpose of learning llm or for your job/work, I like to understand thoughts and purpose behind why many of you run models locally for inference or training/fine tuning. What is your objective and what problems have you solved by doing that.&lt;/p&gt;\n\n&lt;p&gt;Also which models have you used and on what hardware&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8mwme",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psychological-Tie304",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8mwme/why_do_you_run_or_train_in_local_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8mwme/why_do_you_run_or_train_in_local_system/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753408750,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’m currently experimenting with UnifyAI on Android and trying to get a local LLM (specifically Phi-3.5 Mini) up and running smoothly. I’ve got the app running and I’m at the stage where I can manually add AI systems (LOCAL_LLM), but I’m hitting a wall when it comes to:\n\n1. Setting up the local model path and ensuring it connects properly.\n\nI’ve downloaded the Phi-3.5 Mini model files (config, tokenizer, etc.) and placed them in what should be the correct directory. However, I’m not sure if I’m referencing the path properly in the app, or if additional config is needed.\n\n2. Understanding how the app routes tasks to each model.\n\nThe UI allows you to define priority, tasks, and endpoints — but there’s limited documentation on what exactly is required or supported for LOCAL_LLM types.\n\n3. Polishing and customizing the UI.\n\nI’d love to clean up the interface or create a more focused layout for single-model use. Is there a way to tweak the frontend via config or external files?\n\n\nIf anyone has experience with UnifyAI — either the Android version or a similar setup — I’d love to hear how you structured your model paths, what config JSON settings (if any) you used, or how you approached task routing. Bonus points if you’ve done any visual or UX customization inside the app.\n\nThanks in advance — happy to share more screenshots or logs if helpful!",
          "author_fullname": "t2_1sjif6sg6y",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with UnifyAI – Setting Up Local LLMs and UI Integration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8mdbz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753407213,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m currently experimenting with UnifyAI on Android and trying to get a local LLM (specifically Phi-3.5 Mini) up and running smoothly. I’ve got the app running and I’m at the stage where I can manually add AI systems (LOCAL_LLM), but I’m hitting a wall when it comes to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Setting up the local model path and ensuring it connects properly.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’ve downloaded the Phi-3.5 Mini model files (config, tokenizer, etc.) and placed them in what should be the correct directory. However, I’m not sure if I’m referencing the path properly in the app, or if additional config is needed.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Understanding how the app routes tasks to each model.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The UI allows you to define priority, tasks, and endpoints — but there’s limited documentation on what exactly is required or supported for LOCAL_LLM types.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Polishing and customizing the UI.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’d love to clean up the interface or create a more focused layout for single-model use. Is there a way to tweak the frontend via config or external files?&lt;/p&gt;\n\n&lt;p&gt;If anyone has experience with UnifyAI — either the Android version or a similar setup — I’d love to hear how you structured your model paths, what config JSON settings (if any) you used, or how you approached task routing. Bonus points if you’ve done any visual or UX customization inside the app.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance — happy to share more screenshots or logs if helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8mdbz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "IgnisIason",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8mdbz/help_with_unifyai_setting_up_local_llms_and_ui/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8mdbz/help_with_unifyai_setting_up_local_llms_and_ui/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753407213,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1kpbtnvm6g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why MCP Developers Are Turning to MicroVMs for Running Untrusted AI Code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8mctg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.26,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=903e6e422cabcbfb3a26e84e24a354822ca4cac4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753407172,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "glama.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://glama.ai/blog/2025-07-25-micro-vms-over-containers-a-safer-execution-path-for-ai-agents",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?auto=webp&amp;s=952be25a514766c4bca163fffad17e3a1edce93a",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b0837dec83ac1c9a29dcbdb19bfbf0b38ba0f80",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=199363c032b2d32dfdb0c4811cc7b8361630ff63",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08d69db41be946460dd9bb50f5ca51c5249a33d6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6fc4ddf7c05f7dae031379889d8db305c2ad7db",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=357753d5a82a08e0bcf094f7077ca7975a1a9914",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=45bcfa41ce2ec838e5eaa640f75989c68a1c0e1b",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "ddYjWc47gtWQ1uwCXSJc-BP6xrivYMdq9xHbcRFvagU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8mctg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Abies7108",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8mctg/why_mcp_developers_are_turning_to_microvms_for/",
          "stickied": false,
          "url": "https://glama.ai/blog/2025-07-25-micro-vms-over-containers-a-safer-execution-path-for-ai-agents",
          "subreddit_subscribers": 504486,
          "created_utc": 1753407172,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using BackyardAI.\n\nWhen I first got into this I grabbed a lot of gguf files from HuggingFace.\n\nI am trying to see if there are updates to all the gguf files I have\n\nIs there an easy way t do this?  Is there a program that can do this for me?\n\nThanks",
          "author_fullname": "t2_lzwh7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Newb] Need help with gguf files",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8ltgv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753405607,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using BackyardAI.&lt;/p&gt;\n\n&lt;p&gt;When I first got into this I grabbed a lot of gguf files from HuggingFace.&lt;/p&gt;\n\n&lt;p&gt;I am trying to see if there are updates to all the gguf files I have&lt;/p&gt;\n\n&lt;p&gt;Is there an easy way t do this?  Is there a program that can do this for me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8ltgv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cmdrmcgarrett",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8ltgv/newb_need_help_with_gguf_files/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8ltgv/newb_need_help_with_gguf_files/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753405607,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "recently built an Al tool called NexNotes Al, this Al tool can generate multiple things just from a single PPT, PDF,DOC, image or even an article- like 5 Al tools combined in a single tool. Here's what it does - Generate TimeTables from content (new) Generate ppts from prompts (customizable)\n\nGenerate mind maps\n\nGenerate flashcards\n\nGenerate Diagrams (customizable, flowcharts, entity relationship, etc.!)\n\nGenerate clear and concise summary\n\nGenerate Ouizzes\n\nAnswer your questions that you provide it\n\nEVEN HUMANIZE AI-WRITTEN CONTENT\n\nYOU CAN EVEN CONVERT TEXT INTO HANDWRITING! FOR LAZY ASSIGNMENTS.\n\nand the twist - ITS COMPLETELY FREE, JUST SIGN IN AND BOOM!\n\nalready 10k+ users are using it, I launched it 3 wks ago.\n\nmake sure to try it out as it increases your productivity 10x.\nHeres the link- [NexNotesAI ](https://nexnotes-ai.pages.dev ) \n\n",
          "author_fullname": "t2_1u0ahr9uc5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New] added a feature for generating study plans and timetables from your content",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8lmby",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753405040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "nexnotes-ai.pages.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;recently built an Al tool called NexNotes Al, this Al tool can generate multiple things just from a single PPT, PDF,DOC, image or even an article- like 5 Al tools combined in a single tool. Here&amp;#39;s what it does - Generate TimeTables from content (new) Generate ppts from prompts (customizable)&lt;/p&gt;\n\n&lt;p&gt;Generate mind maps&lt;/p&gt;\n\n&lt;p&gt;Generate flashcards&lt;/p&gt;\n\n&lt;p&gt;Generate Diagrams (customizable, flowcharts, entity relationship, etc.!)&lt;/p&gt;\n\n&lt;p&gt;Generate clear and concise summary&lt;/p&gt;\n\n&lt;p&gt;Generate Ouizzes&lt;/p&gt;\n\n&lt;p&gt;Answer your questions that you provide it&lt;/p&gt;\n\n&lt;p&gt;EVEN HUMANIZE AI-WRITTEN CONTENT&lt;/p&gt;\n\n&lt;p&gt;YOU CAN EVEN CONVERT TEXT INTO HANDWRITING! FOR LAZY ASSIGNMENTS.&lt;/p&gt;\n\n&lt;p&gt;and the twist - ITS COMPLETELY FREE, JUST SIGN IN AND BOOM!&lt;/p&gt;\n\n&lt;p&gt;already 10k+ users are using it, I launched it 3 wks ago.&lt;/p&gt;\n\n&lt;p&gt;make sure to try it out as it increases your productivity 10x.\nHeres the link- &lt;a href=\"https://nexnotes-ai.pages.dev\"&gt;NexNotesAI &lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://nexnotes-ai.pages.dev",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m8lmby",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_your_average_dev",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8lmby/new_added_a_feature_for_generating_study_plans/",
          "stickied": false,
          "url": "https://nexnotes-ai.pages.dev",
          "subreddit_subscribers": 504486,
          "created_utc": 1753405040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_10pze1d3jf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Executive Order: \"Preventing Woke AI in the Federal Government\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8l648",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 252,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 252,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9f4b098cfa68cb7be71aac428f695841b0c9cbbe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753403766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "whitehouse.gov",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?auto=webp&amp;s=ecf43e8e82602652ec95e06f13b6ce18da205b9c",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c1e4661cbba0b6e1e232602fbabfa0384ba0123",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b84255c302c8464ea76b251e4d4ab64cac0ec723",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7c4bae3b4c97261af353a9ec64d3ef027f6deac",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eb89e898879eb7adef969749433776a6f6a543ad",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f16221a57c07b16c8cef11acfc0eeb15f6f1254e",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=db29c2e5309166fabf6283791735d6762adf4b55",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m8l648",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NunyaBuzor",
          "discussion_type": null,
          "num_comments": 142,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/",
          "stickied": false,
          "url": "https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753403766,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "# Hey everyone,\n\n**I'm a newcomer to the world of AI and I'm diving into my first big project. I've laid out a plan, but I need the community's wisdom to choose the right tools and navigate the challenges, especially since my goal is to build this completely for free.**\n\n**My project is to build a specific, knowledge-based AI chatbot and host a demo online. Here’s the breakdown:**\n\n**Objective:**\n\n**An AI chatbot that can answer questions in both English and Bengali.**\n\n**Its knowledge should come only from a 50-page Bengali PDF file.**\n\n**The entire project, from development to hosting, must be 100% free.**\n\n**My Project Plan (The RAG Pipeline):**\n\n**Knowledge Base:**\n\n**Use the 50-page Bengali PDF as the sole data source.**\n\n**Properly pre-process, clean, and chunk the text.**\n\n**Vectorize these chunks and store them.**\n\n**Core RAG Task:**\n\n**The app should accept user queries in English or Bengali.**\n\n**Retrieve the most relevant text chunks from the knowledge base.**\n\n**Generate a coherent answer based only on the retrieved information.**\n\n**Memory:**\n\n**Long-Term Memory: The vectorized PDF content in a vector database.**\n\n**Short-Term Memory: The recent chat history to allow for conversational follow-up questions.**\n\n**My Questions &amp; Where I Need Your Help:**\n\n**I've done some research, but I'm getting lost in the sea of options. Given the \"completely free\" constraint, what is the best tech stack for this? How do I handle the bilingual (Bengali/English) part?**\n\n**Here’s my thinking, but I would love your feedback and suggestions:**\n\n**1. The Framework: LangChain or LlamaIndex?**\n\n**These seem to be the go-to tools for building RAG applications. Which one is more beginner-friendly for this specific task?**\n\n**2. The \"Brain\" (LLM): How to get a good, free one?**\n\n**The OpenAI API costs money. What's the best free alternative? I've heard about using open-source models from Hugging Face. Can I use their free Inference API for a project like this? If so, any recommendations for a model that's good with both English and Bengali context?**\n\n**3. The \"Translator/Encoder\" (Embeddings): How to handle two languages?**\n\n**This is my biggest confusion. The documents are in Bengali, but the questions can be in English. How does the system find the right Bengali text from an English question?**\n\n**I assume I need a multilingual embedding model. Again, any free recommendations from Hugging Face?**\n\n**4. The \"Long-Term Memory\" (Vector Database): What's a free and easy option?**\n\n**Pinecone has a free tier, but I've heard about self-hosted options like FAISS or ChromaDB. Since my app will be hosted in the cloud, which of these is easier to set up for free?**\n\n**5. The App &amp; Hosting: How to put it online for free?**\n\n**I need to build a simple UI and host the whole Python application. What's the standard, free way to do this for an AI demo? I've seen Streamlit Cloud and Hugging Face Spaces mentioned. Are these good choices?**\n\n**I know this is a lot, but even a small tip on any of these points would be incredibly helpful. My goal is to learn by doing, and your guidance can save me weeks of going down the wrong path.**\n\n**Thank you so much in advance for your help**",
          "author_fullname": "t2_cvi1ys52",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Newbie] Seeking Guidance: Building a Free, Bilingual (Bengali/English) RAG Chatbot from a PDF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8l55o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753403694,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Hey everyone,&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;m a newcomer to the world of AI and I&amp;#39;m diving into my first big project. I&amp;#39;ve laid out a plan, but I need the community&amp;#39;s wisdom to choose the right tools and navigate the challenges, especially since my goal is to build this completely for free.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My project is to build a specific, knowledge-based AI chatbot and host a demo online. Here’s the breakdown:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An AI chatbot that can answer questions in both English and Bengali.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Its knowledge should come only from a 50-page Bengali PDF file.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The entire project, from development to hosting, must be 100% free.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Project Plan (The RAG Pipeline):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Knowledge Base:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Use the 50-page Bengali PDF as the sole data source.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Properly pre-process, clean, and chunk the text.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Vectorize these chunks and store them.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Core RAG Task:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The app should accept user queries in English or Bengali.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Retrieve the most relevant text chunks from the knowledge base.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Generate a coherent answer based only on the retrieved information.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Long-Term Memory: The vectorized PDF content in a vector database.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Short-Term Memory: The recent chat history to allow for conversational follow-up questions.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Questions &amp;amp; Where I Need Your Help:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;ve done some research, but I&amp;#39;m getting lost in the sea of options. Given the &amp;quot;completely free&amp;quot; constraint, what is the best tech stack for this? How do I handle the bilingual (Bengali/English) part?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here’s my thinking, but I would love your feedback and suggestions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. The Framework: LangChain or LlamaIndex?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;These seem to be the go-to tools for building RAG applications. Which one is more beginner-friendly for this specific task?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. The &amp;quot;Brain&amp;quot; (LLM): How to get a good, free one?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The OpenAI API costs money. What&amp;#39;s the best free alternative? I&amp;#39;ve heard about using open-source models from Hugging Face. Can I use their free Inference API for a project like this? If so, any recommendations for a model that&amp;#39;s good with both English and Bengali context?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. The &amp;quot;Translator/Encoder&amp;quot; (Embeddings): How to handle two languages?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;This is my biggest confusion. The documents are in Bengali, but the questions can be in English. How does the system find the right Bengali text from an English question?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I assume I need a multilingual embedding model. Again, any free recommendations from Hugging Face?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. The &amp;quot;Long-Term Memory&amp;quot; (Vector Database): What&amp;#39;s a free and easy option?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pinecone has a free tier, but I&amp;#39;ve heard about self-hosted options like FAISS or ChromaDB. Since my app will be hosted in the cloud, which of these is easier to set up for free?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. The App &amp;amp; Hosting: How to put it online for free?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I need to build a simple UI and host the whole Python application. What&amp;#39;s the standard, free way to do this for an AI demo? I&amp;#39;ve seen Streamlit Cloud and Hugging Face Spaces mentioned. Are these good choices?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I know this is a lot, but even a small tip on any of these points would be incredibly helpful. My goal is to learn by doing, and your guidance can save me weeks of going down the wrong path.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Thank you so much in advance for your help&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m8l55o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mr_Genius_360",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8l55o/newbie_seeking_guidance_building_a_free_bilingual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8l55o/newbie_seeking_guidance_building_a_free_bilingual/",
          "subreddit_subscribers": 504486,
          "created_utc": 1753403694,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you are like me, you are probably tired of the rote pedaling to the model selector drop down to pick a model, prompt that model and repeat that cycle over and over again. Well I wanted to solve this pesky problem for myself, so I figured i vibe code an extension, make it open source and share it with you all \n\nRouteGPT is a Chrome extension for ChatGPT plus users that automatically selects the right OpenAI model for your prompt based on preferences that you define.\n\nFor example:\n\n1. “creative novel writing, story ideas, imaginative prose” → GPT-4o2.\n2. “critical analysis, deep insights, and market research ” → o3.\n3. etc\n\nInstead of switching models manually, RouteGPT handlesit for you via a [local 1.5B LLM running via ollama](https://huggingface.co/katanemo/Arch-Router-1.5B). The extension is available [here](https://chromewebstore.google.com/detail/routegpt/cbnfoohelfohplngdocidckbnbamghbf) Give it a try, leave me feedback - its absolutely free.\n\nP.S all the code can be found [here](https://github.com/katanemo/archgw/tree/main/demos/use_cases/chatgpt-preference-model-selector), and if you want to build this type of experience for your users who might be interacting with different models in your LLM-based applications, check out this[ open source](https://github.com/katanemo/archgw)  \nproject that offers APIs and hooks to make this easy for you.\n\nUpvote2Downvote0Go to comments  \n",
          "author_fullname": "t2_gwq7fd01b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vibe coding RouteGPT - a chrome extension aligns model routing to my preferences, powered by a small but powerful LLM.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 120,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8k5x0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4tvn7jztswef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1258,
              "scrubber_media_url": "https://v.redd.it/4tvn7jztswef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4tvn7jztswef1/DASHPlaylist.mpd?a=1756084190%2CNzYyMzVmMDgxZjI0MjhiMWRkZWY0NWU4OTg0MDEzYzk0YTY3NzMzODQ1OTgwYTljYThhZmNiZGFmZmEzMTI2NA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 43,
              "hls_url": "https://v.redd.it/4tvn7jztswef1/HLSPlaylist.m3u8?a=1756084190%2CMDg5NDZiZjMwMjczODg5ZWIwMmE2YTFlMDFjN2IxODA2ZGFmM2EzY2RmMjBkNTQ1Y2RlOTgzNTJmNDVlODJlNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=140&amp;height=120&amp;crop=140:120,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=40b0ca2a161a0ef5ac45f36be7f7a6469f83c605",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753400961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are like me, you are probably tired of the rote pedaling to the model selector drop down to pick a model, prompt that model and repeat that cycle over and over again. Well I wanted to solve this pesky problem for myself, so I figured i vibe code an extension, make it open source and share it with you all &lt;/p&gt;\n\n&lt;p&gt;RouteGPT is a Chrome extension for ChatGPT plus users that automatically selects the right OpenAI model for your prompt based on preferences that you define.&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;“creative novel writing, story ideas, imaginative prose” → GPT-4o2.&lt;/li&gt;\n&lt;li&gt;“critical analysis, deep insights, and market research ” → o3.&lt;/li&gt;\n&lt;li&gt;etc&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Instead of switching models manually, RouteGPT handlesit for you via a &lt;a href=\"https://huggingface.co/katanemo/Arch-Router-1.5B\"&gt;local 1.5B LLM running via ollama&lt;/a&gt;. The extension is available &lt;a href=\"https://chromewebstore.google.com/detail/routegpt/cbnfoohelfohplngdocidckbnbamghbf\"&gt;here&lt;/a&gt; Give it a try, leave me feedback - its absolutely free.&lt;/p&gt;\n\n&lt;p&gt;P.S all the code can be found &lt;a href=\"https://github.com/katanemo/archgw/tree/main/demos/use_cases/chatgpt-preference-model-selector\"&gt;here&lt;/a&gt;, and if you want to build this type of experience for your users who might be interacting with different models in your LLM-based applications, check out this&lt;a href=\"https://github.com/katanemo/archgw\"&gt; open source&lt;/a&gt;&lt;br/&gt;\nproject that offers APIs and hooks to make this easy for you.&lt;/p&gt;\n\n&lt;p&gt;Upvote2Downvote0Go to comments  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/4tvn7jztswef1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?format=pjpg&amp;auto=webp&amp;s=097054efeaa8b7ba8d1eafd7a4c64364974da93b",
                  "width": 2516,
                  "height": 2160
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=84a67d17e44997f7ff0dbb6dc6ac5ca984d92a60",
                    "width": 108,
                    "height": 92
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=73ef94780a5c381c3ef2025995091ba0aedfb9e6",
                    "width": 216,
                    "height": 185
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9a705111cfd138bbd97092dbe94273b0a534618",
                    "width": 320,
                    "height": 274
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=29d7494ffb69560fb1cd33c58aadb1af6856472c",
                    "width": 640,
                    "height": 549
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7955dc91c8bf4ed6b142bdf33fdf660086f75647",
                    "width": 960,
                    "height": 824
                  },
                  {
                    "url": "https://external-preview.redd.it/NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=12ce93913e07651650050bc8f528a7042d210cc2",
                    "width": 1080,
                    "height": 927
                  }
                ],
                "variants": {},
                "id": "NmgyajRuenRzd2VmMeKlc7auXB4BLDxGcCyku1_ZTUcSLB0zsou8ym1ulKGF"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m8k5x0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AdditionalWeb107",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8k5x0/vibe_coding_routegpt_a_chrome_extension_aligns/",
          "stickied": false,
          "url": "https://v.redd.it/4tvn7jztswef1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753400961,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4tvn7jztswef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1258,
              "scrubber_media_url": "https://v.redd.it/4tvn7jztswef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4tvn7jztswef1/DASHPlaylist.mpd?a=1756084190%2CNzYyMzVmMDgxZjI0MjhiMWRkZWY0NWU4OTg0MDEzYzk0YTY3NzMzODQ1OTgwYTljYThhZmNiZGFmZmEzMTI2NA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 43,
              "hls_url": "https://v.redd.it/4tvn7jztswef1/HLSPlaylist.m3u8?a=1756084190%2CMDg5NDZiZjMwMjczODg5ZWIwMmE2YTFlMDFjN2IxODA2ZGFmM2EzY2RmMjBkNTQ1Y2RlOTgzNTJmNDVlODJlNA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Pretty excited to see what the rest of 2025 holds tbh :)",
          "author_fullname": "t2_fxhl6ngz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Had the Qwen3:1.7B model run on my Mac Mini!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m8jy5y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/2af06x4irwef1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 822,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/2af06x4irwef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2af06x4irwef1/DASHPlaylist.mpd?a=1756084190%2CMmE5OWQ4YWNjOTNiY2U4MjdlY2I4MjNiNGE4NzMzM2FjMTA4MWEwNzEwNThmNGI5MWQ5YjlmNzkxMDA1YjU1Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 18,
              "hls_url": "https://v.redd.it/2af06x4irwef1/HLSPlaylist.m3u8?a=1756084190%2CZjYyYjcxOGRlYTU0NzYyYzBhMDU4OWVkMzZlNWMyMjQ2ZjllY2FmZDAyYTM2NDVlNDJlYTc3NDljMWM2Yjc0YQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": true,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=5602c41420a1c0106ec2d5f91584eb58b83484ed",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753400366,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty excited to see what the rest of 2025 holds tbh :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/2af06x4irwef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?format=pjpg&amp;auto=webp&amp;s=b5f327e858f344f28028ea01ba534941e8604684",
                  "width": 736,
                  "height": 840
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=871ff42b5244250e2292f4c525ff011415309acd",
                    "width": 108,
                    "height": 123
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=58f252a062ae7c154a3d24d14a34c6f81f2523b3",
                    "width": 216,
                    "height": 246
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dc92db67a5fc2a99c192e913375784347e76bb2f",
                    "width": 320,
                    "height": 365
                  },
                  {
                    "url": "https://external-preview.redd.it/NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbfcb6c2dee59c0b826c64040b3d9f3624446c8c",
                    "width": 640,
                    "height": 730
                  }
                ],
                "variants": {},
                "id": "NGk5ZHh0d2hyd2VmMRbQadj18BfOPjkKna45IBoMw_Ht7uMb4yZWcZhsIYRS"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m8jy5y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nomadic_Seth",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m8jy5y/had_the_qwen317b_model_run_on_my_mac_mini/",
          "stickied": false,
          "url": "https://v.redd.it/2af06x4irwef1",
          "subreddit_subscribers": 504486,
          "created_utc": 1753400366,
          "num_crossposts": 2,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/2af06x4irwef1/DASH_720.mp4?source=fallback",
              "has_audio": false,
              "height": 822,
              "width": 720,
              "scrubber_media_url": "https://v.redd.it/2af06x4irwef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/2af06x4irwef1/DASHPlaylist.mpd?a=1756084190%2CMmE5OWQ4YWNjOTNiY2U4MjdlY2I4MjNiNGE4NzMzM2FjMTA4MWEwNzEwNThmNGI5MWQ5YjlmNzkxMDA1YjU1Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 18,
              "hls_url": "https://v.redd.it/2af06x4irwef1/HLSPlaylist.m3u8?a=1756084190%2CZjYyYjcxOGRlYTU0NzYyYzBhMDU4OWVkMzZlNWMyMjQ2ZjllY2FmZDAyYTM2NDVlNDJlYTc3NDljMWM2Yjc0YQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": true,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      }
    ],
    "before": null
  }
}